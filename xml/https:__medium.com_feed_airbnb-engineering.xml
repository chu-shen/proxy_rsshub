<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 29 Sep 2021 00:41:23 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[The Airflow Smart Sensor Service]]></title>
            <link>https://medium.com/airbnb-engineering/the-airflow-smart-sensor-service-221f96227bcb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/221f96227bcb</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[data-quality]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Yingbo Wang]]></dc:creator>
            <pubDate>Tue, 28 Sep 2021 17:24:05 GMT</pubDate>
            <atom:updated>2021-09-28T18:29:09.737Z</atom:updated>
            <content:encoded><![CDATA[<p>Consolidating long-running, lightweight tasks for improved resource utilization</p><p><strong>By:</strong> <a href="https://www.linkedin.com/in/yingbo-wang-86aa3027/">Yingbo Wang</a>, <a href="https://www.linkedin.com/in/ruiqinyang/">Kevin Yang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3r30u7rnBhR7BJSc" /></figure><h3>Introduction</h3><p>Airflow is a platform to programmatically author, schedule, and monitor data pipelines. A typical Airflow cluster supports thousands of workflows, called DAGs (directed acyclic graphs), and there could be tens of thousands of concurrently running tasks at peak hours. Back in 2018, Airbnb’s Airflow cluster had several thousand DAGs and more than 30 thousand tasks running at the same time. This amount of workload would often result in Airflow’s database being overloaded. It also made the cluster quite expensive since it required a lot of resources to support those concurrent tasks.</p><p>In order to make the system more stable, and to reduce the cost of the cluster, we looked to optimize the Airflow system. We soon found that the long-running lightweight (LRLW) tasks waste a lot of resources, so we proposed a Smart Sensor to consolidate them and address the waste.</p><h3>Long-Running Lightweight Tasks</h3><p>When we investigated the Airflow performance issues, we found that a few kinds of tasks shared the same LRLW patterns. They are the sensor tasks, the subDAGs, and the SparkSubmitOperator.</p><p><strong>Sensors</strong>, or sensor tasks, are a special kind of operator that will keep running until a certain criterion is met. The criterion can be a file landing in HDFS or S3, a partition appearing in Hive, whether some other external task succeeded, or even if it is a specific time of the day.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HPMY9cRlDg7_Y7zj" /><figcaption><strong>Figure 1. The lifespan of a sensor task</strong></figcaption></figure><p>When a sensor task is running, it calls its “poke” function to check the criterion periodically, usually every 3 minutes, and marks the sensor tasks with ‘success’ if their “poke” functions return true or ‘fail’ if sensor timeout. The execution of a “poke” is very fast, mostly less than 100ms, so most of the time sensors are idle, waiting for the next “poke” time to come. The lifespan of a sensor task is from the checking time to the time when the condition is met, which can range from a few minutes to several days.</p><p><strong>SubDAGs</strong> are another example of long-running lightweight tasks. They are used to encapsulate a set of tasks in a DAG and make a complicated DAG’s structure cleaner and more readable. The DAG run is created for a subDAG in the pre_execute function and then subDAG task “poke” the DAG run status in the execute function.</p><p>The <strong>SparkSubmitOperator</strong> is also an example of a long-running lightweight task. The Spark client in Airflow submits the job and polls until completion. All these tasks, after some initialization work, fall into a lightweight and, at times, a long-running status.</p><p>From the previous examples, we can see that these tasks fall into the same “long-running, lightweight” pattern, characterized by the following:</p><ul><li><strong>The resource utilization is very low.</strong> Worker processes for these tasks remain idle 99% of the time.</li><li><strong>These tasks often account for a very large portion of the concurrent running tasks in a large scale cluster.</strong> At Airbnb, more than 70% of running tasks are sensors. At peak hour, they take more than 20Kworker slots.</li><li><strong>There are a lot of duplicate sensor tasks.</strong> More than 40% of sensor jobs are duplicates because many downstream DAGs usually wait for the same partitions from just a few important upstream DAGs.</li></ul><h3>Smart Sensor</h3><p>We proposed the Smart Sensor to consolidate these LRLW tasks. Though originally created to consolidate long-running sensor tasks, it was later expanded to consolidate all LRLW tasks. We kept the name Smart Sensor for this service.</p><h3>How It Works</h3><p>The main idea of the Smart Sensor service is to use centralized processes to execute long-running tasks in batches, instead of using one process for each task.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*H6QTUtYBpgbn2ijm" /><figcaption><strong>Figure 2. Sensors before and after enabling smart sensor</strong></figcaption></figure><p>With the Smart Sensor service, a sensor task is executed in two steps:</p><ol><li>First, each task parses the DAG, gets the task object, runs the pre_execute function, and then registers itself to the Smart Sensor service. In the registration, it persists information required to poll external resources to the Airflow metaDB. After registration succeeds, the task exits and frees up the worker slots.</li><li>Then, a few centralized processes (the Smart Sensor tasks from a built-in DAG) keep checking the database for the latest records of all registered tasks and execute the “poke” function for these tasks in batches. Normally, one Smart Sensor task is able to handle several hundred sensor tasks easily. The Smart Sensor can also combine duplicate sensor tasks into a single instance to save even more resources.</li></ol><p><strong>The Smart Sensor deduplicates tasks and balances workloads by defining the sensor task shards.</strong> The number of concurrently running sensors could be large and there will be multiple Smart Sensor tasks to execute all these jobs in a short period. How to assign sensor tasks to Smart Sensors was one of our key challenges when designing this system. We sought to balance the workload of all Smart Sensor tasks. At the same time, the `duplicated` sensor tasks have to be assigned to the same Smart Sensor so that we can avoid multiple pokes for the same target.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fM_bvm_cykKVz7qd" /><figcaption><strong>Figure 3. Deduplicating tasks by shardcode</strong></figcaption></figure><p>In the Smart Sensor service, the `poke_context` is the signature of a sensor job. It is a dictionary of arguments needed to execute the sensor’s poke function. Two sensors with the same operator class and same `poke_context` are running the same `poke` function and are considered duplicated tasks. By using the hashcode of `poke_context` to do the sharding and make each Smart Sensor task take care of tasks whose hashcode is in a specific range, it should be able to assign `duplicated` sensors to the same smart sensor. Since hashcodes are long, we optimized by using the mod of the hashcode, which can be indexed in the database. We refer to this key as the `shardcode`.</p><p>Figure 3 shows how the sharding works in the Smart Sensor service. Sensor1 and sensor2 have the same `poke_context` and so they have the same `hashcode` and `shardcode`. At runtime, they will be picked up by the same Smart Sensor — e.g., `SmartSensor1`. All duplicated sensors will be poked only once in one poking loop.</p><p><strong>Smart Sensor is a general service for all sensor classes.</strong> The centralized Smart Sensor task is a general framework. It is designed to support various classes. As long as the class has a poke function and the argument for this poke function can be serialized, the Smart Sensor tasks can support them.</p><p><strong>Logs are handled similarly to unconsolidated processes.</strong> Although task execution is consolidated into fewer processes, the Smart Sensor service supports the same ability to read or download logs from the Airflow UI. Users can read logs from the original sensor task’s URL.</p><p><strong>Smart Sensor can be easily applied to an Airflow cluster.</strong> Enabling and disabling the Smart Sensor service is simple, we only need to do a system level configuration change on the `smart_sensor` session in airflow.cfg. The change is transparent to the individual users and there is no need to change existing DAGs. Also, rotating centralized smart sensor tasks will not cause any user’s sensor task to fail.</p><h3>The Efficiency Improvement</h3><p>Upon deploying the first version of Smart Sensor, Airbnb was able to reduce the number of peak-hour, concurrently running tasks by more than 60%. We also reduced the running sensor tasks by 80%. The process slots needed for sensors were reduced from 20,000 to 80. The database load is also greatly reduced due to much fewer running tasks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lkiBWjq8_ezvYC-e" /><figcaption><strong>Figure 4. Number of running tasks after Smart Sensor deployed</strong></figcaption></figure><p>In Smart Sensor, the deduplicate mechanism reduced about 40% of requests to the Hive metastore and hence reduced both the absolute sensor traffic and the load on the underlying data warehouse.</p><h3>Conclusion</h3><p>Smart Sensor is a service which consolidates small, lightweight task traffic into bigger centralized tasks. It can reduce Airflow’s infrastructure cost and improve cluster stability. This is especially true for large clusters with a considerable amount of sensor tasks. For Airbnb’s gigantic Airflow clusters, Smart Sensor reduced a significant amount of cost and greatly improved the overall cluster stability.</p><p>The smart sensor service was released as one of the majority new features in <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/smart-sensors.html">Apache Airflow 2.0</a>, since which it has been used to improve the resource utilization for more airflow users. Because the smart sensor service introduced the idea of splitting task lifespan into multiple processes and unlocked the `async` mode for task execution, the open source community has started to invest in more generic use cases for `async` solutions, among which the <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177050929">deferrable (“Async”) operator</a> is an operator aiming to extend the async mode to more tasks.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=221f96227bcb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/the-airflow-smart-sensor-service-221f96227bcb">The Airflow Smart Sensor Service</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Rachel Zhao]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-rachel-zhao-3302e70c5a54?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/3302e70c5a54</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[search]]></category>
            <category><![CDATA[hiring]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Mon, 27 Sep 2021 22:49:28 GMT</pubDate>
            <atom:updated>2021-09-23T20:45:51.761Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Rachel Zhao</h3><p>From an uncertain software engineering student to Head of Search Engineering.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D5ycTIwTPF9Oo6Onbgrc9w.jpeg" /></figure><p><em>If there’s one thing travel teaches us, it’s that the journey is just as important as the destination. With this in mind, we’re launching a new series of blog posts to bring you the personal stories of our amazing Airfam! How did they initially connect with their passion, what brought them to Airbnb, and what’s fueling them every day?</em></p><p><em>We could think of no one better to kick off this series than </em><a href="https://www.linkedin.com/in/rachelzhao/"><em>Rachel Zhao,</em></a><em> the head of engineering for our Search product group, which contributed to the set of </em><a href="https://news.airbnb.com/2021-release/"><em>incredible features</em></a><em> this year to respond to the changing world of travel. In addition to major initiatives in search and mobile, Rachel’s team is expanding to our new Atlanta hub, which goes hand-in-hand with her personal passion to improve representation in tech. As an immigrant and a woman, Rachel knows the importance of community building and how hard and intimidating it can be to find your place in an engineering career path. Read on for Rachel’s own words on seeing code as a way to communicate, working at the crossroads of UX and data, the tech talent in Atlanta, and more.</em></p><h3>From Beijing to Waterloo</h3><p>I grew up in Beijing and then went to Canada to study engineering at the University of Waterloo. It felt like my classmates all knew that they wanted to do computer science from a very early age. On the other hand, I was a complete newbie. Growing up, I’d been more interested in the arts and communication media professions. But my family decided to immigrate to Canada, and as I knew very little English, I needed to find a major that was more practical for me. It was very random — I just Googled for the top schools and programs in Canada. And that’s how I got into software engineering.</p><p>It was really stressful because it seemed like everyone knew how to program already. I struggled a lot in the first years and felt so behind compared to everyone in the class (learning English and Java at the same time was rough!). But I quickly realized that everyone has strengths and weaknesses. While I was weaker in computer science, I was stronger in other subjects. I could offer help on calculus and physics thanks to my advanced high school curriculum, and in return they would share their programming experience. I was lucky that I had classmates who supported each other, and that got me through the years of uncertainty.</p><h3>Finding my niche in software engineering</h3><p>As a visual person, the typical perception of programming (dark screen with green text, “Matrix”-style) was daunting to me. However, things changed when I took an introduction to user interface class. It made me realize that a big part of software engineering is to create a way to communicate effectively. I could bring my interest in art and media into my career path as an engineer by working on user experience.</p><p>Through many internships during the undergrad years, I also learned that when writing code, making it functional is not the only goal — we’re writing for other developers, current and future, who share the same codebase. They need to understand what you’re trying to achieve and your code needs to be maintainable. It’s not just about the algorithms, but also about organization and communication. And those skills you have as a person also apply to engineering.</p><h3>Discovering my place and my people at Airbnb</h3><p>Throughout my career, I’ve found that I’m motivated to solve user problems: working with research to understand the needs, and prioritizing work that brings value to customers. Therefore the opportunity to come to Airbnb and lead the search group was very exciting to me. Teams in this org are responsible for a crucial path of a guest’s journey: helping people discover and onboard to Airbnb (SEO), showcasing what Airbnb has to offer (Storefronts), capturing what guests are looking for (Search Input) and finding the best matches for them (Search Feed).</p><p>The space we work on is both a product surface and a platform. It’s also the crossroad of user experience and data. As a result, we get to work on many different types of projects, from new user facing features, systems that power product pages, platforms that enhance developer productivity, to work that enhances product experience such as performance improvements… The possibilities are endless.</p><p>Not only is this area a great fit for my passions, but Airbnb is a product that I’ve admired for a long time (ever since discovering the platform as a host back in 2012, when I literally had guests staying on an air mattress in my flat in San Francisco!). One thing I love about Airbnb as a product is that it’s really good at storytelling. You land on the homepage and you see there’s a narrative here, rather than a lot of components.</p><p><strong>“I think that how you deal with a crisis — how you deal with the lows of a company — shows more character of the company than the highs, when everything is going smoothly.”</strong></p><p>What really confirmed my desire to join was seeing how Airbnb handled the challenges of 2020. While seeing layoffs happening all over the tech industry during the pandemic, I was impressed by Airbnb’s response. The communication was very clear from leadership, and the company was generous and considerate in helping people find and land their next job and get through this period financially. There was also <a href="https://news.airbnb.com/a-message-from-co-founder-and-ceo-brian-chesky/">a widely-shared blog post by Brian</a> which I felt set the bar for how to communicate with empathy and compassion while making a difficult decision.</p><p>I think that how you deal with a crisis — how you deal with the lows of a company — shows more character of the company than the highs, when everything is going smoothly. I felt like how Airbnb handled things was a really good sign of the company’s culture and the leadership. And that’s what ultimately inspired me to join the team.</p><h3>Expanding our team in Atlanta</h3><p>I put a lot of value on community building, inclusive communication, and representation. When there’s a lack of diversity at the table, we don’t get to hear different perspectives, and those perspectives are not considered in decision making. That could lead to biased technical decisions, or a product direction with many blindspots.</p><p>That’s why I’m so excited about the team we’re starting in Atlanta. It’s a very important tech hub. Atlanta has great schools and great talent. And the office there will help us operate in a way that’s less Silicon Valley-centric. I think it’s very important to bring different ways of thinking into the company and strengthen the culture instead of simply fitting in the culture.</p><p>I’m also feeling confident that we’ve built up the “muscle” of working remotely and learned how to make everyone feel supported over that past year and half. We’re making sure to integrate new engineers in Atlanta with our existing teams first, so they can learn how Airbnb works, what our tech stack is like, and so on — we’re being very careful about how we equip everyone with that domain knowledge so they’re set up for success.</p><p>At Airbnb, we have a healthy engineering culture of collaboration and knowledge sharing. People are willing to help each other out. We’re here to build products with a mission that everyone can belong anywhere, and there are so many ways to contribute to a team where everyone is sharing their strengths.</p><p>Our team is dedicated to perfecting our new features that help travelers to embrace more flexibility in <a href="https://techcrunch.com/2021/05/24/airbnb-doubles-down-on-flexible-search-improves-the-host-flow-in-preparation-for-summer-2021/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAACTqKJxxNCEClv9r2IB9uYeSHOeH_XMduLIman-CPW8aQtcgupj8ioXo2ZeXXaCEjW2uzTG-JF2fPuRuyLWz5IDZZd4QpXpS-DLrlMyDgtCc2H6mm7raLLBhTRI3Mpo3Dj8MfPSGqeETUld2Cc9JIKIl6I8PF8yGenT4ocEWbhaB">date</a> and <a href="https://news.airbnb.com/unique-stays-hosts-earn-more-than-300-million-since-start-of-pandemic/">destination</a>, as well as improving core functionalities all over the onboarding and search flow. On the platform side, we continue to invest in mobile, to scale and evolve our tech stack and set a new standard for app development. We’re hiring in Atlanta, the Bay Area, and a number of other locations and we’d love to hear from you!</p><p>Check out these related roles:</p><ul><li><a href="https://careers.airbnb.com/positions/2607507/">Senior iOS Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/2756046/">Senior Android Software Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/3178314/">Senior Software Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/3206883/">Staff Fullstack Engineer, Guest Experience</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3302e70c5a54" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-rachel-zhao-3302e70c5a54">My Journey to Airbnb — Rachel Zhao</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Enables Consistent Data Consumption at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-enables-consistent-data-consumption-at-scale-1c0b6a8b9206?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/1c0b6a8b9206</guid>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[metrics]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Shao Xie]]></dc:creator>
            <pubDate>Tue, 21 Sep 2021 17:00:25 GMT</pubDate>
            <atom:updated>2021-09-21T17:00:25.435Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part-III: Building a coherent consumption experience</h4><p><strong>By: </strong><a href="https://www.linkedin.com/in/apahwa/">Amit Pahwa</a>, <a href="https://www.linkedin.com/in/cristianrfr/">Cristian Figueroa</a>, <a href="https://www.linkedin.com/in/donghan-zhang-670990135/">Donghan Zhang</a>, <a href="https://www.linkedin.com/in/haimgrosman/">Haim Grosman</a>, <a href="https://www.linkedin.com/in/john-bodley-a13327133/">John Bodley</a>, <a href="https://www.linkedin.com/in/jonathan-parks-15617820/">Jonathan Parks</a>, <a href="https://www.linkedin.com/in/jialingliu1020/">Jenny Liu</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/shengnan-zhu-89403124/">Maggie Zhu</a>, <a href="https://www.linkedin.com/in/michaelcl/">Mike Lin</a>, <a href="https://www.linkedin.com/in/philip-weiss-391021b1/">Philip Weiss</a>, <a href="https://www.linkedin.com/in/robert-ih-chang/">Robert Chang</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/sylviatomiyama/">Sylvia Tomiyama</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/xiaohui-sun-24bb3017/">Xiaohui Sun</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4n3siF32QWxo31_6okaE9g.jpeg" /></figure><h3>Introduction</h3><p>In the <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">first post</a> of this series, we highlighted the role Minerva plays in transforming how Analytics works at Airbnb. In the <a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">second post</a>, we dove into Minerva’s core compute infrastructure and explained how we enforce data consistency across datasets and teams. In this third and final post, we will focus our story on how Minerva drastically simplifies and improves the data consumption experience for our users. Specifically, we will showcase how a unified metric layer, which we call the Minerva API, helps us build versatile data consumption experiences tailored to users with a wide range of backgrounds and varying levels of data expertise.</p><h3>A Metric-Centric Approach</h3><p>When data consumers use data to frame a business question, they typically think in terms of metrics and dimensions. For example, a business leader may wonder what percentage of bookings (a metric) is made up of long-term stays (a dimension). To answer this question, she needs to find the right set of tables from which to query (where), apply the necessary joins or filters (how), and then finally aggregate the events (how) to arrive at an answer that is, hopefully, correct.</p><p>While many traditional BI tools attempt to abstract this work on behalf of their users, most of their data-serving logic still relies heavily on the users to figure out the “where” and the “how”. At Airbnb, we aspired to build a better user experience — one in which users simply ask for metrics and dimension cuts, and receive the answers without having to worry about the “where” or the “how”. This vision, what we call a “metric-centric approach”, turned out to be a difficult engineering challenge.</p><h4>Challenge One: The “Where”</h4><p>In most traditional data warehouses, data is organized in tables. This means that to answer an inquiry, a BI tool needs to associate the metrics and dimensions in question to the physical tables that contain the relevant answers. However, for a given metric and dimension combination, there might be many datasets from which to source the answers. These tables often have varying degrees of data quality and correctness guarantees, so picking the right tables to serve the data is nontrivial.</p><h4>Challenge Two: The “How”</h4><p>Moving beyond the “where”, the data-serving logic responsible for the “how” also has many nuances. To start, there are different metric types: <em>simple metrics</em> are composed of single materialized events (e.g., bookings); <em>filtered metrics</em> are composed of simple metrics filtered on a dimension value (e.g., bookings in China); and <em>derived metrics</em> are composed of one or more non-derived metrics (e.g. search-to-book rate). Furthermore, while many metrics are additive (e.g., bookings), many other metrics are not: count distincts, percentiles, and point-in-time snapshots cannot simply be calculated by summing individual events. Consistently calculating these various metric types correctly, across all scenarios, is a big challenge.</p><h4>Challenge Three: Integration With Downstream Applications</h4><p>Finally, to make data-informed decisions, data must be used in a wide variety of contexts, applications, and tools. The more prevalent and important the metric is, the more likely it is to be used in a wide variety of settings. For example, gross booking value (GBV), nights booked, and revenue are among the most frequently used metrics at Airbnb. They are used to track business performance, calculate guardrail metrics for randomized controlled experiments, and leveraged as features for machine learning models. Serving these metrics in different use cases, while providing contextual information for users to use them the right way is yet another core challenge for us.</p><h4>Our Solution</h4><p>We have addressed these challenges by building the Minerva API, a metric-serving layer that acts as an interface between upstream data models and downstream applications. With Minerva API, any downstream application is able to serve data consistently and correctly without knowing where the data is stored or how metrics should be computed. In essence, the Minerva API serves as the “how” by connecting the “what” with the “where”.</p><h3>Minerva API</h3><p>The Minerva API consists of the API web server, a metadata fetcher application, and several clients that integrate with <a href="https://superset.apache.org/">Apache Superset</a>, <a href="https://www.tableau.com/">Tableau</a>, <a href="https://www.python.org/">Python</a>, and <a href="https://www.r-project.org/">R</a>. These components serve native NoSQL and SQL metric queries to the downstream applications.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FUP1m7DFm8B6XmaF" /><figcaption>Minerva API serves as the interface between the consumers and the underlying datasets</figcaption></figure><h4>Metadata Fetcher: Abstracting the “Where”</h4><p>We mentioned previously that users simply ask Minerva for metrics and dimension cuts without having to figure out the “where”. When a data request is issued, Minerva spends a great deal of effort figuring out which of its datasets should be used to honor that request.</p><p>Under the hood, Minerva takes into account several factors before picking an optimal data source — one of the most important factors being data completeness. This means that any data source chosen to serve the query should contain all the columns needed for a given user’s query request and must cover the time range required from the query request.</p><p>To accomplish this, we built a service called Metadata Fetcher that periodically fetches data source metadata and caches it in a MySQL database every 15 minutes. Specifically, we periodically fetch the latest copy of the Minerva configuration (stored in Thrift binary) from S3 to get the list of every valid Minerva data source in Druid. For each data source, we query the Druid broker to get its name and a list of associated metrics and dimensions. Furthermore, we can also get the min date, max date, and count of distinct dates from the broker to figure out if there is any missing data. Every time new information is fetched, we update the MySQL database to maintain the source of truth. With this metadata fetcher, we are able to serve the data request using the best data source at any given time.</p><h4>Data API: Abstracting the “How”</h4><p>Imagine a scenario in which a user is interested in knowing the trend of average daily price (ADR), cut by destination region, excluding private rooms for the past 4 weeks in the month of August 2021. The full spec of the example query might look like the following:</p><pre>{</pre><pre>      metric: ‘price_per_night’,</pre><pre>      groupby_dimension: ‘destination_region’,</pre><pre>      global_filter: ‘dim_room_type!=”private-room”’,</pre><pre>      aggregation_granularity: ‘W-SAT’,</pre><pre>      start_date: ‘2021–08–01’,</pre><pre>      end_date: ‘2021–09–01’,</pre><pre>      truncate_incomplete_leading_data: ‘true’,</pre><pre>      truncate_incomplete_trailing_data: ‘true’,</pre><pre>}</pre><p>When Minerva receives such a request, it needs not only to figure out where to fetch the data, but also how to filter, combine, and aggregate the data to create the final result. It employs a strategy for achieving this via the <a href="https://www.jstatsoft.org/article/view/v040i01">Split-Apply-Combine paradigm</a>, commonly used in data analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZBoswC0q8u0DvM82" /><figcaption>Split-Apply-Combine in action for `price_per_night` metric</figcaption></figure><h4>Step 1: Split the Request into Atomic Metric Requests</h4><p>When Minerva API receives a query request such as the one above, the first thing it does is to break up any derived metrics into what we call a Minerva “atomic” metric by creating a set of associated subqueries. If a user query only specifies an atomic Minerva metric, then this first step is essentially a no-op.</p><p>In the example above, given that the `price_per_night` metric is a ratio metric (a special case of derived metric) that contains a numerator (`gross_booking_value_stays`) and a denominator (`nights_booked`), Minerva API breaks up this request into two sub-requests.</p><h4>Step 2: Apply and Execute Each Subquery</h4><p>With the atomic metrics identified from step 1, Minerva leverages metric configurations stored in S3 to extrapolate the associated metric expressions and metadata in order to generate the subqueries. Let’s stick with the same example: Minerva data API looks up the metric definition of `gross_booking_value_stays` and sees that it is a SUM aggregation, and similarly for the `nights_booked` metric. In both requests, a global filter ‘dim_room_type!=”private-room”’ is applied to ensure that private rooms are excluded from the calculation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*utscy1Byao92TR23" /><figcaption>The split-apply-combine paradigm in action for the ADR metric</figcaption></figure><p>Once the associated subqueries are generated for each atomic metric, Minerva API finally sends the queries over to Druid or Presto. It chops up the query into several “slices” that span a smaller time range and then combines the results into a single dataframe if resource limitation is reached. The API also truncates any incomplete leading or trailing data before rolling up the dataframe based on the aggregation granularity.</p><h4>Step 3: Combine Atomic Metric Results Into a Single Dataframe</h4><p>Once Minerva rolls up the dataframes for each atomic metric, it then combines them into a single dataframe by joining the dataframes on the timestamp column. As a final step, Minerva API performs any necessary post-aggregation calculations, applies ordering, and limits before returning the final result to the client in serialized JSON format.</p><p>To recapitulate, with Minerva’s data source API and data API, we are able to abstract away the process of identifying “where” to fetch the data and “how” to return the data. This API serves as the single layer of abstraction for Minerva to honor any request coming from downstream applications. However, our story does not simply end here: many of our engineering challenges involve how to integrate different applications with this API. We will explore these challenges in the next section.</p><h3>The Data Consumption Experience</h3><p>Bearing in mind the diverse set of data consumers within Airbnb, we set out to build tools tailored to different personas and use cases. With the Minerva API, we built a wide range of user interfaces that provide a consistent and coherent data consumption experience. As we mentioned briefly in the first <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">post</a>, there are four major integration endpoints, each supporting a different set of tools and audience:</p><ul><li><strong>Data Analysis: </strong>Integration with Python and R, used mostly for advanced data analytics</li><li><strong>Data Exploration: </strong>Integration with BI tools such as Superset, <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd#c576">Metric Explorer</a>, and Tableau, tailored for data-savvy analysts who drive insights</li><li><strong>Reporting: </strong>Integration with<strong> </strong><a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#efb9">XRF</a> (eXecutive Reporting Framework), tailored for an executive audience who wish to know the current state of the business</li><li><strong>Experimentation: </strong>Integration with<strong> </strong><a href="https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166">ERF</a> (Experimentation Reporting Framework), tailored for any data scientists, engineers, or product managers who run A/B tests at Airbnb</li></ul><p>When building out these features, we were constantly trading off between consistency, flexibility, and accessibility. For example, Metric Explorer is built mostly for non-technical users who are not data experts. This means that it needs to optimize consistency and accessibility over flexibility. Metric Explorer enforces strict guardrails that prevent users from doing the wrong thing, and there is very little opportunity to go off the “paved path”.</p><p>At the other extreme, the R and Python clients that are typically favored by data scientists are much more flexible. Users have full controls on how to leverage the clients’ API to perform custom analysis or visualization. In the next few sections, we will explain how some of these consumption experiences are created behind the scenes.</p><h4>Integration with Metric Explorer</h4><p>Metric Explorer was created at Airbnb so anyone, regardless of their level of data expertise, can leverage data to make informed decisions. Because of its broad target audience, Metric Explorer optimizes accessibility and data consistency over flexibility.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OknXUhNkPk0DtU8P" /><figcaption>The Metric Explorer is great for a non-technical audience who wants to answer high-level business questions</figcaption></figure><p>Under the hood, all of Metric Explorer’s metrics, dimensions, and relevant metadata are sourced from Minerva’s metric repository and ingested into <a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a>. These metadata are conveniently presented on the right sidebar as contexts before users perform any operations on the data.</p><p>When a user chooses to perform data operations such as Group By and Filter, Metrics Explorer presents dimensions in ranked order so that users with little or no business context can easily drill down, without needing to know the dimension values ahead of time — as illustrated above.</p><p>As users slice and dice the data, the Minerva API automatically determines which combination is valid and only surfaces cuts that exist. Nowhere in the experience does a user need to know anything about the underlying physical table from which the metric in question is sourced.</p><h4>Integration with Apache Superset</h4><p>While Metrics Explorer provides high-level information about metrics, more adventurous users who wish to slice and dice the data more can do so in Superset. <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Apache Superset</a> is a homegrown tool at the core of Airbnb’s self-serve BI solutions. Given the ubiquity of Superset inside the company, we knew that we needed to provide a functional SQL-like integration with Superset in order for Minerva to be widely adopted.</p><p>While many applications can be built on top of the Minerva API by talking to its RESTful endpoints directly, the client interfaces for BI tools such as Apache Superset and Tableau are more complex. Commonly, these BI tools speak SQL (via a client), not HTTP requests. This meant that Minerva API needed to support a SQL-like interface that adheres to the <a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a> type query structure. To build such an interface, we added to Minerva API a SQL parser — leveraging <a href="https://pypi.org/project/sqlparse/v">sqlparse</a> — to parse the SQL statement into an AST which is then validated and transformed into native HTTP requests.</p><p>Adhering to the DRY principle, we leveraged <a href="https://calcite.apache.org/avatica/">Apache Calcite Avatica</a>, which defines a generic database wire API between a client and server. The Minerva API serves as the Avatica HTTP server and the client is either a custom <a href="https://www.python.org/dev/peps/pep-0249/">Python Database API</a> database driver with <a href="https://www.sqlalchemy.org/">SQLAlchemy</a> dialect (Superset) or Avatica provided JDBC connector (Tableau).</p><p>Unlike traditional BI tools for which custom business logic is implemented in the tools themselves, Minerva consolidates and obfuscates all this logic via pseudo SQL-like AGG metric expressions. In the table below, we compare and contrast the queries run in a traditional BI tool to those run in Superset:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HK1IgDif4Uz5Qmm8LLiFPA.png" /></figure><p>In the query on the left, a user need not specify where the metric should be computed from, nor do they need to specify the correct aggregation function — these details are abstracted away by Minerva.</p><p>Finally, given that there are 12,000 metrics and 5,000 dimensions in Minerva, not all metric-dimension combinations are valid. For example, active listing can be cut by where the host is located, but not by where the guest is from (i.e. this guest attribute could be different for each booking reservation). We added event listeners to the chart controls to make sure that only eligible metric and dimension combinations are surfaced in the left pane. This design helps to reduce cognitive load and to simplify the data exploration process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CqG4C2B6ND0XW17h" /><figcaption>Superset is metric-centric. Users can query all metrics and dimensions from a single virtual source</figcaption></figure><h4>Integration with XRF — eXecutive Reporting Framework</h4><p>As presented in <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#efb9">Part-I</a>, XRF is a framework for producing succinct, high-fidelity, business critical reports that are consumed by executives and leadership teams. This framework is configured via the Minerva configs and powered entirely by Minerva API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hedx5FI-tvWKqhuA" /><figcaption>XRF automates a lot of repeated manual work and allows us to standardize high fidelity business critical reports</figcaption></figure><p>To curate an XRF report, users first define the reporting config and specify the desired business metrics, dimensional cuts, and global filters to apply. In addition, users can configure other controls such as whether a metric should be calculated as a running aggregation (e.g., MTD, QTD, or YTD), and the appropriate unit for growth rate time ratio comparisons (e.g., YoY, MoM, or WoW). Once these settings are specified, the Minerva API performs the necessary aggregations and final pivots to produce the final report.</p><p>The data output by XRF can be rendered in a Google sheet via a custom GoogleSheetHook as well as in Tableau via Presto connection. By leveraging the metric definitions in Minerva and its aggregation logic, we enforce consistency safeguards in the users’ choice of presentation layer.</p><h4>Integration with ERF — Experimentation Reporting Framework</h4><p>Unlike the analytics or reporting use cases, the experimentation use case is unique in that the metrics used for reporting are only a starting point. To make proper causal inferences, metrics must be joined with experiment assignment data before transforming them into summary statistics that can be used for valid statistical comparisons.</p><p>Typically, Minerva supplies the “raw events” to ERF. Depending on the unit of randomization and unit of analysis, we join the Minerva data to the assignment logs using different subject keys so that each event will have the associated subject, as well as the experiment group attached to it. Summary statistics such as means, percent changes, and p-values are then calculated and surfaced in the ERF scorecard.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wuT_vRG_w43SvZtf" /><figcaption>ERF scorecard showing summary statistics for experiments</figcaption></figure><p>The Experimentation UI also exposes relevant Minerva metadata directly in the tool. Users can view the description and ownership information of the underlying Minerva events. A lineage view, overlayed with ETA information, allows users to <a href="https://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710">track the progress of ERF metrics</a> and helps them contact the relevant Minerva metric owners in case of delays.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZtOE_1oe6GUtuY1T" /><figcaption>ERF displaying metrics metadata, which links to<a href="https://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710"> SLA Tracker</a> to visualize data lineage and timeliness</figcaption></figure><p>In summary, Minerva and its various integrations enable users to easily track metrics within their scheduled reporting, measure movements due to experimentation, and explore unexpected changes — all with the confidence that the data is correct and consistent. This confidence drastically reduces the time spent deriving insights, increases trust in data, and helps to support data-driven decision making.</p><h3>Closing</h3><p>Minerva introduced a novel way of thinking about data, not only is it centered around a business- and metric-centric user interface, we also need to adapt traditional BI tools (that mostly talk SQL) to the interface of Minerva API. In some sense, it is akin to fitting a new square peg (Minerva) into an existing round hole ( BI Tools).</p><p>As more organizations embrace the concept of a metric layer similar to Minerva, we believe there will be a new set of challenges awaiting us. That said, some of this pioneering work will surely bring analytics to the next level, and we are grateful for contributing to the leading edge of this landscape. We hope that soon more companies will follow suit.</p><h3>Acknowledgements</h3><p>Thanks to everyone who <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#8a0a">contributed to the work and outcomes</a> represented in this blog post. In addition to <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#8a0a">previous acknowledgements</a> we would also like to thank those who have partnered with us to adopt Minerva within our consumption landscape.</p><p>All trademarks are the properties of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship or endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1c0b6a8b9206" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-enables-consistent-data-consumption-at-scale-1c0b6a8b9206">How Airbnb Enables Consistent Data Consumption at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Commitment to Craft]]></title>
            <link>https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e36d5a8efe2a</guid>
            <category><![CDATA[culture]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[work-experience]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 16 Sep 2021 16:59:30 GMT</pubDate>
            <atom:updated>2021-09-16T20:03:27.761Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://news.airbnb.com/cto-2018/"><em>Ari Balogh</em></a><em>, CTO at Airbnb</em>, shares how striving towards excellence has served us well in a time of uncertainty.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EoAwRoDa43XnZypIQz7oUQ.jpeg" /></figure><p>If you’ve ever been part of a startup, you understand the importance of speed. Sometimes it feels like nothing else matters, since getting your products to market quickly can determine whether you survive. When Airbnb was an emerging company with a radical new vision for travel, we often had to prioritize speed in making tough engineering tradeoffs.</p><p>These decisions paid off, and Airbnb grew into a platform that supports millions of Hosts and Guests globally. Now, in addition to delivering fast, our success depends on providing an experience of exceptional quality that considers every detail for a diverse community that spans virtually every country. With this in mind, two years ago our tech organization took a holistic look at what we’d built and where to make changes. To prioritize excellence, we committed to creating an environment that enables people to do their best work and nurtures a mindset of quality — a Commitment to Craft.</p><p>Nobody could have predicted what came next. COVID-19 changed the world and impacted travel in unprecedented ways. With our Commitment to Craft already in place, we were better prepared for these challenges. In turn, we’ve developed an even deeper appreciation for quality and efficiency across our technology organization.</p><h3>What is Commitment to Craft?</h3><p>Commitment to Craft consists of several principles. First, it’s about systems built on sound technical foundations that enable easy adaptation and innovation, while ensuring quality. Second, it’s about enabling the people behind the work. To build on these foundations, it takes great talent combined with an environment that fosters creativity and collaboration, with clear accountability for deliverables. People then need the right tools and, importantly, time to do excellent work. Third, it’s about setting measurable goals: problems are clearly defined, and the individual teams create their plans.</p><p>A critical outcome is that these elements combine to create so much of the magic in our products. When people feel their craft is supported, their personal touches of excellence come through to bring true delight for our Hosts and Guests.</p><h3>Building with craft</h3><p>Change takes time. We cascaded goals to every engineer and data scientist (together we call them technologists) so everyone was part of the transformation. We started small, identifying a few concrete areas of improvement around site reliability, performance and developer tooling. These investments laid a foundation for quality and, over time, transformed how people work.</p><p>Commitment to Craft led to many important outcomes across Airbnb, including improving our page performance, <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">investing in our data quality</a>, <a href="https://medium.com/airbnb-engineering/achieving-insights-and-savings-with-cost-data-ec9a49fd74bc">using our compute resources more efficiently</a>, and improving our development practices, including testing.</p><h4>Example: Improving page performance</h4><p>Faster websites lead to happier users. But while we were focused on product innovation and adding more and more features, our pages became significantly slower. One of our Commitment to Craft goals was to reduce page load times. To set a specific target, we decided that “page performance score” — a composite score of user-centric metrics measuring the time it takes for a page to load and feel responsive — was a key measure.</p><p>To enable craft, we built a set of tools to measure fine-grained latencies across many page components. At the same time, our performance experts wrote a “how-to” guide on improving load times. With these resources, individuals across the entire org took ownership of improving the performance of their pages. The improvements were exceptional — we’ll share details in an upcoming blog post.</p><h3>Craft in a crisis</h3><p>We were progressing well on our Commitment to Craft goals when the world changed overnight. COVID-19 hit in early 2020 and Airbnb lost 80% of its business in eight weeks.</p><p>Prior to COVID, one of the elements of the Commitment to Craft program is what we called <em>Performance Efficiency:</em> delivering a reliable, performant experience in a cost-efficient manner. To support this, we started to build tools and techniques to help teams understand and optimize their cost of serving. With COVID, we doubled-down on these tools and techniques — and the use of these tools to improve efficiency and resource utilization. The result was a dramatic reduction in operating costs that helped us weather the storm.</p><h3>Looking ahead</h3><p>Today, Commitment to Craft is more than a set of projects — it’s a philosophy that people have rallied around. We’re starting to see teams across tech choose to prioritize craft simply because it’s “how we do things here.”</p><p>I’ve never been more proud of the work our teams are doing. We still have much to do, but we’re confident that this philosophy will help us navigate the future and keep us focused on what matters. Our goal is to achieve “agility with stability” — have the development agility of a startup combined with the product quality expected of a company of our scale. To get there, we’ll continue investing in the people behind the craft.</p><p><em>-Ari</em></p><p><em>If a culture of craft resonates with you, you might be a great fit for Airbnb. We’re currently hiring for a variety of technical roles, so check out our </em><a href="https://careers.airbnb.com/"><em>career page</em></a><em> for more information.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e36d5a8efe2a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a">Commitment to Craft</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automating Data Protection at Scale, Part 1]]></title>
            <link>https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c74909328e08</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[privacy]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[elizabeth nammour]]></dc:creator>
            <pubDate>Tue, 14 Sep 2021 17:00:16 GMT</pubDate>
            <atom:updated>2021-09-14T17:00:16.451Z</atom:updated>
            <content:encoded><![CDATA[<p>Part one of a series on how we provide powerful, automated, and scalable data privacy and security engineering capabilities at Airbnb.</p><p><a href="https://www.linkedin.com/in/elizabethnammour/">Elizabeth Nammour</a>, <a href="https://www.linkedin.com/in/wendy-jing-jin-81452921/">Wendy Jin</a>, <a href="https://www.linkedin.com/in/shengpu-liu/">Shengpu Liu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u5ErTNsWp-x1GE72" /></figure><p>Our community of hosts and guests trust that we will keep their data safe and honor their privacy rights. With frequent news reports of data security breaches, coupled with global regulations and security requirements, monitoring and protecting data has become an even more critical problem to solve.</p><p>At Airbnb, data is collected, stored, and propagated across different data stores and infrastructures, making it hard to rely on engineers to manually keep track of how user and sensitive data flows through our environment. This, in turn, makes it challenging for them to protect it. While many vendors exist for different aspects of data security, no one tool met all of our requirements when it came to data discovery and automated data protection, nor did they support all of the data stores and environments in our ecosystem.</p><p>In this three-part blog series, we’ll be sharing our experience building and operating a data protection platform at Airbnb to address these challenges. In this first post, we will give an overview of why we decided to build the Data Protection Platform (DPP), walk through its architecture, and dive into the data inventory component, Madoka.</p><h3>Data Protection Platform (DPP)</h3><p>Since no one tool was meeting our needs, we decided to build a data protection platform to enable and empower Airbnb to protect data in compliance with global regulations and security requirements. However, in order to protect the data, we first needed to understand it and its associated security and privacy risks.</p><h4>Understanding Airbnb’s Data</h4><p>At Airbnb, we store petabytes of data across different file formats and data stores, such as MySQL, Hive, and S3. Data is generated, replicated, and propagated daily throughout our entire ecosystem. In order to monitor and gain an understanding of the ever-changing data, we built a centralized inventory system that keeps track of all the data assets that exist. This inventory system also collects and stores metadata around the security and privacy properties of each asset, so that the relevant stakeholders at Airbnb can understand the associated risks.</p><p>Since some data assets may contain sensitive business secrets or public information, understanding what type of data is stored within a data asset is crucial to determining the level of protection needed. In addition, privacy laws, such as the European Union General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA), have granted users the right to access and delete their personal data. However, personal data is a less-than-precise term that represents many different data elements, including email addresses, messages sent on the platform, location info, etc. In order to comply with these laws, we need to pinpoint the exact location of all personal data. To do this, we built a scalable data classification system that continuously scans and classifies our data assets to determine what type of data is stored within them.</p><h4>Enabling Automated Data Protection</h4><p>Based on the understanding of the data, the DPP strives to automate its protection, or enables and notifies teams across the company to protect it. This automation focuses on a few key areas: data discovery, prevention of sensitive data leakages, and data encryption.</p><p>Discovering personal data is the first step to privacy compliance. This is especially true as personal data needs to be deleted or returned to a user upon request. Our platform enables us to automatically notify data owners when new personal data is detected in their data stores and integrate this data with our privacy orchestration service to ensure it gets deleted or returned if needed.</p><p>A common cause of data breaches is when sensitive secrets, such as API keys or credentials, are leaked internally and then make their way into the hands of an attacker. This can come from an engineer logging the secret within their service or committing the secret to code. Our data protection platform identifies potential leaks from various endpoints and notifies the engineer to mitigate the leakage by deleting the secret from the code or log, rotating the secret, and then hiding the new secret with our encryption tool sets.</p><p>One of the most popular and important methods of data protection is encryption, since even in case of an infiltration, attackers won’t be able to get their hands on sensitive data. However, breaches due to unencrypted sensitive data are unfortunately a common occurrence within the industry.</p><p>Why does it still happen? Secure encryption with proper key management is technically challenging, and organizations do not always know where sensitive data is stored. The DPP aims to abstract these challenges by providing a data encryption service and client library that engineers can use. It automatically discovers sensitive data, so that we don’t rely on manual identification.</p><h4>Platform Architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SUuqoIcTshHVhQQ_" /><figcaption>Figure 1: DPP Overview</figcaption></figure><p>The DPP aims to discover, understand, and protect our data. It integrates the services and tools we built to tackle different aspects of data protection. This end-to-end solution includes:</p><ul><li><strong>Inspekt</strong> is our data classification service. It continuously scans Airbnb’s data stores to determine what sensitive and personal data types are stored within them.</li><li><strong>Angmar</strong> is our secret detection pipeline that discovers secrets in our codebase.</li><li><a href="https://medium.com/airbnb-engineering/one-step-forward-in-data-protection-8071e2258d16"><strong>Cipher</strong></a> is our data encryption service that provides an easy and transparent framework for developers across Airbnb to easily encrypt and decrypt sensitive information.</li><li><strong>Obliviate</strong> is our orchestration service, which handles all privacy compliance requests. For example, when a user requests to be deleted from Airbnb, obliviate will forward this request to all necessary Airbnb services to delete the user’s personal data from their data stores.</li><li><strong>Minister</strong> is our third party risk and privacy compliance service that handles and forwards all privacy data subject rights requests to our external vendors.</li><li><strong>Madoka</strong> is our metadata service that collects security and privacy properties of our data assets from different sources.</li><li>Finally, we have our <strong>Data Protection Service</strong>,<strong> </strong>a presentation layer where we define jobs to enable automated data protection actions and notifications using information from Madoka (e.g., automate integrations with our privacy framework)</li></ul><h3>Madoka: A Metadata System</h3><p>Madoka is a metadata system for data protection that maintains the security and privacy related metadata for all data assets on the Airbnb platform. It provides a centralized repository that allows Airbnb engineers and other internal stakeholders to easily track and manage the metadata of their data assets. This enables us to maintain a global understanding of Airbnb’s data security and privacy posture, and provides an essential role in automating security and privacy across the company.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/920/0*OEag9YxDW3CT1iQy" /><figcaption>Figure 2: Madoka Architecture</figcaption></figure><p>Implemented by two different services, a crawler and a backend, Madoka has three major responsibilities: collecting metadata, storing metadata, and providing metadata to other services.The Madoka crawler is a daily crawling service that fetches metadata from other data sources, including Github, MySQL databases, S3 buckets, Inspekt (data classification service), etc. It then publishes the metadata onto an AWS Simple Queue Service (SQS) queue. The Madoka backend is a data service that ingests the metadata from the SQS queue, reconciles any conflicting information, and stores the metadata in its database. It provides APIs for other services to query the metadata findings.</p><p>The primary metadata collected by Madoka includes:</p><ul><li>Data assets list</li><li>Ownership</li><li>Data classification</li></ul><p>For each of the above we handle both MySQL and S3 formats.</p><h4>Data Assets List</h4><p>The first type of metadata that needs to be collected is the list of all data assets that exist at Airbnb, along with their basic metadata such as the schema, the location of the asset, and the asset type.</p><p>For MySQL, the crawler collects the list of all columns that exist within our production AWS account. It calls the AWS APIs to get the list of all clusters in our environment, along with their reader endpoint. The crawler then connects to that cluster using JDBI and lists all the databases, tables, and columns, along with the column data type.</p><p>The crawler retains the following metadata information and passes it along to the Madoka backend for storage:</p><ul><li>Cluster Name</li><li>Database Name</li><li>Table Name</li><li>Column Name</li><li>Column Data Type</li></ul><p>For S3, the crawler collects the list of all objects that exist within all of our AWS accounts.</p><p>At Airbnb, we use <a href="https://www.terraform.io/">Terraform</a> to configure AWS resources in code, including S3 buckets. The crawler parses the Terraform files to fetch the S3 metadata.</p><p>The crawler first fetches the list of all AWS account numbers and names, which are stored in a configuration file in our Terraform repository. It then fetches the list of all bucket names, since each bucket configuration is a file under the account’s subrepo.</p><p>In order to fetch the list of objects within a bucket, the crawler uses <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html">S3 inventory reports</a>, a tool provided by AWS. This tool produces and stores a daily or weekly CSV file of all the objects contained in the bucket, along with their metadata. This is a much faster and less costly way of getting the list compared to calling the List AWS API. We’ve enabled inventory reports on all production S3 buckets in Terraform, and the bucket configuration will specify the location of the inventory report.</p><p>The crawler retains the following information and passes it along to Madoka backend for storage:</p><ul><li>Account Number</li><li>Account Name</li><li>Assume Role Name</li><li>Bucket Name</li><li>Inventory Bucket Account Number</li><li>Inventory Assume Role Name</li><li>Inventory Bucket Prefix</li><li>Inventory Bucket Name</li><li>Object key</li></ul><h4>Ownership</h4><p>Ownership is a metadata property that describes who owns a specific data asset.</p><p>We decided to collect service ownership, which allows us to link a data asset to a specific codebase, and therefore automate any data protection action that requires code changes.</p><p>We also decided to collect team membership, which is crucial to perform any data protection action that requires an engineer to do some work or that requires a stamp of approval. We chose to collect team ownership and not user/employee ownership since team members constantly change, while the data asset remains with the team.</p><p>At Airbnb, since we migrated to a service-oriented architecture (SOA), most MySQL clusters belong to a single service and a single team. To determine service ownership, the crawler fetches the list of the services that connect to a MySQL cluster and will set the service with the most number of connections within the last 60 days as the owner of all the tables within that cluster. There are many services that connect to all clusters for monitoring, observability, and other common purposes, so we created a list of roles that should be filtered out when determining ownership.</p><p>There are still some legacy clusters in use that are shared amongst many services, where each service owns specific tables within the clusters. For those clusters, not all tables will have the correct service owner assigned, but we allow for a manual override to correct these mistakes.</p><p>The crawler uses service ownership to determine team ownership, since at Airbnb, team ownership is defined within the service’s codebase on Git.</p><p>At Airbnb, all S3 buckets have a project tag in their Terraform configuration file, which defines which service owns the bucket. The crawler fetches the service ownership from that file and uses it to determine the team ownership, as described above for MySQL.</p><h4>Data Classification</h4><p>Data classification is a metadata property that describes what type of data elements are stored within the asset — e.g., a MySQL column which stores email addresses or phone numbers would be classified as personal data. Gathering data classifications allows us to understand the riskiness of each data set so we can determine the level of protection needed.</p><p>The crawler fetches the data classification from two different sources. First, it fetches data classifications from our Git repositories, since data owners can manually set the classifications in their data schema. However, relying on manual classifications is insufficient. Data owners do not always know what an asset contains, or they may forget to change the classifications when the data asset is updated to store new data elements.</p><p>The crawler will then fetch data classifications from our automated data classification tool, called Inspekt, which we will describe in detail in a later blog post. Inspekt continuously scans and classifies all of our major data stores, such as MySQL and S3. It outputs what data elements were found in each data asset. This ensures that our data is constantly monitored, and classifications are updated as data changes. As with any automated detection tool, precision and recall are never 100%, so false positives and false negatives may occur.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uArO328-uBKl2WQ-" /><figcaption>Figure 3: Classification Reconciliation</figcaption></figure><p>Since the crawler fetches the data classifications from two different sources, some discrepancies may arise, where the manual classification contains data elements not found by Inspekt or vice versa. The crawler will forward all findings to the Madoka backend, which will resolve any conflicts. The status of the manual classification is marked as <em>new</em> by default and the status of the Inspekt classification is marked as suggested. If the manual classification aligns with the Inspekt result, the classification is automatically confirmed. If there is any discrepancy, we file tickets to the data owners through the data protection service. If the Inspekt classification is correct, the owners may update the data schema in the Git repository, or they can mark the Inspekt classification as incorrect to resolve the conflict.</p><h4>Other Security and Privacy Related Attributes</h4><p>Madoka also stores how data assets have integrated with our security and privacy tools. For example, we may store whether or not the data asset is encrypted using Cipher or is integrated with our privacy compliance service, Obliviate, for data subject rights requests. We built Madoka to be easily extensible and are constantly collecting and storing more security and privacy related attributes.</p><h3>Conclusion</h3><p>In this first post, we provided an overview of why we built the DPP, described the platform’s architecture, and dove into the data inventory component, Madoka. In our next post, we will focus on our data classification system that enables us to detect personal and sensitive data at scale. In our final post we will deep dive into how we’ve used the DPP to enable various security and privacy use cases.</p><h3>Acknowledgements</h3><p>The DPP was made possible thanks to many members of the data security team: Pinyao Guo, Julia Cline, Jamie Chong, Zi Liu, Jesse Rosenbloom, Serhi Pichkurov, and Gurer Kiratli. Thank you to the data governance team members for partnering and supporting our work: Andrew Luo, Shawn Chen, and Liyin Tang. Thank you Tina Nguyen for helping drive and make this blog post possible. Thank you to our leadership, Marc Blanchou, Brendon Lynch, Paul Nikhinson and Vijaya Kaza, for supporting our work. Thank you to previous members of the team who contributed greatly to the work: Lifeng Sang, Bin Zeng, Alex Leishman, and Julie Trias.</p><p>If this type of work interests you, see <a href="https://careers.airbnb.com/">our career page</a> for current openings.</p><p>Tags: data, security</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c74909328e08" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">Automating Data Protection at Scale, Part 1</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Task-Oriented Conversational AI in Airbnb Customer Support]]></title>
            <link>https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5ebf49169eaa</guid>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[nlp]]></category>
            <category><![CDATA[customer-support]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Gavin Li]]></dc:creator>
            <pubDate>Tue, 10 Aug 2021 16:51:16 GMT</pubDate>
            <atom:updated>2021-08-10T17:45:29.187Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb is powering automated support to enhance the host and guest experience</p><p><a href="https://www.linkedin.com/in/gavin-li-64354117/">Gavin Li</a>, <a href="https://www.linkedin.com/in/mia-zhao-964a9213/">Mia Zhao</a></p><figure><img alt="mother and daughter sitting on a chair, scrolling through their phone" src="https://cdn-images-1.medium.com/max/1024/0*46ISdm7kRuJddwie" /></figure><p>Customer Support (CS) can make or break a guest’s travel experience. To support Airbnb’s community of guests and Hosts, we have been investing heavily in developing intelligent CS solutions leveraging state-of-the-art natural language processing (NLP), machine learning (ML), and artificial intelligence (AI) technologies.</p><p>In this blog post, we’ll introduce the automated support system at Airbnb, which employs the latest task-oriented conversational AI technology, through the lens of a recently launched feature called Mutual Cancellation. We will describe in detail how we framed the business problem as an AI problem, how we collected and labeled training data, how we designed and built the ML models, and how the models were deployed in the online system. Throughout each step, we’ll discuss some technical challenges we faced during this project and the solutions we innovated to address these challenges.</p><h4>A Case Study: Mutual Cancellation</h4><p>Prior to the development of the mutual cancellation model, guests needed to involve CS agents, even if they had already reached an agreement with the host for canceling a reservation. This meant that issues took longer to get resolved and precious CS agent hours were wasted. To solve this issue, we developed AI models that help guests and Hosts self-resolve cancellation and refund issues without involving a CS agent. This empowers hosts and guests to decide what is best for them, while allowing us to focus CS agent hours where they are needed most.</p><p>In the rest of the post, we will use the mutual cancellation feature as an example to describe the technical components of Airbnb’s task-oriented AI system.</p><h3>System Architecture</h3><p>Airbnb’s Intelligent Support Platform team develops cutting-edge AI technologies to help guests and hosts solve their issues in the most efficient manner. Based on the chatbot platform we built, <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">ATIS</a>, our AI models aim to learn and mimic how human agents provide warm and effective customer care. A warm and effective customer care experience starts with a personal and intelligent issue identification that aims to quickly understand the user’s situation, needs, questions and concerns with minimum friction.. Once the issue is clearly identified, we generate responses dynamically and guide users through various product workflows to solve their issues or route them to human agents.</p><p>Our intelligent customer support product is designed as a “task-oriented dialog system” (<a href="https://arxiv.org/abs/2007.12720">Zang et al. 2020</a>, <a href="https://arxiv.org/abs/2008.06239">Madotto et al. 2020</a>). Task-oriented dialog systems are gaining more and more interest in recent years, powering AI products ranging from virtual assistants to smart speakers. These models can understand the user’s intent (e.g., ‘play music’), extract needed parameters (e.g., ‘artist name and name of the song’) from the conversation, ask questions to clarify details (e.g., ‘there are two versions of this song, which one do you like to play?’), and complete the task — all while having a dialogue with the user that seems completely natural.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/851/1*LfN3IPLswc0LJcYVQCdmRQ.png" /><figcaption>Figure 1. Airbnb’s Chatbot as a task-oriented dialog system. It detects user intent and generates appropriate responses and completes the task through actions.</figcaption></figure><h3>Customer Support as a Task-Oriented Dialog Problem</h3><p>In real-world machine learning applications, the most crucial piece of the puzzle is how to formulate the problem. Problem formulation has a much more significant impact on the product’s long-term performance than the model itself. There are lots of decisions and trade-offs to be made before a single line of code is written. We designed a multi-layer issue detection and decision-making system to allow both extensibility and domain-specificity for the customer support problem, as demonstrated in Figure 2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/1*TA0BXmY2hYhuswYJNOJrYA.png" /><figcaption>Figure 2. A multi-layer user issue detection and decision-making model structure.</figcaption></figure><p>When a user sends a message in the Airbnb chatbot, the message is processed by the first layer, a domain classification model. The domain classification model determines which domain the message belongs to, for example, a trip rebooking request, a cancellation refund request, or a question that can be answered with a help article recommendation. If the Mutual Cancellation domain is predicted to be the most likely domain, the system triggers the Mutual Cancellation flow and enters the second layer to further our understanding of the user’s intent and checks the eligibility of the Mutual Cancellation.</p><p>For Mutual Cancellation, there are two models in the second layer: the Q&amp;A-based intent understanding model and the “expected refund ratio prediction” model. The Q&amp;A intent model is trained on a manually labeled dataset. The “expected refund ratio prediction” model is trained on historical cancellation data and refund ratio decided by agents. Refund ratios capture many vital characteristics of the trip that are crucial for the AI system to make decisions on behalf of human agents.</p><p>The multi-layer structure has the benefit of:</p><ul><li><strong>Scalable</strong>: it allows the system to be extended to new domains and domain-specific models for existing domains won’t be affected by new domains.</li><li><strong>Effective</strong>: the top-level model is trained on manually labeled data which is usually high quality, but often difficult and expensive to collect. Domain-specific models are mostly trained from historical data, easy to collect but noisy and biased towards past user behavior. The multi-layer structure allows us to leverage human-labeled data to train the top-layer domain prediction model and historical data to train domain-specific models.</li></ul><h3>Collecting and Labeling Training Data</h3><p>A typical task-oriented dialog system builds an intent taxonomy tree where each node represents some intent, and the nodes are mutually exclusive. Airbnb’s customer support, similar to other shared-economy customer support, users’ issues contain complex issues that are less structural than a typical online marketplace. It is challenging, if possible at all, to define a clean taxonomy tree to capture ALL users’ issues and partition them in a hierarchical tree.</p><p>In addition, a taxonomy tree usually implies that we need to traverse from the root node following a path to the leaf node. Along the path, the system asks questions (e.g., “Do you want to cancel the reservation?”) or collects more information (e.g., “Is the user a Guest or a Host?”) to decide on which branch to continue. In Airbnb’s case, users’ issues are much more complicated and may require different sequences of questions to identify the issue efficiently. For Mutual Cancellation, the first question (“if the host and guest agree with each other”) and the second question(“who initiated the cancellation”) capture different aspects of the cancellation and refund process. It can be challenging to design a simple and clean tree structure taxonomy to cover all user issues and rely on the path down the tree to collect the needed information efficiently. Instead, we model intent understanding as a Question &amp; Answer (Q&amp;A) problem.</p><h4>A Q&amp;A Model for Understanding User Intent</h4><p>Given a user’s initial message to our CS platform, we ask a couple of questions about the user’s intent, and then have human agents/labelers answer those questions. Through this setup, we collect data and train a Q&amp;A model. The trained Q&amp;A model is able to answer those questions similarly. Users’ questions can have multiple answers and users often try to describe the problem from different angles. In some cases, the questions can be mutually exclusive, whereas in other cases the questions may contain redundant information.</p><p>Below are a few examples we ask our labeler team:</p><p><strong><em>User’s message to Airbnb:</em></strong></p><p><em>Hello! I made a reservation wrongly. Thinking it was a whole apartment rental when it was actually just a room. I didn’t pay attention. I immediately spoke to my host, she agreed to refund me and asked me to request the refund money from the app, but I can’t find the option.</em></p><p><strong><em>Question: Who initiated the cancellation?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>The host initiated the cancellation, or the host could not accommodate the guest</em></li><li><em>The guest initiated the cancellation</em></li><li><em>Not mentioned</em></li></ol><p><strong><em>Question: Do the host and guest agree on a refund?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Host agrees on offering a refund and the refund amount</em></li><li><em>Host and guest are having some differences on the refund amount</em></li><li><em>Host disagrees with issuing a refund or already declined it</em></li><li><em>Agreement not mentioned about refund</em></li><li><em>Refund not mentioned at all</em></li></ol><p><strong><em>Question: Is the guest asking how they can get what they want? (how to get refund, what to do, etc)</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Yes</em></li><li><em>No</em></li></ol><p><strong><em>Question: Is the guest asking how they can get a refund, if is it possible, or how much refund can they get?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Yes</em></li><li><em>No</em></li></ol><p>Q&amp;A problems with multiple-choice answers are normally modeled as a multi-class classification problem, where each class maps to one question. However, <a href="https://arxiv.org/abs/2011.03292">Jiang et al. (2020)</a> proposed the idea of modeling Q&amp;A problems as single-choice binary classification problems. In modeling the problem this way, the difficulty of the problem increases. Picking the correct answer from multiple options is no longer sufficient － the model must predict the correct choice as positive and all other choices as negative. This approach makes consolidating multiple Q&amp;A problems easier, enabling us to increase the pre-training scale. <a href="https://arxiv.org/abs/2005.00700">Khashabi et al. (2020)</a> similarly found that unifying multiple pre-training datasets can help boost the model performance.</p><p>We follow the single-choice binary setup, which enables us to unify related user intent training labels from different domains to increase the scale of our training data and enhance the performance. As stated above, we continuously review the data labeling quality and refine the labeling questionnaire design. As a result, there are many versions of labeling questions and the answers for each version. A single-choice setup allows us to mix all the different versions of our training questions together in training.</p><p>Figures 3 and 4 show the difference between single-choice and multi-choice setups for an example message “<em>My host agreed to fully refund me, so if I cancel now can I get a full refund?</em>”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/915/1*iHFTwigsFWtMPuQ--aoy5w.png" /><figcaption>Figure 3. Single-choice Q&amp;A model setup</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/885/1*EmjZlJzZMdzpt3POAjMVrw.png" /><figcaption>Figure 4. Multi-choice Q&amp;A setup</figcaption></figure><p>Figure 5 shows the model performance difference in our experiment. Single-choice Q&amp;A setup outperforms traditional multi-class intent classification setup both on offline labeling prediction accuracy and on online conversion prediction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3GUwYj0tepuePtLJ0aBsmA.png" /><figcaption>Figure 5. Accuracy of single-choice vs. multi-class intent classification.</figcaption></figure><h4>Benefits and Challenges of Intent Prediction as Q&amp;A</h4><p>Compared with traditional multi-class classification, the Q&amp;A setup makes the data labeling much more manageable. We can continuously refine the questionnaire design and flexibly merge questions from different dimensions, different angles, or those with redundancy.</p><p>One of the biggest challenges of applying machine learning in real-world problems is the lack of high-quality training data. From a few-shot learning point of view, the single-choice Q&amp;A setup allows us to build many capabilities into the model, even with sparse training data. This setup trains the model to encode information in the user message, the question and the answer. The model can also learn from related questions from other domains. For this reason, it has the capability to understand both questions in training labels and some newly constructed, unseen questions.</p><p>A shortcoming of this setup is that it puts a lot of pressure on the serving latency. For example, if we want to use the model to answer five questions and then take actions based on the five questions, we have to run the model five times. Later in this post, we’ll discuss how we reduce the model latency including using GPU.</p><h3>Model Design and Implementation</h3><p>We use autoencoder transformers as model architecture. We tested all kinds of model choices as the backbone. The results are shown below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JY2SnLt2CGbwHC7gYjQtGw.png" /><figcaption>Figure 6. Results on out-of-sample data from various intent classification models.</figcaption></figure><p>For most of our use cases, Roberta performs the best. However, the Roberta-base and Roberta-large’s performances vary depending on the scale of training labels. In our online product case, where we have around 20K labels, the Roberta-large model achieved the best performance and is the model that we deployed in production. However, with 335M parameters, it is very challenging to run this model online with a given latency budget.</p><p>To improve the performance of this model, we leveraged three key techniques:</p><ul><li>Pre-training our transformer model with transfer learning;</li><li>Translating training labels to utilize a multilingual model; and</li><li>Incorporating multi-turn intent predictions.</li></ul><h4>Pre-training</h4><p>Perhaps the most critical recent development in deep learning is transfer learning and pre-training. It dominates most state-of-the-art models in almost all kinds of NLP, computer vision(CV), and automatic speech recognition(ASR) domains.</p><p>We experimented with different pre-training methods extensively and found two pre-training methods to be particularly effective in boosting the model performance:</p><ul><li><strong>In-Domain Unsupervised Masked Language Model (MLM) Pre-training:</strong> Based on users’ conversations with our customer service platform, the listing descriptions, and the help articles, we construct a 1.08GB (152M word tokens) unsupervised training corpus. This corpus contains 14 different languages, with 56% in English. As shown through the experiment results in Figure 7, the in-domain MLM pre-training helps to boost the model performance for our tasks.</li><li><strong>Cross-Domain Task Finetune Pre-training: </strong>Pretraining a transformer model based on a cross-domain dataset is often helpful for many tasks. It’s also effective in boosting intent detection accuracy in our use cases. Experiments results can be found in Figures 8 and 9.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MxI1RhDvxilRZoll83yPBw.png" /><figcaption>Figure 7. In-domain pre-training performance</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ehncNQ9sU2J-_lySRTYAvA.png" /><figcaption>Figure 8. cross-domain task finetune pre-training performance.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HR9TzCqDhRxUF3lIvVwUHQ.png" /><figcaption>Figure 9. Multilingual task finetune pre-training performance.</figcaption></figure><p>Many challenging cases in our intent understanding problem require the model to have some logical reasoning capability. Similar to the finding in the logical reasoning public dataset in <a href="https://arxiv.org/abs/2002.04326">Yu et al. (2020)</a>, pre-training on the RACE dataset helps to boost the performance the most.</p><h4>Multilingual Model</h4><p>Airbnb customer support serves users from all around the world, currently supporting 14 languages. The top non-English languages, including French, Spanish, German, and Portuguese, represent around 30% of the requests. Since our model is targeted at users who speak all languages but labeled data are mainly in English, we leveraged a translated annotation dataset and multilingual model, XLM-RoBERTa, to boost model performance across all languages.</p><p>Translating the training labels to other languages is an unsupervised data augmentation technique proven effective in many deep learning training cases (<a href="https://arxiv.org/abs/1904.12848">Xie et al., 2020</a>). We translate the labeled English training corpus and the labeling questions and answers into other top languages and include them in the training data to train the XLM-RoBERTa model.</p><p>We also tried training monolingual models on translated text for comparison based on public pre-trained monolingual models. Results show that multilingual models trained on translated datasets significantly outperform the English-only training dataset. Model performance is comparable with monolingual models trained by translated annotation datasets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*usU-lK5fyOuDn5WWol6x6Q.png" /><figcaption>Figure 10. Multilingual vs. monolingual model performance.</figcaption></figure><h4>Incorporating Multi-Turn Intent Prediction</h4><p>When a user comes to chatbot with a Mutual Cancellation request, we pull all the text sequences from the user’s previous conversations and concatenate the text sequences of the previous messages and the current request message together as a new text sequence input to the transformer model. This works as a <strong>dialog state tracking</strong> (<a href="https://arxiv.org/abs/1908.01946">Gao et al., 2019</a>) module to incorporate the signals from user’s past interactions to better understand user intent. We experimented with two offline approaches to better consume this signal: 1) adding the last N round of messages as additional features to the current model, and 2) calculating multi-turn intent predictions on each message threshold and adding max intention score to the downstream model.</p><p>One challenge is that the computation complexity of transformer models is O(n⁴) of the sequence length, including all the previous conversions. The complexity makes it impossible to infer online in real-time. To solve this, we process the historical conversation asynchronously offline ahead of time and store pre-computer scores. During online serving, the model directly query the pre-computed scores associated to the user.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zpDgIz3WO9vsTyk1vrmcBg.png" /><figcaption>Figure 11. Multi-turn intent prediction performance and latency.</figcaption></figure><h3>Online Serving</h3><p>Deploying machine learning models online comes with a few major challenges that need to be managed differently than in the offline world.</p><h4>Online Inference GPU Serving</h4><p>One challenge in online serving is the latency of the model in production. We took two key steps to solve for latency requirements: 1) enabling GPU serving, and 2) leveraging transfer learning. Similar to the discussions in the section above, transfer learning techniques like teacher student model is used to reduce the amount of computation needed in online inference. In this section we mainly focus on how GPU serving helped us address this challenge.</p><p>To support GPU inference, we experimented with an offline benchmark on transformer models with 282M parameters on three different instance types — <em>g4dn.xlarge, p3.2xlarge and r5.2xlarge</em>. Figure 12 shows the latency results across these various instance types. The general trend of latency between CPU and GPU as our input messages grow in length can be seen in Figure 13. Shifting to GPU serving has a significant impact on the online latency and is more cost-efficient.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ihe4tlKCo_r9DGNqz4_Fuw.png" /><figcaption>Figure 12. GPU online serving latency for various instance types.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3EEzjo6bTnB_rZvS" /><figcaption>Figure 13. Latency using CPU vs. GPU with input message length increasing.</figcaption></figure><p>The results from our later online experiment (Figure 14) also show the improvement in latency from shifting to GPU inference on transformer models. With ~1.1B parameters and average input message length of 100 words, we were able to achieve ~60ms on p95, which is 3x faster on single transform and five times faster on batch transform.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hkQzw6r7Hk9K96_3" /><figcaption>Figure 14. Model latency in production before and after switching from CPU to GPU .</figcaption></figure><p>Switching to GPU not only improves the latency, it also allows us to run multiple model scoring in parallel. We leverage the PyTorch platform, which has built-in support for non-blocking model scoring, for better scalability.</p><h4>Contextual Bandit and Reinforcement Learning</h4><p>The second challenge in online serving is to adapt and optimize ML models based on new users’ online behavior. As we described in previous sections, the training data of the initial model is collected from historical user interaction over the product flow before the model is deployed. After the model is deployed, users interact with the system in a very different manner compared to the experience when the training data is collected. If the daily traffic is sufficiently large, we can always relabel the new data and update the model using the new data which reflects the updated user behavior, or directly perform multivariate testing on N policies. However, Airbnb’s CS chatbot traffic volume is relatively small compared to other ML systems such as search ranking. It will take a very long time to see the effect of any model change (either retrained model using new data or hyper parameter change).</p><p>To solve the challenge of low traffic volume, we use <strong>contextual bandit-based reinforcement learning</strong> (<a href="https://arxiv.org/abs/1802.04064">Bietti et al., 2019</a>; <a href="https://arxiv.org/abs/1606.03966">Agarwal et al., 2017</a>) to choose the best model and the most appropriate thresholds. Contextual Reinforcement Learning explores all the alternative problems by maximizing the rewards and minimizing the regrets. This allows us to learn from new behavior by dynamically balancing the exploration and exploitation.</p><p>We view this problem through three different actions in the product:</p><ul><li>a0: User is not directed through the mutual cancellation flow</li><li>a1: User is directed to the mutual cancellation UI for guests who have already agreed with the host on the refund</li><li>a2: User is directed to the mutual cancellation UI for cases where it was not clear if the host and guest have reached a mutual agreement</li></ul><p>Our reward function is <em>mutual cancellation flow entering rate</em> and <em>acceptance rate</em>. The reward at time step 𝑡 for any given action can be formulated as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*unZjK8QE_0njZb43-0yXMw.png" /></figure><p>where c denotes if a mutual cancellation flow is not entered/accepted.</p><p>We then leveraged greedy-epsilon as our first exploration strategy. If it’s in exploration mode, we compute the probabilities for each action based on policies’ preferences and select it based on the chances. If it’s in exploitation mode, we choose the best policy. We compute the models’ thresholds based on a set of logged (x, a, r, p) tuples. We use an self-normalized inverse propensity-scoring (IPS) estimator (<a href="https://papers.nips.cc/paper/2015/hash/39027dfad5138c9ca0c474d71db915c3-Abstract.html">Swaminathan and Joachims 2015</a>) to evaluate each policy:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/491/1*r7eHhtXDjK3Pbi3EfJY4nw.png" /></figure><p>In production, this approach successfully helped us explore many different models and parameter options and make the best use of the limited online traffic.</p><h3>Conclusion</h3><p>In this post, we introduced how we employ state-of-the-art machine learning and AI models to build support products that better serve the needs of our guests and hosts. We described how we leverage a single-choice Q&amp;A-based model, large-scale pretraining, multilingual models, multi-turn dialog state tracking, and GPU serving and successfully tackled the technical challenges.</p><p>Interested in tackling challenges in the machine learning and AI space?</p><p>We invite you to visit our <a href="https://careers.airbnb.com">careers page</a> or apply for these related opportunities:</p><p><a href="https://grnh.se/36f092141us"><strong>Staff Data Architect, Community Support Platform</strong></a></p><p><a href="https://grnh.se/ae09e08b1us"><strong>Staff Software Engineer — Machine Learning Modeling Platform</strong></a></p><p><a href="https://grnh.se/6648b3961us"><strong>Machine Learning Engineer, Search Ranking</strong></a></p><h3>Acknowledgements</h3><p>Thanks to Cassie Cao, Hao Wang, Bo Zeng, Ben Ma, Wayne Zhang, Mariel Young , Shahaf Abileah, Pratik Shah, Brian Wang, Hwanghah Jeong, Amy Guo, Vita Papernov, Courtney Nam, Aliza Hochsztein, Mike Hinckley, Yushuang Dong, Jan Castor, Ivy Cui, Lucia Ciccio for the great contributions to Mutual Cancellation workflow development, ERF analysis and product launches. Special thanks to Alex Deng for the help on contextual bandit and reinforcement learning work; many designs are originally Alex’s idea. We would also like to thank Atul Ktal, Bahador Nooraei, Shaowei Su, Alfredo Luque for the ML infrastructure support on GPU inference. In addition, we would like to thank the contributors of open source ML libraries such as PyTorch and HuggingFace Transformers, which benefited us a lot. Finally, we want to appreciate Ari Balogh, Tina Su, Andy Yasutake, and Joy Zhang’s leadership support in leveraging machine learning on Customer Support Platforms.</p><h3>References:</h3><ol><li>Zang X, Rastogi A, Sunkara S, Gupta R, Zhang J, Chen J (2020) MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines. CoRR abs/2007.12720</li><li>Jiang Y, Wu S, Gong J, Cheng Y, Meng P, Lin W, Chen Z, Li M (2020) Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning. CoRR abs/2011.03292</li><li>Khashabi D, Min S, Khot T, Sabharwal A, Tafjord O, Clark P, Hajishirzi H (2020) UnifiedQA: Crossing Format Boundaries With a Single QA System. In: Cohn T, He Y, Liu Y (eds) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16–20 November 2020. Association for Computational Linguistics, pp 1896–1907</li><li>Yu W, Jiang Z, Dong Y, Feng J (2020) ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26–30, 2020. OpenReview.net</li><li>Madotto A, Liu Z, Lin Z, Fung P (2020) Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems. CoRR abs/2008.06239</li><li>Xie Q, Dai Z, Hovy EH, Luong T, Le Q (2020) Unsupervised Data Augmentation for Consistency Training. In: Larochelle H, Ranzato M, Hadsell R, Balcan M-F, Lin H-T (eds) Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6–12, 2020, virtual</li><li>Bietti, Alberto, Alekh Agarwal, and John Langford. A Contextual Bandit Bake-off. Microsoft Research. 21 Mar. 2019</li><li>Agarwal, Alekh, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making Contextual Decisions with Low Technical Debt. ArXiv.org. 09 May 2017</li><li>Swaminathan A, Joachims T (2015) The Self-Normalized Estimator for Counterfactual Learning. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R (eds) Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7–12, 2015, Montreal, Quebec, Canada. pp 3231–3239</li><li>Gao S, Sethi A, Agarwal S, Chung T, Hakkani-Tür D (2019) Dialog State Tracking: A Neural Reading Comprehension Approach. In: Nakamura S, Gasic M, Zuckerman I, Skantze G, Nakano M, Papangelis A, Ultes S, Yoshino K (eds) Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, SIGdial 2019, Stockholm, Sweden, September 11–13, 2019. Association for Computational Linguistics, pp 264–273</li></ol><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5ebf49169eaa" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa">Task-Oriented Conversational AI in Airbnb Customer Support</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Built “Wall” to prevent data bugs]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ad1b081d6e8f</guid>
            <category><![CDATA[airflow]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-quality]]></category>
            <category><![CDATA[data-engineering]]></category>
            <dc:creator><![CDATA[Subrata Biswas]]></dc:creator>
            <pubDate>Wed, 04 Aug 2021 17:00:02 GMT</pubDate>
            <atom:updated>2021-08-04T17:00:01.993Z</atom:updated>
            <content:encoded><![CDATA[<p>Gaining trust in data with extensive data quality, accuracy and anomaly checks</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LkY1dXWTna9PbJNJ" /></figure><p>As shared in our Data Quality Initiative <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">post</a>, Airbnb has embarked on a project of massive scale to ensure trustworthy data across the company. To enable employees to make faster decisions with data and provide better support for business metric monitoring, we introduced <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469">Midas</a>, an analytical data certification process that certifies all important metrics and data sets. As part of that process, we made robust data quality checks and anomaly detection mandatory requirements to prevent data bugs propagating through the data warehouse. We also created guidelines on which specific data quality checks need to be implemented as part of the data model certification process. Adding data quality checks in the pipeline has become a standard practice in our data engineering workflow, and has helped us detect many critical data quality issues earlier in the pipelines.</p><p>In this blog post we will outline the challenges we faced while adding a massive number of data checks (i.e. data quality, accuracy, completeness and anomaly checks) to prevent data bugs company-wide, and how that motivated us to build a new framework to easily add data checks at scale.</p><h3>Challenges</h3><p>When we first introduced the <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469">Midas</a> analytical data certification process, we created recommendations on what kind of data quality checks need to be added, but we did not enforce how they were to be implemented. As a result, each data engineering team adopted their own approach, which presented the following challenges:</p><h4>1. Multiple approaches to add data checks</h4><p>In Airbnb’s analytical data ecosystem, we use Apache Airflow to schedule ETL jobs or data pipelines. Hive SQL, Spark SQL, Scala Spark, PySpark and Presto are widely used as different execution engines. However, because teams started building similar data quality checks in different execution engines, we encountered other inherent issues:</p><ul><li>We did not have any centralized way to view the data check coverage across teams.</li><li>A change in data check guidelines would require changes in multiple places in the codebase across the company.</li><li>Future-proof implementations were nearly impossible to scale. Teams kept re-inventing the wheel and duplicated code spread across the codebase.</li></ul><h4>2. Redundant efforts</h4><p>Different teams often needed to build tools to meet their own requirements for different data checks. Each Data Engineering (DE) team started to build data check tools in silos. Although each of these teams were building solid tools to meet their individual business needs, this approach was problematic for a few reasons:</p><ul><li>We started to build multiple frameworks in parallel.</li><li>Data check frameworks became costly to maintain and introduced operational overhead.</li><li>Missing features and lack of flexibility/extensibility made these frameworks difficult to reuse across the company.</li></ul><h4><strong>3. Complicated Airflow DAG code</strong></h4><p>Each check was added as a separate task in Airflow as part of the ETL pipeline. Airflow DAG files soon became massive. The operational overhead for these checks grew to the point that it became hard to maintain, because of a few different factors:</p><ul><li>There was no support for blocking vs non-blocking checks. Minor check failures or false alarms often blocked the SLA of critical data pipelines.</li><li>ETL logic and data checks became tightly coupled and not reusable.</li><li>Maintenance became operationally challenging, as we tracked the dependencies manually, which also made it difficult to add more checks.</li></ul><h3>Defining the Requirements</h3><p>To address these tooling gaps, we set out to build a unified data check framework that would meet the following requirements and ensure greater usability overtime:</p><ul><li>Extensible : Unify data check methodologies in use at Airbnb</li><li>Configuration-driven: Define the checks as YAML-formatted files for faster development</li><li>Easy to use: Provide a simplified interface to promote faster adoption company wide</li></ul><h3>Introducing Wall Framework</h3><p>Wall is the paved path for writing offline data quality checks. It is a framework designed to protect our analytical decisions from bad data bugs and ensure trustworthy data across Airbnb.</p><p>Wall Framework is written in Python on top of Apache Airflow. Users can add data quality checks to their Airflow DAGs by writing a simple config file and calling a helper function in their DAG.</p><ul><li>Wall provides most of the quality checks and anomaly detection mechanisms currently available in the company under a common framework, making data checks a lot easier to standardize.</li><li>It supports templated custom SQL-based business logic, accuracy checks, and an extensible library of predefined checks.</li><li>Wall is config driven — no code is required to add checks.</li><li>Checks can be used in the ETL pipeline in a <a href="https://airflow.apache.org/docs/apache-airflow/1.10.2/concepts.html?highlight=branch%20operator#subdags">Stage-Check-Exchange</a> pattern or as standalone checks.</li><li>The framework is extensible — any team can add their team-specific checks to Wall quite easily following the open source model (as per the Data Engineering Paved Path team’s approval).</li><li>Business users can easily add quality checks without creating any airflow DAG or tasks for each check.</li><li>Wall takes care of SQL-based checks and anomaly detection task creations. It also takes care of stage and exchange task creations and setting the appropriate dependency on the checks in a decoupled manner. Hence, after migrating to Wall, ETL pipelines were drastically simplified and we’ve seen cases where we were able to get rid of more than 70% of DAG code.</li></ul><h3>Wall Architecture</h3><p>Following our key requirements, this framework was designed to be extensible. It has three major components — WallApiManager, WallConfigManger and WallConfigModel..</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WkknwY6urhgMsEik" /><figcaption>Wall internal architecture</figcaption></figure><h4>WallApiManager</h4><p>The Wall Api Manager is the public interface to orchestrate checks and exchanges using Wall. Wall users only use this from their DAG files. It takes a config folder path as input and supports a wide variety of ETL operations such as Spark, Hive etc.</p><h4>WallConfigManager</h4><p>The Wall Config Manager parses and validates the check config files and then calls the relevant CheckConfigModels to generate a list of Airflow tasks. Wall primarily uses Presto checks to generate data checks.</p><h4>CheckConfigModel</h4><p>Each Wall check is a separate class that derives from BaseCheckConfigModel. CheckConfigModel classes are primarily responsible for validating check parameters and generating Airflow tasks for the check. CheckConfigModel makes the framework extensible. Different teams can add their own CheckConfigModel if existing models do not support their use cases.</p><h3>Key Features</h3><p>Wall framework provided the following key features to address the requirements we mentioned above.</p><h4>Flexibility</h4><ul><li>Wall configs can be located in the same repository where teams are already defining their data pipeline DAGs — teams or DAG owners can decide where they’re located. Teams can either use a separate YAML file for each table or a single YAML file for a group of tables to define checks.</li><li>Each check config model can define an arbitrary set of parameters and it can override parameters if needed. The same check configs can be orchestrated and run differently based on running context. i.e. as part of ETL’s stage-check-exchange or as pre/post checks.</li><li>A check property can be hierarchical (i.e. it can be defined at team level, file level, table level or at check level). Lower level property values override upper level values. Teams can define their team level defaults in a shared YAML file instead of duplicating the same configurations and checks in different YAML files.</li><li>In the case of stage-check-exchange checks, users can specify blocking and non-blocking checks. It makes Wall more flexible while onboarding new checks.</li></ul><h4>Extensibility</h4><ul><li>It’s easy to onboard a new type of check model. Wall is able to support commonly used data checks/validations mechanisms.</li><li>Each check config model is decoupled from each other and it can define its own set of params, validations, check generation logic, pre-processing etc.</li><li>Check config models can be developed by the data engineering community with the collaboration of the Data Engineering Paved Path team.</li></ul><h4>Simplicity</h4><ul><li>Easy to copy-paste to apply similar checks in different tables or contexts.</li><li>Check models are intuitive.</li><li>Checks are decoupled from DAG definition and ETL pipeline so that they can be updated without updating ETL.</li><li>Easy to test all the checks at once.</li></ul><h3>Adding a Wall check</h3><p>At the every high level, users need to write a yaml config and invoke Wall’s API from their DAG to orchestrate their ETL pipeline with data checks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R6UrxoyuLlaZ4dsI" /><figcaption>High level diagram of how users interact with Wall.</figcaption></figure><p>As an example of how easy it is to add a new data quality check, let’s assume you’d like to add a data quality check — verifying that a partition is not empty — to a table named foo.foo_bar in the wall_tutorials_00 DAG. It can be done by following these two steps:</p><ol><li>Decide on a folder to add your wall checks configs i.e. projects/tutorials/dags/wall_tutorials_00/wall_checks. Create a check config file (i.e. foo.foo_bar.yml ) with the following contents in your wall check config folder:</li></ol><pre>primary_table: foo.foo_bar<br>emails: [&#39;<a href="mailto:subrata.biswas@airbnb.com">subrata.biswas@airbnb.com</a>&#39;]<br>slack: [&#39;#subu-test&#39;]<br>quality_checks:<br>   - check_model: CheckEmptyTablePartition<br>     name: EmptyPartitionCheck</pre><p>Update the DAG file (i.e. wall_tutorials_00.py) to create checks based on the config file.</p><pre>from datetime import datetime</pre><pre>from airflow.models import DAG</pre><pre>from teams.wall_framework.lib.wall_api_manager.wall_api_manager import WallApiManager</pre><pre>args = {</pre><pre>&quot;depends_on_past&quot;: True,</pre><pre>&quot;wait_for_downstream&quot;: False,</pre><pre>&quot;start_date&quot;: datetime(2020, 4, 24),</pre><pre>&quot;email&quot;: [&quot;subrata.biswas@airbnb.com&quot;,],</pre><pre>&quot;adhoc&quot;: True,</pre><pre>&quot;email_on_failure&quot;: True,</pre><pre>&quot;email_on_retry&quot;: False,</pre><pre>&quot;retries&quot;: 2,</pre><pre>}</pre><pre>dag = DAG(&quot;wall_tutorials_00&quot;, default_args=args)</pre><pre>wall_api_manager = WallApiManager(config_path=&quot;projects/tutorials/dags/wall_tutorials_00/wall_checks&quot;)</pre><pre># Invoke Wall API to create a check for the table.</pre><pre>wall_api_manager.create_checks_for_table(full_table_name=&quot;foo.foo_bar&quot;, task_id=&quot;my_wall_task&quot;, dag=dag)</pre><p><strong>Validate and Test</strong></p><p>Now if you check the list of tasks of the wall_tutorials_00 you’ll see the following tasks created by the Wall Framework:</p><pre>&lt;Task(NamedHivePartitionSensor): ps_foo.foo_bar___gen&gt;</pre><pre>   &lt;Task(SubDagOperator): my_wall_task&gt;</pre><p>Wall created a SubDagOperator task and a NamedHivePartitionSensor task for the table in the primary DAG (i.e. wall_tutorials_00). Wall encapsulated all the checks inside the sub-dag. To get the check tasks list you would need to look at the sub-dag tasks i.e. run list_tasks for the wall_tutorials_00.my_wall_task dag. It returns the following list of tasks for this case:</p><pre>&lt;Task(WallPrestoCheckOperator): EmptyPartitionCheck_foo.foo_bar&gt;</pre><pre>   &lt;Task(DummyOperator): group_non_blocking_checks&gt;</pre><pre>      &lt;Task(DummyOperator): foo.foo_bar_exchange&gt;</pre><pre>&lt;Task(DummyOperator): group_blocking_checks&gt;</pre><pre>   &lt;Task(DummyOperator): foo.foo_bar_exchange&gt;</pre><pre>&lt;Task(PythonOperator): validate_dependencies&gt;</pre><p>Note: You probably noticed that Wall created a few DummyOperator tasks and a PythonOperator task in the sub-DAG. It was required to maintain control flows i.e. blocking vs non-blocking checks, dependencies, validation etc. You can ignore those tasks and don’t need to take dependencies on these tasks since they may change or can be deleted in future.</p><p>Now you can test your check tasks just like any airflow tasks i.e.</p><pre>airflow test wall_tutorials_00.my_wall_task EmptyPartitionCheck_foo.foo_bar {ds}</pre><h3>Wall in Airbnb’s Data Ecosystem.</h3><p>Integrating Wall with other tools in Airbnb’s data ecosystem was critical for its long term success. To allow other tools to integrate easily, we publish results from the “check” stage as Kafka events, to which other tools can subscribe. The following diagram shows how other tools are integrated with Wall:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zPIHSSuv-7zdHmzJ" /><figcaption>Wall in Airbnb’s data ecosystem</figcaption></figure><h3>Conclusion</h3><p>Wall ensures a high standard for data quality at Airbnb and that the standard does not deteriorate over time.</p><p>Through enabling standardized but extensible data checks that can be easily propagated across our distributed data engineering organization, we continue to ensure trustworthy, reliable data across the company. As a result all of Airbnb’s critical business and financial data pipelines are using Wall and we have hundreds of data pipelines running thousands of Wall checks every day.</p><p>If this type of work interests you, check out some of our related positions:</p><p><a href="https://grnh.se/706bf4e01us">Senior Data Engineer</a></p><p><a href="https://grnh.se/a207325d1us">Staff Data Scientist- Algorithms, Payments</a></p><p>and more at <a href="https://careers.airbnb.com/">Careers at Airbnb</a>!</p><p>You can also learn more about our <em>Journey Toward High Quality</em> by watching our recent <a href="https://fb.watch/72Oumx3pJJ/">Airbnb Tech Talk</a>.</p><p>With special thanks to <a href="https://www.linkedin.com/in/nikuma/">Nitin Kumar</a>, <a href="https://www.linkedin.com/in/bharatrangan/">Bharat Rangan</a>, <a href="https://www.linkedin.com/in/kenneth-jung-71495256/">Ken Jung</a>, <a href="https://www.linkedin.com/in/victor-ionescu-3a029b5a/">Victor Ionescu</a>, <a href="https://www.linkedin.com/in/siyu-qiu-05b482a1/">Siyu Qiu</a> for being key partners while evangelizing this framework.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ad1b081d6e8f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f">How Airbnb Built “Wall” to prevent data bugs</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Sentiment Score to Assess Customer Service Quality]]></title>
            <link>https://medium.com/airbnb-engineering/using-sentiment-score-to-assess-customer-service-quality-43434dbe199b?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/43434dbe199b</guid>
            <category><![CDATA[ai-model]]></category>
            <category><![CDATA[a-b-testing]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[machine-learning-ai]]></category>
            <category><![CDATA[customer-experience]]></category>
            <dc:creator><![CDATA[Shuai Shao (Shawn)]]></dc:creator>
            <pubDate>Tue, 27 Jul 2021 15:48:00 GMT</pubDate>
            <atom:updated>2021-07-27T21:50:15.137Z</atom:updated>
            <content:encoded><![CDATA[<p>How AI-based Sentiment Models Complement Net Promoter Score</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oVSUzHkUykPwADBZ" /></figure><p>By <a href="https://medium.com/@shuai_shao">Shuai Shao</a>, <a href="https://medium.com/@cenzhao06">Mia Zhao</a>, <a href="https://medium.com/@chloe.yuanyuan.ni">Yuanyuan Ni</a></p><p>Net Promoter Score (NPS) is a well-accepted measurement of customer satisfaction in most customer-facing industries. We leverage NPS at Airbnb to help measure how well we serve our community of guests and hosts through our customer service. But NPS has two major drawbacks: 1) NPS is <em>sparse</em>, given only a fraction of users respond to the survey, and 2) NPS is <em>slow</em>. It takes at least a week for results to show up. Airbnb uses A/B testing heavily across our core products and customer service offerings. In the A/B testing world, the longer it takes to see results and interpret experiments, the longer it takes to iterate on the quality of our customer service. This is why we needed a much more <em>sensitive</em> and <em>robust</em> metric.</p><p>To address these limitations, Airbnb has developed an AI-based sentiment model to complement NPS. Sentiment models process messages users send to customer support (CS) representatives to extract signals reflecting users’ sentiment. Compared to NPS, the sentiment score has the following advantages:</p><ul><li>Higher <em>coverage</em>: we are not limited to those who submit a survey, and therefore more users in a given experiment register a value for this metric;</li><li>Better <em>sensitivity</em>: it takes much less time to reach statistical significance while running an experiment;</li><li><em>Causal relationship</em> with long term customer loyalty: we can ‘translate’ user sentiment scores into long term business values.</li></ul><p>This blog post provides insights on how we developed the sentiment model and the metric which aggregates the raw sentiment scores to measure customer sentiment. We leveraged Entropy Balancing (<a href="https://web.stanford.edu/~jhain/Paper/eb.pdf">Hainmueller, 2012</a>) to create a counterfactual group, in order to detect the relationship between the sentiment metric and future revenue. From our study, we show great results of sentiment metric compared to NPS.</p><h3>Sentiment Model Development</h3><p>Sentiment analysis is a great method to gauge consumers’ feeling of a particular product or service. In Airbnb’s customer support, sentiments from our guests and hosts are important signals for us to build better products and services, and ship changes with our community in mind. .</p><p>There are two main challenges we face when developing sentiment models in the customer support domain.</p><ul><li><strong>Skewed</strong><em> </em><strong>Data</strong><em>:</em> Most text inputs are negative in sentiment. Unlike when leaving reviews or messaging with hosts, guests typically contact customer support when they are experiencing an issue with Airbnb.</li><li><strong>Multilingual Input:</strong> More than 14 languages are supported by Airbnb’s customer service. Hosts and guests might be communicating in different languages in the same support ticket.</li></ul><p>To make a sentiment model tailored to our use case, we developed <strong>customized rating guidelines</strong> for customer support messages to make our model aware of domain-specific knowledge and contextual information. Examples below illustrate how the same messages are labelled differently when presented as a CS message versus a Social Media post or App Store review. In the CS domain, we focus on how well customers “think” the issue gets solved as a <em>positive</em> indication and how frustrating they “feel” the issue is as a <em>negative</em> indication.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WxB1wIGnyMsmpRnn" /></figure><p>We address data skewness via multiple iterations of sampling data for human annotations using ML model and retraining model using newly labelled data. The first round of annotation is performed based on random sampling, while subsequent annotation datasets are stratified on existing model predictions. This leads to a more balanced dataset for training.</p><p>We built and tested two deep learning architectures, both support multilingual inferences:</p><ul><li><a href="https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c">WIDeText</a> uses a CNN-based architecture to process text channels, while all categorical features are processed through the WIDe channel.</li><li><a href="https://arxiv.org/pdf/1911.02116.pdf">XLM-Roberta</a> uses a transformer-based architecture and leverages a pre-trained multilingual model to have CS messages trained in 14 languages. .</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*otPxo8B6qRoHOtv9" /><figcaption>WIDeText Architecture</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/366/0*95onJYQ62BgRs-gI" /><figcaption>Transformer Architecture</figcaption></figure><p>Transformer-based models achieve slightly better performance on English sentiment analysis and much better performance on less frequently used languages. We chose transformer-based classifier for production inference pipeline.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ld-Sd5q4Iznj9hqT-sVqmQ.png" /></figure><h3>Sentiment Metrics Development</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/946/0*r2iNOyss8cXLq35v" /></figure><p>From the raw sentiment scores, we developed the sentiment metric aiming to optimize the following criteria:</p><ul><li>Strong correlation with NPS</li><li>Sensitivity in experimentation</li><li>Demonstrable causal relationship with long-term business gains</li></ul><h3>Correlation with NPS</h3><p>Despite the limitations of NPS, it is still considered to be the gold-standard of users’ sentiment. It is desirable to make the sentiment metric, now more sensitive and robust, correlates well with NPS. We tested various ways to design the metric by aggregating the message-level raw sentiment scores (e.g., mean, cutoff, slope) to correlate with NPS.</p><p>The two charts below illustrate that sentiment scores and NPS correlate well for guest and host sentiment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/863/0*ULuPwKX14HSpn6F8" /></figure><p>NPS (green) vs Sentiment Metrics (orange) on guest sample</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/856/0*ECJNOsyJnhRMin8O" /></figure><p>NPS (green) vs Sentiment Metrics (orange) on host sample</p><h3>Sensitivity in Experimentation</h3><p>We revisited two types of past experiments (Scenario 1 and 2) to compare the sensitivity in experimentation between NPS and sentiment metric. The goal was to determine if sentiment metric can provide quicker or more accurate feedback in response to a shift in user sentiment.</p><h4>Scenario 1</h4><p>In the first type of experiments, a new product/service feature hurt the user experience from user research (e.g., the service required extra steps to contact a support agent), yet these features did not show any statistically significant changes in NPS.</p><p>For example, in one of our Interactive Voice Response (IVR) experiments, we successfully reduced contact rate by adding more questions to our automated phone messaging system. However, this also increased friction for users trying to reach customer support. At the end of the experiment, NPS trended negative but was not statistically significant after running for 30 days.</p><p>When we applied sentiment metrics to this experiment, we were able to detect that the change in new sentiment metrics reached statistical significance within 5 days.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MrcorRynIU3biIsnnkLHCg.png" /></figure><h4>Scenario 2</h4><p>The second type of experiments have features in product/service that hurt the user experience and did impact NPS in a statistically significant way. For example, one of our <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">chatbot </a>experiment decreased both NPS and sentiment metrics but NPS reached statistical significance at day 10, while sentiment metric converged much faster, detecting a shift by day 5.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X8WX3nicGLPVcCtsQX1Gtg.png" /></figure><h3>Relationship with Long-term Customer Loyalty</h3><p>As a low-frequency marketplace, one of the challenges in Airbnb’s experimentation framework is the difficulty of evaluating long-term customer loyalty such as user churn rate and future booking revenue in product iterations. For customer support teams, our products have an especially large impact on users’ experience. The experimentation should help the decision makers to answer the question “Should we launch a product/service feature if it reduces cost but hurts users’ satisfaction levels?”</p><p>Our third assessment quantifies the future booking impact of customer service using the sentiment score metric.</p><p>It would be very expensive, if possible at all, to run A/B tests with two distinct pools of agents who provide different standards of service to different groups of users. Instead, we use a novel causal inference technique to detect sentiment effects on a user’s future one-year booking revenue with observational data.</p><p>We divide the users into two groups: a <em>control</em> group, with comparatively lower sentiment scores, and a <em>treatment</em> group, with higher, more positive sentiment. We need to control for the fact that these two groups may be fundamentally different from each other in many ways, such as their tolerance to different levels of service quality, loyalty to our platform, and historical booking experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/0*Yrn1pWAswJu-JX3t" /></figure><p>Analysis workflow of establishing relationship between sentiment score and future revenue</p><p>In order to evaluate more reliable long-term effects of providing good customer service, we established a procedure to: 1) find confounding factors, 2) control these covariates using entropy balancing, and 3) evaluate treatment effects using weighted data.</p><h4>Confounding Variable Selection</h4><p>It took several rounds of iteration before we were able to narrow down the appropriate confounding variables and generate the covariate matrix. We listed all possible confounding variables that should be taken into account. This covered multiple disciplines including user account information, previous booking behaviors, customer contact habits, etc. We then selected related variables that correlated with both sentiment and future booking. For example, users with more previous bookings tend to book more and are more positive when communicating with customer support agents. Finally,<strong> </strong>we cross checked correlations among all variables to remove redundant ones. This helped us to select a short list of confounding variables.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6KfIB1PS0QaKby0K06fpsA.png" /></figure><h4>Entropy Balancing</h4><p>We use <a href="https://web.stanford.edu/~jhain/Paper/eb.pdf">Entropy Balancing</a> to achieve covariate balance. Entropy Balancing is a maximum entropy reweighting scheme to create balanced samples that satisfy a set of constraints. Here are two most important features in the scheme:</p><p><strong>1. Equalized moments of the covariate distributions. </strong>By assigning weight wi to each sample unit, we want the moments of the covariate distribution (e.g., mean, variance, and skewness) between the treatment and the reweighted control group to be equal (defined in equation 2). A typical balance constraint is formulated with mr containing the rthorder moment of a given variable Xj from the treatment group, whereas the moment functions are specified for the control group as cri(Xij)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aA3mjawjoqS17c6s6iZ7SA.png" /></figure><p><strong>2. Minimized distance from base weights</strong>. We also want to minimize the. distance between estimated weights wiand base weight qi(usually set as 1/n0 , uniformly distributed) to retain information as much as possible (defined in equation 3).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GuVv6BTriCaFHg5jEJpQWg.png" /></figure><p>Compared to more frequently used Propensity Score Matching, Entropy Balancing has several proven advantages:</p><ul><li><strong>It is good at balancing results even to high degrees of moments</strong>. In contrast to most other preprocessing methods that involve multiple rounds of manual adjustments on both model and matching until reaching balanced results (which often fails on high dimensional samples), entropy balancing directly searches for weights that can achieve exact covariate balance in finite samples. It significantly improves the balance that can be obtained by other methods, which are validated by an insurance use case <a href="https://www.thieme-connect.de/products/ejournals/abstract/10.1055/a-1009-6634">Matschinger (2019)</a>.</li><li><strong>It retains valuable information without discarding units</strong>. Entropy Balancing retains valuable information by allowing the unit weights to vary smoothly across units, so that we don’t have to throw away any unmatched data.</li><li><strong>It is versatile.</strong> The weights we get can be used to almost any standard estimation of treatment effects such as weighted mean and weighted regression.</li><li><strong>It is computationally inexpensive</strong>. It only takes a couple seconds to get balanced results for over 1M records.</li></ul><p>Evaluating Treatment Effects</p><p>We were able to reach balanced results for all confounding variables after using entropy reweighting:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/374/0*1JfKfSricjCrVLaf" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/378/0*rUOEG3pjW3Qtsbja" /></figure><p>With the weighted results, we found that <strong>guests on Airbnb with higher sentiment (potentially good CS experiences with sentiment metrics &gt;= 0.1) produce significantly more revenue in the subsequent 12 months</strong>. This result can be applied to trade-off analysis whenever we see an opposite result in cost and user CS sentiment score and help us make the right launch decision taking long-term revenue into consideration.</p><h3>Takeaways</h3><p>In this blog post, we provided details of sentiment model development and the framework of assessing sentiment metrics.</p><p>For ML practitioners, the success of a sentiment analysis depends on domain-specific data and annotation guidelines. Our experiments show transformer-based classifiers perform better than CNN-based architectures, especially in less frequently used languages.</p><p>For customer service providers who struggled with the pain of NPS, sentiment analysis has provided promising results to describe customers’ satisfaction levels. If you have user communication text, exploring sentiment analysis may solve the long lasting pain of NPS. However, if you only have phone call recordings, exploring audio to text transcription could be a good start before exploring emotion detection in audio.</p><p>For data analysts and data scientists, the framework of metrics development from a new signal (model output) is reusable: considering many user feedback metrics are either slow or sparse, data professionals can assess the new signals from coverage, sensitivity, and causal relationship with business values. For causal analysis challenges, it is worth spending some time to explore the new Entropy Balancing techniques, which may save you time from Propensity Score Matching.</p><p>If this type of work interests you, check out some of our related positions:</p><p><a href="https://grnh.se/049bfea61us">Senior Data Scientist — Analytics, Support Products</a></p><p><a href="https://grnh.se/36f092141us">Staff Data Architect, Community Support Platform</a></p><p>and more at <a href="https://careers.airbnb.com/">Careers at Airbnb</a>!</p><h3>Acknowledgements</h3><p>Thanks to Zhiying Gu and Lo-hua Yuan for providing important knowledge support on causal inference. Thanks to Mitral Akhtari and Jenny Chen for knowledge sharing on <a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">Airbnb’s Future Incremental Value system</a>. We would also like to thank Bo Zeng for sentiment modeling guidance, Mariel Young for the metrics iteration, and Aashima Paul, Evan Lin, and Keke Hu for their hard work on labelling the sentiment data. Last but not least, we appreciate Joy Zhang, Nathan Triplett, and Shijing Yao for their guidance.</p><h3>References:</h3><ol><li>Jens Hainmueller (2012) Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies, <em>Political Analysis</em>, 20:25−46 doi:10.1093/pan/mpr025</li><li>Herbert Matschinger, Dirk Heider, Hans-Helmut König (2020) A Comparison of Matching and Weighting Methods for Causal Inference Based on Routine Health Insurance Data, or: What to do If an RCT is Impossible,<em>Gesundheitswesen, </em>82(S 02): S139-S150 DOI: 10.1055/a-1009–6634</li></ol><p>Further Reading</p><p><a href="https://docs.google.com/document/d/1irNF89bKsTGgjNCmPoJXBVEaNPv00Fl3skwCAjBHK8M/edit">WIDeText: Multimodal Deep Learning Framework, and its application on Room Type Classification</a> goes into the details of Deep learning framework used in Airbnb</p><p><a href="https://ieeexplore.ieee.org/document/8964147">Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform</a> goes into the details of the Airbnb Machine Learning Infrastructure. <em>DSAA</em>’2019</p><p><a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">How Airbnb Measures Future Value to Standardize Tradeoffs</a> goes into details of how Airbnb optimizes for long-term decision-making through the propensity score matching model</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=43434dbe199b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/using-sentiment-score-to-assess-customer-service-quality-43434dbe199b">Using Sentiment Score to Assess Customer Service Quality</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Measures Future Value to Standardize Tradeoffs]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/3aa99a941ba5</guid>
            <category><![CDATA[data-platforms]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Jenny Chen]]></dc:creator>
            <pubDate>Tue, 13 Jul 2021 17:00:41 GMT</pubDate>
            <atom:updated>2021-08-27T13:55:53.714Z</atom:updated>
            <content:encoded><![CDATA[<h4>The propensity score matching model powering how we optimize for long-term decision-making</h4><p>By <a href="https://www.linkedin.com/in/mitra-akhtari/">Mitra Akhtari</a>, <a href="https://www.linkedin.com/in/jennychen96/">Jenny Chen</a>, <a href="https://www.linkedin.com/in/amelialemionet">Amelia Lemionet</a>, <a href="https://www.linkedin.com/in/dan-nguyen-b8817a34/">Dan Nguyen</a>, <a href="https://www.linkedin.com/in/hassan-obeid/">Hassan Obeid</a>, <a href="https://www.linkedin.com/in/yunshanz/">Yunshan Zhu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u3P2iMzEh4mLglRq" /></figure><p>At Airbnb, we have a <a href="https://news.airbnb.com/brian-cheskys-open-letter-to-the-airbnb-community-about-building-a-21st-century-company/">vision</a> to build a 21st century company by operating over an infinite time horizon and balancing the interests of all stakeholders. To do so effectively, we need to be able to compare, in a common currency, both the short and long-term value of actions and events that take place on our platform. These actions could be a guest making a booking or a host adding amenities to their listing, to name just two examples.</p><p>Though randomized experiments measure the initial impact of some of these actions, others, such as cancellations, are difficult to evaluate using experiments due to ethical, legal, or user experience concerns. Metrics in experiments can be hard to interpret as well, especially if the experiment affects opposing metrics (e.g., bookings increase but so do cancellations). Additionally, regardless of our ability to assess causal impact with A/B testing, <a href="https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7">experiments are often run only for a short period of time</a> and do not allow us to quantify impact over an extended period.</p><p>So what did we build to solve this problem?</p><h3><strong>Introducing Future Incremental Value (FIV)</strong></h3><p>We are interested in the long-term causal effect or “future incremental value” (FIV) of an action or event that occurs on Airbnb. We define “long-term” as 1 year, though our framework can adjust the time period to be as short as 30 days or as long as 2 years.</p><p>To use a concrete example, assume we would like to estimate the long-term impact of a guest making a booking. Denote the <em>n1</em> number of users who make a booking within a month as <em>i ∈a1</em>and the <em>n0</em> number of users who do not make a booking in that time period as <em>i∈a0</em>. In the following year, each of these users generates revenue (or any other outcome of interest) denoted by <em>y</em>. The naive approach to computing the impact of making a booking would be to simply look at the average differences between users who made a booking versus those that did not:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Vxqdv4sg3gdaHP5p" /></figure><p>However, these two groups of users are very different: those who made a booking “selected” into doing so. This selection bias obscures the true causal effect of the action, <em>FIV(a)</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/826/0*EiXkA-AcIEMhDMB2" /></figure><p>Our goal is to exclude the bias from the naive estimate to identify <em>FIV(a)</em>.</p><h3><strong>The Science Behind FIV</strong></h3><p>To minimize selection bias in estimating the FIV of an action, we need to compare observations from users or listings that are similar in every way except for whether or not they took or experienced an action. The well-documented, quasi-experimental methodology we have chosen for this problem is <a href="https://academic.oup.com/biomet/article/70/1/41/240879?login=true">propensity score matching</a> (PSM). We start by separating users or listings into two groups: observations from those that took the action (“focal”) during a given timeframe and observations from those that did not (“complement”). Using PSM, we construct a “counterfactual” group, a subset of the complement that matches the characteristics of the focal as much as possible, except that these users or listings did not take the action. The assumption is that “assignment” into focal versus counterfactual is as good as random.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F43MRwKtSDShhbRi" /><figcaption><em>Figure 1. Overview of methodology behind FIV</em></figcaption></figure><p>The specific steps we take for eliminating bias from the naive method are:</p><ol><li><em>Generate the Propensity Score:</em> Using a set of pre-treatment or control features describing attributes of the user or listing (e.g., number of past searches), we build a binary, tree-based classifier to predict the probability that the user or listing took the action. The output here is a propensity score for each observation.</li><li><em>Trim for Common Support: </em>We remove from the dataset any observations that have no “matching twin” in terms of propensity score. After splitting the distribution of propensity scores into buckets, we discard observations in buckets where either the focal or complement have little representation.</li><li><em>Match Similar Observations: </em>To create the counterfactual, we use the propensity score to match each observation in the focal to a counterpart in the complement. Various <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">matching strategies</a> can be used, such as matching in bins or via nearest neighbors.</li><li><em>Results: </em>To get the FIV, we compute the average of the outcome or target feature in the focal minus the average in the counterfactual.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/952/0*P3Og2RJ1mLeasghj" /></figure><h4>Evaluation</h4><p>In a supervised machine learning problem, as more data becomes available and future outcomes are actualized, the model is either validated or revised. This is not the case for FIV. The steps above give us an estimate of the incremental impact of an action, but the “true” incremental impact is never revealed. In this world, how do we evaluate the success of our model?</p><p><em>Common Support: </em>One of the assumptions of using PSM for causal inference is “common support”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*POao7OeMFaC3Sv5V" /></figure><p>where <em>D = 1 </em>denotes observations in the focal group and <em>X</em> are the controlling features. This assumption rules out the possibility of “perfect predictability” to guarantee that observations with the same <em>X</em> values have a positive probability of belonging to both groups and thus can be matched together to provide valid comparisons. Plotting the distribution of propensity scores for the focal and the complement group allows for a visual inspection of this assumption. Interestingly, in the case of causal inference with PSM, a high Area Under the Curve (AUC), a desirable feature for most prediction models, means that the model is able to distinguish between focal and complement observations too well, reducing our matching quality. In such cases, we assess whether those control features are confounders that affect the output metrics and eliminate them.</p><p><em>Matching Evaluation:</em> Observations are considered “similar” if the distributions of key features in the focal closely match the distributions of those in the counterfactual. But how close is close enough? To quantify this, we compute three metrics to assess the quality of the matching, as described in <a href="http://sekhon.berkeley.edu/papers/other/Rubin2001.pdf">Rubin (2001)</a>. These metrics identify whether the propensity score and key control features have similar distributions in the focal and counterfactual groups. Additionally, we are currently investigating whether to apply an additional regression adjustment to correct for any remaining imbalance in the key control features. For instance, after the matching stage, we could run a regression that directly controls for key features that we want an almost exact match for.</p><p><em>Past experiments:</em> Company-wide, we run experiments to test various hypotheses on how to improve the user experience, potentially leading to positive outcomes such as a significant increase in bookings. These experiments generate a source of variation in the likelihood of guests making a booking that does not suffer from selection bias, due to the randomization of treatment assignment in the experiment. By tracking and comparing the users in the control group to users in the treatment groups of these experiments, we observe the “long-term impact of making a booking”, which we can compare to our FIV estimate for “guest booking”. While the FIV estimate is a global average and experiments often estimate <a href="https://www.aeaweb.org/articles?id=10.1257/000282806776157641">local average treatment effects</a>, we can still use experimental benchmarks as an important gut check.</p><h3><strong>Adapting FIV for Airbnb</strong></h3><p>While PSM is a well-established method for causal inference, we must also address several additional challenges, including the fact that Airbnb operates in a two-sided marketplace. Accordingly, the FIV platform must support computation from both the guest and the listing perspective. Guest FIV estimates the impact of actions based on activity a guest generates on Airbnb after experiencing an action, while listing FIV is from the lens of a listing. We are still in the process of developing a “host-level” FIV. One challenge in doing so will be sample size: we have fewer unique hosts than listings.</p><p>To arrive at a “platform” or total FIV for an action, we cannot simply add guest and listing FIVs together because of double counting. We simplify the problem and only count the value from the guest-side or the listing-side depending on which mechanisms we believe drive the majority of the long-term impact.</p><p>Another feature of our two-sided market is cannibalization, especially on the supply-side: if a listing gets more bookings, some portion of this increase is taking away bookings from similar listings. In order to arrive at the true “incremental” value of an action, we apply cannibalization haircuts to listing FIV estimates based on our understanding of the magnitude of this cannibalization from experimental data.</p><h3><strong>The Platform Powering FIV</strong></h3><p>FIV is a data product and its clients are other teams within Airbnb. We provide an easy to use platform to organize, compute, and analyze actions and FIVs at scale. As part of this, we have built components that take in input from the client, construct and store necessary data, productionize the PSM model, compute FIVs, and output the results. The machinery, orchestrated through <a href="https://airflow.apache.org/docs/apache-airflow/2.0.1/">Airflow</a> and invisible to the client, looks as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IqZbWBgn0wJFgXGd" /><figcaption><em>Figure 2. Overview of FIV Platform</em></figcaption></figure><h4>Client Input</h4><p>Use cases begin with a conversation with the client team to understand the business context and technical components of their desired estimate. An integral part of producing valid and useful FIV estimates is establishing well-defined focal and complement groups. Additionally, there are cases when the FIV tools are not applicable, such as when there is limited observational data (e.g., a new feature) or small group sizes (e.g., a specific funnel or lever).</p><p>The client submits a configuration file defining their focal and complement groups, which is essentially the only task the client does in order to use the FIV platform. Below is the config for the FIV of “guest booking”: a visitor who booked a home on our site (focal) versus one who did not book a home (complement).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/585c5f1c3b0f0789f6369b41006ad5c1/href">https://medium.com/media/585c5f1c3b0f0789f6369b41006ad5c1/href</a></iframe><p>The <em>cohort</em> identifies the maximum set of users to consider (in this case, all visitors to Airbnb’s platform), some of which are removed from consideration by the <em>filter_query</em> (in this case, users who also booked an Airbnb experience are removed). From the remaining set of users, the <em>action_event_query</em> allocates users to the focal with leftovers automatically assigned to the complement.</p><p>After the client’s config is reviewed, it is merged into the FIV repository and automatically ingested into our pipelines. We assign a version to each unique config to allow for iteration while storing historical results.</p><p>We have designed the platform to be as easy to use as possible. No special knowledge of modeling, inference, or complex coding is needed. The client only needs to provide a set of queries to define their groups and we take care of the rest!</p><h4>Data Pipeline</h4><p>The config triggers a pipeline to construct the focal and complement, join them with control and target features, and store this in the Data Warehouse. Control features will later serve as inputs into the propensity score model, whereas target features will be the outcomes that FIV is computed over. Target features are what allow us to convert actions from different contexts and parts of Airbnb into a “common currency”. This is one of FIV’s superpowers!</p><p>Leveraging <a href="https://databricks.com/session/zipline-airbnbs-machine-learning-data-management-platform">Zipline</a> as our feature management system, we currently have approximately 1,000 control features across both guests and listings, such as region, cancellations, or past searches. Though we have the capability to compute FIV in terms of numerous target features, we have a few target features that give us a standardized output, such as revenue, cost, and bookings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*keT6ycZkmSs4an8L" /><figcaption><em>Figure 4. Steps to compute the raw data needed for FIV, after taking in client input</em></figcaption></figure><p>The version of the config is also used here to automate backfills, significantly decreasing manual errors and intervention. There are multiple checks on the versioning to ensure that the data produced is always aligned with the latest config.</p><h4>Modeling Pipeline</h4><p>Because the focal and complement groups can be very large and costly to use in modeling, we downsample and use a subset of our total observations. To account for sampling noise, we take multiple samples from the output of our data pipeline and feed each sampling round into our modeling pipeline. Sampling improves our SLA, ensures each group has the same cardinality and allows us to get a sense of sampling noise. Outliers are also removed to limit the noisiness of our estimates.</p><p>The PSM model is built on top of <a href="https://ieeexplore.ieee.org/document/8964147">Bighead</a>, Airbnb’s machine learning platform. After fetching the sampled data, we perform feature selection, clean the features, and run PSM to produce FIVs in terms of each target feature before finally writing our results into the Data Warehouse. In addition to the FIVs themselves, we also collect and store evaluation metrics as well as metrics such as feature importance and runtime.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Hm6aj1TSpXLjd5kU" /><figcaption><em>Figure 5. Modeling steps needed to compute FIV, after the raw data has been generated</em></figcaption></figure><p>On top of the modeling pipeline we have built the ability to prioritize actions and rate limit the number of tasks we launch, giving us a big picture view of the resources being used.</p><h4>FIVs!</h4><p>Next we pull our FIVs into a <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Superset</a> dashboard for easy access by our clients. FIV point estimates and confidence intervals (estimated by <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf">bootstrapping</a>) are based on the last 6 months of available data to smooth over seasonality or month-level fluctuations. We distinguish between the value generated by the action itself (tagged as “Present” below) and the residual downstream value (“Future”) of the action.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wM0yfjWPOP1NU9Yb" /><figcaption><em>Figure 6. Snapshot of the dashboard as seen by clients</em></figcaption></figure><h3><strong>FIV as a Product</strong></h3><p>Airbnb’s two-sided marketplace creates interesting but complicated tradeoffs. To quantify these tradeoffs in a common currency, especially when experimentation is not possible, we have built the FIV framework. This has allowed teams to make standardized, data-informed prioritization decisions that account for both immediate <em>and</em> long-term payoffs.</p><p>Currently, we have scaled to work with <em>all</em> teams from across the company (demand-side, supply-side, platform teams like Payments and Customer Support, and even the company’s most recent team, <a href="https://www.airbnb.org/">Airbnb.org</a>) and computed over 150 FIV action events from the guest and listing perspective. Use cases range from return on investment calculations (what is the monetary value of a “perfect” stay?) to determining the long-term value of guest outreach emails that may not always generate immediate output metrics. We have also used FIV to inform the <a href="https://www.linkedin.com/pulse/overall-evaluation-criterion-oec-ronny-kohavi/">overall evaluation criteria</a> in <a href="https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669">experiments</a> (what weights do we use when trading off increased bookings and cancellations?) and rank different listing levers to understand what to prioritize (what features or amenities are most useful for a host to adopt?).</p><p>In the absence of a centralized, scalable FIV platform, each individual team would need to create their own framework, methodology, and pipelines to assess and trade off long-term value, which would be inefficient and leave room for errors and inconsistencies. We have boiled down this complex problem into essentially writing two queries with everything else done behind the scenes by our machinery.</p><p>Yet, our work is not done–we plan to continue improving the workflow experience and explore new models in order to improve our estimates. The future of FIV at Airbnb is bright!</p><h3><strong>Acknowledgments</strong></h3><p>FIV has been an effort spanning multiple teams and years. We’d like to especially thank <a href="https://www.linkedin.com/in/diana-chen-42332939/">Diana Chen</a> and <a href="https://www.linkedin.com/in/xuyuhe/">Yuhe Xu</a> for contributing to the development of FIV and the teams who have onboarded and placed trust into FIV.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3aa99a941ba5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">How Airbnb Measures Future Value to Standardize Tradeoffs</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Deep Dive into Airbnb’s Server-Driven UI System]]></title>
            <link>https://medium.com/airbnb-engineering/a-deep-dive-into-airbnbs-server-driven-ui-system-842244c5f5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/842244c5f5</guid>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[mobile-app-development]]></category>
            <category><![CDATA[server-driven-ui]]></category>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[web]]></category>
            <dc:creator><![CDATA[Ryan Brooks]]></dc:creator>
            <pubDate>Tue, 29 Jun 2021 17:00:44 GMT</pubDate>
            <atom:updated>2021-06-29T21:05:35.690Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb ships features faster across web, iOS, and Android using a server-driven UI system named Ghost Platform 👻.</p><p>By <a href="https://www.linkedin.com/in/rbro112/">Ryan Brooks</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CedYKpSYMIGEiX7m" /></figure><h4>Background: Server-Driven UI</h4><p>Before we dive into Airbnb’s implementation of server-driven UI (SDUI), it’s important to understand the general idea of SDUI and how it provides an advantage over traditional client-driven UI.</p><p>In a traditional world, data is driven by the backend and the UI is driven by each client (web, iOS, and Android). As an example, let’s take Airbnb’s listing page. To show our users a listing, we might request listing data from the backend. Upon receiving this listing data, the client transforms that data into UI.</p><p>This comes with a few issues. First, there’s listing-specific logic built on each client to transform and render the listing data. This logic becomes complicated quickly and is inflexible if we make changes to how listings are displayed down the road.</p><p>Second, each client has to maintain parity with each other. As mentioned, the logic for this screen gets complicated quickly and each client has their own intricacies and specific implementations for handling state, displaying UI, etc. It’s easy for clients to quickly diverge from one another.</p><p>Finally, mobile has a versioning problem. Each time we need to add new features to our listing page, we need to release a new version of our mobile apps for users to get the latest experience. Until users update, we have few ways to determine if users are using or responding well to these new features.</p><h4>The Case for SDUI</h4><p>What if clients didn’t need to know they were even displaying a listing? What if we could pass the UI directly to the client and skip the idea of listing data entirely? That’s essentially what SDUI does — we pass both the UI and the data together, and the client displays it agnostic of the data it contains.</p><p>Airbnb’s specific SDUI implementation enables our backend to control the data and how that data is displayed across all clients at the same time. Everything from the screen’s layout, how sections are arranged in that layout, the data displayed in each section, and even the actions taken when users interact with sections is controlled by a single backend response across our web, iOS, and Android apps.</p><h3>SDUI at Airbnb: The Ghost Platform 👻</h3><p>The Ghost Platform (GP) is a unified, opinionated, server-driven UI system that enables us to iterate rapidly and launch features safely across web, iOS, and Android. It is called Ghost because our primary focus is around ‘<strong>G</strong>uest’ and ‘<strong>Host</strong>’ features, the two sides to our Airbnb apps.</p><p>GP provides web, iOS, and Android frameworks in each client’s native languages (Typescript, Swift, and Kotlin, respectively) that enable developers to create server-driven features with minimal setup.</p><p>The core feature of GP is that features can share a library of generic sections, layouts, and actions, many backward compatible, enabling teams to ship faster and move complicated business logic to a central location on the backend.</p><h4>A Standardized Schema</h4><p>The backbone of the Ghost Platform is a standardized data model that clients can use to render UI. To make this possible, GP leverages a shared data layer across backend services using a unified data-service mesh, called <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>.</p><p>The key decision that helped us make our server-driven UI system scalable was to use a single, shared GraphQL schema for Web, iOS, and Android apps — i.e., we’re using the same schema for handling responses and generating strongly typed data models across all of our platforms.</p><p>We’ve taken time to generalize the shared aspects of different features and to account for each page’s idiosyncrasies in a consistent, thoughtful way. The result is a universal schema that’s capable of rendering all features on Airbnb. This schema is powerful enough to account for reusable sections, dynamic layouts, subpages, actions, and more, and the corresponding GP frameworks in our client applications leverage this universal schema to standardize UI rendering.</p><h3>The GP Response</h3><p>The first fundamental aspect of GP is the structure of the overall response. There are two main concepts used to describe UI in a GP response: sections and screens.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Qx23_meafmhc7-xQ" /><figcaption>Figure 1. <em>How users see Airbnb features on GP vs. how GP sees those same features as screens and sections.</em></figcaption></figure><ul><li><strong>Sections:</strong> Sections are the most primitive building block of GP. A section describes the data of a cohesive group of UI components, containing the exact data to be displayed — already translated, localized, and formatted. Each client takes the section data and transforms it directly into UI.</li><li><strong>Screens:</strong> Any GP response can have an arbitrary number of screens. Each screen describes the layout of the screen and, in turn, where sections from the sections array will appear (called placements). It also defines other metadata, such as how to render sections — e.g., as a popover, modal, or full-screen — and logging data.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e7e7ae8430106ed099e41067cbbd626d/href">https://medium.com/media/e7e7ae8430106ed099e41067cbbd626d/href</a></iframe><p>A feature’s backend built with GP will implement this GPResponse (fig. 2) and populate the screens and sections depending on their use case. GP client frameworks on web, iOS, and Android provide developers standard handling to fetch a GPResponse implementation and translate that to UI with minimal work on their part.</p><h3>Sections</h3><p>Sections are the most basic building block of GP. The key feature of GP sections is that they are entirely independent of other sections and the screen on which they are displayed.</p><p>By decoupling sections from the context around them, we gain the ability to reuse and repurpose sections without worrying about a tight coupling of business logic to any specific feature.</p><h4>Section Schema</h4><p>In GraphQL schema, GP sections are a union of all possible section types. Each section type specifies the fields they provide to be rendered. Sections are received in a GPResponse implementation with some metadata and are provided through a SectionContainer wrapper, which contains details about the section’s status, logging data, and the actual section data model.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a95a97b2f0c56c0c99ba4117c91d17c2/href">https://medium.com/media/a95a97b2f0c56c0c99ba4117c91d17c2/href</a></iframe><p>One important concept to touch on is SectionComponentType. SectionComponentType controls <em>how</em> a section’s data model is rendered. This enables one data model to be rendered in many different ways if needed.</p><p>For example, the two SectionComponentTypes TITLE and PLUS_TITLE might use the same backing TitleSection data model, but the PLUS_TITLE implementation will use Airbnb’s Plus-specific logo and title style to render the TitleSection. This provides flexibility to features using GP, while still promoting schema and data reusability.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i-Mi5mngBjWXQkFk" /><figcaption>Figure 4. <em>Example of rendering a TitleSection data model differently using SectionComponentType.</em></figcaption></figure><h4>Section Components</h4><p>Section data is transformed into UI through “Section Components”. Each section component is responsible for transforming a data model and a SectionComponentType into UI components. Abstract section components are provided by GP on each platform in their native languages (i.e., Typescript, Swift, Kotlin) and can be extended by developers to create new sections.</p><p>Section components map a section data model to <strong>one</strong> unique rendering and therefore pertain only to one SectionComponentType. As mentioned previously, sections are rendered without any context from the screen they are on or the sections around them, so each section component has no feature-specific business logic provided to it.</p><p>I’m an Android developer, so let’s take an Android example (and because Kotlin is great 😄). To build a title section, we have the code snippet seen below (fig. 5). Web and iOS have similar implementations — in Typescript and Swift respectively — for building section components.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1fc6b8a69d9799529da8817ff11621f5/href">https://medium.com/media/1fc6b8a69d9799529da8817ff11621f5/href</a></iframe><p>GP provides many “core” section components, such as our example TitleSectionComponent above (fig. 5), meant to be configurable, styleable, and backward compatible from the backend so we can adapt to any feature’s use case. However, developers building new features on GP can add new section components as needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8jjmsp_zOQfl0-Q8" /><figcaption>Figure 6. <em>GP takes section data, uses a section component to turn it into UI (TitleSectionComponent from fig. 5), and presents the built section UI to the user.</em></figcaption></figure><h3>Screens</h3><p>Screens are another building block of GP, but unlike sections, screens are mostly handled by GP client frameworks and are more opinionated in usage. GP screens are responsible for the layout and organization of sections.</p><h4>Screens schema</h4><p>Screens are received as a ScreenContainer type. Screens can be launched in a modal (popup), in a bottom sheet, or as a full screen, depending on values included in the screenProperties field.</p><p>Screens enable dynamic configuration of a screen’s layout and, in turn, arrangement of sections through a LayoutsPerFormFactor type. LayoutsPerFormFactor specifies the layout for compact and wide breakpoints using an interface called ILayout, which will be elaborated on below. The GP framework on each client then uses screen density, rotation, and other factors to determine which ILayout fromLayoutsPerFormFactor to render.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/183497776e1051791c38fb590888117c/href">https://medium.com/media/183497776e1051791c38fb590888117c/href</a></iframe><h4>ILayouts</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vSJJB0hlGHrzSlu6" /><figcaption>Figure 8. <em>A few examples of ILayout implementations, which are used to specify various placements.</em></figcaption></figure><p>ILayouts enable screens to change layouts depending on the response. In schema, ILayout is an interface with each ILayout implementation specifying various placements. Placements contain one or many SectionDetail types that point to sections in the response’s outermost sections array. We point to section data models rather than including them inline. This shrinks response sizes by reusing sections across layout configurations (LayoutsPerFormFactor from fig. 7).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7bfa81b59a9b172db7178c3c9db31aa2/href">https://medium.com/media/7bfa81b59a9b172db7178c3c9db31aa2/href</a></iframe><p>GP client frameworks inflate ILayouts for developers, as ILayout types are more opinionated than sections. EachILayout has a unique renderer in each client’s GP framework. The layout renderer takes each SectionDetail from each placement, finds the proper section component to render that section, builds the section’s UI using that section component, and finally, places the built UI into the layout.</p><h3>Actions</h3><p>The last concept of GP is our action and event handling infrastructure. One of the most game-changing aspects of GP is that in addition to defining the sections and layout of a screen from the network response, we also can define actions taken when users interact with UI on the screen, such as tapping a button or swiping a card. We do this through an IAction interface in our schema.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c66091e20e170cff94bc3bb43d63c078/href">https://medium.com/media/c66091e20e170cff94bc3bb43d63c078/href</a></iframe><p>Recall from earlier (fig. 6) that a section component is what translates our TitleSection to UI on each client. Let’s take a look at the same Android example of a TitleSectionComponent with a dynamic IAction fired on the click of the subtitle text.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cc64c3b4a32e77b593f830d6bcb080bf/href">https://medium.com/media/cc64c3b4a32e77b593f830d6bcb080bf/href</a></iframe><p>When a user taps the subtitle in this section, it fires the IAction passed for the onSubtitleClickAction field in TitleSection. GP is responsible for routing this action to an event handler defined for the feature, which will handle the IAction that was fired.</p><p>There is a standard set of generic actions that GP handles universally, such as navigating to a screen or scrolling to a section. Features can add their own IAction types and use those to handle their feature’s unique actions. Since the feature-specific event handlers are scoped to the feature, they can contain as much feature-specific business logic as they wish, enabling freedom to use custom actions and business logic when specific use cases arise.</p><h3>Bringing It All Together</h3><p>We’ve gone over several concepts, so let’s take an entire GP response and see how it’s rendered to tie everything together.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dfe0f7b05badb9997ab1c0ab0d75bce2/href">https://medium.com/media/dfe0f7b05badb9997ab1c0ab0d75bce2/href</a></iframe><h4>Creating the Section Components</h4><p>Features using GP will need to fetch a response implementing GPResponse mentioned above (fig. 2). Upon receiving a GPResponse, GP infra handles parsing this response and building the sections for the developer.</p><p>Recall that each section in our sections array has a SectionComponentType and a section data model. Developers working on GP add section components, using SectionComponentType as the key for how to render the section data model.</p><p>GP finds each section component and passes it the corresponding data model. Each section component creates UI components for the section, which GP will insert into the proper placement in the layout below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2SLVYciGoKNbdau6" /><figcaption>Figure 13. <em>Transforming section data to UI.</em></figcaption></figure><h4>Handling Actions</h4><p>Now that each section component’s UI elements are set up, we need to handle users interacting with the sections. For example, if they tap a button, we need to handle the action taken on click.</p><p>Recall earlier that GP handles routing events to their proper handler. The example response above (fig. 12) contains two sections that can fire actions, toolbar_section and book_bar_footer. The section component for building both of these sections simply needs to take the IAction and specify when to fire it, which in both cases will be when a button is clicked.</p><p>We can do this through click handlers on each client, which will use GP infra to route the event on a click event.</p><pre>button(<br>  onClickListener = {<br>    GPActionHandler.handleIAction(section.button.onClickAction)<br>  }<br>)</pre><h4>Setting Up the Screen and Layout</h4><p>To arrange a fully interactive screen for our users, GP looks through the screens array to find a screen with the “ROOT” id (GP’s default screen id). GP will then find the proper ILayout type depending on the breakpoint and other factors relevant to the specific device the user is using. To keep things simple, we’ll use the layout from the compact field, a SingleColumnLayout.</p><p>GP will then find a layout renderer for SingleColumnLayout, where it’ll inflate a layout with a top container (the nav placement), a scrollable list (the main placement), and a floating footer (the footer placement).</p><p>This layout renderer will take the models for the placements, which contain SectionDetail objects. These SectionDetails contain some styling information as well as the sectionId of the section to inflate. GP will iterate through these SectionDetail objects and inflate sections into their respective placements using the section components we built earlier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8mJWV_F1FPq0U5eI" /><figcaption>Figure 14. <em>GP Infra takes built sections with action handlers added, adds sections to ILayout placements.</em></figcaption></figure><h3>What’s Next for GP?</h3><p>GP has only existed for about a year, but a majority of Airbnb’s most used features (e.g., search, listing pages, checkout) are built on GP. Despite the critical mass of usage, GP is still in its infancy and there’s much more to be done.</p><p>We have plans for a more composable UI through “nested sections”, improving discoverability of elements that already exist through our design tools, such as Figma, and WYSIWYG editing of sections and placements, enabling no-code feature changes.</p><p>If you’re passionate about server-driven UI or building UI systems that scale, there’s so much more to be done. We encourage you to apply to the <a href="https://careers.airbnb.com/">open roles</a> on our engineering team.</p><h4>Re-engineering Travel Tech Talk</h4><p>Server-driven UI is complex. Countless hours have gone into creating a robust schema, client frameworks, and developer documentation that enables GP to be successful.</p><p>If you’d like a more high-level overview of SDUI and GP, I recently had the opportunity to speak at Airbnb’s <a href="https://www.facebook.com/AirbnbTech/videos/1445539065813160/">Re-engineering Travel tech talk</a> presenting GP. I’d encourage you to check it out for a general overview of server-driven UI and GP (skip to the 31-minute mark if you’re short on time).</p><h4>Special Thanks</h4><p>Special thanks to <a href="https://www.linkedin.com/in/abhinavvohra/">Abhi Vohra</a>, <a href="https://www.linkedin.com/in/wensheng-mao-76ab7142/">Wensheng Mao</a>, <a href="https://www.linkedin.com/in/jnvollmer/">Jean-Nicolas Vollmer</a>, <a href="https://www.linkedin.com/in/pranayairan/">Pranay Airan</a>, <a href="https://www.linkedin.com/in/stephen-herring-00381a6a/">Stephen Herring</a>, <a href="https://www.linkedin.com/in/jsperl/">Jasper Liu</a>, <a href="https://www.linkedin.com/in/kevinchrisweber/">Kevin Weber</a>, <a href="https://www.linkedin.com/in/rodolphe-courtier-97b32610/">Rodolphe Courtier</a>, <a href="https://www.linkedin.com/in/danielgarciacarrillo/">Daniel Garcia-Carrillo</a>, <a href="https://www.linkedin.com/in/fidelsosa/">Fidel Sosa</a>, <a href="https://www.linkedin.com/in/roshan-goli-03977a25/">Roshan Goli</a>, <a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a>, <a href="https://www.linkedin.com/in/chen-wu-a5677649/">Chen Wu</a>, <a href="https://www.linkedin.com/in/nickbryanmiller/">Nick Miller</a>, <a href="https://www.linkedin.com/in/yanlin-chen-58a41411/">Yanlin Chen</a>, <a href="https://www.linkedin.com/in/rsusandang/">Susan Dang</a>, and <a href="https://www.linkedin.com/in/amitywang/">Amity Wang</a>, as well as many more behind the scenes, for their tireless work building and supporting GP.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=842244c5f5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/a-deep-dive-into-airbnbs-server-driven-ui-system-842244c5f5">A Deep Dive into Airbnb’s Server-Driven UI System</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>