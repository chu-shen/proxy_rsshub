<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 24 May 2022 01:41:49 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Dynamic Kubernetes Cluster Scaling at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d79ae3afa132</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[cluster-autoscaler]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[David Morrison]]></dc:creator>
            <pubDate>Mon, 23 May 2022 17:35:24 GMT</pubDate>
            <atom:updated>2022-05-23T17:35:24.292Z</atom:updated>
            <content:encoded><![CDATA[<p>Authors: <a href="https://www.linkedin.com/in/evansheng112/">Evan Sheng</a>, <a href="https://www.linkedin.com/in/david-morrison-9419b110/">David Morrison</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Elojmgc7Y06tItOaLdB0Cw.jpeg" /></figure><h3>Introduction</h3><p>An important part of running Airbnb’s infrastructure is ensuring our cloud spending automatically scales with demand, both up <strong>and </strong>down. Our traffic fluctuates heavily every day, and our cloud footprint should scale dynamically to support this.</p><p>To support this scaling, Airbnb utilizes Kubernetes, an open source container orchestration system. We also utilize OneTouch, a service configuration interface built on top of Kubernetes, and is described in more detail in a previous <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">post</a>.</p><p>In this post, we’ll talk about how we dynamically size our clusters using the Kubernetes Cluster Autoscaler, and highlight functionality we’ve contributed to the <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">sig-autoscaling community</a>. These improvements add customizability and flexibility to meet Airbnb’s unique business requirements.</p><h3>Kubernetes Clusters at Airbnb</h3><p>Over the past few years, Airbnb has shifted almost all online services from manually orchestrated EC2 instances to Kubernetes. Today, we run thousands of nodes across nearly a hundred clusters to accommodate these workloads. However, this change didn’t happen overnight. During this migration, our underlying Kubernetes cluster setup evolved and became more sophisticated as more workloads and traffic shifted to our new technology stack. This evolution can be split into three stages.</p><p>Stage 1: Homogenous Clusters, Manual Scaling</p><p>Stage 2: Multiple Cluster Types, Independently Autoscaled</p><p>Stage 3: Heterogeneous Clusters, Autoscaled</p><h4>Stage 1: Homogenous Clusters, Manual Scaling</h4><p>Before using Kubernetes, each instance of a service was run on its own machine, and manually scaled to have the proper capacity to handle traffic increases. Capacity management varied per team and capacity would rarely be un-provisioned once load dropped.</p><p>Our initial Kubernetes cluster setup was relatively basic. We had a handful of clusters, each with a single underlying node type and configuration, which ran only stateless online services. As some of these services began shifting to Kubernetes, we started running containerized services in a multi-tenant environment (many pods on a node). This aggregation led to fewer wasted resources, and consolidated capacity management for these services to a single control point at the Kuberentes control plane. At this stage, we scaled our clusters manually, but this was still a marked improvement over the previous situation.</p><figure><img alt="An EC2 node running a single application vs. a Kubernetes node running 3 applications." src="https://cdn-images-1.medium.com/max/1024/0*xgJUXKfck5DuQOg1" /><figcaption>Figure 1: EC2 Nodes vs Kubernetes Nodes</figcaption></figure><h4>Stage 2: Multiple Cluster Types, Independently Autoscaled</h4><p>The second stage of our cluster configuration began when more diverse workload types, each with different requirements, sought to run on Kubernetes. To accommodate their needs, we created a cluster type abstraction. A “cluster type” defines the underlying configuration for a cluster, meaning that all clusters of a cluster type are identical, from node type to different cluster component settings.</p><p>More cluster types led to more clusters, and our initial strategy of manually managing capacity of each cluster quickly fell apart. To remedy this, we added the Kubernetes <a href="https://github.com/kubernetes/autoscaler">Cluster Autoscaler</a> to each of our clusters. This component automatically adjusts cluster size based on pod requests — if a cluster’s capacity is exhausted, and a pending pod’s request could be filled by adding a new node, Cluster Autoscaler launches one. Similarly, if there are nodes in a cluster that have been underutilized for an extended period of time, Cluster Autoscaler will remove these from the cluster. Adding this component worked beautifully for our setup, saved us roughly 5% of our total cloud spend, and the operational overhead of manually scaling clusters.</p><figure><img alt="Different Kubernetes clusters for different types of applications (CPU-bound or GPU-bound applications, for example)." src="https://cdn-images-1.medium.com/max/1024/0*XevtJSPUAo9vTpJn" /><figcaption>Figure 2: Kubernetes Cluster Types</figcaption></figure><h4>Stage 3: Heterogeneous Clusters, Autoscaled</h4><p>When nearly all online compute at Airbnb shifted to Kubernetes, the number of cluster types had grown to over 30, and the number of clusters to 100+. This expansion made Kubernetes cluster management tedious. For example, cluster upgrades had to be individually tested on each of our numerous cluster types.</p><p>In this third phase, we aimed to consolidate our cluster types by creating “heterogeneous” clusters that could accommodate many diverse workloads with a single Kubernetes control plane. First, this greatly reduces cluster management overhead, as having fewer, more general purpose clusters reduces the number configurations to test. Second, with the majority of Airbnb now running on our Kubernetes clusters, efficiency in each cluster provides a big lever to reduce cost. Consolidating cluster types allows us to run varied workloads in each cluster. This aggregation of workload types — some big and some small — can lead to better bin packing and efficiency, and thus higher utilization. With this additional workload flexibility, we had more room to implement sophisticated scaling strategies, outside of the default Cluster Autoscaler expansion logic. Specifically, we aimed to implement scaling logic that was tied to Airbnb specific business logic.</p><figure><img alt="A single Kubernetes cluster with multiple different node types: an Intel compute node, an AMD compute node, a high-memory node, and a GPU node." src="https://cdn-images-1.medium.com/max/1024/0*1GUcmg4jijWf_fdA" /><figcaption>Figure 3: A heterogeneous Kubernetes cluster</figcaption></figure><p>As we scaled and consolidated clusters so they were heterogeneous (multiple instance types per cluster), we began to implement specific business logic during expansion and realized some changes to the autoscaling behavior were necessary. The next section will describe some of the changes we’ve made to Cluster Autoscaler to make it more flexible.</p><h3>Cluster Autoscaler Improvements</h3><h4>Custom gRPC Expander</h4><p>The most significant improvement we made to Cluster Autoscaler was to provide a new method for determining node groups to scale. Internally, Cluster Autoscaler maintains a list of node groups which map to different candidates for scaling, and it filters out node groups that do not satisfy pod scheduling requirements by running a scheduling simulation against the current set of Pending (unschedulable) pods. If there are any Pending (unschedulable) pods, Cluster Autoscaler attempts to scale the cluster to accommodate these pods. Any node groups that satisfy all pod requirements are passed to a component called the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders">Expander</a>.</p><figure><img alt="A depiction of Cluster Autoscaler, which calls the Expander to determine which type of node to add in a heterogeneous Kubernetes cluster." src="https://cdn-images-1.medium.com/max/1024/0*ryQyolVdPY6bbSQy" /><figcaption>Figure 4: Cluster Autoscaler and Expander</figcaption></figure><p>The Expander is responsible for further filtering the node groups based on operational requirements. Cluster Autoscaler has a number of different built-in expander options, each with different logic. For example, the default is the random expander, which selects from available options uniformly at random. Another option,and the one that Airbnb has historically used, is the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/priority">priority expander</a>, which chooses which node group to expand based on a user-specified tiered priority list.</p><p>As we moved toward our heterogeneous cluster logic, we found that the default expanders were not sophisticated enough to satisfy our more complex business requirements around cost and instance type selection.</p><p>As a contrived example, say we want to implement a weighted priority expander. Currently, the priority expander only lets users specify distinct tiers of node groups, meaning it will always expand tiers deterministically and in order. If there are multiple node groups in a tier, it will break ties randomly. A weighted priority strategy of setting two node groups in the same tier, but expanding one 80% of the time, and another 20% of the time, is not achievable with the default setup.</p><p>Outside of the limitations of the current supported expanders, there were a few operational concerns:</p><ol><li>Cluster Autoscaler’s release pipeline is rigorous and changes take time to review before being merged upstream. However, our business logic and desired scaling strategy is continuously changing. Developing an expander to fill our current needs today may not fulfill our needs in the future</li><li>Our business logic is specific to Airbnb and not necessarily other users. Any changes we implement specific to our logic would not be useful to contribute back upstream</li></ol><p>From these, we came up with a set of requirements for a new expander type in Cluster Autoscaler:</p><ol><li>We wanted something that was both extensible and usable by others. Others may run into similar limitations with the default Expanders at scale, and we would like to provide a generalized solution and contribute functionality back upstream</li><li>Our solution should be deployable out of band with Cluster Autoscaler, and allow us to respond more rapidly to changing business needs</li><li>Our solution should fit into the Kubernetes Cluster Autoscaler ecosystem, so that we do not have to maintain a fork of Cluster Autoscaler indefinitely</li></ol><p>With these requirements, we came up with a design that breaks out the expansion responsibility from the Cluster Autoscaler core logic. We designed a pluggable “<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/grpcplugin">custom Expander</a>.” which is implemented as a gRPC client (similarly to the <a href="https://github.com/kubernetes/autoscaler/blob/68c984472acce69cba89d96d724d25b3c78fc4a0/cluster-autoscaler/proposals/plugable-provider-grpc.md">custom cloud provider</a>). This custom expander is broken into two components.</p><p>The first component is a gRPC client built into Cluster Autoscaler. This Expander conforms to the same interface as other Expanders in Cluster Autoscaler, and is responsible for transforming information about valid node groups from Cluster Autoscaler to a defined <a href="https://developers.google.com/protocol-buffers/docs/overview">protobuf</a> schema (shown below), and receives the output from the gRPC server to transform back to a final list of options for Cluster Autoscaler to scale up.</p><pre>service Expander {<br>  rpc BestOptions (BestOptionsRequest) returns (BestOptionsResponse) <br>}</pre><pre>message BestOptionsRequest {<br>  repeated Option options;<br>  map&lt;string, k8s.io.api.core.v1.Node&gt; nodeInfoMap;<br>}</pre><pre>message BestOptionsResponse {<br>  repeated Option options;<br>}</pre><pre>message Option {<br>  // ID of node to uniquely identify the nodeGroup<br>  string nodeGroupId;<br>  int32 nodeCount;<br>  string debug;<br>  repeated k8s.io.api.core.v1.Pod pod;<br>}</pre><p>The second component is the gRPC server, which is left up to the user to write. This server is intended to be run as a separate application or service, which can run arbitrarily complex expansion logic when selecting which node group to scale up, with the given information passed from the client. Currently, the protobuf messages passed over gRPC are slightly transformed versions of what is passed to the Expander in Cluster Autoscaler.</p><p>From our aforementioned example, a weighted random priority expander can be implemented fairly easily by having the server read from a priority tier list and weighted percentage configuration from a configmap, and choose accordingly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MldTcDs1Df38IfHE" /><figcaption>Figure 5: Cluster Autoscaler and Custom gRPC Expander</figcaption></figure><p>Our implementation includes a failsafe option. It is recommended to use the option to pass in <a href="https://github.com/kubernetes/autoscaler/pull/4233">multiple expanders</a> as arguments to Cluster Autoscaler. With this option, if the server fails, Cluster Autoscaler is still able to expand using a fallback Expander.</p><p>Since it runs as a separate application, expansion logic can be developed out of band with Cluster Autoscaler, and since the gRPC server is customizable by the user based on their needs, this solution is extensible and useful to the wider community as a whole.</p><p>Internally, Airbnb has been using this new solution to scale all of our clusters without issues since the beginning of 2022. It has allowed us to dynamically choose when to expand certain node groups to meet Airbnb’s business needs, thus achieving our initial goal of developing an extensible custom expander.</p><p>Our custom expander was <a href="https://github.com/kubernetes/autoscaler/pull/4452">accepted</a> into the upstream Cluster Autoscaler earlier this year, and will be available to use in the next version (v1.24.0) release.</p><h3>Other Autoscaler Improvements</h3><p>Over the course of our migration to heterogeneous Kubernetes clusters, we identified a number of other bugs and improvements that could be made to Cluster Autoscaler. These are briefly described below:</p><ul><li><a href="https://github.com/kubernetes/autoscaler/pull/4489">Early abort for AWS ASGs with no capacity</a>: Short circuit the Cluster Autoscaler loop to wait for nodes it tries to spin up to see if they are ready by calling out to an AWS EC2 endpoint to check if the ASG has capacity. With this change enabled, users get much more rapid, yet correct scaling. Previously, users using a priority ladder would have to wait 15 minutes between each attempted ASG launch, before trying an ASG of lower priority.</li><li><a href="https://github.com/kubernetes/autoscaler/pull/4073">Caching launch templates to reduce AWS API calls</a>: Introduce a cache for AWS ASG Launch Templates. This change unlocks using large numbers of ASGs, which was critical for our generalized cluster strategy. Previously, for empty ASGs (no present nodes in a cluster), Cluster Autoscaler would repeatedly call an AWS endpoint to get launch templates, resulting in throttling from the AWS API.</li></ul><h3>Conclusion</h3><p>In the last four years, Airbnb has come a long way in our Kubernetes Cluster setup. Having the largest portion of compute at Airbnb on a single platform provided a strong, consolidated lever to improve efficiency, and we are now focused on generalizing our cluster setup (think <a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">“cattle, not pets”</a>). By developing and using a more sophisticated expander in Cluster Autoscaler (as well as fixing a number of other minor issues with the Autoscaler), we have been able to achieve our goals of developing our complex, business specific scaling strategy around cost and mixed instance types, while also contributing some useful features back to the community.</p><p>For more details on our heterogeneous cluster migration, watch our Kube-Con <a href="https://www.youtube.com/watch?v=GCCSY7ERXj4&amp;ab_channel=CNCF%5BCloudNativeComputingFoundation%5D">talk</a> and we’re also at KubeCon EU this year, come talk to us! If you’re interested in working on interesting problems like the ones we’ve described here, we’re hiring! Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/3949745/">Engineering Manager- Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3903900/">Senior Front End Engineer</a></p><p><a href="https://careers.airbnb.com/positions/2623004/">Senior Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4168852/">Software Engineer, Observability</a></p><p><a href="https://careers.airbnb.com/positions/3696687/">Software Engineer, Developer Infrastructure</a></p><h3>Acknowledgements</h3><p>The evolution of our Kubernetes Cluster setup is the work of many different collaborators. Special thanks to Stephen Chan, Jian Cheung, Ben Hughes, Ramya Krishnan, David Morrison, Sunil Shah, Jon Tai and Long Zhang, as this work would not have been possible without them.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d79ae3afa132" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132">Dynamic Kubernetes Cluster Scaling at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Kamini Dandapani]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/7f51f1fbb2bb</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[leadership]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[people]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Wed, 11 May 2022 19:35:36 GMT</pubDate>
            <atom:updated>2022-05-11T19:35:36.825Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Kamini Dandapani</h3><p>Airbnb’s VP of Engineering on why you don’t have to change your natural self to be a leader</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/0*t-dDS7QYW1gsBtG5" /></figure><p><a href="https://www.linkedin.com/in/kaminidandapani/">Kamini Dandapani</a>, VP of Engineering at Airbnb, leads the Infrastructure Engineering organization, which is in many ways the backbone of the company: responsible for powering the systems that keep Airbnb running smoothly and help new products reach millions of people. With a passion for how platforms can support and sustain the business and product, Kamini developed her considerate and welcoming leadership style at eBay and LinkedIn before joining Airbnb two years ago. In addition to her Infra role, she champions diversity and belonging in the workplace and is co-sponsor for Airbnb’s tech diversity council, which aims to create the most diverse and inclusive community in the tech industry.</p><p><em>Want to hear Kamini and other Infrastructure team members talk about some of the team’s latest projects? Check out the </em><a href="https://www.facebook.com/AirbnbTech/videos/635338454172729/"><em>“Powering Our Platform” Airbnb Tech Talk</em></a><em> from March 2022. You’ll hear about some of the major initiatives we’re working on in next-generation service mesh, observability, feature engineering, and scalable storage.</em></p><h3>From Chennai to Chicago</h3><p>Growing up in India, I was the youngest of three girls. Despite facing skepticism and criticism from others around them, my parents invested heavily in our education and gave us a very strong footing, without which I don’t think I would be where I am today.</p><p>I started familiarizing myself with the engineering world, and found that I immensely enjoyed it. I did my undergrad in electronics and communication, and with my dad’s encouragement — he camped out overnight in the line in front of the US Consulate to get a visa — I came to the US to pursue my master’s in computer science.</p><p>In Chicago, I had to adjust to a lot of new experiences (including the winter cold!). In India, I never did anything alone, but here I had to do everything independently, from managing my finances to driving a car. After I graduated, I felt very fortunate to get a job in Silicon Valley, and I’ve stayed here ever since.</p><h3>Leading at the intersection of platform and product</h3><p>Effective infrastructure can’t be built in a vacuum. Rather, it requires close partnerships with our product engineers to support both our product and overall business strategies. My professional sweet spot is where the platform architecture meets the end-user experience — plus scale!</p><p>In my engineering career, I worked at eBay for 12 years and grew into a director position, leading international expansion. After that, I was at LinkedIn for six years, leading infrastructure and tools for the consumer app, and that’s where I learned how to operate and develop a platform at scale. When Airbnb got in touch with me, I wasn’t looking for a change. But with every conversation that I had, there was something truly magical about the place — from the leadership, to the inclusivity, to the company’s mission — and I am so grateful that I made the leap.</p><p>What excited me most was bringing dozens of years of operating at scale to Airbnb. And one key component to operating at scale is working effectively and smoothly cross-functionally, and building close relationships with our product teams and key partners across the business. I’ve seen some truly incredible teamwork within my own team, and across all of Airbnb.</p><h3>Building the tech backbone of Airbnb</h3><p>Most of the technical foundation that powers Airbnb comes from the Infrastructure organization. The impact that this group has is so wide and profound.</p><p>The Infrastructure organization has several key pillars:</p><ul><li><strong>Search Infrastructure</strong>, which powers the backend systems for our guest search experience</li><li><strong>Data Platform</strong>, for storing, processing and managing all the data that powers every user experience</li><li><strong>Developer Platform</strong>, which helps make Airbnb engineers’ lives friction-free by building tools, services and environments to help them develop, build, test and deploy their code</li><li><strong>Cloud Infrastructure,</strong> which delivers and operates the cloud environment that powers Airbnb</li><li><strong>Reliability Engineering, </strong>which remedies and prevents site performance issues through tooling and automation</li></ul><p>Within each of these areas, we have many long-term, multi-year projects, all part of what we’re calling Tech Stack 2.0: an evolution and modernization of our technology. Some sample initiatives include <a href="https://news.airbnb.com/unique-stays-hosts-earn-more-than-300-million-since-start-of-pandemic/">flexible search for guests</a> and UDS, our pioneering next-generation storage system.</p><h3>My identity: female, South Asian, immigrant</h3><p>People often point out that I’m unique for being a female leader in tech. But in reality, there are three important aspects of my identity: yes, I’m a woman, but I’m also South Asian and an immigrant. All of these have shaped who I am today.</p><p>I grew up in a very different culture. We were discouraged from challenging the status quo, and for my parents and grandparents, the idea was that if you work extremely hard, recognition will follow. That’s not the way it works here: it sometimes seems like you need to have an opinion and advocate for yourself in order to be taken seriously.</p><p>In many ways, I think being different is an advantage as a leader. While I encourage everyone on the team to make sure their voice is being heard, I also believe in being your natural self. That’s how I’ve been able to build trust with my teams, by letting them see the real me. My philosophy is that no one can be an expert in everything. What you’ll see is varying degrees in people — and I want to fully support that diversity of thought and experience, because a team that’s well-rounded is more effective.</p><h3>Bringing people along</h3><p>When joining Airbnb, I asked to have the dedicated time and agency to do work around diversity and gender parity. I’m now the co-sponsor for the Tech Diversity Council alongside <a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-lucius-diphillips-79d1f0bc72a2">Lucius DiPhillips</a> (CIO), where we advocate for diversity-related projects around the Tech org. I’m also one of the advisors for our Asians@ employee resource group.</p><p>There’s something special about these employee resource groups here at Airbnb that I haven’t seen before. It’s a very small close-knit group, and we can relate to our similar upbringing and cultural norms. We genuinely look out for each other and amplify our Asian@ colleagues’ voices.</p><p>There’s a saying that “if you want to go fast, go alone, but if you want to go far, go together.” Whether it’s sharing context about our work, being vulnerable about my mistakes, or building a diverse organization, I very much believe in bringing people along. I couldn’t be at a better place than here at Airbnb, where our company’s mission is for anyone to belong anywhere.</p><p><em>Interested in working at Airbnb? We’re hiring! Check out these open roles:</em></p><p><a href="https://careers.airbnb.com/positions/3029584/">Staff Software Engineer, Distributed Storage</a></p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=3da3a8881us">Senior Frontend Infrastructure Engineer, Web Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3747712/">Senior/Staff Backup and Recovery Engineer, Storage Infrastructure</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7f51f1fbb2bb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb">My Journey to Airbnb — Kamini Dandapani</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Continuous Delivery at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/6ac042bc7876</guid>
            <category><![CDATA[migration]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[deployment]]></category>
            <dc:creator><![CDATA[jens vanderhaeghe]]></dc:creator>
            <pubDate>Fri, 22 Apr 2022 17:09:17 GMT</pubDate>
            <atom:updated>2022-04-22T17:09:17.675Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/939/1*_bvb5WtcQRE3mL-32b0F4g@2x.png" /></figure><p><a href="https://www.linkedin.com/in/jensvanderhaeghe">Jens Vanderhaeghe</a>,<a href="https://www.linkedin.com/in/manishma"> </a><a href="mailto:manish.maheshwari@airbnb.com">Manish Maheshwari</a></p><h3>Introduction</h3><p>Over the years, Airbnb’s tech stack has <a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b">shifted</a> from a monolith to 1,000+ services in our service-oriented architecture (SOA). While this migration solved our problems scaling our application architecture, it also introduced an array of new challenges.</p><p>In this blog post we’ll cover the deployment challenges faced on the road to our current architecture and how we’ve solved those problems by adopting Continuous Delivery best practices on top of <a href="https://spinnaker.io/">Spinnaker</a>. We’ll do a deep dive into how we’ve solved such a large scale migration in a short timespan while maintaining developer productivity along the way.</p><h3>From Deployboard to Spinnaker</h3><p><a href="https://medium.com/airbnb-engineering/introducing-deploy-pipelines-to-airbnb-fc804ac2a157">Deployboard</a>, Airbnb’s legacy deployment tool, was designed for a monolith having a few centrally managed pipelines. As we started moving to SOA, thousands of code changes across hundreds of service teams were being deployed. Deployboard was not designed for the SOA architecture, which is characterized by decentralized deployments. We needed something much more templated so that teams could quickly get a standard, best-practice pipeline, rather than start from scratch for every new service. Rather than continuing to build in-house solutions with siloed knowledge, it made the most sense for us to adopt open source solutions built from the ground-up for decentralized, SOA pipelines.</p><p>Spinnaker is proven at Airbnb’s scale, and beyond, by industry peers like Google and Netflix. We believe continuous delivery isn’t a problem unique to Airbnb, and decided we’d benefit from collaborating with the larger community. We chose Spinnaker as the replacement for Deployboard in part because we could bridge functionality gaps by plugging in custom logic easily, without forking the core code. Also, it was important to us that Spinnaker automated canary analysis (ACA), an extremely effective strategy in reducing the blast radius of buggy deployments.</p><h3>Migrating to Spinnaker</h3><p>When deciding to switch rather than evolve, we created a new problem: How do we get a globally distributed team of thousands of engineers working on thousands of services (each with their own deploy pipeline), working under business pressure to continuously improve their product and code base, to change one of the most important tools they depend on for day-to-day productivity.</p><p>We were particularly worried about the “long-tail migration problem,” where we successfully get 80% of the services migrated in the first year or so, but the remaining ones become stuck indefinitely on the old system. Having to operate in such a hybrid mode is costly, and it also is a reliability and even security risk, because the “legacy” systems (including the legacy deploy system) receive less and less attention over time.</p><p>Rather than forcing yet another new tool on our engineers, we came up with a migration strategy based on three pillars: focus on the benefits, automated onboarding, and data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/732/0*2fjFQ8VIEm7LD_Iz" /><figcaption>The 3 pillars of our migration strategy</figcaption></figure><h4>Focus on Benefits</h4><p>By focusing on the benefits of Spinnaker, we encouraged engineering teams to adopt Spinnaker voluntarily rather than forcing them.</p><p>We started out by manually onboarding a small group of early adopters. We identified a set of services that were prone to causing incidents or had a complicated deployment process. By migrating these services onto Spinnaker and automating their release process using a deployment pipeline with ACA, we were quickly able to demonstrate value. As we onboarded more teams, we iterated on the feature gaps between Deployboard and Spinnaker. These early services served as case studies, proving to both the rest of engineering as well as leadership that adopting an automated and standardized deployment process provides huge benefits.</p><p>These early adopters saw benefits so significant that they ended up becoming evangelists for continuous delivery and Spinnaker, spreading the word to other teams organically.</p><h4>Automated Onboarding</h4><p>As more and more services started adopting Spinnaker, the Continuous Delivery team could no longer keep up with demand. We switched gears and focused on building automated tooling to onboard services to Spinnaker.</p><p>At Airbnb, we store configuration as code <a href="https://www.infoq.com/presentations/airbnb-kubernetes-services/">using a framework called OneTouch</a>. This allows engineers to make changes to the code as well as the infrastructure running their code in a single commit and in the same folder. All infrastructure changes are version controlled.</p><figure><img alt="Example of a codified Spinnaker pipeline" src="https://cdn-images-1.medium.com/max/1024/0*1fytVc5gswBQQYwZ" /><figcaption>Example of a codified Spinnaker pipeline</figcaption></figure><p>Following the OneTouch philosophy, we created an abstraction layer on top of Spinnaker that enables all continuous delivery configuration to be source controlled and managed by our existing tools and processes.</p><p>Today, when new services are created they get Spinnaker integration, including ACA, for free out of the box.</p><h4>Data</h4><p>In addition to focusing on the benefits and making it easy to onboard, we wanted to clearly communicate the value-add of adopting Spinnaker in a data-driven way. We automatically instrumented <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Superset</a> dashboards for each service that adopted Spinnaker.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UaIkauaDzJcPZuOZ" /></figure><p>Example of an instrumented dashboard for a service that has adopted Spinnaker</p><p>Service owners get insight into deployment data like deploy frequency and number of regressions prevented by ACA. Most service owners saw a significant increase in deployment frequency and a marked decrease in production incidents by adopting our new tooling. By arming our users with the right data, they can more easily advocate for the benefits of adopting continuous delivery.</p><h3>Clearing the final hurdle</h3><p>As expected, we eventually hit an inflection point in adoption. Organic adoption slowed as we reached ~85% of deployments being done on Spinnaker.</p><p>Once we hit this point, it was time to switch our strategy again, to adopt the lagging services. Our plan consisted of the following steps.</p><ol><li><strong><em>Stop the bleeding</em></strong> <br>The first thing we did is stop any new service from being deployed with Deployboard. This kept our list of remaining services to adopt static. We did this by giving engineers ample heads-up that this change was coming.</li><li><strong><em>Announce deprecation date + increase friction </em></strong><br>We gradually increased friction when using Deployboard over Spinnaker by adding a banner and warning inside Deployboard. We also instituted an exemption process that would allow us to catch major blockers well before the actual deprecation date without hurting customer experience.</li><li><strong><em>Send out automated PRs for the remaining services. </em></strong><br>To ensure we could also help onboard services where owners are resource constrained we once again leveraged tools like our in-house refactor tool,Refactorator, to do the heavy lifting.</li><li><strong>Deprecation date and post-deprecation follow-up. </strong><br>On deprecation date, we had code in place that blocked any OneTouch deploy from Deployboard. We had some loopholes in place in case there were services that still needed to use Deployboard for emergency reasons. The exemption list allows them to temporary get access to Deployboard. Engineers on the CD team can also still deploy with Deployboard, a simple page to the on-call can quickly help service owners in this case. As of today, the number of those cases remains very minimal given the amount of preparation we’ve done.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vXsxUvTvvx0rP7gw" /><figcaption>By adding a banner to Deployboard recommending engineers to adopt Spinnaker, we were able to drive adoption more quickly.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-QuiF4QGkdQN1MV_" /><figcaption>Example of an automated Pull Request that migrates a service from Deployboard to Spinnaker with minimal engineering effort.</figcaption></figure><h3>Future Plans and Opportunities</h3><p>Now that we’ve standardized our deployment process, we’re excited to integrate various existing tools at Airbnb into our continuous delivery pipelines. In 2022 and beyond, we are investing resources into integrating automated load testing, providing a way to safely toggle feature flags, and enabling blue/green deployments to facilitate instant rollbacks. More broadly, we see Spinnaker not only as a tool for code deployments, but also for the automation of various manual processes, allowing engineers to orchestrate any arbitrary workload as a pipeline.</p><p>During our migration, we’ve made a ton of modifications, both large and small, to Spinnaker, which is a testament to how flexible the tool is. We will be focused on upgrading to the latest open-source version and are looking forward to contributing some of our changes back to the open-source community.</p><h3>Conclusion</h3><p>In our move from a monolithic architecture to SOA, we needed to rethink the way we do deployments at Airbnb.</p><p>By creating a Continuous Delivery team focused on delivering great tools to safely and easily deploy code, we were able to migrate from our in-house tool, Deployboard, to Spinnaker. This was a very carefully planned and crafted migration. To adopt the majority of services, we focused on the benefits using a data-driven and automated approach to migration.</p><p>As expected, there was a long tail of services that didn’t organically adopt our new tools. We were able to get to the 100% finish line by shifting our strategy towards adding more friction and eventually deprecating our old tool.</p><p>This migration now serves as a blueprint for other infrastructure related migrations at Airbnb and we look forward to continuing iterating on our strategies for bringing better tools to our engineers while maintaining existing productivity and reducing toil.</p><h3>Acknowledgments</h3><p>All of our achievements wouldn’t have been possible without support of the entire Continuous Delivery team: <a href="mailto:jerry.chung@airbnb.com">Jerry Chung</a>, Freddy Chen, Alper Kokmen, Brian Wolfe, <a href="mailto:dion.hagan@airbnb.com">Dion Hagan</a>, Ryan Zelen, Greg Foster, Jens Vanderhaeghe, Mohamed Mohamed, Jake Silver, Manish Maheshwari and Shylaja Ramachandra. The entire Developer Platform organization rallied behind this effort. We’re also grateful to the countless engineers at Airbnb that have adopted Spinnaker over the years and have provided us with valuable feedback. We’d also like to thank all of the people at our peer companies and volunteers who have spent countless hours working on the open source Spinnaker project.</p><p><strong><em>Interested in working at Airbnb? Check out these open roles:</em></strong></p><p><a href="https://careers.airbnb.com/positions/3696687/?gh_src=08eeee991us">Senior/Staff Software Engineer, Developer Infrastructure</a><br><a href="https://careers.airbnb.com/positions/3903900/?gh_src=e91bd0291us">Senior Frontend Infrastructure Engineer, Web Platform</a></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ac042bc7876" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876">Continuous Delivery at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Florian Andes]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-florian-andes-5080685262d3?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5080685262d3</guid>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[business-development]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[program-management]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Tue, 12 Apr 2022 17:56:39 GMT</pubDate>
            <atom:updated>2022-04-12T17:56:06.942Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Florian Andes</h3><p>From building airplanes to Staff Technical Program Manager at Airbnb</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gHm8C-tJ3j2bavkT" /></figure><p><a href="https://www.linkedin.com/in/floandes/"><em>Florian Andes</em></a><em> is a Staff Technical Program Manager at Airbnb. He has over 10 years of experience that spans the software, manufacturing, and strategy consulting industry. He studied in Frankfurt, London, Singapore, and Boston, where he received a bachelor’s and MBA degree in Business and Entrepreneurship.</em></p><p><em>Though it can be hard and intimidating to find your place in the “big tech” industry in Silicon Valley, Florian has relied on curiosity and openness to establish a successful career at Airbnb. Read on for Florian’s own words on working at the intersection of business and software engineering, transferring to the US and scaling tech programs from zero to 10x for Airbnb, and more.</em></p><h3>A global citizen finding a home for his career</h3><p>Many years ago, Airbnb had a tagline that really inspired me: “Don’t Go There. Live There.” I’ve tried to embrace that idea as much as I could. I’ve had the chance to live and work in several different countries — Germany, the UK, Singapore, China, and now the US, where I’m currently based in San Francisco.</p><p>I grew up in Southern Germany, specifically a smaller city called Ulm (which is famous for Albert Einstein, and for having the tallest church building in the world). Growing up, my dad was a great inspiration for me. In his twenties, he started his own business. His journey taught me a lot about perseverance, dedication, and visionary thinking. I’m still inspired by how he pioneered the intersection of hardware and software engineering in the very early days of computers.</p><p>In my career, I’ve broadly moved from hardware to software engineering across different industries. (My first job was folding pizza boxes at a friend’s restaurant.) In Germany, there’s a large hardware and manufacturing presence: cars, industrial machines, and appliances in general. I started off at Airbus, building airplanes with advanced plastics and fiber materials, and then moved into the automotive industry and strategy consulting in the mobility space.</p><p>Along the way, I became really interested in software companies and internet technology. In Singapore, where I studied for my MBA, I joined a fast-growing tech startup to manage their partnerships across APAC. I loved it. I was flying around Southeast Asia to speak at startup events, pitch to investors, explain the idea and what problem we were solving — it was all very exciting to me.</p><p>I ended up attending a fireside chat with Mike Curtis, Airbnb’s VP of Engineering at that time, hosted by a local startup co-working space, where he shared lessons he picked up on his journey. I was inspired and ended up connecting with Mike and some folks from Airbnb after the talk, and the opportunity for me to join Airbnb grew organically from there.</p><h3>Pioneering projects at the intersection of tech and business</h3><p>TPMs are involved in every major release at Airbnb. Some TPMs are more product and business-focused (that’s me), and others are more infrastructure and platform-oriented. The potential for impact is high because you’re often very close to engineering leaders. You have conversations with the CTO, and many senior leaders in engineering — and on our product teams, as well.</p><p>I’ve been with Airbnb now for more than five years, and over time, my work has expanded in technical depth and also in breadth. I started out with a focus on business operations and strategy in EMEA (based in Berlin), then transferred to the US to build out an entirely new API program from scratch, and I’m now overseeing all technical programs related to hosting products at Airbnb. It’s a really interesting area that bridges the gap between the tech (engineering, product, data science, and design) and commercial parts of our business.</p><p>There are two projects I have been involved in recently that demonstrate the breadth of our programs. One is the adoption of our <a href="https://airbnb.design/building-a-visual-language/">Design Language System</a> (DLS), a repository of prebuilt components that designers and engineers can reuse instead of building something new. So whenever an engineer at Airbnb has to implement a button, they don’t have to build this from scratch. Another is reducing our <a href="https://medium.com/airbnb-engineering/our-journey-towards-cloud-efficiency-9c02ba04ade8">AWS costs</a>. We introduced a new attribution model at Airbnb that directly associates AWS costs to different teams, and my role was monitoring our organization’s consumption and championing a lot of cultural change to think about AWS cost-efficiency whenever we build products.</p><h3>Strategies for remote program management</h3><p>Recently a big focus has been, naturally, keeping teams engaged and collaborative during remote work. Communicating a clear program vision and using frameworks to keep projects on track are two strategies that are more important than ever.</p><p>One other thing that I consider critical is celebrating wins and key milestones. Before, when we were in the office, it was much easier to celebrate big wins. At Airbnb, we have a tool where you can send appreciation to others. I use it all the time, because when I work with multiple stakeholders across design, product, engineering, and QA, people are generally contributing at different stages. When the project ends, you might not have this big forum anymore, because most people have moved on already. But it’s important to still give credit to the work that everyone contributed along the way and make sure their managers have visibility into their contributions.</p><h3>What makes the TPM role unique at Airbnb</h3><p>TPM is relatively new as a function itself, and every company approaches it slightly differently. Even at Airbnb, it’s continuously evolving and progressing. One thing that’s always the case, though, is that TPMs need to use influence to lead programs, teams, and products — without exercising direct authority. As a TPM, you have to champion ideas, be a great communicator, and work with your partners to align priorities and push projects forward.</p><p>You don’t have to study software engineering to be a technical program manager — I think you need to show that you have the ability to grow and learn on the technical side, and this can happen before the job or on the job. For instance, at first I didn’t have much context about AWS optimization, frontend design language systems, or open source, but people at Airbnb are super open to sharing knowledge which allowed me to quickly onboard and take on these programs. You just need to be curious and open to learn.</p><p>I love working at Airbnb because of the people and the mission-driven nature of the company. And I think what makes being a TPM at Airbnb extremely unique is that there’s still a lot of “greenfield” areas and unexplored territory, where you can really help define a new strategy and vision.</p><p><em>Interested in joining Airbnb as a TPM? Check out these open roles:</em></p><p><a href="https://careers.airbnb.com/positions/4024213/?gh_src=6b8b81e61us">Senior TPM, Guest and Host Technology</a></p><p><a href="https://careers.airbnb.com/positions/3955056/?gh_src=8a794a6e1us">Staff TPM, Insurance Platform</a></p><p><a href="https://careers.airbnb.com/positions/3651016/?gh_src=77ab28041us">Staff TPM, Infrastructure Regionalization</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5080685262d3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-florian-andes-5080685262d3">My Journey to Airbnb — Florian Andes</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hacking Human Connection: The Story of Awedience]]></title>
            <link>https://medium.com/airbnb-engineering/hacking-human-connection-the-story-of-awedience-ebf66ee6af0e?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ebf66ee6af0e</guid>
            <category><![CDATA[connection]]></category>
            <category><![CDATA[meetings]]></category>
            <category><![CDATA[pandemic]]></category>
            <category><![CDATA[virtual-events]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Avand Amiri]]></dc:creator>
            <pubDate>Tue, 05 Apr 2022 17:29:35 GMT</pubDate>
            <atom:updated>2022-04-11T22:22:11.531Z</atom:updated>
            <content:encoded><![CDATA[<h4>How a home-grown product helps Airbnb employees feel more connected during solitary times</h4><figure><img alt="A screenshot of Awedience during an all-company meeting. Brian Chesky, CEO, is in the center and is surrounded by thousands of employees represented by tiny squares. Each square is either a picture of the employee, a color, or a letter that people have arranged to spell words or draw shapes. Emojis are captured emerging from some of the seats." src="https://cdn-images-1.medium.com/max/1024/1*u9pnag1uWfgwnT8tv42I1g.jpeg" /></figure><h3>Introduction</h3><p>This is the story of how Airbnb employees stayed connected during a time they had never felt more apart. In this post, you’ll learn how an idea turned into an internal product that is now a core part of how Airbnb operates.</p><p>When you walk through the doors of an Airbnb office, you feel an energy that’s both inspiring and intimidating. After more than five years with the company, I explain this duality as Airbnb being both incredibly entrepreneurial and aspirational.</p><p>Airbnb company meetings are no different. Brian Chesky and his team keep our all-hands meetings exciting. I know what you’re thinking: “exciting meetings?!” But in all seriousness our all-hands are not just informative, they’re spectacular. Whether it’s drinking a smoothie of dehydrated bugs in solidarity with Engineering or eating spicy chicken wings in a Hot Ones-style Q&amp;A, our meetings are informative, energizing, and engaging. In somber moments, they’re human and heartfelt.</p><p>The pandemic changed that. Feeling connected to the presenter, feeling connected to our peers, or feeling the presenter’s connection to the audience all vanished. Instead, we each separately watched the presenter streaming through a 16:9 rectangle on our laptops. Other people were watching concurrently (one could assume based on the invite) but it couldn’t be <em>felt</em>. Inspired, I set out to solve this, wondering,“<a href="https://en.wikipedia.org/wiki/The_Six_Million_Dollar_Man">we have the technology</a>, why can’t we see and interact with everyone watching live?”</p><h3>Inspiration</h3><p>It’s impossible to know under which circumstances inspiration will find us, and in hindsight it seems perfectly planned.</p><p>Airbnb is a community based on connection and belonging, which is the reason so many of us came to work here. However, in March of 2020, as the spread of COVID-19 forced us to shelter in place, we struggled to find ways to preserve those traits within our company culture. What it meant to feel connected to the world and to each other was being redefined by emojis over video. Video chat helps us stay in communication, but it’s <a href="https://www.youtube.com/watch?v=DYu_bGbZiiQ">comically clunky</a>, dry, and lifeless. If these technologies feel contrived it’s because they are. We all know what an authentic human connection actually feels like, and it’s not that.</p><p>For almost two years before the pandemic hit, I volunteered to help produce Airbnb’s quarterly engineering all-hands. Unbeknownst to me, I was actually studying the question, “how does one make people feel more connected?” Whether it meant hosting an orca-themed relay race across our Portland, Seattle, and San Francisco offices, or projecting live chat in auditoriums to make the audience more engaged, I had been finding ways to foster human connection.</p><figure><img alt="Engineers dressed up in costumes working in pairs at laptops. Ping pong balls are being throw at them from the crowd, while they try to focus on solving engineering problems on the computers." src="https://cdn-images-1.medium.com/max/1024/1*SIXAHtcfgeFICxZ6B11UwA.gif" /><figcaption>Pairs of engineers complete programming challenges, while being pelted with ping pong balls by the audience. This was just one of the many ridiculous segments of Nerds@, the engineering all-hands meeting I voluntarily produced for two years.</figcaption></figure><p>Isolated at home with all these insights into event production, it’s not surprising that, while I watched our CEO, Brian Chesky, address thousands of employees via live stream, my attention drifted to the little number in the top right corner that showed how many other people were watching. These were my colleagues, my friends — some of the most talented people in the world. In our isolation, the only affordance that existed to capture our experience of watching this live stream together was an aggregate count. Instagram and YouTube allowed people to express themselves with emojis and comments during live broadcasts. Internally, we could not.</p><figure><img alt="Photos of various employees at-home workstations. Some photos are laptops on a desk or coffee table. Others are projected onto a TV in a living room. Two of the photos feature pets in the background." src="https://cdn-images-1.medium.com/max/1024/1*t-XAOqp7jvfTDsFhQpSejQ.png" /><figcaption>A look at life before Awedience: everyone watching the same stream at home alone, with the exception of some furry pals.</figcaption></figure><p>An idea started to emerge: supplement the live stream with small, thumbnail-sized videos of all the viewers’ webcams and capture audience sentiment with emojis. It seemed simple enough.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rat4XcYbupe-pho3KM1wqw.jpeg" /><figcaption>The original sketch. Simple, right?</figcaption></figure><p>Yet even this concept proved untenable. I had never built a fully real-time web application and would likely need months to figure out the webcam piece. I didn’t have months. I was only willing to dedicate a few days to see if this idea had merit.</p><p><a href="https://entrepreneurshandbook.co/how-airbnb-founders-sold-cereal-to-keep-their-dream-alive-d44223a9bdab">Cereal Entrepreneurs</a> will attest that ideas are cheap — nobody copies ideas. Only <em>proven</em> ideas get copied, and this idea was definitely not proven. So I started to collaborate with some of my peers and <a href="https://www.linkedin.com/in/stepan-parunashvili-65698932">Stepan Parunashvili</a>, helped us get the ball rolling. “Punt on video for now,” he said, “start with profile pictures from our company directory. Firebase can handle all the real-time stuff, and we already have an internal authentication service. Boom!”</p><p>Stepan continued to offer his support with the initial infrastructure. We needed a name and thought about it for all of a minute. This product was all about the “audience” and “aww” is one sound an audience makes when they’re experiencing something together. Inspiring “awe” was also part of the motivation of this work so we decided on “Awedience.”</p><p>Within a few hours, Stepan had the scaffolding complete. People could sign in and <a href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">“hello world”</a> with other users. Our authentication service played gatekeeper, guaranteeing that the application would have access to an employee’s LDAP username after they signed in. That username enabled me to load a profile picture from our company directory. <a href="https://firebase.google.com/docs/database/">Firebase</a> served as the realtime database, and React sat right on top of it, bound (almost directly) to Firebase events. I could finally focus on my specialty, UI and UX. With an iframe embedded front and center, the UI naturally formed a U-shaped auditorium with virtual seats. When you clicked a seat, your picture would appear, and as you reacted, emojis would float out of your seat for everyone to see. You could also write short messages and they would pop out of your seat too, simulating shouts to the crowd.</p><p>We had built something. Now, would anyone care?</p><h3>Growth</h3><p>Attention is an incredibly valuable resource and there are lots of ways to get it. Attention is commonly bought. Attention can be diverted from other channels. It can even be stolen.</p><p>But attention can also be <em>earned</em>. When people <em>love</em> a product, it not only has true staying power but will grow organically. Therefore, one way to see if people love something new is to say nothing and observe.</p><p>I invited close colleagues to try Awedience for an all-hands to see what would happen. Intuitively, they took a seat, started reacting, and used it throughout the meeting. The feedback was <a href="https://sive.rs/hellyeah">overwhelmingly positive</a>. Awedience didn’t do much but what it did do, it did well enough.</p><figure><img alt="A screenshot of Airbnb’s Slack while Audience was being used for the first time. Some messages say “we’re sitting together,” “love this feature it’s so cute,” “this is the coolest,” and “I noticed you sat next to me.”" src="https://cdn-images-1.medium.com/max/1024/1*DPnN31xW0LgGXbbysUO_fw.png" /><figcaption>A glimpse into our company Slack: first reactions to Awedience at Airbnb.</figcaption></figure><p>At the time, Brian hosted all-company Q&amp;As weekly. I created a calendar event alongside the all-company one with an alternative URL and only invited people that had previously used Awedience. Within a few months, Awedience was so popular that it was offered as a secondary option in the official calendar invites.</p><p>With the increased popularity, it was hard to support Awedience on nights and weekends. I asked my team for time to redirect my focus to Awedience for a few months, and they were supportive. The only request was that I figure out the product’s future by the end of that time and not leave things open-ended. Would it become a part of <a href="https://www.airbnb.com/s/experiences/online">Online Experiences</a>? Would it become a part of another team’s roadmap? We even speculated that it could be a completely new line of business.</p><p>While it was tempting to keep adding functionality, resourcefulness was now the name of the game. Awedience was crashing during peak moments so performance was and still is the most important feature. Before we implemented throttling, we were binding reactions directly to our app state, which triggered a deluge of re-renders:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9cc2af478f93a52dbd1e74dfafeb2e11/href">https://medium.com/media/9cc2af478f93a52dbd1e74dfafeb2e11/href</a></iframe><p>A crude <em>batchedThrottle</em> function reduced renders when users mashed an emoji button:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e7477b5316cc110ea5df6409c5d86694/href">https://medium.com/media/e7477b5316cc110ea5df6409c5d86694/href</a></iframe><p>Later, additional performance gains were found by detaching the React UI from Firebase real-time callbacks. Eventually, reactions would be managed natively without React at all:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/de71ca2d02402ba3df6f5db2228c4141/href">https://medium.com/media/de71ca2d02402ba3df6f5db2228c4141/href</a></iframe><p>There were more affordances I wanted to explore. At sporting events, attendees often hold up signs, flags, or even paint things on their bodies to spell out a message. Replicating this in Awedience was a huge hit. Rather than a profile picture, attendees could now pick a color, letter, or a portion of a graphic to display from their seat. People show up early and coordinate amongst themselves to spell out messages to represent their team, city, or the company. The result is magical. Awedience didn’t make it easy to tell your neighbor to change their seat picture. People were going out of their way to coordinate with one another. Connection between colleagues was happening organically and it was thrilling to see.</p><figure><img alt="A screenshot of Awedience during an all-company meeting. Brian Chesky, CEO, is in the center and is surrounded by thousands of employees represented by tiny squares. The squares spell out words like “insurance,” “human,” or “trust.” There are also a few Airbnb logos being drawn out in 5x5 tiles." src="https://cdn-images-1.medium.com/max/1024/1*DsAVOD_cZgUwAP39X6qQZQ.jpeg" /><figcaption>An earlier version of Awedience where people were spelling words, representing their teams, cities, and the company. Or the CEO’s face.</figcaption></figure><p>Making sure there were enough seats for everyone was also a challenge. Too few seats and some people can’t participate; too many and the auditorium feels empty. To handle this, Awedience does something its skeuomorphic counterpart can’t do: adding seats as needed. This feature felt vital for a product built at a company so focused on belonging. Later, we would improve on this feature by increasing seating density such that almost 1,000 people are visible “above the fold.”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kwLwo5MNpcT8HLk8xq3L7g.gif" /><figcaption>Adding rows of seats to the bottom worked for a long time but limited users from seeing everyone at once. It took the addition of virtual aisles to afford seats being added horizontally without compromising user-generated seat art.</figcaption></figure><p>Self-service features were also prioritized. Brian’s staff immediately wanted to know what content was spurring engagement — a question they hadn’t been able to answer since going remote. Cumulative data from each event was piped into a graphing library for quick and dirty analytics. Similarly, our video production team wanted to be able to create and edit auditoriums without relying on me so that self-service tooling came as well.</p><figure><img alt="A stacked line graph of emojis over time. One line, applause, for example goes up to over 300 reactions for a moment. Later, hearts, spike to nearly 150. Each emoji has its own usage graphed on the chart." src="https://cdn-images-1.medium.com/max/1024/1*ceUehJwN6__G7Q_OIQoZrA.png" /><figcaption>Awedience helps presenters understand exactly which parts of their presentation landed for their audience.</figcaption></figure><p>Later, during a hackathon, we even created applause sound effects that naturally scale up from the sound of a few hands clapping to an uproar based on audience engagement.</p><figure><a href="https://video.airbnb.com/media/t/1_o3egj5jk"><img alt="A video demo of Awedio, in which you can see Brian Chesky being interviewed on CBS while members of the audience applause around him. In addition to seeing the applause emojis rise from the audience, you can also hear the applause generated by the software." src="https://cdn-images-1.medium.com/max/1024/1*N_NlPojNdwUIK83bev6Rug.jpeg" /></a><figcaption>Awedio allows you to hear the audience’s applause reactions.</figcaption></figure><h3>Moments</h3><p>Awedience made Airbnb feel like Airbnb again. There’s now a place where you can see everyone and feel connected to them. It’s become home to celebratory moments and a place where we can sit alongside one another during somber ones.</p><p>When Airbnb announced a cut back to our workforce, there was an all-hands scheduled to honor and appreciate the employees who were leaving. However, with VPN access cut to roughly 2,000 soon-to-be alumni, Awedience was suddenly only accessible to the spared employees. Resident security guru, <a href="https://www.linkedin.com/in/keeleysam">Sam Keeley</a>, and I committed to making Awedience accessible outside of VPN and almost overnight switched authentication to Google IAP. When the founders addressed the company, they invited a standing ovation for our departing peers and Awedience obliged. It’s hard to imagine what kind of impersonal and solitary send-off we would have had without Awedience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*znVI-1UvXbpPEn9WClZzJw.gif" /><figcaption>Hundreds of employees joined our founders in a virtual standing ovation for the members of our team that were let go as a result of the pandemic cut backs.</figcaption></figure><p>In May, in the wake of social action around George Floyd’s murder, the company met to address the Black Lives Matter movement. At the end of this meeting, Brian invited the company to take an 8 minute and 46 second moment of silence.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-7x_Yi1B3DiKxQTq6yv_aA.gif" /><figcaption>Employees join in a somber yet moving virtual moment of silence for George Floyd.</figcaption></figure><h3>Conclusion</h3><p>At Airbnb, Awedience is here to stay and now receives ongoing support and maintenance. In collaboration with our Employee Experience team, we found a home where it would make sense long term. In fact, if this is the kind of work you find interesting, you may even consider joining our team to help us build internal tools to foster connection — <a href="https://careers.airbnb.com/positions/">we’re hiring</a>!</p><p>I feel fortunate to work for a company that creates space to bring these types of projects to life. Airbnb is an inspiring place — the combination and culmination of a rigorous entrepreneurial spirit and an ongoing commitment to outdo the status quo. That’s the type of environment you need for ideas like Awedience to flourish.</p><p>Awedience is more than just a triumph of passion and creativity. The spark of the idea was just that: a spark. In the words of <a href="https://twitter.com/richardbranson/status/264067714266587136">Richard Branson</a>, “opportunities are like the buses — there’s always one coming.” Without the help and support of many amazingly talented colleagues, there would literally be nothing to write about.</p><p>What makes Awedience awesome is the people. Big ideas are rarely the consequence of one person’s ideas or effort. It takes a lot of people to do incredible things.</p><h3>Acknowledgements</h3><p>To Stepan Parunashvili for fueling the fire and bootstrapping the infrastructure that got Awedience going. Without you, it would not have been possible. Thank you.</p><p>To Sam Keeley for enabling and evolving Awedience access for the entire company.</p><p>To Joe Gebbia for creating some air space for Awedience to grow and evolve.</p><p>To Byoung Bae, Allison Frelinger, Darrick Brown, and Judd Antin of my former team for taking a gamble on Awedience with me.</p><p>To Liz Kleinman and Beth Axelrod for creating a role for me to continue this work.</p><p>To Shawdi Ilbagian Hahn, Dave O’Neill, Kylie McQuain, Kelly Bechtel, Kate Walsh, Benny Etienne, Carrie Kissell, Alyce Thompson, John Lawrence, and Samantha Eaton for your collaboration and partnership in keeping the company engaged and connected.</p><p>To Cory Boldt, Steven McNellie, Garrett McGrath, Alex Lacayo, John Espey, and Scott Ethersmith for your help and creativity on the technical productions.</p><p>To Jenna Cushner, Ortal Yahdav, Lucille Hua, Christian Williams, Shawn Terasaki, Brian Wallerstein, Ben Muschol, Mike Fowler, Jason Goodman, Caty Kobe, Joe Lencioni, Nicolas Haunold, Christian Baker, Alan Sun, and Jacqui Watts for your early contributions and feedback.</p><p>To Kevin Swint, Danielle Zloto, Christine Berry, Federica Petruccio, and Consuelo Hernandez for going above and beyond to try Awedience with Online Experiences and the powerful insights that were created as a result.</p><p>To Nicholas Roth, Izzy Rattner, Jonathan Lieberman, Stephen Gikow, Steve Flanders, Lonya Breitel, Alan Shum, Brian Savage, Veronica Mariano, Allie Hastings, Alica Del Valle, Rajiv Patel, and Emily Bullis for your legal support in protecting Awedience’s intellectual property and making external partnerships possible.</p><p>To Sarah Baker for always rallying people together to create seat artwork.</p><p>To Gaurav Mathur, Hope Eckert, Sean Abraham, Jessie Li, Vaithiyanathan Sundaram, Andy Yasutake, Virginia Vickery, Jonathan Rahmani, Andrew Pariser, Sunakshi Kapoor, Diane Ko, Biki Berry, Francisco Diaz, Erik Ritter, Tony Gamboa, Mohsen Azimi, Bruce Paul, Omari Dixon, Sonia Anderson, CJ Cipriano, Chihwei Yeh, Arie Van Antwerp, Victor De Souza, Sam Shadwell, Deanna Bjorkquist, Jenna Cushner, Richard Kirk, Jake Silver, Alex Rosenblatt, David He, LA Logan, Ryan Booth, Pistachio Matt, Melanie Cebula, Brian Morearty, and Victor Caruso for your participation and support!</p><p>To Stephanie Wei, Micah Roumasset, Ryland Harris, Waylon Janowiak, and Ben Arnon for your willingness to try Awedience outside of Airbnb.</p><p>To Jerry Chabolla, Nicholas Schell, Ryan Jespersen, Sergio Garcia Murillo, Wes Dagget, and the entire team at Millicast for enabling real-time streaming.</p><p>To Brett Bukowski, Cara Moyer, Nicki Williams, Dylan Hurd, and Lauren Mackevich for encouragement and support in writing this blog post.</p><p>And lastly to Danee Chavez for powering the light bulb. 💡</p><p><strong>Interested in working at Airbnb? Check out these open roles:</strong></p><p><a href="https://careers.airbnb.com/positions/3714489/">Senior Software Engineer, Airfam Products</a></p><p><a href="https://careers.airbnb.com/positions/3955056/">Staff Technical Program Manager, Insurance Platform</a></p><p><a href="https://careers.airbnb.com/positions/3988445/">Staff Automation Engineer, BizTech Global Ops</a></p><p><a href="https://careers.airbnb.com/positions/4003012/">Operations Engineer</a></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ebf66ee6af0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/hacking-human-connection-the-story-of-awedience-ebf66ee6af0e">Hacking Human Connection: The Story of Awedience</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Measuring Latency Overhead with Own Time]]></title>
            <link>https://medium.com/airbnb-engineering/measuring-latency-overhead-with-own-time-f4373f586ca?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f4373f586ca</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[service-mesh]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Jimmy O’Neill]]></dc:creator>
            <pubDate>Mon, 21 Mar 2022 20:53:37 GMT</pubDate>
            <atom:updated>2022-03-21T23:05:47.028Z</atom:updated>
            <content:encoded><![CDATA[<p>by: <a href="https://www.linkedin.com/in/jimmyoneill">Jimmy O’Neill</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JQ6ZtmHuSu-86FXV1i7Lmg.jpeg" /></figure><h4>A new metric to quantify the latency overhead of our Viaduct framework</h4><p><a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, a GraphQL-based data-oriented service mesh, is Airbnb’s paved road solution for fetching internal data and serving public-facing API requests. As a unified data access layer, the Viaduct framework handles high throughput and is capable of dynamically routing to hundreds of downstream destinations when executing arbitrary GraphQL queries.</p><h3>Performance Challenges in Viaduct</h3><p>Viaduct’s role as a data access layer puts it in the critical path of most activity on Airbnb. This makes runtime performance of utmost importance as overhead in the framework will apply universally and can have a multiplicative effect. At the same time, Viaduct accepts arbitrary queries against the unified data graph. In practice, this amounts to many thousands of heterogeneous queries in production, each of which is capable of making an arbitrary number of downstream and often concurrent calls during the course of query execution.</p><p>This presented a challenge for us. Runtime overhead in Viaduct is crucial for us to monitor and improve, but we did not have a good measure for it. Metrics on end-to-end query latencies are confounded by the performance of downstream services, making it difficult to accurately judge the effect of a performance intervention in Viaduct. We needed a metric that <em>isolates </em>the performance impact of Viaduct changes from the performance impact of downstream services.</p><h3>Defining Own Time</h3><p>To do this, we created a metric called “own time”. Own time measures the portion of a request’s wall-clock time that occurs when there are zero downstream requests in flight. The following is pseudocode to compute own time given a root request time span and a set of downstream fetch time spans:</p><pre>def calculateOwnTime(rootSpan, fetchSpans):<br>  ownTime = 0<br>  maxEndTimeSoFar = rootSpan.startTime<br>  sortedFetchSpans = fetchSpans sorted by increasing start-time<br>  for fetchSpan in sortedFetchSpans:<br>    if (maxEndTimeSoFar &lt; fetchSpan.startTime)<br>      ownTime += (fetchSpan.startTime - maxEndTimeSoFar)<br>    maxEndTimeSoFar = max(maxEndTimeSoFar, fetchSpan.endTime)<br>    ownTime += (rootSpan.endTime - maxEndTimeSoFar)<br>  return ownTime</pre><p>The own time metric allows us to focus on aspects of Viaduct’s overhead that are clearly unrelated to downstream service dependencies. While it does not capture <em>all </em>aspects of Viaduct’s overhead, we’ve found it captures enough to be a valuable indicator of overhead costs.</p><p><strong>Examples</strong></p><p>In the trivial case where all downstream calls are made serially, own time is a simple span difference of the root operation span and the sum of the downstream time spans.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-fwKbBCLNP-GkWJP" /></figure><p>When there are multiple downstream calls, they may be made fully or partially in parallel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DVUD3CPpix-Kt6h3" /></figure><p>In this example, the downstream calls happen partially in parallel, and the resulting own time value doesn’t include the time that any downstream request is in flight, parallel or not.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ezhwxOXWnSHMf1wj" /></figure><h3>Identifying and Reducing Runtime Latency</h3><p><strong>Measuring the influence of CPU vs. I/O on request latency</strong></p><p>Normalizing operation own time by overall operation latency gives us an estimate of how CPU-bound vs. I/O-bound an operation is. We call this the “own time ratio” of a query. For example, the Viaduct operation graphed below had an own time ratio of 20%, indicating that 20% of the request runtime in Viaduct was spent with no downstream request in flight. After deploying an internal Viaduct performance improvement, this operation’s own time ratio dropped to 17%, since Viaduct overhead improved while downstream performance remained constant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wy6PADUW1fd6-F-X" /><figcaption>This graph shows a day-over-day reduction in own time ratio for a Viaduct operation after a runtime overhead improvement was deployed.</figcaption></figure><p>A low own time ratio for an operation indicates that the biggest overall latency gains will likely be found by optimizing downstream services, not Viaduct. A high own time ratio indicates that the meaningful latency gains can come from optimizing internal Viaduct runtime for the operation. When making such optimizations for the sake of one operation, we can also use own time ratios across all operations, and especially low-ratio ones, to ensure we aren’t introducing a regression more broadly.</p><p><strong>Quantifying the impact of query size on runtime overhead</strong></p><p>Viaduct users reported that large queries were running slower than expected, attributing the slow execution to Viaduct overhead. Before own time, we had no metrics to assess such reports. After introducing own time, we had a starting point, but we needed to refine the metric further for this use case.</p><p>One would expect own time to increase as the number of fields returned by an operation increases. But was that a reasonable expectation? We found that normalizing own time by the count of fields returned by an operation yields a metric that more usefully indicates, across a heterogeneous set of operations, when own time is excessive. We defined field count to include both object fields and individual array elements.</p><p>The following graph shows that there is indeed an overall relationship between own time and field count across our set of operations, as well as some outliers that have unusually high own-time-to-field-count ratios.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MoOIoSyyD2fDoUog" /><figcaption>This chart plots the number of fields resolved against own time for unique GraphQL operations.</figcaption></figure><p>This relationship between field count and own time encouraged us to focus on framework logic that runs on every field for all operations, rather than other parts of the codebase. Through some CPU profiling, we were able to quickly identify bottlenecks. One resulting improvement was a change to our multithreading model for field execution, which decreased own time for all operations by 25%.</p><p><strong>Quantifying the impact of internal caching on runtime overhead</strong></p><p>Viaduct saw another performance issue. For some operations, latency appeared to vary an unusual amount, even between identical requests. Here again, we used own time to guide our investigation into root causes.</p><p>Viaduct relies on a number of internal caches to ensure that execution is fast, such as a cache for parsed and validated GraphQL documents. Own time metrics indicated that Viaduct runtime overhead, not downstream service dependencies, was causing the variance in latencies. We theorized that cache misses were the culprit. To test this theory, we instrumented our caches to report whether any lookup miss occurred during an operation execution and attached this hit/miss status to our own time metric output. This allowed us to report on own time by cache hit/miss status on a per-cache, per-operation basis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E3j_g3qCzFxhw_IN" /></figure><p>Adding this information to own time allowed us to both confirm our theory and quantify the potential benefit of implementing a solution, such as additional cache warming or moving in-memory caches to distributed caches, prior to committing actual engineering resources. Migrating the in-memory cache that stores the validation state of GraphQL documents to a distributed cache reduced miss rates. This had a significant impact on tail latencies, especially for low QPS operations that were more likely to encounter cold cache states.</p><h3>Setting Runtime Overhead Goals</h3><p>Establishing the own time metric normalized by field count ended up being a great way to account for changes in query patterns. Thus, we now use this metric, aggregated across all operations, to set framework-level performance targets that are isolated from changes in client query patterns. In particular, after measuring the base rate of normalized own time at the beginning of a quarter, we set a goal to improve normalized own time by a specific percentage quarter-over-quarter.</p><p>We also use this metric, aggregated on a per-operation basis, to let operation owners know how their operation overhead compares to the rest of the system.</p><h3>Integrating Own Time Into The Release Cycle</h3><p>To quantify the runtime performance impact of a change, we can set up experiments where two staged control and treatment applications receive identical production traffic replay. We can then graph the difference in own time between them. This allows us to quantify the impact of various framework interventions on runtime overhead and measure each intervention’s impact against our performance goals.</p><p>While replay experiments help us to assess the potential runtime improvements of a change on a limited set of use cases, narrowly-targeted optimizations can lead to broader performance regressions may still happen accidentally. To guard against such regressions, we leverage an automated canary analysis process before deployment. A canary instance and baseline instance receive identical production replay traffic for a period of time, and large discrepancies between them can automatically stop the deployment process. By inspecting the own time difference between the canary and baseline instances, we can identify unexpected performance regressions prior to the regression making it to production.</p><p>In addition to automated canary analysis, graphing day-over-day, week-over-week and month-over-month own time in production shows us long-term isolated performance trends and allows us to bisect any regressions that make it to production.</p><h3>Limitations and Future Work</h3><p>By ignoring what Viaduct does during all downstream calls, own time does not account for possible optimizations to a call pattern of the downstream requests themselves. For example, a request execution may be sped up by increasing concurrency of downstream calls or removing some calls altogether.</p><p>Although own time gives a measure of wall-clock runtime service overhead, it does not say <em>what</em><strong> </strong>is causing the overhead or how to best improve it, which will vary across operations in a GraphQL server. However, tracking downstream request spans in memory provides baseline data that can be enriched with other metadata and further filtered to measure the contribution of application-specific activity to own time.</p><p>Tracking down the root cause of unexpected own time changes or understanding why an operation is an own time outlier requires manual inspection and sometimes additional one-off measurements, which take valuable engineering time. We can automate the first steps in these investigations by measuring the contribution of various parts of the application to own time. This would speed up root cause analysis and limit time spent manually profiling CPU usage.</p><h3>Conclusion</h3><p>Own time has allowed us to isolate the runtime performance characteristics of Viaduct, our GraphQL-based data-oriented service mesh. Using own time, we can precisely measure the production runtime performance effects of application changes, set downstream-independent performance goals, and measure our long-term progress against those goals for an arbitrary underlying application. Enriching own time with application-specific data, such as fetched field counts and cache hit/miss states in Viaduct, gives us an overarching view of the relationship between an application’s state and its runtime performance characteristics.</p><h3>Acknowledgements</h3><p>Thanks to everyone who made this work possible by supporting the Viaduct framework, brainstorming ideas and providing feedback on this post, including Aileen Chen, Yuchun Chen, Zoran Dimitrijevic, Adam Miskiewicz, Parth Shah, Raymie Stata, and Kim Strauch.</p><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f4373f586ca" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/measuring-latency-overhead-with-own-time-f4373f586ca">Measuring Latency Overhead with Own Time</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Artificial Counterfactual Estimation (ACE): Machine Learning-Based Causal Inference at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/artificial-counterfactual-estimation-ace-machine-learning-based-causal-inference-at-airbnb-ee32ee4d0512?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ee32ee4d0512</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[causality]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[causal-inference]]></category>
            <dc:creator><![CDATA[zhiying gu]]></dc:creator>
            <pubDate>Wed, 16 Mar 2022 19:34:04 GMT</pubDate>
            <atom:updated>2022-03-16T19:34:04.610Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EQ_C2aqZE91XHEJ4" /></figure><p><strong>By:</strong><a href="https://www.linkedin.com/in/zhiying-gu-2293a353/"><strong> Zhiying Gu</strong></a><strong>, </strong><a href="https://www.linkedin.com/in/qianrongwu/"><strong>Qianrong Wu</strong></a></p><h3>Summary</h3><p>What if you wanted to measure the impact of a change to your business, but it was not possible to run a randomized controlled experiment? That’s exactly the problem we faced when measuring the benefit of a new tool used by Airbnb operations to automate part of their workflow. Due to organizational constraints, it was simply not possible to randomly assign the tool to operations agents; even if we could make random assignments, the sample sizes were too small to generate sufficient statistical power. So what did we do? We imagined a parallel universe in which the operations agents who did not use the new tool were identical in all respects to those who did–in other words, a world in which the assignment criteria were as good as random. In this blog post, we explain this new methodology, called <strong>ACE (Artificial Counterfactual Estimation)</strong>, which leverages machine learning (ML) and causal inference to artificially reproduce the “counterfactual” scenario produced by random assignment. We’ll explain how this works in practice, why it is better than other methods such as matching and synthetic control, and how we overcame challenges associated with this method.</p><h3>The Non-Randomizable Operations Problem</h3><p>There are two key assumptions undergirding randomized controlled <a href="https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7">experiments</a> (often referred to as “A/B tests”):</p><ol><li>The treatment and control groups are similar. When you have similar groups, outcomes are independent of group attributes such as age, gender, and location, meaning that any difference between the groups can be attributed to a treatment that was received by one group but not the other. In statistical terms, we assume that we have controlled all confounders, thereby reducing the bias of our estimates.</li><li>The sample sizes are sufficiently large. Large sample sizes serve to reduce the magnitude of chance differences between the two randomized groups, giving us confidence that the treatment has a true causal impact. In technical lingo, we assume that we have reduced the variance of our estimates enough to give us appropriate statistical power.</li></ol><p>Given the need for similar groups and large sample sizes when running A/B tests, any organization with operational teams presents challenges. To start, there are general concerns about unfairness and disruptive experience when running randomized experiments on operations agents. Second, the operational sites are located in different countries with varied amounts of employees, skill levels and so on so we cannot simply assign certain geographies to treatment and some to control without introducing apples-to-oranges comparison, which will lead to bias of the measurement. Finally, we have millions of customers but not millions of operations agents, so the sample size for this test is always going to be much smaller than that of other experiments.</p><h3>ACE to the Rescue</h3><p>With the ACE (<strong>Artificial Counterfactual Estimation)</strong>, we have the next best thing to a randomized experiment. The trick is to achieve <strong>bias reduction and variance reduction</strong> at the same time using a machine learning-based causal impact estimation technique.</p><p>Causal inference is a process of estimating the counterfactual outcome that would have occurred had the treated units not been treated. In our case, we want to know how productive our operations agents would have been, had they not used the new workflow automation tool. There are many ways to construct such a counterfactual outcome, but the most common methods are:</p><ul><li>Use the control group from a randomized controlled experiment (unfortunately, is often times not possible in our case)</li><li>Construct a group that is similar to the treated group using matching methods such as Propensity Score Matching (Weighting), <a href="https://gking.harvard.edu/files/political_analysis-2011-iacus-pan_mpr013.pdf">Coarsened Exact Matching</a>, or <a href="https://web.stanford.edu/~jhain/Paper/PA2012.pdf">Entropy Balancing</a></li><li>Construct the counterfactual outcome with time-series predictions (e.g., <a href="https://research.google/pubs/pub41854/">Causal Impact Model</a>)</li><li>Construct the counterfactual outcome as the weighted average of all non-treated units (<a href="https://economics.mit.edu/files/11859">Synthetic Control</a>, <a href="https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3">Generalized Synthetic Control</a>)</li></ul><p>We can construct the counterfactual outcome by ML prediction using both confounding and non-confounding factors as features. In a nutshell, we use a holdout group (i.e., the group not treated)) to train an ML model that predicts the counterfactual outcome being not treated in the post-treatment period. We then apply the trained model to the treated group <strong>for the same period. </strong>The<strong> </strong>predicted outcome serves as the counterfactual (new control) representing the imagined scenario in which the treatment group had not been treated in the post-treatment period<strong> </strong>(<em>Y’’</em><strong> </strong>in the equation below).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/194/1*NKttYN8--2qKUiGFtU6b3w.png" /></figure><p><strong>In the equation above,<em> t </em>is the difference between the observed treatment group outcome </strong>(Y)<strong> and the predicted outcome </strong>(<em>Y’’</em>). It represents <strong>a <em>naive</em> estimate of the impact </strong>because it<strong> is <em>biased</em>. </strong>The following graph illustrates ACE at a high level. It has the following steps as illustrated in Figure 1:</p><ol><li>We train a machine learning model using data from a hold out group, i.e. a group without treatment.</li><li>We apply the trained model on the treatment group to obtain the predicted outcome had we not applied treatment on this group.</li><li>The difference between the actual and the predicted outcome for the treatment group is the estimated impact.</li></ol><p>We will flesh out the detailed challenges in a later section before its application.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1007/0*GymYMQAYRWRjpbf-" /><figcaption>Figure 1: Estimation Process</figcaption></figure><h3>Challenges of ACE, and Solutions</h3><p>There are two major challenges in developing ACE: bias estimation and construction of confidence intervals.</p><h3>Challenge 1: Bias estimation</h3><p>The predicted outcome <strong><em>Y’’</em> </strong>from the machine learning models is often biased for two reasons, causing the estimated causal impact <em>t</em> to also be biased (see <a href="https://academic.oup.com/ectj/article/21/1/C1/5056401">Chernozhukov et. al. (2018)</a>). The two reasons for bias are 1) regularization, and 2) overfitting.</p><p>The figure below shows the ML model prediction error on 100 synthetic A/A tests, for which the estimated impact should always be zero. Clearly, however, the distribution of estimates is not centered around zero. The average prediction error is actually 2%, meaning that the ML prediction <em>Y’’</em> is, on average, overestimated by 2%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/492/0*AQMF97y43O2klwcr" /><figcaption>Figure 2: Prediction Bias</figcaption></figure><h3>Challenge 2: Construction of Confidence Intervals</h3><p>Unlike in a traditional t-test for A/B testing, there is no analytical solution for confidence intervals when we are doing ACE. As a result, we have to construct empirical confidence intervals for the estimates. To address these two challenges, we took an empirical approach to removing bias from the prediction and then constructed our confidence intervals based on that same empirical approach.</p><p>In ACE, we use A/A tests both for debiasing and for constructing confidence Intervals.</p><h3>Solution to Challenge 1: Debias</h3><p>One natural idea is that if we can confidently estimate the magnitude of the bias, we can simply adjust the prediction by the estimated bias. The estimation then becomes:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/408/1*tf49yKKBFKPqxqyxkTa4EA.png" /></figure><p>Practitioners can freely choose any machine learning models to use — <em>f(X) </em>— for the prediction of <em>Y’’.</em> Figure 2 shows a 2% bias for 100 A/A samples. The question is: can we say the true bias is 2%? If we can verify that the bias is systematically 2% (i.e., consistent across different A/A samples during the same periods and repeatable across different time periods), we can say bias = 2%. Figure 3 shows the repeatability of the bias estimation over time. The estimates are always biased upwards and the average estimates of bias are around 2%. Figure 4 shows the average prediction error after removing the bias (2%). With bias correction, the distribution of estimated impact is centered around zero.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3VxewhXE-I6q6e3B" /><figcaption>Figure 3: the stability of bias estimation</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/846/0*BIyfw_s9Jo80_zya" /><figcaption>Figure 4: Distribution of impact estimates based on A/A after bias correction</figcaption></figure><h3>Solution to Challenge 2: Construct Empirical Confidence Intervals</h3><p>We can use data from A/A tests to construct empirical confidence intervals and p-values.</p><ul><li>Empirical confidence interval: to be more specific, the 95% confidence interval is constructed by looking at the distribution of 100 bootstrapped A/A samples. Given that we know the true differences of A/A tests are 0, and if 5% of estimated impacts from 100 A/A tests are outside [-0.2, 0.2] range, then we know the 95% confidence interval is [-0.2, 0.2].</li><li>Empirical p-value: we can estimate Type I error via A/A tests estimated from ML models as follows. Suppose we estimated a 3% of the impact for the treatment. P-value is to estimate the probability of obtaining an estimate that is outside [-3%, 3%] when the null hypothesis is true — there is no impact. This probably is estimated with the empirical distribution of iterative A/A tests. If the probability is 1%, we will conclude that we have at least 98% (i.e 100% — (1%*2)) confidence that the alternative hypothesis — the impact is not zero — is true.</li></ul><h3>Validation</h3><p>To validate if ACE can accurately measure the impact, we further ACE to the data from a large scale randomized A/B data and compared ACE results with the A/B tests results. The result from the A/B test is considered as ground truth for validation because A/B testing is the gold standard for measurement. The results are nearly identical.</p><h3>Advantages of ACE</h3><p>There are several advantages of ACE over other estimation methods:</p><ul><li>It is flexible in the choice of estimation model. We can freely choose any cutting-edge ML models to achieve desired level of accuracy, based on various use cases and data properties..</li><li>Its validity and accuracy can be easily assessed during the design phase of the measurement plan by conducting A/A tests.</li><li>It can be applied on both experimental data for variance reduction and on non-experimental data for bias correction as well as for variance reduction.</li><li>For experimental data:<br>- It is less prone to biases compared to regression adjustments. <br>- It has more power compared to stratification when the ML model has a good performance. <br>- It estimates the magnitude of the impacts instead of only the existence of the impacts compared to rank tests.</li></ul><p>You’ll recall that we applied ACE to estimate the incremental benefit of a tool that helps operations agents to automate part of their workflow. We generated p-values for three different measurement methodologies: (1) classic t-test; (2) <a href="https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">non-parametric rank test</a> and (3) ACE non-parametric test based on the empirical confidence interval we described in the previous section. The following is a performance comparison for t-test, rank test, and ML-based methods for the same sample size, in particular, when sample size is small when we try to conduct inference with classic t-test as we do in A/B testing.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EnfVbOcLKkcDHoEa" /></figure><h3>Recap</h3><p>In this blog post, we explained how one can leverage ML for counterfactual prediction, using an estimation problem for the efficacy of an agent tool as our motivating example.</p><p>Combining statistical inference and machine learning methods is a powerful approach when it’s not possible to run an A/B test. However, as we have seen, it can be dangerous to apply ML methodologies if intrinsic model bias is not addressed.. This post outlined a practical and reliable way to correct for this intrinsic bias, while minimizing Type I error relative to competing methods.</p><p>Currently, we are working to turn our code template into an easy-to-use Python package that will be accessible to all data scientists within the company.</p><p>If this type of work interests you, check out some of our related positions!</p><p><a href="https://careers.airbnb.com/positions/3859241/">Senior Data Scientist — Payments</a></p><h3>Acknowledgments</h3><p>Thanks to Alex Deng and Lo-hua Yuan for providing feedback on the development of ACE and spending time reviewing the work. We would also like to thank Airbnb Experiment Review Committee Members for feedback and comments. Last but not least, we really appreciate Joy Zhang and Nathan Triplett for their guidance, and feedback and support from Tina Su, Raj Rajagopal and Andy Yasutake.</p><h3>References</h3><ul><li>Stefano M. Iacus, King, Gary, Giuseppe Porro, 2017. <a href="https://gking.harvard.edu/files/political_analysis-2011-iacus-pan_mpr013.pdf">Causal Inference without Balance Checking: Coarsened Exact Matching</a>, <em>Political Analysis.</em></li><li>Jens Hainmueller, 2012, <a href="https://web.stanford.edu/~jhain/Paper/PA2012.pdf">Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies</a>, <em>Political Analysis.</em></li><li><a href="https://research.google/people/KayBrodersen/">Kay H. Brodersen</a>, Fabian Gallusser, Jim Koehler, <a href="https://research.google/people/NicolasRemy/">Nicolas Remy</a>, Steven L. Scott, 2015. <a href="https://research.google/pubs/pub41854/">Inferring causal impact using Bayesian structural time-series models</a>, <em>Annals of Applied Statistics</em>.</li><li>Alberto Abadie, Alexis Diamond, and Jens Hainmueller, 2010. <a href="https://economics.mit.edu/files/11859">Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program</a>, <em>Journal of the American Statistical Association.</em></li><li>Yiqing Xu, 2017.<a href="https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3">Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models</a>, <em>Political Analysis.</em></li><li>Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins, 2018. <a href="https://academic.oup.com/ectj/article/21/1/C1/5056401">Double/debiased machine learning for treatment and structural parameters</a>,<em> The Econometrics Journal.</em></li></ul><h3>Further Reading on Similar Topic</h3><ul><li><a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">How Airbnb Measures Future Value to Standardize Tradeoff</a></li></ul><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee32ee4d0512" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/artificial-counterfactual-estimation-ace-machine-learning-based-causal-inference-at-airbnb-ee32ee4d0512">Artificial Counterfactual Estimation (ACE): Machine Learning-Based Causal Inference at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rebuilding Payment Orchestration at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/rebuilding-payment-orchestration-at-airbnb-341d194a781b?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/341d194a781b</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[payments]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Bryon Ross]]></dc:creator>
            <pubDate>Thu, 24 Feb 2022 19:47:55 GMT</pubDate>
            <atom:updated>2022-02-24T19:48:41.588Z</atom:updated>
            <content:encoded><![CDATA[<h4>How we maintained reliable money movement while migrating Airbnb’s payment orchestration system from the legacy monolithic application to a service-oriented architecture</h4><p><strong>By:</strong> <a href="https://www.linkedin.com/in/bryon-ross/">Bryon Ross</a>, <a href="https://www.linkedin.com/in/feifeng-yang-339b8b33/">Feifeng Yang</a>, <a href="https://www.linkedin.com/in/sophie-behr-6874b734/">Sophie Behr</a>, <a href="https://www.linkedin.com/in/johnsont/">Theresa Johnson</a>, <a href="https://www.linkedin.com/in/xin-lin-39527b58/">Xin Lin</a>, <a href="https://www.linkedin.com/in/yunjincho/">Yun Jin Cho</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xkVmheTk8AghzmfFEJuIKg.jpeg" /></figure><h3>Introduction</h3><p>Airbnb’s payment orchestration system is responsible for ensuring reliable money movement between hosts, guests, and Airbnb. In short, guests should be charged the right amount at the right time using their selected payment methods; hosts should be paid the right amount at the right time to their desired payout methods. For historical reasons, Airbnb’s billing data, payment APIs, payment orchestration, and user experiences were tightly coupled with the concept of a reservation for a stay. Unfortunately, this meant that a payment-related feature for stays had to be rebuilt for other products — for example, Airbnb Experiences — and each implementation may have its own product-specific quirks. As you can imagine, this approach is neither scalable nor easy to maintain.</p><p>For several years, Airbnb has been migrating away from our monolithic Ruby on Rails application toward a service-oriented architecture (SOA). This migration has been discussed extensively in several Airbnb <a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b">tech</a> <a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-2-142be1c5d506">blog</a> <a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-3-ac6d4972fc2d">posts</a>. We will gloss over some of the technical discussions common to those migrations and instead focus on some of the aspects that were unique to migrating our payments systems. While many teams at Airbnb chose to create a one-to-one replacement when migrating to SOA, the payments organization instead decided to use it as an opportunity to fundamentally redesign our services to provide a sound technical foundation for future growth. As a consequence of this decision, the migration process took longer to complete than a more straightforward one-to-one replacement.</p><h3>Why Redesign?</h3><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FMssx8PleeYc%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMssx8PleeYc&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FMssx8PleeYc%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/20c047e574ac84523c61f08a3e5c51b0/href">https://medium.com/media/20c047e574ac84523c61f08a3e5c51b0/href</a></iframe><p>As Brian shared in the video above, support for on-platform payments has played a critical role in establishing trust among Airbnb’s hosts and guests. Airbnb has grown significantly since our first payment system was created over a decade ago and, with that growth, the scope and scale of payments at Airbnb have also grown and changed. Many of the original payment models were tied closely to reservations for a stay. This made sense in the early days of Airbnb as there was only one product, and the engineers working on payments at that time did an excellent job developing a solution that solved the needs of guests and hosts. While these original models used for payments have proven extremely versatile and powerful, this tight coupling between stays and payments has led to increased complexity when adding new products like Experiences or features like the Resolution Center.</p><p>When planning for the SOA migration, Airbnb’s payments teams made a bold decision to fundamentally redesign the payments system. Our goal was to create a payment platform that would allow teams across Airbnb to quickly, easily, and safely integrate new features and products with payments. It’s not feasible to list all of the enhancements in a single blog post, so this post will focus on some design highlights affecting the new payment orchestration system: idempotency, platformization, and data immutability.</p><h4>Idempotent Orchestration</h4><p>As discussed in an <a href="https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb">earlier blog post</a>, idempotency is a common technique to maintain consistency among distributed services. The new payment orchestration system was designed around Orpheus (the idempotency framework described in that post). Every major workflow is divided into a directed acyclic graph (DAG) of retryable idempotent steps, each with well-defined behavior. This allows the payment orchestration layer to maintain eventual consistency with other key services (such as the payment gateway layer and product fulfillment services). This approach has led to five 9s (99.999%) of consistency for payments.</p><p>The idempotency framework works well for both synchronous and asynchronous communication between services. For asynchronous communication, payments services primarily use a Kafka-based message bus to send “events” to one another. Event processors use the idempotency framework to enhance the at-least-once guarantee of Kafka into an exactly-once guarantee. The transactional integrity analysis tools described in <a href="https://medium.com/airbnb-engineering/measuring-transactional-integrity-in-airbnbs-distributed-payment-ecosystem-a670d6926d22">this post</a> provide an additional layer of confidence by ensuring consistency between events and transactional data sources.</p><h4>Product-Agnostic Platform</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wXTahHCWmRKVpdsQeFwa3w.png" /><figcaption>The payments SOA migration decoupled product fulfillment, payment orchestration, and pricing</figcaption></figure><p>A significant disadvantage of our legacy payment data models is that they were closely tied to a single product, reservations for stays. For this reason, our new payment orchestration service was intentionally architected to avoid tightly coupling the payments system to any particular product. Instead, the new orchestration layer was designed around generic payment-specific workflows (e.g., validation, payment processing, financial reporting) with payment-specific logic and product-specific logic isolated from one another, with the exception of a few well-defined integration points. When combined with the generic billing and pricing APIs described in <a href="https://medium.com/airbnb-engineering/scaling-airbnbs-payment-platform-43ebfc99b324">this blog post</a>, this approach allows new products to integrate quickly and easily with existing generic payment flows, drastically reducing both engineering effort and time to delivery. Additionally, as new features are added to the payment systems, these features can be easily adopted by other products.</p><h4>Data Immutability</h4><p>Immutable data is easier to understand, audit, and reconcile. All of the new payment services were built around the idea of data immutability. For payment orchestration, data immutability manifests in two major forms: persistent events and versioning. Events are naturally append-only. It is the responsibility of the event consumer to determine if a new event represents a modification to an existing event. When an existing product is altered (e.g., adding another night to a stay), the modifications to the payment orchestration plan are modeled as a new version in a sequence of plans for that product. The combined information from all the versions provides a complete history of the intended and actual money movement related to that product.</p><h3>A Phased Migration</h3><p>Various teams at Airbnb took different approaches to the migration towards a service-oriented architecture (SOA). Many teams chose to migrate functionality in small blocks, replacing the legacy implementation with an equivalent SOA one. Generally, with this approach, the existing system would be broken down into discrete, cohesive, functional blocks. Each block could be migrated mostly independently of the others. The behavior of each block would be well defined and the result could be trivially compared across both systems to ensure consistent results.</p><p>The Airbnb payments organization took a different approach for the migration of the various payments systems. Instead of small functional blocks, the migration for the payments systems was broken down into four major phases: Pricing, Payouts, Bookings, and Data Migration. The Pricing phase remodeled each of the product-specific pricing models into a generic model that could be used across all Airbnb products. The Payouts and Bookings phases fundamentally redesigned the way that money movement is orchestrated at Airbnb to more easily support new products, features, and business needs. The majority of the work related to payment orchestration was contained within these phases. The Data Migration phase migrated existing bookings from the legacy system to SOA, allowing the legacy system to be wound down and deprecated.</p><p>Within each phase, the migration was divided into smaller migrations, usually by feature or product. For example, in the Bookings phase, bookings for stays were migrated independently from bookings for experiences. When reasonable, those subphases were further broken down as well. The migration of bookings for stays was subdivided into over 30 milestones based on characteristics of the bookings. The relatively small scope of each milestone allowed engineers and data scientists to thoroughly test and validate each set of migrations. Additionally, the relatively independent nature of each milestone allowed many of them to be completed in parallel.</p><h3>Maintaining Two Systems</h3><p>The new payment orchestration system introduced a fundamentally redesigned data model based around the concept of a bill. Unlike the legacy model, the new data model is not tied to any specific product, but rather focuses on being sufficiently powerful, extensible, and generic to be useful for existing and future Airbnb products. One important consequence of fundamentally redesigning the payment data model was that it became non-trivial to convert from one data model to another.</p><p>In general, historical bookings and payouts were not moved from one system to another as part of the initial migration process. Rather, new bookings and payouts would be routed to SOA if they were deemed eligible. Otherwise, they would continue to be routed to the legacy system. Throughout most of the migration process, existing bookings would continue to proceed through their lifecycle in the legacy monolithic system. Only at the tail end of the migration were active bookings transitioned from the legacy system to SOA. As a result, engineering teams needed to maintain two parallel payment orchestration systems throughout virtually the entire migration process.</p><p>Most consumers of payments data don’t actually care whether the data is stored in the legacy or SOA system; they just want the data. In order to provide an easy and consistent experience for those client services, a new transformation layer was built to transparently retrieve data from the correct underlying source and to seamlessly convert them into a unified data model that could be consumed by all clients. The translation layer proved incredibly valuable as it decoupled the work of the teams working on the migration from the work of the client teams.</p><p>Nothing happens in a vacuum. While the migration was in progress, business needs arose and features had to be added to the payment orchestration system. For each feature, teams had to decide whether the changes should be implemented in only one system or in both. In many cases, this led to twice as much work in order to maintain a consistent user experience across both systems. In other cases, features were simply deferred or redesigned to avoid duplication of effort.</p><p>Finally, special care had to be taken to ensure that both systems behaved in the way that our guests and hosts expected. Ideally, guests and hosts wouldn’t even notice the difference apart from some improvements in performance. Additional tooling and workflows were created to ensure that Airbnb’s support ambassadors continued to provide a consistent experience for our guests and hosts regardless of which system was used to orchestrate payments.</p><p>One key learning from this experience was how critical it is to communicate with all stakeholders to ensure that everyone is aligned on timelines, constraints, and priorities. Maintaining two parallel systems over an extended period of time creates a lot of overhead and slows down iteration speeds for new features. It is vital to ensure that the broader organization is aligned on the timeline so that product teams aren’t unnecessarily slowed down by unexpected work related to a partially migrated system. Splitting the migration into phases helps reduce the time during which teams are impacted.</p><h3>Commitment to Craft</h3><p>Perhaps the most important part of the migration process was ensuring that the new system was built with Airbnb’s <a href="https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a">Commitment to Craft</a> in mind and thoroughly validated before being rolled out. A dedicated team of quality assurance engineers performed comprehensive manual testing of hundreds of scenarios to help to ensure consistency with the legacy system across a wide spectrum of use cases. In addition, an extensive set of unit tests, shallow integration tests, and end-to-end integration tests were created across the entire payments engineering organization to ensure the correct behavior of key payment flows. As an additional safeguard, whenever possible, asynchronous “matchup” jobs would compare the new data model and the old data model to validate that both codepaths produced consistent results.</p><h3>Conclusion</h3><p>Payments systems are complex. Taking the time to thoughtfully redesign the system can lead to improvements in maintainability, extensibility, performance, and resiliency. However, there are also noteworthy disadvantages to a long-lived migration process. The process can lead to uncertainty among clients of the service and consume resources that might otherwise be spent creating new features or optimizing existing flows. It is possible to mitigate some of these concerns by dividing the migration into smaller, well-defined milestones and ensuring regular communication with stakeholders. A thorough testing and validation plan is vital for ensuring that the new service can seamlessly replace legacy systems. By following this approach, we were able to launch a new payment orchestration system that is faster, easier to maintain, and can more easily support new products, features, and business needs.</p><p>Watch the recording of the <a href="https://www.facebook.com/AirbnbTech/videos/airbnb-tech-talk-make-money-moves/349403395953262/">Make Money Moves tech talk</a> for a more in-depth discussion of the migration of payments services to SOA.</p><p>If this type of work interests you, check out some of our related positions:</p><p><a href="https://careers.airbnb.com/positions/3393082/?gh_src=5a0351831us">Senior Software Engineer, Payments</a> (San Francisco or Seattle)</p><p><a href="https://careers.airbnb.com/positions/3393185/?gh_src=3eaf43fe1us">Staff Software Engineer, Payments</a> (San Francisco or Seattle)</p><p><a href="https://careers.airbnb.com/positions/2768475/">Manager, Engineering Payments Compliance</a> (Bangalore, India)</p><p><a href="https://careers.airbnb.com/positions/2925359/">Senior Software Engineer, Payments Compliance</a> (Bangalore, India)</p><p><a href="https://careers.airbnb.com/positions/2773515/">Staff Software Engineer, Payments Compliance</a> (Bangalore, India)</p><p><a href="https://careers.airbnb.com/positions/3197040/">Software Engineer </a>— Cities (Bangalore, India)</p><h3>Acknowledgments</h3><p>This migration has been a long journey that wouldn’t have been possible without the contributions of many people at Airbnb across several organizations including Payments, Hosting, Guest Experience, QA, and Finance. Too many people helped on this project to thank all of them here, but the authors would like to recognize Musaab At-Taras, Xuemei Bao, Ryan Bi, Abhijit Borude, Ben Bowler, Yizheng Cai, Jiaqi Chen, Haoran Cheng, Cynthia Adams, Pat Connors, David Cordoba, Chong Chung, Anqi Dai, Xinyue Deng, Rex Du, Ali Goksel, Ömer Faruk Gül, Jiajia Han, Jing Hao, Toland Hon, Jeremy Kane, Hide Kato, Fanchen Kong, Victoria Ku, Pasha Lahutski, Serena Li, Tina Li, Harry Liu, Michael Liu, Wenguo Liu, Yixia Mao, Elena Moskvichev, Eric Ning, Ika Ogeil, Christina Ou, Payut Pantawongdecha, Yixiao Peng, Yaritza Perez, Wentao Qi, Zachary Sabin, Rajen Shah, Patrick Shay, Bo Shi, Derek Shimozawa, Erika Stott, Huayan Sun, Sam Tang, Claire Thompson, Neo Tong, Alex Virrueta, Jing Wang, Bryan Wehner, Michel Weksler, Claudio Wilson, Xuanxuan Wu, Liang Xiao, Chao Xin, Serdar Yildirim, Hang Yuan, Brian Zhang, Yunfei Zhao, Jaclyn Zhong, and Linglong Zhu for their contributions over the lifetime of this project.</p><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=341d194a781b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/rebuilding-payment-orchestration-at-airbnb-341d194a781b">Rebuilding Payment Orchestration at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Lucius DiPhillips]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-lucius-diphillips-79d1f0bc72a2?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/79d1f0bc72a2</guid>
            <category><![CDATA[diversity]]></category>
            <category><![CDATA[leadership-development]]></category>
            <category><![CDATA[leadership]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 17 Feb 2022 19:54:26 GMT</pubDate>
            <atom:updated>2022-02-17T19:56:34.003Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Lucius DiPhillips</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hnM9txCDUVpdDN-u9Z9lJQ.jpeg" /></figure><p>Airbnb’s CIO on sponsorship, belonging, and the power of human connection</p><p><em>Lucius DiPhillips is the Chief Information Officer (CIO) at Airbnb. He has over 20 years of experience that spans Product Development, Information Technology, Customer Service, Financial Services, Payments, eCommerce, and Trust &amp; Safety. He has a Degree in Management Information Systems from Rensselaer Polytechnic Institute and serves as the executive sponsor for several diversity and belonging groups and initiatives across the company. Through his sponsorship, Lucius has been instrumental in helping to improve the ways in which Airbnb attracts and retains diverse technical talent.</em></p><h3>Breaking barriers and growing from adversity</h3><p>I grew up in Upstate New York, in a small town called Hudson Falls. I was raised by a single-parent mom, and I’m an only child. Growing up, we struggled financially, and I helped out wherever I could. From delivering papers as a ten-year-old to waiting tables as a high-schooler, I always had something to balance. That’s helped me as a leader to this day: balance and having that hard work ethic, and seeing my mom’s struggle as a single mom.</p><p>I’m multiracial: my mom is white and my dad has Black heritage. At an early age, I became acutely aware that I was different, and sometimes I didn’t feel like I belonged — because of how I looked, because I didn’t have both parents in the picture, and because we didn’t have a lot financially. Rather than letting my differences hold me back, I immersed myself in many things, from playing sports to the school choir and musicals. I wanted to get to know a lot of different people and ultimately go beyond the superficial labels and barriers between us.</p><p>That has become part of how I lead to this day, and how I build teams. It’s very much about connecting with people beyond what you might see on the surface, and really trying to find common ground. It was a skill that came from a dark place early on in my career, that has now become a skill that helps me be a coach, mentor, and sponsor, who invests in others and leads them to develop in their own careers.</p><h3>A career path shaped by curiosity, connections, and conversations</h3><p>I first got interested in tech in the 90s, as a student curious about this new thing called the internet. While my career started in traditional IT, I later got into product development and software development for end-users. Normally, those are two different profiles, but I’m more of a hybrid with a broad understanding of all facets of tech. I love technology, but I also love operations, leadership, and people. I have an appreciation for how we make sure that we connect what’s happening with tech to real customers, real people at the end of the day.</p><p>Making and maintaining many different personal connections with mentors and colleagues has led to a variety of opportunities in my career, and eventually brought me to Airbnb. I first came to Airbnb leading our Payments technology organization. Airbnb has a structured framework for career conversations that involves assessing your dream job, your story, your strengths, and what you want to do better, and from there identifying career development opportunities. This process led to my current role as the first CIO at Airbnb.</p><p>I feel like I have the best job as a CIO. I feel like I work for the best company in the world at Airbnb. That’s why I’m still glowing and here, going on four years later. And to me, this is just the beginning.</p><h3>Diversity and belonging in tech</h3><p>I am the co-sponsor of the Tech Diversity Council, a group of senior technical leaders at Airbnb tasked with amplifying and advocating for diversity-related projects and initiatives across our Tech org. It’s one of the most important roles that I have to play, if not the most important. And we’ve created the Council because we still have a long way to go in terms of representation across tech and across Airbnb. To me, the best way to get involved is to give my time and push ideas and vision into action that creates impact.</p><p>There isn’t just one initiative to talk about here, but rather many parallel efforts that span from wide-scale to personal. In addition to the Tech Diversity Council, we have a hyperfocus on Black in Tech as a group, and we have the Black Sponsorship Program, developed and led by Airbnb’s Black Employee Resource Group, Black@. I lead a monthly series for anyone who’s in technology that self-identifies as Black, who optionally wants to come together, to have a safe place to share, to contribute.</p><p>At Airbnb, we’ve always prided ourselves in standing for what we believe in unapologetically. With the Black Lives Matter movement and the George Floyd protests, I felt like we had the support to speak up about what we were feeling and the experiences we were having. The Black@ group created <a href="https://news.airbnb.com/activism-and-allyship-guide/">a guide</a> on how to be an ally, and we hosted many conversations about what it means to be Black in America. To me, that was the most important thing we could have been doing in that moment of time, and we continue doing it.</p><h3>Redesigning our hiring process</h3><p>I’m proud of Airbnb, because most companies don’t even talk about where they hope they go. They don’t share representation numbers. We not only share them, we say we can do better, and we’re going to do better, and here’s how and when.</p><p>One of the things we’ve done is go back to the drawing board to redesign our hiring process. Having a love of operations, data, and driving process improvements, I approached our hiring process like a product. Step by step, we looked at the data and asked “Is there a disproportionate drop-off here for certain demographics? What can we do differently?”</p><p>We realized that engineers are very unique — not just in their gender and ethnicity and work experience, but in how they’re most comfortable interviewing. So we decided to give candidates more flexibility: they might choose either to do a take-home coding test or show us some open source work they’re proud of. We also got more managers involved at the ground level to support our diversity and inclusion candidates and help them feel seen, understood, and connected to their future team. Engaging more of our diverse engineers early on in the hiring process had a huge impact.</p><p>Starting from my time with the Payments organization, I recognized the urgency of the moment and the stakes at play for underrepresented candidates and pushed the recruiting team to put changes into action rather than waiting or holding back. I call it breaking some glass — you need to break some glass every once in a while to challenge the status quo.</p><h3>A human-centric way to lead</h3><p>If you focus on belonging and engagement, and you make it a priority, then you can create a better environment for your team. When traumatic things happen, it’s important to educate others so they can be allies, as well as creating a safe space for people to share. As part of our wellness programs, we host “listening sessions.” I’ve hosted them with my leadership team, for Black@, for what was happening with the Afghan refugee situation, or when the COVID-19 pandemic was spiking in India.</p><p>I’m also passionate about demystifying the fact that work-life balance is a real thing you can actually conquer. It’s a pet peeve of mine that we talk about work-life balance. My balance is very different than any one of yours individually. So let’s talk about flexibility, and having empathy for each other’s unique needs and situations.</p><p>I care about being transparent and available, and part of the way I do that is by offering an office hours slot twice a day that anyone can sign up for at any time. We actually took that idea and scaled it with something called coffee chats. Anyone in our organization can sign up to have a coffee chat with someone else. You don’t know who it’s going to be until you show up. And that’s what I loved about my office hour slots, being able to ask, “What’s your story? How long have you been here? What’s one personal thing you’re thinking about?” At heart I want to connect with people. I want to demonstrate a human-centric way to lead.</p><p>–</p><p>Interested in working with Lucius at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/3897689/">Senior Engineering Manager, Tax Platform</a></p><p><a href="https://careers.airbnb.com/positions/3873636/">Systems Engineer, Client Engineering</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=79d1f0bc72a2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-lucius-diphillips-79d1f0bc72a2">My Journey to Airbnb — Lucius DiPhillips</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Past, Present, and Future of react-dates]]></title>
            <link>https://medium.com/airbnb-engineering/the-past-present-and-future-of-react-dates-b351ab739d3f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/b351ab739d3f</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[frontend]]></category>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[react]]></category>
            <category><![CDATA[javascript]]></category>
            <dc:creator><![CDATA[Diane Ko]]></dc:creator>
            <pubDate>Fri, 21 Jan 2022 17:45:50 GMT</pubDate>
            <atom:updated>2022-01-24T22:44:18.497Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://www.linkedin.com/in/kodiane/">Diane Ko</a></p><figure><img alt="Silhouettes of two people in front of the inside of a large, clear clock overlooking a city." src="https://cdn-images-1.medium.com/max/1024/1*VymqCVttV2_VOqmmApgakw.jpeg" /></figure><p>In 2016, Airbnb released react-dates, a React date picker component library. The <a href="https://github.com/airbnb/react-dates/stargazers">project has amassed more than 11,000 stars</a>. GitHub also tells us that <a href="https://github.com/airbnb/react-dates/network/dependents">react-dates is used by over 30,000 repos</a>.</p><p>In more recent years, Airbnb’s requirements for a date picker have changed in a way that has diverged from react-dates. If we were to have made those changes to the library, it would have severely limited the flexibility of the library, one of its key features. To better support the react-dates community, we’ve made the decision to transfer ownership of the react-dates repo to a new <a href="https://github.com/react-dates">react-dates GitHub organization</a>. We believe this new home will better serve the community and continue to evolve the original goals of react-dates.</p><p>If you want to help react-dates grow, please check out the <a href="https://github.com/airbnb/react-dates/issues">open issues</a> and <a href="https://github.com/airbnb/react-dates/pulls">pull requests</a> — the <a href="https://github.com/airbnb/react-dates/labels/pull%20request%20wanted">“pull request wanted” tag</a> is a great starting point.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b351ab739d3f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/the-past-present-and-future-of-react-dates-b351ab739d3f">The Past, Present, and Future of react-dates</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>