<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 08 Aug 2022 01:30:55 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Incident Management]]></title>
            <link>https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ae863dc5d47f</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[incident-management]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[reliability-engineering]]></category>
            <category><![CDATA[automation]]></category>
            <dc:creator><![CDATA[Vlad Vassiliouk]]></dc:creator>
            <pubDate>Wed, 27 Jul 2022 16:39:43 GMT</pubDate>
            <atom:updated>2022-07-27T16:39:43.843Z</atom:updated>
            <content:encoded><![CDATA[<h3>Automated Incident Management Through¬†Slack</h3><p>How Airbnb automates incident management in a world of complex, rapidly evolving ensemble of microservices.</p><p><a href="https://www.linkedin.com/in/vladimir-vassiliouk">Vlad Vassiliouk</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hP8PSrLw_LTyLjhbR6LRxg.jpeg" /></figure><h3>Incident Management</h3><p>Incidents are unforeseeable events that disrupt normal business operations and are inevitable in complex systems that must be up and running 24/7. This is why it‚Äôs important to prepare and to train people to handle incidents in a timely and organized manner. Although each incident is unique, we follow the same procedure for detection, escalation, management, and resolution of incidents.</p><p>At Airbnb, we utilize a <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> which involves many interconnected services managed by small teams. Quickly figuring out what service is in trouble, and who to page is paramount to timely incident resolution. We found that our teams spent a lot of time switching between applications such as Slack, Pagerduty and Jira to raise an incident, page responders, and provide context. In order to have quick resolutions of incidents, we developed an incident management bot, a centralized automation tool for incident management.</p><h3>Incident Management Slack¬†bot</h3><p>Our goal was to centralize incident management in Slack. Everyone at Airbnb is familiar with and has access to Slack, and it‚Äôs easy to bring people and resources together in an incident channel. In addition, the incident channel acts like a timeline of events which makes putting together a post mortem report¬†easy.</p><p>Our requirements were as¬†follows:</p><ul><li>Run in Airbnb‚Äôs <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> and have full support from our¬†team.</li><li>Standardize incident-related communications in all tools such as Jira, Slack, PagerDuty.</li><li>Centralize incident management in¬†Slack.</li><li>Single intake funnel for incidents with clearly defined¬†steps.</li><li>Automate post-incident tasks such as setting up meetings and archiving channels.</li><li>Provide incident timelines and¬†metrics.</li></ul><p>We decided to build our own app to meet our exact specifications and allow us to easily customize and develop further. We also chose to build the app in Golang, because of the great community, and their well documented <a href="https://pkg.go.dev/github.com/slack-go/slack">slack¬†library</a>.</p><p>Finally, we decided to use chat commands instead of <a href="https://slack.com/help/articles/201259356-Slash-commands-in-Slack">slash commands</a> so that all commands sent to the bot would be visible to the members of the Slack¬†channel.</p><p>Our incident management bot achieves incident response automation through four key commands:</p><ul><li><strong>new incident &lt;summary&gt;: </strong>Create a Jira ticket and page incident managers.</li><li><strong>new channel &lt;ticket&gt;: </strong>Create an incident Slack channel for an open incident¬†ticket.</li><li><strong>page &lt;service|user&gt;: </strong>Page the on-call(s) for a PagerDuty Service or a user directly.</li><li><strong>get timeline:</strong> Compile a concise timeline of important chat events for post-incident analysis.</li></ul><h3>Incident Response Lifecycle</h3><p>We have defined four separate phases of an incident: detection, communication, escalation and resolution. Each of the bot‚Äôs commands automates tasks that would normally require coordination during these distinct¬†phases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x2vbM-iepMqqsrH9" /></figure><h3>Detection</h3><p>Most of our incidents are detected by our <a href="https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f">monitoring and alerting tools</a>, although sometimes we learn about incidents from our team members or customers. No matter how an incident is detected, having a single intake funnel for all incidents is crucial for effective incident detection. Our bot solves this by providing the ‚Äúnew incident‚Äù command.</p><h4>New incident &lt;summary&gt;</h4><p>This command creates a blank JIRA ticket with default settings and asks the user if they‚Äôd like to page an incident¬†manager.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*W275nEZINolYZ6pn" /></figure><p>Regardless of the user‚Äôs choice to page an incident manager, a popup appears to the user asking for additional information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/519/0*cwNt9XjvCIiVXNS2" /></figure><p>This allows us to escalate incidents quickly while still allowing the incident responder to provide valuable information for the incident managers. These fields are optional in the interests of urgency and can be filled out later if¬†needed.</p><h3>Communication</h3><p>Another important first step is to set up communication channels and provide as much context as possible to responders.</p><h4>New channel [Jira¬†ticket]</h4><p>This command takes an optional Jira ticket as a URL or key. If none is provided, it will show the last 5 recently opened incident tickets for the user to choose. A channel is then created using the Jira ticket key, the summary as the title, and all incident managers are¬†invited.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/495/0*TfXcDuFds_fv8K-8" /></figure><p>To provide context to all users invited, the channel‚Äôs topic is set to the Jira ticket link along with the summary of the Jira ticket. In addition, we update the Jira ticket with a link to the newly created Slack¬†channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/328/0*2WJcyBs7LocZ4yjP" /></figure><h3>Escalation</h3><p>You may have heard about the<a href="https://nvd.nist.gov/vuln/detail/CVE-2021-44228"> Log4j security vulnerability</a> which was characterized as the single biggest and most critical vulnerability of the last decade. Within 72 hours of vulnerability disclosure, there were reports of <a href="https://arstechnica.com/information-technology/2021/12/hackers-launch-over-840000-attacks-through-log4j-flaw">840,000 attacks</a> on companies globally, which turned into 100 internet wide attacks per minute over the following weekend.</p><p>At Airbnb, we have over a thousand micro services with hundreds of small teams managing them, which offered a unique challenge for us. We had to identify all vulnerable services, and quickly reach out to their respective owners for quick mitigation. This is where our Slack bot really shined, allowing our Incident Managers to quickly reach out to service owners and coordinate rolling out the fix much quicker than before. In a matter of minutes, the bot was used to page over 300 teams to assist with assessing impact and deploying patches. This equated to 4 hours saved compared to paging these teams manually, not to mention reducing the time spent in a vulnerable state.</p><h4>Page &lt;shortcut|service name|slack user&gt;</h4><p>The page command can be given a service shortcut, service name, or a slack¬†user.</p><p>To get started, the user can view a list of shortcuts by typing in ‚Äúpage¬†list‚Äù</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/264/0*DVEqiuHCfuSqyruA" /></figure><p>Each shortcut corresponds to a PagerDuty service ID which will be used when creating a PagerDuty incident. The shortcuts are easily customizable by editing a YAML¬†file.</p><p>If a user types in a service name which doesn‚Äôt match any shortcut, a search is done in the PagerDuty service directory and results are displayed for the user to¬†choose.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/426/0*yym1kmhJGZTzsa4c" /></figure><p>Once a user chooses the service they want to page they‚Äôre asked to confirm and a new PagerDuty incident is created for that¬†service.</p><p>We also allow paging Slack users directly for when additional responders are required outside of those¬†on-call.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/0*pw130BpOe5IFD7CZ" /></figure><p>Once the page command is sent, the bot creates a new incident in PagerDuty with the Jira ticket, summary, and slack channel to provide context to the on-call person. After the on-call person is paged, the bot announces who was paged and invites them to the¬†channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/457/0*f0cPvK_oG91okdgm" /></figure><h3>Resolution</h3><p>Once responders confirm there is no further user impact and a root cause is known, the incident is considered resolved and the team transitions to the post-incident phase. A robust timeline is required to have an effective post-incident review and an effective <a href="https://www.atlassian.com/incident-management/postmortem">post mortem</a>¬†report.</p><h4>Get timeline</h4><p>This command will search the incident channel for all chat messages marked with a specific emoji which designates the message as a timeline event, and direct message the user a compiled timeline.</p><p>For example, we use the üìù emoji to designate important events in the chat. As the incident is ongoing, anyone can add the emoji as a reaction to important chat events. Post-incident, the ‚Äúget timeline‚Äù command will compile these chat events into an easy to copy paste timeline to be used in the post-incident report.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*vL094xETdA2Re2cR" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/472/0*Z2WAZ9sw8X_Fg0-J" /></figure><h4>Incident Review</h4><p>At Airbnb, we have after action review meetings (AAR) weekly where we review recent high severity incidents, post-incident reports, and ensure any corrective actions are called out and assigned. As soon as the Jira ticket tracking the incident is updated with the AAR meeting date, the bot will notify the person owning the Jira ticket when the meeting will be and what is expected of¬†them.</p><h4>Followup Tracking</h4><p>Oftentimes, during our <a href="https://www.atlassian.com/incident-management/postmortem/blameless">blameless postmortem process</a>, tickets for corrective actions are created and assigned to teams to avoid similar incidents in the future. To encourage quick resolutions we set a strict deadline for these tickets. Our bot will send a warning message over Slack a couple of days before the deadline, and another message if the deadline has lapsed to the user assigned to the¬†ticket.</p><h4>Archiving Incident¬†Channels</h4><p>To keep our Slack workspace tidy, the bot automatically archives incident channels ten days after the incident‚Äôs Jira ticket has been¬†closed.</p><h3>Results</h3><p>Since launch, our bot has saved our Incident Managers and responders many hours through its automation and centralization of incident management within Slack. By measuring the average amount of time each task takes to complete manually compared to the bot‚Äôs automation, we determined an estimated 44 hours of time saved so far in¬†2022.</p><h3>What‚Äôs Next?</h3><p>To further streamline our incident response from Slack, we plan to enhance our integration with PagerDuty.</p><p>Currently, every time the page command is used a new PagerDuty incident is created. Instead, we plan to unify all pages under a single PagerDuty incident to take advantage of PagerDuty‚Äôs incident metrics and to provide more context to responders.</p><p>Lastly, after a PagerDuty service is paged using the bot, we don‚Äôt have visibility of the status of the PagerDuty incident in Slack. Was the page acknowledged? Did the on-call not respond? Was it escalated and to who? We plan to build automation to follow the PagerDuty incident and report the current status to the incident‚Äôs channel. This will also allow us to record the timeline of actions taken in the PagerDuty incident after paging the¬†service.</p><h3>Attribution and¬†Thanks</h3><ul><li><a href="https://medium.com/u/af76cda83a53">Stephen</a>: for being a great partner on the Airbnb Incident Management team and helping to define the incident management bot‚Äôs feature¬†roadmap</li></ul><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ae863dc5d47f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f">Incident Management</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb‚Ää‚Äî‚ÄäBeti Gathegi]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/61c2db3d8546</guid>
            <category><![CDATA[program-management]]></category>
            <category><![CDATA[√©ducation]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[bootcamp]]></category>
            <category><![CDATA[onboarding]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 21 Jul 2022 17:28:47 GMT</pubDate>
            <atom:updated>2022-07-21T17:28:47.687Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb‚Ää‚Äî‚ÄäBeti¬†Gathegi</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mhJHc6Qqjet_mKa-bzwu-w.jpeg" /></figure><p>From exploring careers across continents to now helping others find their place at¬†Airbnb.</p><p><em>After trying a series of careers ranging from television production to university communications and marketing, </em><a href="https://www.linkedin.com/in/betigathegi"><em>Beti Gathegi</em></a><em> works as a Senior Program Manager on the TechED (technical education) team at Airbnb. When she‚Äôs not lurking in the #bookworms Airbnb Slack channel, you can find Beti leading Bootcamp, our onboarding program for new technical hires, which takes engineers and data scientists through their first commit at Airbnb. Before this role, Beti was a recruiting program manager for Connect, Airbnb‚Äôs engineering apprenticeship program targeted at people from non-traditional technical backgrounds.</em></p><p><em>Beti herself has a non-traditional background, with a degree in journalism and several experiences outside the tech industry, including substantial time abroad. She is a major advocate for diversity and inclusion; part of her role in leading Bootcamp involves setting the company‚Äôs culture and encouraging new hires to shape the culture in their own unique¬†ways.</em></p><h3>Setting my own direction</h3><p>I describe myself as half East Coast, half West Coast, with a bit of time abroad added in. I‚Äôm the child of Kenyan immigrants and I grew up in the San Francisco Bay Area, in a town called Albany, California. When I was 15, I moved to the East Coast, and it would be many years before I found myself back in the Bay¬†Area.</p><p>For a long time, I wanted to be a journalist. To that end, I studied journalism in college as part of my communications degree. I was never fixated on a specific path and certainly explored a lot to reach where I am now. My father, who pivoted later in life by getting a law degree around age 40, was my guiding light in terms of being willing to try new things. I find personal exploration liberating‚Ää‚Äî‚Ääcrafting my own, organic path gave me a chance to figure out my likes and dislikes, as well as my skills and growth opportunities.</p><p>Part of my outlook on life is that it‚Äôs okay to stop something that isn‚Äôt right for yourself. Sometimes there can be a lot of inertia that makes it hard to pause and change directions, but I think making a decision to pursue another path is really brave and can be worn as a badge of honor. In my case, I started a master‚Äôs in liberal arts in which I was studying the South Asian diaspora and the children of Indian immigrants in particular. Inspired by the stories of others, I was eager to discover more about my own background and history. I chose to leave my program to go live in Kenya and experience Kenyan culture for myself. Until that point, I‚Äôd only been to Kenya with my family, so this was a new lens to see the country on my¬†own.</p><h3>Living and working in three continents</h3><p>Living in Kenya was a transformative experience and helped me understand my own identity more deeply. Having previously been told, by some, that I‚Äôm not Kenyan enough or not American enough, actually living in Kenya and encountering the sheer diversity of people made me realize there‚Äôs no singular way to be Kenyan, just like there‚Äôs no one way to be American or any culture for that matter. During my time abroad, I also realized I was ready to get more hands-on experience and enter the working¬†world.</p><p>Ready to live a life of adventure, I moved to New York City but stumbled into a financial crisis when seemingly everyone was getting laid off. I worked retail for a little while but otherwise didn‚Äôt last too long in New York. This was just the first in a series of new experiences, my next being at a TV production firm where I was an assistant, and where to this day I have an IMDB credit for four episodes of the show <em>Swamp Men </em>on National Geographic. If that wasn‚Äôt enough, I also had jobs writing TV quizzes for Nielsen, doing marketing for the University of South Florida, and working at an Australian Aboriginal art¬†gallery.</p><p>Eventually, I happened upon the tech industry when a friend recruited me to join Lyft in a customer support role. This was a completely new universe to me and I took every opportunity to get involved and apply my skills to a growing company. Practically by accident, my initiative to gather people via the company‚Äôs internal email list turned into Employee Resource Groups or ERGs. I helped form the Black ERG and Women‚Äôs luncheon, while also supporting others who wanted to create similar spaces for their communities. Very organically, I was taking a big part in the diversity and belonging conversation and making sure to educate myself to be a thoughtful contributor to these discussions. Later, this turned into an official job focused on Lyft‚Äôs culture, and afterward I moved to Pandora as a diversity and belonging program¬†manager.</p><h3>Onboarding to¬†Airbnb</h3><p>By the time I applied to join Airbnb, I had shifted to leading Pandora‚Äôs university recruiting program. I noticed a tremendous amount of potential in students, and found it really impactful to work on this key pipeline. That said, going directly from college to the tech industry isn‚Äôt the only way, my own career being a prime example. I jumped at the opportunity to join Airbnb as an apprenticeship program manager where I had the chance to revamp this pathway for engineers with unconventional backgrounds to join the¬†company.</p><p>I ran the apprenticeship program for two years and while the role was primarily on the recruiting side, we worked very closely with engineering and I started to develop an interest in TechED, the team I‚Äôm on now that owns the onboarding process for Airbnb‚Äôs technical hires. I was already meeting regularly with my manager at the time, Leo, about growing in my career, so I started by sharing my goals in that setting. Leo was incredibly supportive and epitomizes how Airbnb has a culture of empowering people to explore their passions and what energizes them.</p><p>I reached out to the manager on the TechED team to express my interest in the space and not too long after, a role happened to open up. After interviewing, I got my current position leading Airbnb‚Äôs Bootcamp program, the onboarding process all software engineers and data scientists go through in their first weeks at the company. It took a ton of experiences across many other roles to arrive at my current spot, but that came with immeasurable learning and I wouldn‚Äôt have it any other way! I feel uniquely equipped to welcome people to a new experience or challenge, having gone through so many of my¬†own.</p><h3>Leading Bootcamp and onboarding others to¬†Airbnb</h3><p>My current role aligns with my passion for helping people acclimate and providing them with the resources they ended to be successful. It‚Äôs fulfilling to advocate for an incredible new hire experience, help new team members feel confident in their respective roles at Airbnb, and support them towards making their first commit or code change. I strive to be really respectful of people‚Äôs time and make Bootcamp as relevant, engaging, and valuable as possible to everyone who participates.</p><p>Onboarding is a challenging task because there are multiple variables. Typically you are onboarding people in various roles, at various levels, to various teams, which may use their own tools and process. There is a balancing act between providing general information and hyper-relevant but also highly specific information. Remote onboarding also adds its own set of challenges.</p><p>That being said, I love co-creating solutions. I get to work with incredibly smart people on the engineering and data science teams to identify and clarify our challenges, workshop ideas, execute solutions, monitor progress, and iterate. I get a ton of energy from that process and from our collaborations.</p><h3>How the first weeks at work can leave a lasting¬†impact</h3><p>I‚Äôm also grateful to the many volunteers I partner with to shape the onboarding experience for our technical hires and set them up for success. For example, we pair each new hire with a buddy from their team. They serve both to scope the hire‚Äôs starter project as well as to answer the many questions that inevitably pop up in onboarding.</p><p>We have volunteers from various teams who raise their hands to host Bootcamp and lead sessions for each new hire cohort, and most of them are driven by providing a sense of belonging. Additionally, there‚Äôs a great community of collaborators across the industry to benchmark with and get mentorship from, since onboarding is a challenging problem that a lot of companies work¬†on.</p><p>Beyond the technical parts of onboarding, Bootcamp plays a critical role in setting Airbnb‚Äôs culture. Especially in a remote work environment, the quality of onboarding can make or break whether new hires feel a sense of community and feel comfortable engaging with it themselves. We emphasize belonging and inclusivity as core values of our culture, and we welcome new hires to bring their own special qualities to integrate into our ever-evolving culture.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61c2db3d8546" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546">My Journey to Airbnb‚Ää‚Äî‚ÄäBeti Gathegi</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Safeguards Changes in Production]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/9fc9024f3446</guid>
            <category><![CDATA[a-b-testing]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[continuous-delivery]]></category>
            <category><![CDATA[spinnaker]]></category>
            <dc:creator><![CDATA[Michael Lin]]></dc:creator>
            <pubDate>Mon, 11 Jul 2022 17:02:38 GMT</pubDate>
            <atom:updated>2022-07-11T18:13:11.762Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part I: Evolution of Airbnb‚Äôs experimentation platform</h4><p>By: <a href="https://www.linkedin.com/in/michaelcl/">Michael Lin</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/zack-loebel-begelman-85407698/">Zack Loebel-Begelman</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0J4whYTNqGPUdUme" /></figure><h3>Introduction</h3><p>As Airbnb has grown to a company with over 1,200 developers, the number of platforms and channels for pushing changes to our product‚Ää‚Äî‚Ääand the number of daily changes we push into production‚Ää‚Äî‚Äähas also grown tremendously. In the face of this growth, we constantly need to scale our ability to detect errors before they reach production. However, errors inevitably slip past pre-production validation, so we also invest heavily in mechanisms to detect errors quickly when they do make it to production. In this blog post we will cover the motivations and foundations for a system for safeguarding changes in production, which we call Safe Deploys. Two following posts will cover the technical architecture in detail for how we applied this to traditional A/B tests, and code deploys respectively.</p><h3>Continuous Delivery and¬†Beyond</h3><p>Airbnb‚Äôs continuous delivery team recently wrote about <a href="https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876">our adoption of Spinnaker</a>, a modern CI/CD orchestrator. Spinnaker supports <a href="https://spinnaker.io/docs/guides/user/canary/">Automated Canary Analysis (ACA)</a> during deployment, splitting microservice traffic by request to compare versions of code to see if performance, error rates, or other key metrics are negatively impacted. If metrics for the new version regress, Spinnaker automatically rolls back the deployment, significantly reducing the time to remediate a bad¬†push.</p><p>ACA at Airbnb has indeed caught a large number of errors early in the deployment process. However, it has a number of limitations:</p><ul><li><strong>Channels: </strong>Spinnaker‚Äôs ACA tests against changes to microservices. However, microservice updates are not the only source of errors that can be pushed into production. For instance, Android and iOS apps follow a release process through their respective app stores. Many ‚Äúproduction pushes‚Äù at Airbnb may involve no new code at all, and are strictly applied through configuration changes. These changes include marketing campaigns or website content created with Airbnb‚Äôs <a href="https://medium.com/airbnb-engineering/airbnbs-promotions-and-communications-platform-6266f1ffe2bd">internal content management systems</a>. While seemingly benign, pushes through these systems can have dramatic effects. For example an incident was once caused when a marketing campaign was mistakenly applied to all countries except one, instead of the original intent of targeting one specific country. This simple mistake led to empty search results for nearly all users globally, and required over an hour to identify and¬†revert.</li><li><strong>End-to-end business metrics: </strong>Spinnaker‚Äôs ACA is driven by local system metrics, such as a microservice‚Äôs local performance and error rates; not end-to-end business metrics, such as search click-through rates and booking rates. While roll-backs based on local system metrics are valuable, they aren‚Äôt sufficient, as some of our most costly bugs impact end-to-end business metrics but not local system metrics. For instance in 2020, a simple frontend change was deployed to production without being tested on a specific browser that did not support the CSS used, preventing users on that browser from booking trips. This had no impact on system metrics, but directly impacted business metrics. <br><br>Unfortunately, adding business metrics to Spinnaker‚Äôs ACA system is not possible because Spinnaker randomizes traffic by request, therefore the same user may be exposed to multiple variants. Business metrics, however, are generally user based and require each user to have a fixed variant assignment. More fundamentally, it‚Äôs not possible because business metrics need to be measured end-to-end and when two microservices undergo ACA at the same time, Spinnaker has no way of distinguishing the respective impact of those two services on end-to-end business¬†metrics.</li><li><strong>Granularity: </strong>Spinnaker‚Äôs ACA tests at the level of the entire microservice. However, it‚Äôs often the case that two features are being worked on at the same time within a microservice. When ACA fails, it can be hard to tell which feature caused the¬†failure.</li></ul><p>While we heavily depend upon Spinnaker‚Äôs ACA at Airbnb, it became clear there was an opportunity to complement it and address the above limitations where the circumstances call for¬†it.</p><h3>Experimentation Reporting Framework (ERF)</h3><p>A/B testing has long been a fixture in product development at Airbnb. While sharing some qualities with ACA in counterfactual analysis, A/B testing has focused on determining whether a new feature improves business outcomes, versus determining whether that feature causes a system regression. Over the years Airbnb has developed our Experimentation Reporting Framework (ERF) to run hundreds of concurrent A/B experiments across a half dozen platforms to determine whether a new feature will have a positive¬†impact.</p><p>ERF addresses the limitations of ACA listed¬†above:</p><ul><li><strong>Channels: </strong>With each new platform, an ERF client has been introduced to support A/B testing on it. This includes mobile, web, and backend microservices. APIs were also introduced to provide config systems an avenue to treat config changes as A/B¬†tests.</li><li><strong>End-to-end business metrics: </strong>ERF is driven <em>primarily</em> by end-to-end business metrics. On the technical side, it randomizes by user, not request, and it is able to distinguish the impact of hundreds of experiments running concurrently. ERF taps into Airbnb‚Äôs <a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">central metrics system</a> to access the thousands of business metrics and dimensions Product and Business teams have defined to measure what matters most to Airbnb¬†overall.</li><li><strong>Granularity: </strong>Where Spinnaker‚Äôs ACA runs its experiments at the level of an entire microservice, ERF runs its experiments based on what are basically feature flags embedded into the code. Thus, if multiple features are being developed concurrently in the same microservice, ERF can determine which one is impacting the business¬†metrics.</li></ul><p>The above characteristics of ERF address the limitations of ACA, but ERF also had a limitation of its own: it was a daily-batch system generating interactive reports intended to be consumed by human decision makers. To address the limitation of Spinnaker‚Äôs ACA, ERF needed to evolve into a near real-time system that can directly control the deployment process without human intervention.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rOQST4-KYXGe253y" /><figcaption>Figure 1: Areas of the ERF Platform augmented to support near real-time experimentation</figcaption></figure><p>This evolution had implications on both the data science behind ERF, and its software architecture. We describe the former in this post, and will describe the latter in the next post of this¬†series.</p><h3>Realtime ERF‚Ää‚Äî‚ÄäThe Data¬†Science</h3><p>The foundation of solid data science is solid data engineering. On the data engineering side, we needed to revisit the definitions of the business metrics to be computed in real-time. The metrics computed by the batch ERF system were designed for accuracy, and could take advantage of complex joins and pre-processing to achieve this. Near real-time metrics did not have this luxury, and required simplification to meet low latency requirements.</p><p>Not only did we have to build new metrics, but we knew we would have to build new statistical tests as well. It is imperative for safe deployment systems to not be noisy, otherwise people will stop using it. Traditional methods like T-Test suffer from a variety of issues that would be extremely problematic when implemented in a real-time system. Two issues in particular are false positives due to (1) <a href="http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1517.pdf">peeking</a> (looking before a predetermined amount of time) and (2) heavily skewed¬†data.</p><p>When monitoring whether or not a metric has changed in real-time, users want to be notified as soon as the model has the confidence that this is true. However, doing so naively results in the first issue, peeking. In traditional A/B testing, the statistical test is only applied once after a predetermined time, because there is a chance that a significant result is due to randomness and not an actual effect. For real-time ERF, we aren‚Äôt making just one test, since, depending on how long we wait to take the test, we‚Äôre at risk for either taking too long to detect some errors, or missing other errors that take longer to surface. Instead, we want to check (peek at) the model every 5 minutes so that we can react quickly. With a p-value of 0.05 running 100 A/A comparisons, one could expect to have ~5 significant results that are actually false positives. We can transfer this issue to computing p-values on the same data set multiple times. Each evaluation results in a 5% chance of a false positive and so over multiple evaluations, the chance of having 1 or more false positives approaches 100%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/894/0*GHdgh0pCLSG87czD" /><figcaption>Figure 2: Increasing evaluations inevitably lead to false positives</figcaption></figure><p>To balance early detection without noisiness, we utilize <a href="https://en.wikipedia.org/wiki/Sequential_analysis">sequential analysis</a>. Sequential methods do not assume a fixed sample size (i.e., checking the model once) and allow us to continually monitor a metric without worrying about false positives incurred due to peeking. One way to correct for false positives (<a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">Type 1 Errors</a>) is by applying a <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>. If you check your model for statistical significance four times and want to guarantee a 5% overall false positive rate, you need to divide your p-value by four, meaning only results with p-value at or under 1.25% are valid. However, doing so is too conservative since each check is dependent. They are dependent because each check has the same base of data only adding additional observations as time goes on. Sequential models take this dependence into account while guaranteeing false positives rates more efficiently than Bonferroni. We use two different sequential models, SSRM (<a href="https://arxiv.org/abs/2011.03567">Sequential Sample Ratio Mismatch</a>) for count metrics, and <a href="https://arxiv.org/abs/1906.09712">Sequential Quantiles</a> (Howard, Ramdas) for quantile¬†metrics.</p><p>The second issue that we needed to solve in order to be robust is handling skewed data. Performance metrics like latency can have extremely heavy tails. Models that assume a normal distribution won‚Äôt be effective because the Central Limit Theorem does not come into effect. By applying Sequential Quantiles, we can ignore assumptions about the metric‚Äôs distribution and directly measure the difference between arbitrary quantiles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/892/0*_UUUc7wmcL3jUKei" /><figcaption>Figure 3: Metrics may have non-normal distributions</figcaption></figure><p>Lastly, many important measures are not independent. Metrics like latency and impressions have within-user correlation, so each event in the data cannot be treated as an independent unit. In order to counteract skew, we aggregate all measures into user metrics first before evaluating statistical models.</p><h3>Conclusion</h3><p>With the statistical methods in place to evaluate business metrics in near real-time, we could now detect problems that were invisible to Spinnaker, or required too much lead time to rely on traditional ERF experiments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nmC6mnUBfe98hcxo" /><figcaption>Figure 4: How Real-time ERF fits between Spinnaker and Traditional ERF</figcaption></figure><p>Operationalizing the newly created near real-time metrics and statistical methods required further engineering, but more challenging, it required changing the experimentation culture at Airbnb. In the following post we will detail how our near real-time metrics pipeline was built, how these metrics powered automated decision making, and how we drove adoption across the¬†company.</p><p>Interested in working at Airbnb? Check out these open¬†roles:</p><p><a href="https://careers.airbnb.com/positions/4262326/">Senior Software Engineer, Metric Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4216371/">Staff Software Engineer, Real-time Stream Processing Platform</a></p><p><a href="https://careers.airbnb.com/positions/2403782/">Staff Software Engineer‚Ää‚Äî‚ÄäML Ops¬†Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><h3>Appreciations</h3><p>Thanks to <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/alex-shaojie-deng-b572347/">Alex Deng</a>, <a href="https://www.linkedin.com/in/antoinecreux/">Antoine Creux</a>, <a href="https://www.linkedin.com/in/erikriverson/">Erik Iverson</a>, <a href="https://www.linkedin.com/in/george-l-9b946655/">George Li</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/preetiramasamy/">Preeti Ramasamy</a>, <a href="https://www.linkedin.com/in/rstata/">Raymie Stata</a>, Reid Andersen, <a href="https://www.linkedin.com/in/ronnyk/">Ronny Kohavi</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/tatiana-xifara/">Tatiana Xifara</a>, <a href="https://www.linkedin.com/in/vincent-chan-70080423/">Vincent Chan</a>, <a href="https://www.linkedin.com/in/xin-tu/">Xin Tu</a> and the OMNI¬†team.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fc9024f3446" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446">How Airbnb Safeguards Changes in Production</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[T-LEAF: Taxonomy Learning and EvaluAtion Framework]]></title>
            <link>https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/30ae19ce8c52</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[taxonomy]]></category>
            <dc:creator><![CDATA[Cen(Mia) Zhao]]></dc:creator>
            <pubDate>Thu, 23 Jun 2022 17:20:01 GMT</pubDate>
            <atom:updated>2022-06-28T21:16:44.392Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>How we applied qualitative learning, human labeling and machine learning to iteratively develop Airbnb‚Äôs Community Support Taxonomy.</strong></p><p><strong>By:</strong> <a href="https://medium.com/@cenzhao06">Mia Zhao,</a> <a href="https://www.linkedin.com/in/peggyshao">Peggy Shao</a>, <a href="https://www.linkedin.com/in/maggiekhanson/">Maggie Hanson</a>, <a href="https://medium.com/@wangpengcqb">Peng Wang</a>, <a href="https://www.linkedin.com/in/bo-zeng-71915624">Bo¬†Zeng</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X863W9ZDWmr7SlKW" /></figure><h3>Background</h3><p>Taxonomies are knowledge organization systems used to classify and organize information. Taxonomies use words to describe things‚Ää‚Äî‚Ääas opposed to numbers or symbols‚Ää‚Äî‚Ääand hierarchies to group things into categories. The structure of a taxonomy expresses how those things relate to each other. For instance, a <em>Superhost</em> is a type of <em>Host</em> and a <em>Host</em> is a type of Airbnb <em>User</em>. Taxonomies provide vital terminology control and enable downstream systems to navigate information and analyze consistent, structured data.</p><p>Airbnb uses taxonomies in front-end products to help guests and hosts discover exciting stays or experiences, as well as inspirational content and customer support offerings. Airbnb also uses taxonomies in backstage tooling to structure data, organize internal information, and support machine learning applications.</p><p>Classifying the types of issues Airbnb community members face is vital for several¬†reasons:</p><ul><li><strong>Hosts and guests</strong> need to be able to describe issues to Airbnb in order to receive relevant help suggestions or get connected with the best¬†support.</li><li><strong>Support Ambassadors</strong> (Airbnb‚Äôs Community Support specialists) need quick and easy access to workflows that help them resolve issues for guests and¬†Hosts.</li><li><strong>Airbnb business units</strong> need to understand where and why guests and Hosts encounter problems so that we can improve our product and make the Airbnb experience better.</li></ul><p>The Contact Reasons taxonomy is a new, consolidated issue taxonomy that supports all of these use cases. Before Contact Reasons, Community Support had siloed taxonomies for guests and Hosts, Support Ambassadors, and machine learning models that each used different words and structures to classify the same issues and relied on manual mapping efforts to keep in¬†sync.</p><p>The consolidation of disjointed issue taxonomies into Contact Reasons was the first project of its kind at Airbnb. The development of such a new taxonomy requires iterative learning: create/revise the taxonomy by taxonomists; roll out to train ML model, product and services; evaluate the quality of the taxonomy and identify areas for improvement. Before this work, there was no systematic process in place to evaluate taxonomy development or performance and the iteration was mostly subjective and qualitative. To accelerate the iterative development with more quantitative and objective evaluation of the quality of the taxonomy, we created T-LEAF, a <strong><em>T</em></strong><em>axonomy </em><strong><em>L</em></strong><em>earning and </em><strong><em>E</em></strong><em>valu</em><strong><em>A</em></strong><em>tion </em><strong><em>F</em></strong><em>ramework</em>, to quantitatively evaluate taxonomy from three perspectives: coverage, usefulness, and agreement.</p><h3>Challenges in Evaluating the New¬†Taxonomy</h3><p>In the Airbnb Community Support domain, new taxonomies or taxonomy nodes often need to be created before we have either real-world data or clear downstream workflow applications. Without a consistent quantitative evaluation framework to generate input metrics, it‚Äôs difficult to gauge the quality of a new taxonomy (or a taxonomy version) when directly applying it to downstream applications.</p><h3>Lack of quantitative evaluation framework</h3><p>Taxonomies are typically developed by qualitative-centric approaches¬π. When we started prototyping the new taxonomy, we evaluated feedback from existing users, and recruited guests and Hosts for several rounds of user research to generate insights. While qualitative evaluation like domain expert review is helpful in identifying high-level challenges and opportunities, it is insufficient for providing evaluation at scale, due to small sample sizes and potential sample bias from users participating in the research.</p><h3>Lengthy and iterative product cycle for taxonomy¬†launches</h3><p>Developing and launching a taxonomy can be a lengthy and iterative process that requires several quarters of use to get substantive and reliable quantitative feedback. A typical process includes:</p><ul><li><strong>Taxonomy discovery and development</strong> based on product requirement or data-driven analysis</li><li><strong>Production changes</strong> to integrate backend environments and frontend surfaces, including necessary design and content¬†updates</li><li><strong>ML model</strong> (re)label training data, retraining, and deployment</li><li><strong>Logging and data analysis on user¬†feedback</strong></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2xY0JS_w7ZTjcm4Q" /><figcaption>Figure 1. Typical taxonomy development iteration cycle.</figcaption></figure><p>Before T-LEAF, the taxonomy development process relied solely on output metrics to measure the effectiveness of a new taxonomy, which means that: 1) major changes take a long time to experiment and test; and 2) minor changes like adding or updating new nodes aren‚Äôt tested. These two pain points can be addressed with the T-LEAF framework by consistent and periodic¬†scoring.</p><p>T-LEAF has been developed to include more quantitative evaluation in the taxonomy development and address the above mentioned two pain points to accelerate the taxonomy development iteration.</p><h3>Taxonomy Learning and EvaluAtion Framework (T-LEAF)</h3><h3>Quality of a¬†Taxonomy</h3><p>T-LEAF framework measures the quality of a taxonomy in three aspects: 1) coverage, 2) usefulness and 3) agreement.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/866/1*Sab-WsEGFK1FXkDTM4AbEA.png" /><figcaption>Figure 2. T-LEAF Structure</figcaption></figure><h3>Coverage</h3><p>Coverage indicates how well a taxonomy can classify the scope of real-world objects. In Contact Reasons, coverage score evaluates how well the taxonomy captures the reasons guests and Hosts contact Airbnb‚Äôs Community Support team. When ‚Äòcoverage‚Äô is low, a lot of user issues (data objects) will not be covered by the taxonomy and become ‚ÄòOther‚Äô or ‚ÄòUnknown‚Äô.</p><blockquote>Coverage Score = 1 - percentage of data classified as ‚Äúother‚Äù or ‚Äúundefined.‚Äù</blockquote><h3>Usefulness</h3><p>Usefulness shows how evenly objects distribute across the structure of the taxonomy into meaningful categories. If a taxonomy is too coarse, i.e., has too few nodes or categories, the limited number of options may not adequately distinguish between the objects that are being described. On the other hand, if a taxonomy is too granular, it may fail to explain similarities between¬†objects.</p><p>In T-LEAF, for a benchmark dataset with n examples (e.g., distinct user issues), we hypothesize that a taxonomy with sqrt(n) number of nodes¬≤ gives a good balance between ‚Äòtoo coarse‚Äô and ‚Äòtoo granular‚Äô. For any input <em>x</em>, we compute a split score from (0,1] to evaluate the ‚Äòusefulness‚Äô:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*z45U60-CPSN1R5jdN76BIQ.png" /></figure><p>We want to evaluate the data deviation by assuming the normal distribution. For example, with 100 distinct user issues, if we split into 1 (‚Äòtoo coarse‚Äô) or 100 categories (‚Äòtoo granular‚Äô), the usefulness score would be close to 0; if we split into 10 categories, the usefulness score would be¬†1.</p><h3>Agreement</h3><p>Agreement captures the inter-rater reliability given the taxonomy. We propose two ways to evaluate agreement.</p><h4>Human Label Inter-rater Agreement</h4><p>Multiple human annotators annotate the same data according to the taxonomy definition and we calculate the inter-rater reliabilities using Cohen‚Äôs Keppa in the range of [-1,¬†1]:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GacnudDKA1g-QVKLeGsiKw.png" /></figure><h4>ML Model Training¬†Accuracy</h4><p>Having multiple human raters annotate one data set can be expensive. In reality, most data is annotated by just one human. In Airbnb‚Äôs Community Support, each customer issue/ticket is processed by one agent and agents label the ticket‚Äôs issue type based on the taxonomy. We train a ML model based on this single-rater labeled training data and then apply the model over the training data to measure the training accuracy. If the taxonomy is well defined (i.e., with high ‚Äòagreement‚Äô), then similar issues (data points) should have similar labels even though these labels come from different agents. ML models trained over highly agreed(consistent) training dataset should have high training accuracy.</p><p>We have done experiments comparing the multi-label inter-rater agreement approach and ML training accuracy over single-rated training¬†data.</p><p>Results are shown in Table 1. We observed that for both methods: 1) accuracies were similar for the top two levels of the taxonomy (L1 and L2 issues are defined in the next section) and; 2) there were similar areas of confusion in both approaches. If taxonomy nodes are clear enough for humans to perform tagging, the consistency rate increases and the model can better capture human intent. The opposite is also true; model training accuracy is negatively impacted if end users are confused by options or unable to choose proper categories.</p><p>It took 1 analyst and 9 annotators about a month to create the multi-rater dataset. In contrast, it took one ML engineer a day to train a ML model over the single-rated data and calculate the training accuracy. As shown in Table 1, ML Training accuracy provides a similar evaluation of taxonomy‚Äôs ‚Äòagreement‚Äô quality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YgpIzpXbhVKc9-GwRrljSQ.png" /><figcaption>Table 1. Comparison between multi-rater labeling approach and ML-model over single-rater training¬†data.</figcaption></figure><h3>Developing the Contact Reason Taxonomy using¬†T-LEAF</h3><p>The Contact Reasons taxonomy consists of nearly 200 nodes, spread across a hierarchy that goes from broad categories in Level 1 (L1) to narrower categories in Level 2 (L2) to specific issues in Level 3 (L3). For¬†example:</p><ul><li>Problems with your reservation (L1)</li><li>Cleanliness and health concerns¬†(L2)</li><li>Smoke or other odors in listing¬†(L3)</li></ul><p>While the old taxonomy had unpredictable levels of granularity, depending on the section, Contact Reasons has a consistent, three-level structure that better supports our continuous evaluation framework. We utilized T-LEAF in the transition from the old taxonomy to the new taxonomy (Contact Reasons) to enable a faster feedback loop and provide a quantified quality control before launching the new taxonomy into production environments (Figure¬†3).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jT-HinGVLNec_9sG" /><figcaption>Figure 3. Iterative process of taxonomy development, evaluation, and deployment with¬†T-LEAF.</figcaption></figure><p>First, we sent a real-world dataset to Airbnb Community Support Labs (CS Labs)‚Ää‚Äî‚Ääa group of skilled and tenured Support Ambassadors‚Ää‚Äî‚Ääfor human annotation. Then, we used T-LEAF scores as an input to the taxonomy development process. Using that input,the Core Machine Learning (CoreML) Engineering team and the Taxonomy team collaborated to significantly improve T-LEAF scores before running experiments in production.</p><p>To evaluate the Contact Reasons taxonomy in one of these production environments, we reviewed its performance in Airbnb bot¬≥. Airbnb bot is one of Community Support‚Äôs core products that helps guests and Hosts self-solve issues and connect to Support Ambassadors when necessary. We found that the improvements to the Contact Reason taxonomy as measured by T-LEAF‚Äôs metrics of coverage, usefulness, and agreement also translated to actual improvements in issue coverage, self-solve effectiveness, and issue prediction accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JbLYJHtVd1E3AWCCtMPDnQ.png" /><figcaption>Table 2. T-LEAF scores between old and new taxonomies</figcaption></figure><h3>A higher T-LEAF coverage score leads to greater issue coverage in production</h3><p>After launching the Contact Reasons taxonomy, we examined 4-months of production data and found that 1.45% of issues were labeled ‚ÄúIt‚Äôs something else,‚Äù which is 5.8% less than the old taxonomy. This is consistent with T-LEAF coverage score improvement (5.3% more coverage than the previous version).</p><h3>A higher usefulness score leads to more issues being resolved through self-service</h3><p>For example, in the new taxonomy, there are two new nodes called ‚Äú<em>Cancellations and refunds &gt; Canceling a reservation you booked &gt; Helping a Host with a cancellation</em>‚Äù and ‚Äú<em>Cancellations and refunds &gt; Canceling a reservation you‚Äôre hosting &gt; Helping a guest with a cancellation.</em>‚Äù The old taxonomy only have nodes for ‚Äú<em>Reservations &gt; Cancellations &gt; Host-initiated</em>‚Äù and ‚Äú<em>Reservations &gt; Cancellations &gt;Guest-initiated</em>‚Äù, which did not have granularity to determine when the guest or Host seeking support is not the one requesting the cancellation.</p><p>With the new nodes, we developed a machine learning model that drives traffic to tailored cancellation workflows‚Å¥. This ensures that guests receive the appropriate refund and Host cancellation penalties are applied only when relevant, all without needing to contact Airbnb Support Ambassadors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*h9c4FN7fDUPhXet6R-B4ow.png" /><figcaption>Figure 4. Airbnb Chatbot self-solve solutions</figcaption></figure><h3>A higher T-LEAF agreement score results in more accurate issue prediction</h3><p>Compared to issue prediction models built on the old taxonomy, the model built on the new taxonomy has improved accuracy by<strong> 9%.</strong> This means that the category the ML model predicts for an issue is more likely to match the category selected by the Support Ambassador.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/0*2FMAyCXyP75We2ku" /><figcaption>Figure 5. User/Agent and ML Model Agreement</figcaption></figure><h3>Conclusion</h3><p>A quantitative framework to evaluate taxonomy supports faster iterations and reduces the risk of launching major taxonomy transformations, which has positive impacts for all of our audiences: guests, Hosts, Support Ambassadors, and Airbnb businesses. The T-LEAF framework that scores the quality of taxonomy in the aspects of coverage, usefulness, agreement, has now been applied to a production taxonomy in Community Support and results show that using this methodology for quantitative taxonomy evaluation can lead to better model performance and larger issue coverage.</p><p>Developing, piloting, and establishing T-LEAF as part of our continuous improvement framework for taxonomy evolution has been a collaborative effort across teams. The CoreML team partnered closely with Taxonomy, Product, and CS Labs to create this new model for iterative development of issue categorization and prediction. Having piloted this new way of working on Contact Reasons, we‚Äôre confident we‚Äôll see more positive results as we continue to apply the T-LEAF methodology to future taxonomy initiatives</p><p>[1]: Szopinski, D., Schoormann, T., &amp; Kundisch, D. (2019). Because Your Taxonomy is Worth IT: towards a Framework for Taxonomy Evaluation. <em>ECIS</em>. <a href="https://aisel.aisnet.org/ecis2019_rp/104/">https://aisel.aisnet.org/ecis2019_rp/104/</a></p><p>[2]: Carlis, J., &amp; Bruso, K. (2012). RSQRT: AN HEURISTIC FOR ESTIMATING THE NUMBER OF CLUSTERS TO REPORT. Electronic commerce research and applications, 11(2), 152‚Äì158. <a href="https://doi.org/10.1016/j.elerap.2011.12.006">https://doi.org/10.1016/j.elerap.2011.12.006</a></p><p>[3]: Intelligent Automation Platform: Empowering Conversational AI and Beyond at Airbnb. <a href="https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2">https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2</a></p><p>[4]: Task-Oriented Conversational AI in Airbnb Customer Support. <a href="https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa">https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa</a></p><p>Interested in working at Airbnb? Check out these open¬†roles:</p><p><a href="https://careers.airbnb.com/positions/3919138/">Senior Staff Data Architect, Community Support¬†Platform</a></p><p><a href="https://careers.airbnb.com/positions/4044524/">Sr. Product Manager, Community Experience Products</a></p><p><a href="https://careers.airbnb.com/positions/3887689/">Product Manager, Claims Experience</a></p><h3>Acknowledgments</h3><p>Thanks to CS Labs for labeling support on existing and new taxonomies!</p><p>Thanks to Pratik Shah, Rachel Lang, Dexter Dilla, Shuo Zhang, Zhiheng Xu, Alex Zhou, Wayne Zhang, Zhenyu Zhao, Jerry Hong, Gavin Li, Kristen Jaber, Aliza Hochsztein, Naixin Zhang, Gina Groom, Robin Foyle, Parag Hardas, Zhiying Gu, Kevin Jungmeisteris, Jonathan Li-On Wing, Danielle Martin, Bill Selman, Hwanghah Jeong, Stanley Wong, Lindsey Oben, Chris Enzaldo, Jijo George, Ravish Gadhwal, and Ben Ma for supporting our successful CS taxonomy launch and workflow related applications!</p><p>Thank Joy Zhang, Andy Yasutake, Jerry Hong, Lianghao Li, Susan Stevens, Evelyn Shen, Axelle Vivien, Lauren Mackevich, Cynthia Garda, for reviewing, editing and making great suggestions to the blog¬†post!</p><p>Last but not least, we appreciate Joy Zhang, Andy Yasutake, Raj Rajagopal, Tina Su and Cynthia Garda for leadership support!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30ae19ce8c52" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52">T-LEAF: Taxonomy Learning and EvaluAtion Framework</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Airbnb‚Äôs Trip to Linaria]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/dc169230bd12</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[web-performance]]></category>
            <category><![CDATA[atomic-css]]></category>
            <category><![CDATA[css-in-js]]></category>
            <category><![CDATA[css]]></category>
            <dc:creator><![CDATA[Joe Lencioni]]></dc:creator>
            <pubDate>Thu, 16 Jun 2022 17:41:49 GMT</pubDate>
            <atom:updated>2022-06-16T17:41:48.753Z</atom:updated>
            <content:encoded><![CDATA[<h4>Learn how Linaria, Airbnb‚Äôs newest choice for web styling, improved both developer experience and web performance</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-qT4pQIPIsxHBZj22sQtag.jpeg" /></figure><p>CSS is a critical component of every web application, and many solutions have evolved for how styles are written by developers and delivered to visitors. In this post we‚Äôll take you through Airbnb‚Äôs journey from Sass to CSS-in-JS and show you why we landed on <a href="https://github.com/callstack/linaria">Linaria, a zero-runtime CSS-in-JS library</a>, and the impact it has had on the developer experience and performance of Airbnb‚Äôs web¬†app.</p><h3>From Sass to CSS-in-JS</h3><p>In 2016, our web frontend was in a monolithic <a href="https://rubyonrails.org/">Ruby on Rails</a> app using a combination of <a href="https://github.com/rails/sprockets">Sprockets</a>, <a href="https://browserify.org/">Browserify</a>, and <a href="https://sass-lang.com/">Sass</a>. We had a <a href="https://getbootstrap.com/">Bootstrap</a>-inspired internal toolkit for styling, but we weren‚Äôt using anything like <a href="https://github.com/css-modules/css-modules">CSS Modules</a> or¬†<a href="http://getbem.com/">BEM</a>.</p><p>Production bugs were often caused by our styling‚Ää‚Äî‚Ääsometimes the correct stylesheet was missing from some pages and other times styles from different stylesheets conflicted unexpectedly.</p><style>body[data-twttr-rendered="true"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true"><p>&#x200a;&mdash;&#x200a;<a href="https://twitter.com/thomasfuchs/status/493790680397803521">@thomasfuchs</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height);resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute("width")); if ( 500  < maxWidth) {window.frameElement.setAttribute("width", "500");}}</script><p>Additionally, developers <a href="https://css-tricks.com/how-do-you-remove-unused-css-from-a-site/">rarely removed styles once added since it was hard to know whether they were still needed</a>. These issues compounded as our product surface area rapidly expanded.</p><p>As we began to build our <a href="https://www.youtube.com/watch?v=fHQ1WSx41CA">Design System</a> in React, we landed on CSS-in-JS as an exciting new option. At the time, CSS-in-JS was still in its infancy‚Äìonly a few libraries existed and <a href="https://styled-components.com/">Styled Components</a> had not been invented yet. We chose <a href="https://github.com/khan/aphrodite">Aphrodite</a>, but didn‚Äôt want to be directly coupled to Aphrodite‚Äôs implementation for two reasons: since CSS-in-JS was a nascent space we wanted to have the flexibility to switch implementations at a later date, and we also wanted something that would work for open source projects where people might not want Aphrodite. So we created an abstraction layer called <a href="https://github.com/airbnb/react-with-styles">react-with-styles</a>, which gave us a <a href="https://reactjs.org/docs/higher-order-components.html">higher-order component (HOC)</a> to define themeable styles.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href">https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href</a></iframe><p>This allowed components to be styled in the same file, making repo organization more convenient. More importantly, <strong>moving from a globally-aware styling system to a component-based styling system gave us guarantees around how styles would be applied and what files were needed to render every component correctly on every page</strong>. This enabled us to rely on <a href="https://happo.io/">Happo, our screenshot testing tool of choice</a>, and as a result visual regressions plummeted (disclosure: I am the co-creator of¬†Happo).</p><p>Though react-with-styles has served us well for years, it comes with <strong>performance</strong> and <strong>developer experience</strong> tradeoffs. The styles and runtime libraries increase critical path JS bundle size, and applying styles at render-time comes with a CPU cost (10‚Äì20% of our component mount times). While we get the aforementioned guarantees about styles, actually writing styles in JavaScript objects feels awkward compared to regular CSS syntax. These tradeoffs led us to reconsider how we style the web at¬†Airbnb.</p><h3>Considering Our¬†Options</h3><p>To address the problems with react-with-styles, we formed a working group of engineers from various teams. We considered a number of directions, which fit into the following high-level categories:</p><ul><li>Static extraction of CSS from react-with-styles at build¬†time</li><li>Write our own framework</li><li>Investigate and adopt an existing framework</li></ul><p>We decided against <strong>static extraction</strong> from react-with-styles at build time because it would require a lot of effort. Additionally, it would be home-grown and therefore lack benefits of a community. Finally, it does not address developer ergonomics issues.</p><p>Similarly, <strong>writing our own framework</strong> would have had a high cost of initial implementation, maintenance, and support. Additionally, there were existing solutions for this problem that we wanted to leverage and contribute back¬†to.</p><figure><img alt="An xkcd comic titled ‚ÄúHow Standards Proliferate: (see: A/C chargers, character encodings, instant messaging, etc). Panel 1: Situation: There are 14 competing standards. Panel 2: ‚Äú14?! Ridiculous! We need to develop one universal standard that covers everyone‚Äôs use cases.‚Äù ‚ÄúYeah!‚Äù Panel 3: Soon: Situation: There are 15 competing standards." src="https://cdn-images-1.medium.com/max/500/0*7Kwk-MuLZOIUhgnv" /><figcaption>Comic from <a href="https://xkcd.com/927/">https://xkcd.com/927/</a> by Randall Munroe and is used under a CC-BY-NC 2.5¬†license.</figcaption></figure><p>After evaluating several <strong>existing frameworks</strong> against our requirements, we narrowed down candidates for building a proof of¬†concept:</p><ul><li><a href="https://emotion.sh/docs/introduction">Emotion</a>: CSS-in-JS, with a low runtime¬†cost</li><li><a href="https://github.com/callstack/linaria">Linaria</a>: zero-runtime CSS-in-JS (static CSS extraction)</li><li><a href="https://github.com/seek-oss/treat">Treat</a>: near zero-runtime CSS-in-JS (static CSS extraction)</li></ul><p>The proof-of-concepting work was done in a new repo that implemented a server-rendered client-hydrated unstyled version of Airbnb‚Äôs logged in homepage. For each framework, this allowed us¬†to:</p><ul><li>Understand what changes might need to be made to our build¬†system</li><li>Try out framework APIs and get a feel for developer ergonomics</li><li>Assess how each framework supports our web styling requirements</li><li>Gather performance metrics</li><li>Serve as a starting point for a migration plan</li></ul><p>Frameworks were evaluated against each other based on the following ranked list of criteria:</p><ol><li><strong>Performance</strong></li><li><strong>Community</strong> (i.e. support and adoption)</li><li><strong>Developer experience</strong></li></ol><h3>Performance Analysis</h3><p>Using <a href="https://www.speedcurve.com/">SpeedCurve</a>, local benchmarking, and the <a href="https://reactjs.org/docs/profiler.html">React &lt;Profiler /&gt;</a>, we ran performance benchmarking tests for each framework. All results were calculated as the median of 200 runs on a throttled MacBook Pro, and are statistically significantly different from control with a p-value of &lt;=¬†0.05.</p><p>Informed by <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb‚Äôs Page Performance Score</a> (similar to <a href="https://web.dev/performance-scoring/">Lighthouse‚Äôs performance score</a>), we focused on the following metrics to give us an idea of how each framework performed and would impact the user experience:</p><ul><li><a href="https://web.dev/tbt/">Total blocking time¬†(TBT)</a></li><li>Bundle size</li><li>Update layout tree count and¬†duration</li><li>Composite layers count and¬†duration</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href">https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href</a></iframe><p>It is clear that the frameworks are divided into two groups: <strong>runtime frameworks</strong> (react-with-styles, Emotion) and <strong>build-time frameworks</strong> (Linaria, Treat).</p><p>Benchmarks of the server-rendered and client-hydrated version of our homepage showed Treat and Linaria performing 36% and 22% better than Emotion on <strong>Total Blocking Time</strong>, respectively. All frameworks performed significantly better than react-with-styles, ranging from a 32‚Äì56% improvement. <em>(Note that these numbers should not be used to estimate expected improvements in production, as this is a very specific benchmark designed to test differences between frameworks, not expected savings in production.)</em></p><p><strong>Bundle size</strong> differences also fall into these two categories‚Ää‚Äî‚Ääwith savings on the order of 80 KiB (~12%) for the Linaria/Treat group.</p><p>The CSS metrics (<strong>update layout tree</strong> and <strong>composite layers</strong>) show that, on average, there is roughly one more layout tree update and layer composition event for react-with-styles/Emotion. This is likely due to the insertion and hydration of stylesheets with JavaScript that is not necessary with a CSS extraction library like Linaria or¬†Treat.</p><p>This performance investigation shows that either Linaria or Treat would be promising options to adopt, and that all frameworks considered are a statistically significant improvement over react-with-styles with Aphrodite.</p><h3>What We Liked About¬†Linaria</h3><p>The above <strong>performance</strong> improvements were largely thanks to Linaria extracting the styles from JS to static CSS files at build time, so there is no JS bundle or runtime CPU overhead‚Ää‚Äî‚Äägiving it a slight edge over the near-zero runtime Treat. Also, this brings caching benefits since these static CSS files may change at a different cadence than the JS files. Since the styles are extracted at build time, Linaria has the opportunity to automatically remove unused styles‚Ää‚Äî‚Ääthis also opens the door to the possibility of deduplicating styles (i.e. <a href="https://css-tricks.com/lets-define-exactly-atomic-css/">Atomic CSS</a>). Additionally, Linaria supports injecting the critical CSS for server-side rendering, which we had wanted to preserve from our react-with-styles integration.</p><p><a href="https://snyk.io/advisor/npm-package/linaria">Linaria also seemed to be a healthy project</a> that saw a good amount of activity, <strong>community</strong> involvement, documentation, and adoption. Its good trajectory gave us confidence that it would continue to improve and that we would be able to contribute back.</p><p>We found Linaria‚Äôs <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> API that enables developers to use CSS syntax to be an attractive improvement over the JS object HOC API that we built for react-with-styles. Additionally, off-the-shelf integrations were available for stylelint, CSS autocompletion, and syntax highlighting, which enriched the <strong>developer experience</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hyXFKX-bB-ixvHsE" /><figcaption>Off-the-shelf integrations for stylelint, CSS autocompletion, and syntax highlighting working with Linaria in¬†action.</figcaption></figure><p>We also found value in the similarities between Linaria and our existing solution. The co-location of styles within the component file was a big feature that tipped the scales in favor of Linaria over Treat for us, and the familiar API smoothed the transition for developers and gave us confidence that migration efforts could be eased with automation.</p><h3>Migration Strategy</h3><p>To roll out this big change, we adopted an incremental migration strategy that is largely automated by <a href="https://medium.com/airbnb-engineering/turbocharged-javascript-refactoring-with-codemods-b0cae8b326b9">codemods</a> we‚Äôve written. We are leaning heavily on our <a href="https://happo.io/">Happo screenshot tests</a> to ensure that our components look the same after they are migrated. This allows sections of our codebase to be migrated by running a script and following up with any necessary tweaks, similar to <a href="https://medium.com/airbnb-engineering/ts-migrate-a-tool-for-migrating-to-typescript-at-scale-cd23bfeb5cc">the approach we took when adopting TypeScript</a>.</p><p>The first phase of the migration was handled by the web styling working group and targeted converting a subset of components on a few select pages with varying performance characteristics. This phase was gated on A/B tests which ensured that our initial understanding of the performance held up under the specifics of our app and assured us that there were no hidden problems.</p><p>Once we were confident about the performance and correctness of our Linaria integration, we allowed teams to start using Linaria in new code. We also encouraged teams to migrate their existing code using our codemods. Although the migration has proceeded at a good pace organically, we plan to ensure that all code has moved off of react-with-styles so that we can eventually remove the runtime dependencies from the bundles entirely. This consistency will give us an additional performance boost and reduce the cost of <a href="https://en.wikipedia.org/wiki/Decision_fatigue">decision¬†fatigue</a>.</p><h3>Contributing Back</h3><p>Once we started using Linaria, we discovered that automatic style deduplication (i.e. Atomic CSS) would give us not just a performance boost, but also would fix some non-performance-related hiccups we ran¬†into.</p><p>The selectors that Linaria generates are all of the same <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Specificity">specificity</a>. Since CSS selectors of the same specificity depend on their declaration order, the order that the bundler builds these files becomes important. This is problematic when sharing styles between files, since we cannot predict or maintain the order of the styles as the shape of the dependency graph¬†changes.</p><p>We initially approached this problem by creating a new <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> for CSS fragments which allows for the styles to be interpolated into Linaria‚Äôs CSS tagged template literals. This works okay, but it is unintuitive, defeats <a href="https://github.com/prettier/prettier/blob/d13feed42b6478710bebbcd3225ab6f203a914c1/src/language-js/embed.js#L90-L121">tooling that expects styles to be defined in CSS tagged template literals</a>, and leads to the styles being included several times in the CSS bundles (which is suboptimal for performance).</p><p>Josh Nelson, a member of our web styling working group, <a href="https://github.com/callstack/linaria/pull/867">contributed Atomic CSS support back to Linaria</a> and the Linaria community has been very supportive. The change adds a new <a href="https://npmjs.com/@linaria/atomic">@linaria/atomic</a> package that when imported instead of <a href="https://www.npmjs.com/package/@linaria/core">@linaria/core</a> will generate Atomic CSS at build time. This means that if you write your code like¬†this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href">https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href</a></iframe><p>Instead of generating output like this (without Atomic¬†CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href">https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href</a></iframe><p>The generated output will look something like this (with Atomic¬†CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href">https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href</a></iframe><p>The order of appearance problem is solved by build time analysis that chains class names based on the order they are passed in to the cx function to increase specificity when necessary.</p><h3>Reception</h3><p>Our engineers have reacted positively to Linaria. Here are some¬†quotes:</p><blockquote>‚ÄúLinaria opens up a world where we can code like it‚Äôs 1999, in old school pure on CSS. It advises against bad patterns, but gives us the flexibility to build amazing experiences. We‚Äôre not fighting the platform anymore, we‚Äôre harnessing it and it feels incredibly powerful.‚Äù‚Ää‚Äî‚ÄäCallie¬†Riggins</blockquote><blockquote>‚ÄúCompared to react-with-styles, I care more about what I‚Äôm creating now. Linaria is so good.‚Äù‚Ää‚Äî‚ÄäIan Demattei-Selby</blockquote><blockquote>‚ÄúI really liked being able to write CSS again. It gives you so much more control over what you can style in the component.‚Äù‚Ää‚Äî‚ÄäBrie¬†Bunge</blockquote><blockquote>‚ÄúIt‚Äôs great to be writing actual CSS again.‚Äù‚Ää‚Äî‚ÄäVictor¬†Lin</blockquote><p>Thanks to its familiar CSS syntax, style extraction into static stylesheets, and application of styles using class names, Linaria <strong>increases product development speed</strong> and <strong>unlocks new styling capabilities not possible with react-with-styles and Aphrodite</strong>.</p><h3>Performance Impact</h3><p>Though we are still at the beginning of our migration, we have run some A/B tests that give us an encouraging look at the real world performance impact of switching to Linaria for a large group of visitors in the¬†wild.</p><p>In one experiment, we converted about 10% of the components rendered on the airbnb.com homepage from react-with-styles to Linaria, and saw Homepage <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Page Performance Score</a> improve by 0.26%. <a href="https://web.dev/fcp/">Time to First Contentful Paint (TTFCP)</a> improved by 0.54% (mean of 790ms), while <a href="https://web.dev/tbt/">Total Blocking Time (TBT)</a> also had a strong improvement of 1.6% (mean of 1200ms). To put this in perspective, hydrating the homepage with React takes around 200ms for most people, so improvements of this order of magnitude are significant. We believe these performance improvements with Linaria are attributable to no longer generating CSS styles at render-time, which improves render times on both server and¬†client.</p><p>Assuming the performance improvements will scale linearly (which is a big assumption), converting the remaining 90% of the components <em>might</em> result in a 2.6% improvement to Page Performance Score, 5.4% improvement to Time to First Contentful Paint (TTFCP), and 16% improvement to Total Blocking Time¬†(TBT).</p><p>Note that direct comparisons with other industry numbers are a little tricky here, given the different ways we define pages especially with regard to client¬†routing.</p><h3>What Does This Mean for react-with-styles?</h3><p>Given that we still have many components that still depend on react-with-styles and that it will take a while for us to complete our migration, <strong>we will put react-with-styles in maintenance mode</strong> until we approach the end of our migration. At that point, <strong>we intend to sunset react-with-styles</strong> and the related packages.</p><p>By removing an option from the marketplace we hope to help the community coalesce towards a common solution and invest in better frameworks. If you are looking for a new tool, we think Linaria is a great¬†choice!</p><h3>Conclusion</h3><p>Styling infrastructure is still an exciting space, rich with opportunities. At Airbnb, we‚Äôve found big improvements to the <strong>developer experience</strong> by adopting a framework that allows regular CSS syntax to be used alongside our React component code. And by replacing a runtime styling library with one that compiles to static CSS files at build time, we are able to continue driving toward faster <strong>performance</strong>. Thanks to the Linaria <strong>community</strong> and our collaboration, we expect this library to continue to improve for many¬†years.</p><p>Interested in working at Airbnb? Check out these open¬†roles:</p><p><a href="https://grnh.se/ebfa55151us">Frontend Infrastructure Engineer, Web Platform</a><br><a href="https://grnh.se/b5afa9151us">Staff Software Engineer, Data Governance </a><br><a href="https://grnh.se/92c32fed1us">Staff Software Engineer, Cloud Infrastructure </a><br><a href="https://grnh.se/bbe55fe81us">Staff Database Engineer </a><br><a href="https://grnh.se/21e5c2011us">Staff Software Engineer‚Ää‚Äî‚ÄäML Ops Platform </a><br><a href="https://grnh.se/ee114dfc1us">Senior/Staff Software Engineer, Service Capabilities</a></p><h3>Acknowledgments</h3><p>We have a lot of appreciation for the folks at <a href="https://www.callstack.com/">callstack</a> and the <a href="https://github.com/callstack/linaria#contributors">Linaria community</a> for building such a great tool and for collaborating with us to make it even better. Also for <a href="https://www.khanacademy.org/">Khan Academy</a> for giving us Aphrodite which served us well for many years. This has been a huge effort at Airbnb that would not have been possible without all the work put in by so many people at Airbnb, including Mars Jullian, Josh Nelson, Nora Tarano, Alan Wright, Jimmy Guo, Ian Demattei-Selby, Victor Lin, Nnenna John, Adrianne Soike, Garrett Berg, Andrew Huth, Austin Wood, Chris Sorenson, and Miles Johnson. Finally, thank you to Surashree Kulkarni for help editing this blog post. Thank you¬†all!</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dc169230bd12" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12">Airbnb‚Äôs Trip to Linaria</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Graph Machine Learning at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f868d65f36ee</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Devin Soni]]></dc:creator>
            <pubDate>Tue, 14 Jun 2022 15:59:22 GMT</pubDate>
            <atom:updated>2022-06-14T15:59:22.004Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>How Airbnb is leveraging graph neural networks to up-level our machine¬†learning</strong></h4><p>By:<a href="https://www.linkedin.com/in/devinsoni/"> Devin¬†Soni</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bEZU2cupMt44ke6mkK-low.jpeg" /></figure><h3>Introduction</h3><p>Many real-world machine learning problems can be framed as graph problems. On online platforms, users often share assets (e.g. photos) and interact with each other (e.g. messages, bookings, reviews). These connections between users naturally form edges that can be used to create a¬†graph.</p><p>However, in many cases, machine learning practitioners do not leverage these connections when building machine learning models, and instead treat nodes (in this case, users) as completely independent entities. While this does simplify things, leaving out information around a node‚Äôs connections may reduce model performance by ignoring where this node is in the context of the overall¬†graph.</p><p>In this blog post, we will explain the benefits of using graphs for machine learning, and show how leveraging graph information allows us to learn more about our users, in addition to building more contextual representations of them [4]. We will then cover specific graph machine learning methods, such as Graph Convolutional Networks, that are being used at Airbnb to improve upon existing machine learning¬†models.</p><p>The motivating use-case for this work is to build machine learning models that protect our community from harm, but many of the points being made and systems being built are quite generic and could be applied to other tasks as¬†well.</p><h3>Challenges</h3><h4><strong>The problem</strong></h4><p>When building trust &amp; safety machine learning models around entities such as users or listings, we generally begin by reaching for features that directly describe the entity. For example, in the case of users, we may use features such as their location, account age, or number of bookings. However, these simple features do not adequately describe the user in the context of the overall Airbnb platform and their interactions with other¬†users.</p><p>Consider the hypothetical scenario in which a new host joins Airbnb. A week into their hosting journey, we likely do not have a lot of information about them other than what they have directly told us. This could include their listing‚Äôs location or their phone number. These direct attributes given to us by the host are relatively surface level and do not necessarily help us understand their trustworthiness or reputation.</p><p>In this state, it is hard for Airbnb to provide this new host with the best possible experience since we do not know what their usage pattern of the platform will be. Lacking information, we might then make this new host go through a slower onboarding process or request a lot of information up-front. Understanding this user‚Äôs relationships to the rest of the platform is data we can leverage to provide them with an improved experience.</p><h4><strong>An illustration of the usefulness of¬†graphs</strong></h4><p>While we do not have much direct information about the new host, what we can do is leverage their surroundings to try and learn more. One example of this is the connections that they have to other¬†users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/1*BAJt6yhBPetG4HwwUylGiQ.png" /></figure><p>We can first take a look at their one-hop neighborhood, or in other words, the set of users whom this host has a direct connection with. In this example, we can see that this new host shares a listing photo with an existing, tenured host. We can also see that the new host‚Äôs listing is in the same house as listings from three other hosts. With this additional knowledge, we now know more about the new host; they might be working with other hosts who have rooms in the same house. However, we can‚Äôt be completely sure how all of the connected hosts relate to each other without looking at more of the¬†graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/971/1*9u8zd4_qXAA46RGCUWa0kw.png" /></figure><p>Let‚Äôs further expand our view and consider the two-hop neighborhood of the new host. This expands our view to users who are not necessarily directly connected to the new host. In this expanded network we now see that many of the hosts with listings in the location are connected to each other through a shared business name. It now becomes very likely that this new host is part of an existing group of hosts that rent out rooms in the same house, and has not yet updated their profile to reflect¬†this.</p><p>Using the power of graphs, we were able to learn about a new host simply by inspecting their connections to other users within Airbnb. This additional knowledge extracted from the graph provides us with an improved glimpse into who our new host is. We are subsequently able to deliver an improved experience to this new host, all without requiring them to provide any more information to¬†Airbnb.</p><p>Supplementing our models with graph information is one way to bootstrap our models. Using graphs, we can construct a detailed understanding of our users in scenarios where we have little historical data or observations pertaining to a user. While the semantic information we gain from the graph is often inferred and not directly told to us by the user, it can give us a strong baseline level of knowledge until we have more factual information about a¬†user.</p><h3><strong>Graph Machine¬†Learning</strong></h3><p>We have established that we want our machine learning models to be able to ingest graph information. The main challenge is figuring out how best to condense everything a graph can represent into a format that our models can use. Let‚Äôs dig into some of the options and explore the solution that we ultimately implemented.</p><p>One simple option is to calculate statistics about nodes and use them as numeric features. For example, we can calculate the number of users a user is connected to or how many listing photos they share with other hosts. These metrics are straightforward to calculate and give us a basic sense of the node‚Äôs role in the overall structure of the graph. These metrics are valuable but do not leverage the node‚Äôs features. As such, simple statistics cannot go beyond representing graph structure.</p><p>What we really want is to be able to produce an aggregation of a node‚Äôs neighborhood in the graph that captures both the node‚Äôs structural role in the graph and its node features. For example, we want to know more than how many users a user is connected to; we also want to understand the type of users they are connected to (e.g. their account tenure, or past booking counts) because that gives us more hints about the original user than simple edge¬†counts.</p><h4><strong>Graph Convolutional Networks</strong></h4><p>To capture both graph structure and node features, we can use a type of graph neural network architecture called a graph convolutional network. Graph convolutional networks (GCN) are neural networks that generally take as input a matrix of node features in addition to an adjacency matrix of the graph and outputs a node-level output. This type of network architecture is preferred over simply concatenating pre-computed structural features with node features because it is able to jointly represent the two types of information, likely producing richer embeddings.</p><p>Graph convolutional networks consist of multiple layers. A single GCN layer aims to learn a representation of the node that aggregates information from its neighborhood (and in most cases, combines its neighborhood information with its own features). Using the example of our newly created host account, one GCN layer would be the host‚Äôs one-hop neighborhood. A second GCN layer representing the host‚Äôs two-hop neighborhood can be introduced to capture additional information. Since the output of GCN layer N is used to produce the representations used in GCN layer N+1, adding layers increases the span of the aggregation used to generate node representations [1].</p><p>Drawing from the example in the previous section, we would need a GCN with two layers in order to produce a graph embedding that captures the illustrated subgraph. We could go even deeper and expand further to third-order, fourth-order, and so on. In practice, however, a small number of layers (e.g. 2‚Äì4) is sufficient, as the connections beyond that point are likely to be very noisy and unlikely to be relevant to the original¬†user.</p><h4><strong>Model architecture &amp;¬†training</strong></h4><p>Having decided to use GCNs, we must now consider how complex we want each layers‚Äô method of aggregating neighboring nodes‚Äô features to be. There are a wide variety of aggregation methods which can be used. These include mean pooling, sum pooling, as well as more complex aggregators involving attention mechanisms [5].</p><p>When it comes to trust &amp; safety, we often work in adversarial problem domains where frequent model retraining is required due to concept drift. Limiting model complexity, and limiting the number of models that must be retrained is important for reducing maintenance complexity.</p><p>One might assume that GCNs with more complex, expressive aggregation functions are always better. This is not necessarily the case. In fact, several papers have shown that in many cases very simple graph convolutional networks are all that is needed for state-of-the-art performance in practical tasks [2, 3]. The Simplified GCN (SGC) architecture showed that we can achieve performance comparable to more complex aggregators using GCN layers that do not have trainable weights [2]. The Scalable Graph Inception Network (SIGN) architecture showed that, in general, we can precompute multiple aggregations without trainable weights, and use them in parallel as inputs into a downstream model [3]. SIGN and SGC are very related; SIGN provides a general framework for precomputing graph aggregations, and SGC provides the most straightforward aggregator to use within the SIGN framework.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5h9Tp1c8MkyEJAE5ZR2hBA.png" /></figure><p>Using SIGN and SGC, the GCN is purely a fixed feature extractor that does not need to learn anything itself‚Ää‚Äî‚Ääit has no weights that must be tuned during training. In this setting we are able to fundamentally treat the GCN as a fixed mathematical formula applied to its inputs. This aspect is very convenient because we do not need to worry about supervised training or pre-training of the GCN¬†itself.</p><h4><strong>Model serving</strong></h4><p>When serving a graph neural network, the main considerations are around freshness of the data and how to obtain the inputs for the model in a production setting. Our primary concern is the trade-offs related to data freshness. The decision between real-time or batch methods has an impact on how up to date the information is.</p><p>Real-time methods can provide downstream models with the most up-to-date information. This increased freshness does, however, require more effort to serve the embeddings. In addition, it often relies on a downsampled version of the graph to handle nodes with many edges, such as in the GraphSAGE algorithm [4].</p><p>Offline batch methods are able to calculate all node embeddings at once. This provides a distinct advantage over real-time methods by reducing implementation complexity. Unfortunately, the tradeoff does come at a cost. We will not necessarily be able to serve the most recent node embedding as we will only be able to leverage information present in the last run of the pipeline.</p><h3><strong>Chosen solution</strong></h3><p>Given all the tradeoffs and our requirements, we ultimately decided to use a periodic offline pipeline which leverages the SIGN method for our initial implementation. The easy maintenance of batch pipelines and relative simplicity of SIGN allows us to optimize for learning instead of performance initially.</p><p>Despite the fact that many of our trust &amp; safety models are run online in real-time, we decided to start with an offline graph model. Features are computed using a snapshot of the graph and node features. When fetching these features online, the downstream model simply looks up the previous run‚Äôs output from our feature store, rather than having to compute the embedding in real-time. The alternative of a real-time graph embedding solution would involve a significant amount of additional implementation complexity.</p><h3>Benefits Realized</h3><p>With the batch pipeline implemented, we can now have access to new information as features in downstream models. Our existing feature sets did not capture this information and it has resulted in significant gains in our models. Components of the embedding are often among the top 10 features in downstream models based on feature importance computed using <a href="https://github.com/slundberg/shap">the SHAP approach</a>.</p><p>These positive results encourage further investment in the area of graph embeddings and graph signals, and we plan to explore other types of graphs &amp; graph edges. Investigating how to make our embedding more powerful either through improving the freshness of the data or using other algorithms has become a priority for us based on the success of augmenting our existing models with graph knowledge.</p><h3>Conclusion</h3><p>In this blog post, we showed how leveraging graph information can be broadly useful and discussed our approach to implementing graph machine learning. We ultimately decided to use a SIGN architecture that leverages a batch pipeline to calculate graph embeddings. These are subsequently fed into downstream models as features. Many of the new features have led to notable performance gains in the downstream models.</p><p>We hope that this information helps others understand how to leverage graph information to improve their models. Our various considerations provide insight into what one must be aware of when deciding to implement a graph machine learning¬†system.</p><p>Graph machine learning is an exciting area of research in Airbnb, and this is only the beginning. If this type of work interests you, check out some of our related positions:</p><p><a href="https://careers.airbnb.com/positions/3910069/">Senior Machine Learning¬†Engineer</a></p><p><a href="https://careers.airbnb.com/positions/4113532/">Senior Software Engineer, Trust</a></p><h3>Acknowledgments</h3><p>This project couldn‚Äôt have been done without the great work of many people and teams. We would like to¬†thank:</p><ul><li>Owen Sconzo for working on this project and reviewing all of the¬†code.</li><li>The Trust Foundational Modeling team for providing the foundational data for graph modeling.</li><li>Members of the Fraud &amp; Abuse Working Group for supporting this project, reviewing this blog post, and providing suggestions.</li></ul><h3>References</h3><p>[1] <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks.</a></p><p>[2] <a href="https://arxiv.org/abs/1902.07153">Simplifying Graph Convolutional Networks</a></p><p>[3] <a href="https://arxiv.org/abs/2004.11198">SIGN: Scalable Inception Graph Neural¬†Networks</a></p><p>[4] <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large¬†Graphs</a></p><p>[5] <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></p><p>****************</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f868d65f36ee" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee">Graph Machine Learning at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unified Payments Data Read at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e613e7af1a39</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[payments]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Alican G√ñKSEL]]></dc:creator>
            <pubDate>Thu, 09 Jun 2022 22:23:30 GMT</pubDate>
            <atom:updated>2022-06-10T21:47:52.619Z</atom:updated>
            <content:encoded><![CDATA[<p>How we redesigned payments data read flow to optimize client integrations, while achieving up to 150x performance gains.</p><p>By: <a href="https://www.linkedin.com/in/ali-can-g%C3%B6ksel-7189214a">Ali Goksel,</a> <a href="https://www.linkedin.com/in/linglongzhu">Linglong Zhu</a>, <a href="https://www.linkedin.com/in/yixiamao">Yixia¬†Mao</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fWhQRxAT0vMQwTEW" /></figure><h3>Introduction</h3><p>In recent years, Airbnb migrated most of its backend services from a monolith to a service-oriented architecture (SOA). This industry standard architecture brings countless benefits to a company that is at the scale of Airbnb; however, it is not free of challenges. With data scattered across many services, it‚Äôs difficult to provide all the information clients need in a simple and performant way, especially for complex domains such as payments. As Airbnb grew, this problem started to crop up for many new initiatives such as host earnings, tax form generation, and payout notifications, all of which required data to be read from the payments¬†system.</p><p>In this blog post, we introduce Airbnb‚Äôs unified payments data read layer. This read layer was custom built to reduce the friction and complexity for client integrations, while greatly improving query performance and reliability. With this re-architecture, we were able to provide a greatly optimized experience to our host and guest communities, as well as for internal teams in the trust, compliance, and customer support¬†domains.</p><h3>Evolution of Airbnb‚Äôs Payments¬†Platform</h3><p>Payments is one of the earliest functionalities of the Airbnb app. Since our co-founder Nate‚Äôs first commit, Payments Platform has grown and evolved tremendously, and it continues to evolve at an even faster pace given our expanding global presence.</p><p>Similar to other companies, Airbnb started its journey with a monolithic application architecture. Since the feature set was initially limited, both write and read payment flows were ‚Äúrelatively‚Äù simple.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*dsBzT9rDVCNoG8MM" /><figcaption>Overly simplified diagram of Airbnb‚Äôs old monolithic architecture. Payments schemas were not very complex, and the feature set was¬†limited.</figcaption></figure><p>Predictably, this architecture couldn‚Äôt scale well with the rapid growth and expansion of our company. Payments, along with most other parts of the tech stack, started to migrate to the SOA architecture. This brought a significant overhaul of the existing architecture and provided many advantages, including:</p><ul><li>We had clear boundaries between different services, which enabled better domain ownership and faster iterations.</li><li>Data was separated into domains in a very normalized shape, resulting in better correctness and consistency.</li></ul><p>For more, take a peek at our <a href="https://medium.com/airbnb-engineering/rebuilding-payment-orchestration-at-airbnb-341d194a781b">blog post</a> detailing the payments SOA migration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6lC8Bu1u3DA0WAzV" /><figcaption>After the SOA migration, every payments subdomain has its own service(s) and tables with clear boundaries, but more features leads to more complex and normalized data.</figcaption></figure><h3>New Architecture Introduces New Challenges</h3><p>Payments SOA provided us with a more resilient, scalable, and maintainable payments system. During this long and complex migration, correctness of the system was our top priority. Data was normalized and scattered across many payments domains according to each team‚Äôs responsibilities. This subdivision of labor had an important side effect: presentation layers now often needed to integrate with multiple payments services to fetch all the required¬†data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*itWZQ8kaxNjoZ-7f" /><figcaption>How payments data read flows looked after the SOA migration. Presentation services called one or more payments services and aggregated data at the application layer.</figcaption></figure><p>At Airbnb, we believe in being transparent with our host and guest communities. Our surfaces related to payments and earnings display a range of details including fees, transaction dates, currencies, amounts, and total earnings. After the SOA migration, we needed to look into multiple services and read from even more tables than prior to the migration to get all the requested information. Naturally, this foundation brought challenges when we wanted to add new surfaces with payments data, or when we wanted to extend the existing surfaces to provide additional details. There were three main challenges that we needed to¬†solve.</p><p>The first challenge was that <em>clients now needed to understand the payments domain well enough</em><strong> </strong>to pick the correct services and APIs. For client engineers from other teams, this required a non-trivial amount of time investment and slowed down overall time to market. On the payments side, engineers needed to provide continuous consultation and guidance, occupying a significant portion of their work¬†time.</p><p>The second challenge was that there were many instances in which we had to change multiple payments APIs at the same time in order to meet the client requirements. When there are<em> too many touchpoints</em>, it becomes <em>hard to prioritize requests</em> since many teams have to be involved. This problem also caused significant negative impact to time to market. We had to slow down or push back feature releases when the alignment and prioritization meetings did not go smoothly. Similarly, when payments teams had to update their APIs, they had to make sure that all presentation services adopted these changes, which slowed down progress on the payments¬†system.</p><p>Last but not least, the technical quality of the complex read flows was not where we wanted it to be. Application-level aggregations worked fine for the average use case, but we had space for improvement when it came to our large hosts and especially for our prohosts, who might have thousands of yearly bookings on our platform. To have confidence in our system over the long term, we needed to find a solution that provided inherently better <strong><em>performance, reliability, and scalability</em>.</strong></p><h3>Introducing the Payments Unified Data Read¬†Layer</h3><p>To achieve our ambitious goals for payments, we needed to re-think how clients integrate with our payments platform.</p><h3>Unified Entry¬†Points</h3><p>Our first task was to unify the payments data read entry points. To accomplish this, we leveraged <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, Airbnb‚Äôs data-oriented service mesh, where clients query for the ‚Äúentity‚Äù instead of needing to identify dozens of services and their APIs. This new architecture required our clients to worry only about the requisite data entity rather than having to communicate with individual payments services.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rtNAGGJFpwHfnZ5V" /><figcaption>Instead of communicating with individual payments services, presentation services just use the read¬†layer.</figcaption></figure><p>In these entry points, we provided as many filtering options as possible so each API could hide filtering and aggregation complexity from its clients. This also greatly reduced the numbers of APIs we needed to¬†expose.</p><h3>Unified Higher-Level Data¬†Entities</h3><p>Having a single entry point is a good start, but it does not resolve all the complexity. In payments, we have 100+ data models, and it requires a decent amount of domain knowledge to understand their responsibilities clearly. If we just expose all of these models from a single entry point, there would still be too much context required for client engineers.</p><p>Instead of making our clients deal with this complexity, we opted to hide payments internal details as much as possible by coming up with <strong>higher-level domain entities</strong>. Through this process, we were able to reduce the core payments data to fewer than ten<strong> </strong>high level entities, which greatly reduced the amount of exposed payments internal details. These new entities also allowed us to guard clients against changes made in Payments platform. When we internally update the business logic, we keep the entity schema the same without requiring any migrations on the client side. Our principles for the new architecture were the following:</p><ul><li><strong>Simple</strong>: Design for non-payments engineers, and use common terminology.</li><li><strong>Extensible</strong>: Maintain loose coupling with storage schema, and encapsulate concepts to protect from payments internal changes while allowing quick iterations.</li><li><strong>Rich</strong>: Hide away the complexity but not the data. If clients need to fetch data, they should be able to find it in <strong><em>one</em></strong> of the entities.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hPMYFXmawEXCM8tf" /><figcaption>Expose cleaner higher-level domain entities to hide payments internal details while guarding clients from frequent API migrations.</figcaption></figure><h3>Materialize Denormalized Data</h3><p>With unified entry points and entities, we greatly reduced the complexity for client onboardings. However, the ‚Äú<strong><em>how</em></strong>‚Äù of fetching the data, combined with expensive application layer aggregations, was still a big challenge. While it‚Äôs important that clients are able to integrate with the payments system smoothly, our valued community should also enjoy the experience on our platform.</p><p>The core problem we identified was <strong><em>dependency on many tables and services during client queries</em></strong>. One of the promising solutions was denormalization‚Äìessentially, moving these expensive operations from query time to ingestion time. We explored different ways of pre-denormalizing payments data and materializing it reliably with less than 10 seconds replication lag. To our great luck, our friends in the Homes Foundation team were piloting a Read-Optimized Store Framework, which takes an event-driven lambda approach to materializing secondary indices. Using this framework, teams are able to get both near real-time data via database change capture mechanisms and historical data leveraging our daily database dumps stored in Hive. In addition, the maintenance requirements of this framework (e.g., single code for online and offline ingestion, written in Java) were much lower compared to other existing internal solutions..</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aEEGisG92n5f9mc8" /><figcaption>A high-level look at the read-optimized store framework usage by payments. It provides ingestion flows for both offline and near real-time data with shared business logic between¬†them.</figcaption></figure><p>After combining all of above improvements, our new payments read flow looked like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kWOXKjHjm7Jvskze" /><figcaption>The final shape of the payments data read architecture. Clients do not need to know any payments services or internals.</figcaption></figure><p>We provide data in a reliable and performant way via denormalized read-optimized store¬†indices.</p><h3>Results</h3><h3>Migrate and Elevate: Transaction History</h3><p>The first test surface for the new unified data read architecture was Transaction History (TH). Hosts on our platform use the <a href="https://www.airbnb.com/users/transaction_history">Transaction History page</a> to view their past and future payouts along with top-level earning metrics (e.g., total paid out¬†amount).</p><p>On the technical side, this was one of the most complex payments flows we had. There were many different details required, and the data was coming from <strong>10+</strong> payments tables. This had caused issues in the past, including timeouts, slow loading times, downtime due to hard dependencies, and slow iteration speed as a result of complex implementations. While doing the initial technical design for TH migration from Airbnb monolith to SOA, we took the hard path of re-architecting this flow instead of applying band-aids. This helped to ensure long-term success and provide the best possible experience to our host community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*homIPsg24j6ZA1FT" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8SKW7SUEmrbGMFO1" /><figcaption>Transaction History page and simplified high level architecture. Airbnb monolith app behaves like a presentation service and fetches data from multiple payment services and also from legacy databases.</figcaption></figure><p>This use case was a great fit for our unified read layer. Using the data used by TH as a starting point, we came up with a new API and high-level entity to serve all data read use cases from similar¬†domains.</p><p>After locking down the entity and its schema, we started to denormalize the data. Thanks to the read-optimized store framework, we were able to denormalize all the data from 10+ tables into a couple of Elasticsearch indices. Not only did we greatly reduce the touchpoints of the query, we were also able to paginate and aggregate much more efficiently by leveraging the storage layer instead of doing the same operations on the application layer. After close to two years of work, we migrated 100% of traffic and achieved up to <strong><em>150x</em></strong> latency improvements, while improving the reliability of the flow from ~96% to¬†<strong><em>99.9+%</em></strong><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NvD6A6Q0AswwXib-" /><figcaption>After the re-architecture, payments data needed by Transaction History is provided by payments read-optimized store and accessed by clients using a well-defined and extensible payout schema over the unified data read¬†layer.</figcaption></figure><h3>Unlocking New Experiences: Guest Payment¬†History</h3><p>Our next use case, called Guest Payment History, came out of Airbnb‚Äôs annual company-wide hackathon. This hackathon project aimed to provide a detailed and easy way for our guest community to track their payments and refunds. Similar to Transaction History, this scenario also required information from multiple payments services and databases, including many legacy databases.</p><p>Guest Payment History (GPH) also helped to showcase many benefits brought by the unified read layer: a new unified entity to serve GPH and future similar use cases, along with an extensible API which supported many different filters. We denormalized and stored data from legacy and SOA payment tables using the read-optimized store framework into a single Elasticsearch index, which reduced the complexity and cost of queries¬†greatly.</p><p>We released this new page to our community with our <a href="https://news.airbnb.com/2021-winter-release/">2021 Winter launch</a> and achieved a huge reduction on customer support tickets related to questions about guest payments; which resulted in close to $1.5M cost savings for 2021. It also illustrated our move towards a stronger technical foundation with high reliability and low¬†latency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KNPtvQ1odi8dp2Xw" /><figcaption>Guests can track their payments and refunds using Guest Payment¬†History.</figcaption></figure><p>The architecture is very similar to TH, where data is provided to clients via unified API and schema, backed by a secondary store.</p><p>After exposing these new entities via TH and GPH, we started to onboard many other critical use cases to leverage the same flow in order to efficiently serve and surface payments¬†data.</p><h3>Conclusion</h3><p>Microservice/SOA architectures greatly help backend teams to independently scale and develop various domains with minimal impact to each other. It‚Äôs equally important to make sure the clients of these services and their data will not be subject to additional challenges under this new industry-standard architecture.</p><p>In this blog post, we illustrated some potential solutions, such as unified APIs and higher-level entities to hide away the internal service and architectural complexities from the callers. We also recommend leveraging denormalized secondary data stores to perform expensive join and transformation operations during ingestion time to ensure client queries can stay simple and performant. As we demonstrated with multiple initiatives, complex domains such as payments can significantly benefit from these approaches.</p><p>If this type of work interests you, take a look at the following related positions:</p><p><strong>US</strong>:</p><p><a href="https://careers.airbnb.com/positions/3393185/">Staff Software Engineer, Payments</a></p><p><strong>India</strong>:</p><p><a href="https://careers.airbnb.com/positions/3153981/">Senior Software Engineer, Cities Bangalore</a></p><p><a href="https://careers.airbnb.com/positions/3842855/">Engineering Manager, Ambassador Platform¬†Products</a></p><p><a href="https://careers.airbnb.com/positions/2768475/">Manager, Engineering Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2773515/">Staff Software Engineer, Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2925359/">Senior Software Engineer, Payments Compliance</a></p><h3>Acknowledgments</h3><p>We had many people at Airbnb contributing to this big re-architecture, but countless thanks to Mini Atwal, Yong Rhyu, Musaab At-Taras, Michel Weksler, Linmin Yang, Linglong Zhu, Yixiao Peng, Bo Shi, Huayan Sun, Wentao Qi, Adam Wang, Erika Stott, Will Koh, Ethan Schaffer, Khurram Khan, David Monti, Colleen Graneto, Lukasz Mrowka, Bernardo Alvarez, Blazej Adamczyk, Dawid Czech, Marcin Radecki, Tomasz Laskarzewski, Jessica Tai, Krish Chainani, Victor Chen, Will Moss, Zheng Liu, Eva Feng, Justin Dragos, Ran Liu, Yanwei Bai, Shannon Pawloski, Jerroid Marks, Yi He, Hang Yuan, Xuemei Bao, Wenguo Liu, Serena Li, Theresa Johnson, Yanbo Bai, Ruize Lu, Dechuan Xu, Sam Tang, Chiao-Yu Tuan, Xiaochen He, Gautam Prajapati, Yash Gulani, Abdul Shakir, Uphar Goyal, Fanchen Kong, Claire Thompson, Pavel Lahutski, Patrick Connors, Ben Bowler, Gabriel Siqueira, Jing Hao, Manish Singhal, Sushu Zhang, Jingyi Ni, Yi Lang Mok, Abhinav Saini, and Ajmal Pullambi. We couldn‚Äôt have accomplished this without your invaluable contributions.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e613e7af1a39" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39">Unified Payments Data Read at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Faster JavaScript Builds with Metro]]></title>
            <link>https://medium.com/airbnb-engineering/faster-javascript-builds-with-metro-cfc46d617a1f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/cfc46d617a1f</guid>
            <category><![CDATA[module-bundler]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[metro-bundler]]></category>
            <category><![CDATA[webpack]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Rae Liu]]></dc:creator>
            <pubDate>Tue, 24 May 2022 17:39:39 GMT</pubDate>
            <atom:updated>2022-06-28T19:59:44.037Z</atom:updated>
            <content:encoded><![CDATA[<p><em>How Airbnb migrated from Webpack to Metro and made the development feedback loop nearly instantaneous, the largest production build 50% faster, with marginal end-user runtime improvements.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RZFWkaoezUfVzTxvpfm2gQ.jpeg" /></figure><p><strong>By:</strong> <a href="https://www.linkedin.com/in/raejin/">Rae¬†Liu</a></p><h3>Introduction</h3><p>In 2018, the frontend Airbnb infrastructure relied on Webpack for JavaScript bundling which had served us well up until then; however, with our codebase almost having quadrupled in the previous year, the frontend team was noticing a significant impact on the development experience. Not only was build performance slow, but the average page refresh time for a trivial one-line code change was anywhere between 30 seconds and 2 minutes depending on the project size. In order to mitigate this, the team decided to migrate to¬†<a href="https://facebook.github.io/metro/">Metro</a>.</p><p>Thanks to the switch to Metro, we‚Äôve improved our build performance. In development, the time it takes for a simple UI change to be reflected and loaded (<a href="https://developer.mozilla.org/en-US/docs/Glossary/Time_to_interactive">Time to Interactive TTI metric</a>) is <strong>80% faster</strong>. The slowest production build compiling ~49k modules (JavaScript files) is <strong>55% faster</strong> (down from 30.5 minutes to 13.8 minutes). As an added bonus, we‚Äôve observed improvements in the <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb Page Performance Scores</a> by ~<strong>1%</strong> for pages built with¬†Metro.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*Og_DCsn0p_8RkRR3eu40hw.png" /></figure><p>Scaling issues with JavaScript bundlers certainly isn‚Äôt a unique problem to Airbnb. In this blog post, we want to highlight the key architectural differences between Webpack and Metro as well as some of the migration challenges we faced in both development and production builds. If you anticipate one of your own projects to scale up significantly in the future, we hope this post can provide useful insights on solving this¬†problem.</p><h3>What is¬†Metro?</h3><p><a href="https://facebook.github.io/metro/">Metro</a> is the open source JavaScript bundler for React Native. While <a href="https://medium.com/airbnb-engineering/sunsetting-react-native-1868ba28e30a">Airbnb no longer uses React Native</a>, we believed the infrastructure could be leveraged for the web as well. After numerous consultations with the Metro folks at Meta as well as some of our own modifications, we managed to build a flavor of Metro that now powers both development and production bundling for all Airbnb websites.</p><p>Conceptually, Metro breaks down bundling to three steps in the following order: <a href="https://facebook.github.io/metro/docs/concepts#resolution">resolution</a>, <a href="https://facebook.github.io/metro/docs/concepts#transformation">transformation</a> and <a href="http://serialization">serialization</a>.</p><ul><li>Resolution deals with how to resolve import/require statements.</li><li>Transformation is responsible for transpiling code (source-to-source compiler which converts modern TypeScript/JavaScript source code into functionally equivalent JavaScript code that‚Äôs more optimized and backwards compatible with older browsers), an example tool would be¬†<a href="https://babeljs.io/">babel</a>.</li><li>Serialization combines the transformed files into JavaScript bundles.</li></ul><p>These three concepts are the fundamental building blocks to understand how Metro works. In the following sections, we highlight the key architectural differences between Metro and Webpack to provide deeper context into Metro‚Äôs strengths.</p><h3>Key architectural differences between Metro and¬†Webpack</h3><h3>Process JS bundles on demand in development</h3><p>When we talk about bundles, a JavaScript bundle is technically just a serialized dependency graph, where an entry point is the root of the graph. At Airbnb, a web page maps to a single entry point. In development, Webpack (even the latest v5 version) requires knowing <a href="https://webpack.js.org/concepts/entry-points/">the entry points</a> for all pages before it can start bundling. On the other hand, the Metro development server processes the requested JavaScript bundles on the¬†fly.</p><p>More specifically, at Airbnb, every frontend project has a Node server which matches a route to a specific entry point. When a web page is requested, the DOM includes script tags with the development JavaScript URLs. The browser loads the page, and makes requests to the Metro development server for the JavaScript bundles. In Figure 1, we illustrate the difference between our Metro &amp; Webpack development setup:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/813/0*d0S7RQA6IXt1YqAO" /></figure><p>Figure 1: Differences between the JS bundle development setups for Metro and¬†Webpack</p><p>In this example, there is a web project with three entry points: entryPageA.js, entryPageB.js, and entryPageC.js. A developer makes changes to Page A, which includes only the entryPageA.js bundle. As you can see in Figure 1, in both scenarios, the browser loads Page A (1), then requests the entryPageA.js file from the bundler (2), and finally the bundler responds to the browser with the appropriate bundles (4). With the Webpack bundler (1a), even though the browser only requests entryPageA.js, Webpack compiles all entry points on start-up before it can respond to the entryPageA.js request from the browser. On the other hand, with the Metro bundler (1b), we see that the development server does not spend any time compiling entryPageB.js or entryPageC.js, instead only compiling entryPageA.js before responding to the browser¬†request.</p><p>One of the biggest frontend projects at Airbnb has ~26k unique modules, with the median number of modules per page being ~7.2k modules. Because we also do server side rendering, the number of modules we ultimately have to process doubles to roughly ~48k. With Metro‚Äôs development model, we saved ~70% of work by compiling JavaScript on¬†demand.</p><p>This key architectural difference improves the developer experience, as Metro only compiles what is needed (JavaScript bundles on the pages requested), whereas Webpack pre-compiles the entire project on start-up.</p><h3>Multi-layered cache</h3><p>Another powerful Metro feature we leverage is its <a href="https://facebook.github.io/metro/docs/caching">multi-layered caching</a> feature, which makes setting up both persistent and non-persistent caches straightforward. While Webpack 5 also comes with <a href="https://webpack.js.org/guides/build-performance/#persistent-cache">a disk persistent cache</a>, it isn‚Äôt as flexible as Metro‚Äôs multi-layered cache. Webpack offers two <a href="https://webpack.js.org/configuration/cache/#cachetype">distinct cache types</a>: ‚Äúfilesystem‚Äù or ‚Äúmemory‚Äù, which is limited to memory or disk cache, no remote cache capability is possible. In comparison, Metro provides more flexibility by allowing us to define the cache implementation, including mixing different types of cache layers. If a layer has a cache miss, Metro attempts to retrieve the cache from the next layer and so¬†on.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/653/0*bvgMUBI6wm9xe0fZ" /></figure><p>Figure 2: How Airbnb configures the multi-cache layers with¬†Metro</p><p>The ordering of the caches determines the cache priority. When retrieving a cache, the first cache layer with a result will be used. In the setup illustrated in Figure 2, the fastest in-memory cache layer is prioritized at the top, followed by the file/disk cache, and lastly the remote read-only cache. Compared with the default Metro implementation without a cache, hitting a remote read-only cache resulted in a 56% faster server build in a project compiling 22k¬†files.</p><p>One contributing factor to Metro‚Äôs performance is its built-in worker support which amplifies the effect of the multi-layer cache. While Webpack requires careful configuration to leverage workers via a <a href="https://webpack.js.org/loaders/thread-loader/">third-party plugin</a>, Metro by default spins up workers to offload expensive transforms, allowing for increased parallelization without configuration.</p><p>But why use a remote read-only cache instead of a regular remote cache (read &amp; write)? We discovered that not writing to the remote cache saved an additional 17%<strong> </strong>build time in development for the same project with 22k files. Writing to the remote cache incurs network calls that can be costly, especially on a slower network. To populate the cache, instead of remote cache writes, we introduced a CI job that runs periodically on the default branch¬†commit.</p><h3>Serialization</h3><p>In the bundler context, serialization means combining the transformed source files into one or multiple bundles. In Webpack, the concept of serialization is encapsulated in <a href="https://webpack.js.org/api/compilation-hooks/#root">the compilation hooks</a> (Webpack‚Äôs public APIs). In Metro, a serializer function is responsible for combining source files into¬†bundles.</p><p>For one example of the importance of serialization, let‚Äôs take a look at Internationalization support. We currently support Airbnb websites in around 70 locales, and in 2020, our<a href="https://medium.com/airbnb-engineering/building-airbnbs-internationalization-platform-45cf0104b63c"> internationalization platform</a> served more than 1 million pieces of content. To support internationalization with JS bundles, we need to implement specific logic in the serialization step. Although we had to implement similar internationalization logic when serializing bundles for both Metro and Webpack, Webpack required lots of source code reading to find the appropriate compilation hooks for us to implement the support. On top of all that, it also required understanding the intricacies of concepts like what dependency templates are and how to write our own. Comparatively, it is a breath of fresh air to implement the same internationalization support with Metro. We only have to focus on how to serialize JS bundles with translation content and all the tasks are done in the single serializer function. The simplicity of Metro‚Äôs bundling concepts makes implementing any bespoke feature straightforward.</p><h3>Challenges of Adopting Metro at¬†Airbnb</h3><p>Even though Metro has the architectural advantages described above, it also brought challenges to overcome in order to leverage it fully for the web. Because Metro is designed for use in a React Native environment, we needed to write <em>more code</em> to achieve feature parity with Webpack, so the decision to switch to Metro came at the expense of reinventing some wheels and learning the inner working of a JavaScript bundler that is usually abstracted away from¬†us.</p><p>In development, we had to create a Metro server with custom endpoints to handle building dependency graphs, translation, bundling JS &amp; CSS files, and building source maps. For production builds, we ran Metro as a Node API to handle resolution, transformation, and serialization.</p><p>The surface area of the full migration was substantial, so we broke it down into two phases. Because the slow iteration speed of our Webpack setup incurred significant costs around developer productivity, we addressed the slow Webpack development experience with the Metro development server as our first priority. In the second phase, we brought Metro to feature parity with Webpack and ran an A/B test between Metro and Webpack in production. The two biggest challenges we faced along the way are outlined¬†below.</p><h3>Bundle Splitting</h3><p>The out-of-the-box Metro setup for development produced giant ~5MiB bundles per entry point, since a single bundle is the intended use case for React Native. For the Web, this bundle size was taxing on browser resources and network latency. Every code change resulted in a 5MiB bundle being processed and downloaded, which was inefficient and could not be HTTP-cached. Even if the changed code recompiled instantly, we still needed to reduce the size and improve browser cacheability.</p><p>To improve the performance of Metro in the Web environment, we split the bundles by <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import#dynamic_import">dynamic import</a> boundaries, a technique also known as <a href="https://developer.mozilla.org/en-US/docs/Glossary/Code_splitting">code splitting</a>. The code splitting boundaries enabled us to leverage HTTP caching effectively.</p><p>In Figure 3, import(‚Äò./file‚Äô) represents the dynamic import boundaries. The bundle on the left hand side (3a) is broken down to three smaller bundles on the right (3b). The additional bundles are requested when the import(‚Äò./file‚Äô) statements are executed.</p><p>In Figure 3a, suppose fileA.js has changed, the entire bundle needs to be re-downloaded for the browser to pick up the change in fileA.js. With bundles split by dynamic import illustrated in Figure 3b, a change in fileA.js only results in re-downloading of the fileA.js bundle. The rest of the bundles can reuse browser¬†cache.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/975/0*R2QCSRzc7ysunr3_" /></figure><p>Figure 3: Splitting bundles by dynamic import boundaries. A bundle is represented by the rectangular boxes with a pink background.</p><p>When we began to think about production bundles, we wanted to optimize a bit differently than in development. It takes time to run the bundle splitting algorithm, and we didn‚Äôt want to waste time on optimizing bundle sizes in development. Instead, we prioritized the page load performance over minimizing bundle¬†sizes.</p><p>In production, we wanted to ship fewer and smaller JavaScript bundles to the end user so the page loads faster and the user experience is performant. There is no Metro development server in production, so all the bundles are pre-built. This makes bundle splitting the biggest blocking feature needed to make our Metro build production ready. With some inspiration from Webpack‚Äôs bundle splitting algorithm, we implemented a similar mechanism to split the Metro dependency graphs. The resulting bundle sizes decreased by ~20% (1549 KB ‚Äì&gt; 1226 KB) on airbnb.com as compared to the development splitting by dynamic import boundaries.</p><p>On comparing the bundle splitting results between Metro and Webpack‚Äôs implementations, we realized both provided bundles of comparable sizes with a few pages shipping a slightly higher number of Javascript bundles with Metro. Despite the slightly heavier page weight, <a href="https://developer.mozilla.org/en-US/docs/Glossary/First_contentful_paint">TTFCP</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Largest_Contentful_Paint_API">largest contentful paint</a>, and <a href="https://web.dev/lighthouse-total-blocking-time/">Total Blocking Time</a> metrics are comparable between Metro and¬†Webpack.</p><h3>Tree-shaking</h3><p>Bundle splitting alone decreased bundle sizes significantly, however we were able to make bundles even smaller by deleting dead code. However, it is not always obvious to identify what is considered dead code in a project, as some ‚Äúdead code‚Äù in a project may be ‚Äúused code‚Äù in the other projects. This is where tree-shaking came into play. It relied on the consistent usages of <a href="https://tc39.es/ecma262/#sec-modules">ECMAScript modules</a> (ESM) <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import">import</a>/<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/export">export</a> statements in the code base. Based on the import/export usages in a project, we analyzed what specific export statements were not imported by any file in the project. Finally, the bundler removes the unused export statements, making the overall bundle sizes¬†smaller.</p><p>One challenge we faced while implementing the tree-shaking algorithm for Metro production builds was the risk of mistakenly removing code that is executed at runtime. For example, we ran into multiple bugs related to <a href="https://developer.mozilla.org/en-US/docs/web/javascript/reference/statements/export#re-exporting_aggregating">re-export statements</a>. Since Webpack handles ESM import/export statements in a different way, there was no comparable prior art for reference. After multiple iterations of tree-shaking algorithm implementation, the following table captures how much dead code we were finally able to drop given the project¬†size.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WlsTTqWIGeJzk_ccUzFuBw.png" /></figure><h3>Conclusion</h3><p>The Metro migration brought forth some very significant improvements. The biggest Airbnb frontend project compiling ~48k modules (including server and browser compilations) saw a drop in the average build time by ~55% from 30.5 minutes to 13.8 minutes. Additionally, we saw improvements on the <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb Page Performance Scores</a> with the pages built by Metro, ranging around +1%. The end user performance improvement was a nice surprise, as we initially aimed for achieving neutral experiment results.</p><p>The simplicity of Metro‚Äôs architecture has benefited us in many ways. Engineers from other teams have ramped up quickly to contribute to Airbnb‚Äôs Metro implementation, which means there is a lower barrier to entry for contributing to the bundling system. The <a href="https://facebook.github.io/metro/docs/caching">multi-layered cache system</a> is straightforward to work with, making experimentation with caching possible. The bespoke bundler feature integrations are made obvious and easier to implement.</p><p>We acknowledge that the landscape has changed since we evaluated <a href="https://parceljs.org/">Parcel</a>, <a href="https://webpack.js.org/">Webpack 4</a>, and <a href="https://facebook.github.io/metro/">Metro</a> back in 2018. There are other tools, such as <a href="https://rollupjs.org/guide/en/">rollup.js</a> and <a href="https://esbuild.github.io/">esbuild</a>, that we haven‚Äôt explored much, and we know that Metro isn‚Äôt a general-purpose JavaScript bundler when compared to Webpack. However, after a few years of working on Metro feature parity, the results we have seen have proven to us that it was a good decision to pursue Metro. Metro <strong>solved</strong> our most desperate scaling issues by dropping development and production build times. We are more productive than ever with instantaneous development feedback loops and faster production builds. If you would like to help us continue to improve our JavaScript tooling and build optimization, or tackle other web infrastructure challenges, check out these open roles at¬†Airbnb:</p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=61d6ab411us">Senior Frontend Infrastructure Engineer, Web¬†Platform</a></p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=61d6ab411us">Engineering Manager, Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/2623004/">Senior Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4168852/">Senior/Staff Software Engineer, Observability</a></p><h3>Acknowledgments</h3><p>Thank you everyone who has contributed to this multi-year project. We couldn‚Äôt have done it without any of you! Special shoutout to my lovely team <a href="mailto:michael.james@airbnb.com">Michael James</a> and <a href="mailto:noah.sugarman@airbnb.com">Noah Sugarman</a> for driving the Metro production migration to the finish line. Thank you <a href="mailto:breanna.bunge@airbnb.com">Brie Bunge</a>, <a href="mailto:dan.beam@airbnb.com">Dan Beam</a>, <a href="mailto:ian.myers@airbnb.com">Ian Myers</a>, <a href="mailto:ian.remmel@airbnb.com">Ian Remmel</a>, <a href="mailto:joe.lencioni@airbnb.com">Joe Lencioni</a>, <a href="mailto:madison.capps@airbnb.com">Madison Capps</a>, <a href="mailto:michael.james@airbnb.com">Michael James</a>, <a href="mailto:noah.sugarman@airbnb.com">Noah Sugarman</a> for reviewing and giving great feedback on this blog¬†post.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cfc46d617a1f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/faster-javascript-builds-with-metro-cfc46d617a1f">Faster JavaScript Builds with Metro</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dynamic Kubernetes Cluster Scaling at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d79ae3afa132</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[cluster-autoscaler]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[David Morrison]]></dc:creator>
            <pubDate>Mon, 23 May 2022 17:35:24 GMT</pubDate>
            <atom:updated>2022-05-23T17:35:24.292Z</atom:updated>
            <content:encoded><![CDATA[<p>Authors: <a href="https://www.linkedin.com/in/evansheng112/">Evan Sheng</a>, <a href="https://www.linkedin.com/in/david-morrison-9419b110/">David¬†Morrison</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Elojmgc7Y06tItOaLdB0Cw.jpeg" /></figure><h3>Introduction</h3><p>An important part of running Airbnb‚Äôs infrastructure is ensuring our cloud spending automatically scales with demand, both up <strong>and </strong>down. Our traffic fluctuates heavily every day, and our cloud footprint should scale dynamically to support¬†this.</p><p>To support this scaling, Airbnb utilizes Kubernetes, an open source container orchestration system. We also utilize OneTouch, a service configuration interface built on top of Kubernetes, and is described in more detail in a previous¬†<a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">post</a>.</p><p>In this post, we‚Äôll talk about how we dynamically size our clusters using the Kubernetes Cluster Autoscaler, and highlight functionality we‚Äôve contributed to the <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">sig-autoscaling community</a>. These improvements add customizability and flexibility to meet Airbnb‚Äôs unique business requirements.</p><h3>Kubernetes Clusters at¬†Airbnb</h3><p>Over the past few years, Airbnb has shifted almost all online services from manually orchestrated EC2 instances to Kubernetes. Today, we run thousands of nodes across nearly a hundred clusters to accommodate these workloads. However, this change didn‚Äôt happen overnight. During this migration, our underlying Kubernetes cluster setup evolved and became more sophisticated as more workloads and traffic shifted to our new technology stack. This evolution can be split into three¬†stages.</p><p>Stage 1: Homogenous Clusters, Manual¬†Scaling</p><p>Stage 2: Multiple Cluster Types, Independently Autoscaled</p><p>Stage 3: Heterogeneous Clusters, Autoscaled</p><h4>Stage 1: Homogenous Clusters, Manual¬†Scaling</h4><p>Before using Kubernetes, each instance of a service was run on its own machine, and manually scaled to have the proper capacity to handle traffic increases. Capacity management varied per team and capacity would rarely be un-provisioned once load¬†dropped.</p><p>Our initial Kubernetes cluster setup was relatively basic. We had a handful of clusters, each with a single underlying node type and configuration, which ran only stateless online services. As some of these services began shifting to Kubernetes, we started running containerized services in a multi-tenant environment (many pods on a node). This aggregation led to fewer wasted resources, and consolidated capacity management for these services to a single control point at the Kuberentes control plane. At this stage, we scaled our clusters manually, but this was still a marked improvement over the previous situation.</p><figure><img alt="An EC2 node running a single application vs. a Kubernetes node running 3 applications." src="https://cdn-images-1.medium.com/max/1024/0*xgJUXKfck5DuQOg1" /><figcaption>Figure 1: EC2 Nodes vs Kubernetes Nodes</figcaption></figure><h4>Stage 2: Multiple Cluster Types, Independently Autoscaled</h4><p>The second stage of our cluster configuration began when more diverse workload types, each with different requirements, sought to run on Kubernetes. To accommodate their needs, we created a cluster type abstraction. A ‚Äúcluster type‚Äù defines the underlying configuration for a cluster, meaning that all clusters of a cluster type are identical, from node type to different cluster component settings.</p><p>More cluster types led to more clusters, and our initial strategy of manually managing capacity of each cluster quickly fell apart. To remedy this, we added the Kubernetes <a href="https://github.com/kubernetes/autoscaler">Cluster Autoscaler</a> to each of our clusters. This component automatically adjusts cluster size based on pod requests‚Ää‚Äî‚Ääif a cluster‚Äôs capacity is exhausted, and a pending pod‚Äôs request could be filled by adding a new node, Cluster Autoscaler launches one. Similarly, if there are nodes in a cluster that have been underutilized for an extended period of time, Cluster Autoscaler will remove these from the cluster. Adding this component worked beautifully for our setup, saved us roughly 5% of our total cloud spend, and the operational overhead of manually scaling clusters.</p><figure><img alt="Different Kubernetes clusters for different types of applications (CPU-bound or GPU-bound applications, for example)." src="https://cdn-images-1.medium.com/max/1024/0*XevtJSPUAo9vTpJn" /><figcaption>Figure 2: Kubernetes Cluster¬†Types</figcaption></figure><h4>Stage 3: Heterogeneous Clusters, Autoscaled</h4><p>When nearly all online compute at Airbnb shifted to Kubernetes, the number of cluster types had grown to over 30, and the number of clusters to 100+. This expansion made Kubernetes cluster management tedious. For example, cluster upgrades had to be individually tested on each of our numerous cluster¬†types.</p><p>In this third phase, we aimed to consolidate our cluster types by creating ‚Äúheterogeneous‚Äù clusters that could accommodate many diverse workloads with a single Kubernetes control plane. First, this greatly reduces cluster management overhead, as having fewer, more general purpose clusters reduces the number configurations to test. Second, with the majority of Airbnb now running on our Kubernetes clusters, efficiency in each cluster provides a big lever to reduce cost. Consolidating cluster types allows us to run varied workloads in each cluster. This aggregation of workload types‚Ää‚Äî‚Ääsome big and some small‚Ää‚Äî‚Ääcan lead to better bin packing and efficiency, and thus higher utilization. With this additional workload flexibility, we had more room to implement sophisticated scaling strategies, outside of the default Cluster Autoscaler expansion logic. Specifically, we aimed to implement scaling logic that was tied to Airbnb specific business¬†logic.</p><figure><img alt="A single Kubernetes cluster with multiple different node types: an Intel compute node, an AMD compute node, a high-memory node, and a GPU node." src="https://cdn-images-1.medium.com/max/1024/0*1GUcmg4jijWf_fdA" /><figcaption>Figure 3: A heterogeneous Kubernetes cluster</figcaption></figure><p>As we scaled and consolidated clusters so they were heterogeneous (multiple instance types per cluster), we began to implement specific business logic during expansion and realized some changes to the autoscaling behavior were necessary. The next section will describe some of the changes we‚Äôve made to Cluster Autoscaler to make it more flexible.</p><h3>Cluster Autoscaler Improvements</h3><h4>Custom gRPC¬†Expander</h4><p>The most significant improvement we made to Cluster Autoscaler was to provide a new method for determining node groups to scale. Internally, Cluster Autoscaler maintains a list of node groups which map to different candidates for scaling, and it filters out node groups that do not satisfy pod scheduling requirements by running a scheduling simulation against the current set of Pending (unschedulable) pods. If there are any Pending (unschedulable) pods, Cluster Autoscaler attempts to scale the cluster to accommodate these pods. Any node groups that satisfy all pod requirements are passed to a component called the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders">Expander</a>.</p><figure><img alt="A depiction of Cluster Autoscaler, which calls the Expander to determine which type of node to add in a heterogeneous Kubernetes cluster." src="https://cdn-images-1.medium.com/max/1024/0*ryQyolVdPY6bbSQy" /><figcaption>Figure 4: Cluster Autoscaler and¬†Expander</figcaption></figure><p>The Expander is responsible for further filtering the node groups based on operational requirements. Cluster Autoscaler has a number of different built-in expander options, each with different logic. For example, the default is the random expander, which selects from available options uniformly at random. Another option,and the one that Airbnb has historically used, is the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/priority">priority expander</a>, which chooses which node group to expand based on a user-specified tiered priority¬†list.</p><p>As we moved toward our heterogeneous cluster logic, we found that the default expanders were not sophisticated enough to satisfy our more complex business requirements around cost and instance type selection.</p><p>As a contrived example, say we want to implement a weighted priority expander. Currently, the priority expander only lets users specify distinct tiers of node groups, meaning it will always expand tiers deterministically and in order. If there are multiple node groups in a tier, it will break ties randomly. A weighted priority strategy of setting two node groups in the same tier, but expanding one 80% of the time, and another 20% of the time, is not achievable with the default¬†setup.</p><p>Outside of the limitations of the current supported expanders, there were a few operational concerns:</p><ol><li>Cluster Autoscaler‚Äôs release pipeline is rigorous and changes take time to review before being merged upstream. However, our business logic and desired scaling strategy is continuously changing. Developing an expander to fill our current needs today may not fulfill our needs in the¬†future</li><li>Our business logic is specific to Airbnb and not necessarily other users. Any changes we implement specific to our logic would not be useful to contribute back¬†upstream</li></ol><p>From these, we came up with a set of requirements for a new expander type in Cluster Autoscaler:</p><ol><li>We wanted something that was both extensible and usable by others. Others may run into similar limitations with the default Expanders at scale, and we would like to provide a generalized solution and contribute functionality back¬†upstream</li><li>Our solution should be deployable out of band with Cluster Autoscaler, and allow us to respond more rapidly to changing business¬†needs</li><li>Our solution should fit into the Kubernetes Cluster Autoscaler ecosystem, so that we do not have to maintain a fork of Cluster Autoscaler indefinitely</li></ol><p>With these requirements, we came up with a design that breaks out the expansion responsibility from the Cluster Autoscaler core logic. We designed a pluggable ‚Äú<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/grpcplugin">custom Expander</a>.‚Äù which is implemented as a gRPC client (similarly to the <a href="https://github.com/kubernetes/autoscaler/blob/68c984472acce69cba89d96d724d25b3c78fc4a0/cluster-autoscaler/proposals/plugable-provider-grpc.md">custom cloud provider</a>). This custom expander is broken into two components.</p><p>The first component is a gRPC client built into Cluster Autoscaler. This Expander conforms to the same interface as other Expanders in Cluster Autoscaler, and is responsible for transforming information about valid node groups from Cluster Autoscaler to a defined <a href="https://developers.google.com/protocol-buffers/docs/overview">protobuf</a> schema (shown below), and receives the output from the gRPC server to transform back to a final list of options for Cluster Autoscaler to scale¬†up.</p><pre>service Expander {<br>  rpc BestOptions (BestOptionsRequest) returns (BestOptionsResponse) <br>}</pre><pre>message BestOptionsRequest {<br>  repeated Option options;<br>  map&lt;string, k8s.io.api.core.v1.Node&gt; nodeInfoMap;<br>}</pre><pre>message BestOptionsResponse {<br>  repeated Option options;<br>}</pre><pre>message Option {<br>  // ID of node to uniquely identify the nodeGroup<br>  string nodeGroupId;<br>  int32 nodeCount;<br>  string debug;<br>  repeated k8s.io.api.core.v1.Pod pod;<br>}</pre><p>The second component is the gRPC server, which is left up to the user to write. This server is intended to be run as a separate application or service, which can run arbitrarily complex expansion logic when selecting which node group to scale up, with the given information passed from the client. Currently, the protobuf messages passed over gRPC are slightly transformed versions of what is passed to the Expander in Cluster Autoscaler.</p><p>From our aforementioned example, a weighted random priority expander can be implemented fairly easily by having the server read from a priority tier list and weighted percentage configuration from a configmap, and choose accordingly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MldTcDs1Df38IfHE" /><figcaption>Figure 5: Cluster Autoscaler and Custom gRPC¬†Expander</figcaption></figure><p>Our implementation includes a failsafe option. It is recommended to use the option to pass in <a href="https://github.com/kubernetes/autoscaler/pull/4233">multiple expanders</a> as arguments to Cluster Autoscaler. With this option, if the server fails, Cluster Autoscaler is still able to expand using a fallback Expander.</p><p>Since it runs as a separate application, expansion logic can be developed out of band with Cluster Autoscaler, and since the gRPC server is customizable by the user based on their needs, this solution is extensible and useful to the wider community as a¬†whole.</p><p>Internally, Airbnb has been using this new solution to scale all of our clusters without issues since the beginning of 2022. It has allowed us to dynamically choose when to expand certain node groups to meet Airbnb‚Äôs business needs, thus achieving our initial goal of developing an extensible custom expander.</p><p>Our custom expander was <a href="https://github.com/kubernetes/autoscaler/pull/4452">accepted</a> into the upstream Cluster Autoscaler earlier this year, and will be available to use in the next version (v1.24.0) release.</p><h3>Other Autoscaler Improvements</h3><p>Over the course of our migration to heterogeneous Kubernetes clusters, we identified a number of other bugs and improvements that could be made to Cluster Autoscaler. These are briefly described below:</p><ul><li><a href="https://github.com/kubernetes/autoscaler/pull/4489">Early abort for AWS ASGs with no capacity</a>: Short circuit the Cluster Autoscaler loop to wait for nodes it tries to spin up to see if they are ready by calling out to an AWS EC2 endpoint to check if the ASG has capacity. With this change enabled, users get much more rapid, yet correct scaling. Previously, users using a priority ladder would have to wait 15 minutes between each attempted ASG launch, before trying an ASG of lower priority.</li><li><a href="https://github.com/kubernetes/autoscaler/pull/4073">Caching launch templates to reduce AWS API calls</a>: Introduce a cache for AWS ASG Launch Templates. This change unlocks using large numbers of ASGs, which was critical for our generalized cluster strategy. Previously, for empty ASGs (no present nodes in a cluster), Cluster Autoscaler would repeatedly call an AWS endpoint to get launch templates, resulting in throttling from the AWS¬†API.</li></ul><h3>Conclusion</h3><p>In the last four years, Airbnb has come a long way in our Kubernetes Cluster setup. Having the largest portion of compute at Airbnb on a single platform provided a strong, consolidated lever to improve efficiency, and we are now focused on generalizing our cluster setup (think <a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">‚Äúcattle, not pets‚Äù</a>). By developing and using a more sophisticated expander in Cluster Autoscaler (as well as fixing a number of other minor issues with the Autoscaler), we have been able to achieve our goals of developing our complex, business specific scaling strategy around cost and mixed instance types, while also contributing some useful features back to the community.</p><p>For more details on our heterogeneous cluster migration, watch our Kube-Con <a href="https://www.youtube.com/watch?v=GCCSY7ERXj4&amp;ab_channel=CNCF%5BCloudNativeComputingFoundation%5D">talk</a> and we‚Äôre also at KubeCon EU this year, come talk to us! If you‚Äôre interested in working on interesting problems like the ones we‚Äôve described here, we‚Äôre hiring! Check out these open¬†roles:</p><p><a href="https://careers.airbnb.com/positions/3949745/">Engineering Manager- Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3903900/">Senior Front End¬†Engineer</a></p><p><a href="https://careers.airbnb.com/positions/2623004/">Senior Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4168852/">Software Engineer, Observability</a></p><p><a href="https://careers.airbnb.com/positions/3696687/">Software Engineer, Developer Infrastructure</a></p><h3>Acknowledgements</h3><p>The evolution of our Kubernetes Cluster setup is the work of many different collaborators. Special thanks to Stephen Chan, Jian Cheung, Ben Hughes, Ramya Krishnan, David Morrison, Sunil Shah, Jon Tai and Long Zhang, as this work would not have been possible without¬†them.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d79ae3afa132" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132">Dynamic Kubernetes Cluster Scaling at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb‚Ää‚Äî‚ÄäKamini Dandapani]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/7f51f1fbb2bb</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[leadership]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[people]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Wed, 11 May 2022 19:35:36 GMT</pubDate>
            <atom:updated>2022-05-11T19:35:36.825Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb‚Ää‚Äî‚ÄäKamini Dandapani</h3><p>Airbnb‚Äôs VP of Engineering on why you don‚Äôt have to change your natural self to be a¬†leader</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/0*t-dDS7QYW1gsBtG5" /></figure><p><a href="https://www.linkedin.com/in/kaminidandapani/">Kamini Dandapani</a>, VP of Engineering at Airbnb, leads the Infrastructure Engineering organization, which is in many ways the backbone of the company: responsible for powering the systems that keep Airbnb running smoothly and help new products reach millions of people. With a passion for how platforms can support and sustain the business and product, Kamini developed her considerate and welcoming leadership style at eBay and LinkedIn before joining Airbnb two years ago. In addition to her Infra role, she champions diversity and belonging in the workplace and is co-sponsor for Airbnb‚Äôs tech diversity council, which aims to create the most diverse and inclusive community in the tech industry.</p><p><em>Want to hear Kamini and other Infrastructure team members talk about some of the team‚Äôs latest projects? Check out the </em><a href="https://www.facebook.com/AirbnbTech/videos/635338454172729/"><em>‚ÄúPowering Our Platform‚Äù Airbnb Tech Talk</em></a><em> from March 2022. You‚Äôll hear about some of the major initiatives we‚Äôre working on in next-generation service mesh, observability, feature engineering, and scalable¬†storage.</em></p><h3>From Chennai to¬†Chicago</h3><p>Growing up in India, I was the youngest of three girls. Despite facing skepticism and criticism from others around them, my parents invested heavily in our education and gave us a very strong footing, without which I don‚Äôt think I would be where I am¬†today.</p><p>I started familiarizing myself with the engineering world, and found that I immensely enjoyed it. I did my undergrad in electronics and communication, and with my dad‚Äôs encouragement‚Ää‚Äî‚Äähe camped out overnight in the line in front of the US Consulate to get a visa‚Ää‚Äî‚ÄäI came to the US to pursue my master‚Äôs in computer¬†science.</p><p>In Chicago, I had to adjust to a lot of new experiences (including the winter cold!). In India, I never did anything alone, but here I had to do everything independently, from managing my finances to driving a car. After I graduated, I felt very fortunate to get a job in Silicon Valley, and I‚Äôve stayed here ever¬†since.</p><h3>Leading at the intersection of platform and¬†product</h3><p>Effective infrastructure can‚Äôt be built in a vacuum. Rather, it requires close partnerships with our product engineers to support both our product and overall business strategies. My professional sweet spot is where the platform architecture meets the end-user experience‚Ää‚Äî‚Ääplus¬†scale!</p><p>In my engineering career, I worked at eBay for 12 years and grew into a director position, leading international expansion. After that, I was at LinkedIn for six years, leading infrastructure and tools for the consumer app, and that‚Äôs where I learned how to operate and develop a platform at scale. When Airbnb got in touch with me, I wasn‚Äôt looking for a change. But with every conversation that I had, there was something truly magical about the place‚Ää‚Äî‚Ääfrom the leadership, to the inclusivity, to the company‚Äôs mission‚Ää‚Äî‚Ääand I am so grateful that I made the¬†leap.</p><p>What excited me most was bringing dozens of years of operating at scale to Airbnb. And one key component to operating at scale is working effectively and smoothly cross-functionally, and building close relationships with our product teams and key partners across the business. I‚Äôve seen some truly incredible teamwork within my own team, and across all of¬†Airbnb.</p><h3>Building the tech backbone of¬†Airbnb</h3><p>Most of the technical foundation that powers Airbnb comes from the Infrastructure organization. The impact that this group has is so wide and profound.</p><p>The Infrastructure organization has several key¬†pillars:</p><ul><li><strong>Search Infrastructure</strong>, which powers the backend systems for our guest search experience</li><li><strong>Data Platform</strong>, for storing, processing and managing all the data that powers every user experience</li><li><strong>Developer Platform</strong>, which helps make Airbnb engineers‚Äô lives friction-free by building tools, services and environments to help them develop, build, test and deploy their¬†code</li><li><strong>Cloud Infrastructure,</strong> which delivers and operates the cloud environment that powers¬†Airbnb</li><li><strong>Reliability Engineering, </strong>which remedies and prevents site performance issues through tooling and automation</li></ul><p>Within each of these areas, we have many long-term, multi-year projects, all part of what we‚Äôre calling Tech Stack 2.0: an evolution and modernization of our technology. Some sample initiatives include <a href="https://news.airbnb.com/unique-stays-hosts-earn-more-than-300-million-since-start-of-pandemic/">flexible search for guests</a> and UDS, our pioneering next-generation storage¬†system.</p><h3>My identity: female, South Asian, immigrant</h3><p>People often point out that I‚Äôm unique for being a female leader in tech. But in reality, there are three important aspects of my identity: yes, I‚Äôm a woman, but I‚Äôm also South Asian and an immigrant. All of these have shaped who I am¬†today.</p><p>I grew up in a very different culture. We were discouraged from challenging the status quo, and for my parents and grandparents, the idea was that if you work extremely hard, recognition will follow. That‚Äôs not the way it works here: it sometimes seems like you need to have an opinion and advocate for yourself in order to be taken seriously.</p><p>In many ways, I think being different is an advantage as a leader. While I encourage everyone on the team to make sure their voice is being heard, I also believe in being your natural self. That‚Äôs how I‚Äôve been able to build trust with my teams, by letting them see the real me. My philosophy is that no one can be an expert in everything. What you‚Äôll see is varying degrees in people‚Ää‚Äî‚Ääand I want to fully support that diversity of thought and experience, because a team that‚Äôs well-rounded is more effective.</p><h3>Bringing people¬†along</h3><p>When joining Airbnb, I asked to have the dedicated time and agency to do work around diversity and gender parity. I‚Äôm now the co-sponsor for the Tech Diversity Council alongside <a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-lucius-diphillips-79d1f0bc72a2">Lucius DiPhillips</a> (CIO), where we advocate for diversity-related projects around the Tech org. I‚Äôm also one of the advisors for our Asians@ employee resource¬†group.</p><p>There‚Äôs something special about these employee resource groups here at Airbnb that I haven‚Äôt seen before. It‚Äôs a very small close-knit group, and we can relate to our similar upbringing and cultural norms. We genuinely look out for each other and amplify our Asian@ colleagues‚Äô voices.</p><p>There‚Äôs a saying that ‚Äúif you want to go fast, go alone, but if you want to go far, go together.‚Äù Whether it‚Äôs sharing context about our work, being vulnerable about my mistakes, or building a diverse organization, I very much believe in bringing people along. I couldn‚Äôt be at a better place than here at Airbnb, where our company‚Äôs mission is for anyone to belong anywhere.</p><p><em>Interested in working at Airbnb? We‚Äôre hiring! Check out these open¬†roles:</em></p><p><a href="https://careers.airbnb.com/positions/3029584/">Staff Software Engineer, Distributed Storage</a></p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=3da3a8881us">Senior Frontend Infrastructure Engineer, Web¬†Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3747712/">Senior/Staff Backup and Recovery Engineer, Storage Infrastructure</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7f51f1fbb2bb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb">My Journey to Airbnb‚Ää‚Äî‚ÄäKamini Dandapani</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>