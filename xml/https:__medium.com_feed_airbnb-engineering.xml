<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 25 Aug 2022 16:14:16 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[My Journey to Airbnb — Veerabahu Chandran]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/70468aa3bc06</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[india]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Lauren Mackevich]]></dc:creator>
            <pubDate>Thu, 18 Aug 2022 18:50:01 GMT</pubDate>
            <atom:updated>2022-08-18T18:50:01.231Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Veerabahu Chandran</h3><p>Learning and growing in Airbnb’s new Bangalore Tech Center</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wwf3CMkjhKPlaxichQJd1g.jpeg" /></figure><p><em>Veera Chandran is an engineer in Airbnb’s new Bangalore Tech Center, where his team builds out technical systems to support hosts. As a lifelong learner, he has a passion for exploring new technologies and diving into practical problems. He’s excited to be tackling both the technical challenges of building new architecture and the organizational challenges of building out the capabilities of a new office.</em></p><p><em>Here’s Veera’s story:</em></p><h3>Learning and exploring</h3><p>I grew up in Tamil Nadu, in the South of India. I was always a curious kid, trying to understand how everything worked, so when it came to choosing a course of study, engineering was a natural fit. I feel lucky that I had a lot of education opportunities in front of me, and I was able to choose the path I wanted to take.</p><p>My first exposure to computers came when I was in 8th grade. These were still the relatively early days of the computer, and my dad brought one home so he could learn to use it. I found it fascinating and learned BASIC and Logo. These are simple languages, but I was excited that I could use them to make drawings and text appear on the screen. Those rudimentary programs opened me to the world of what’s possible with computer science.</p><h3>Studying and practical experience</h3><p>I went to the College of Engineering, Guindy to study Computer Science and Engineering. My studies covered a lot of subjects, but the one that excited me most was networking. I was really curious to understand how data moves from one place to another.</p><p>My first practical networking experience came while still in school. There were a bunch of gamers I knew, and they wanted to set up a LAN to play <em>Age of Empires</em> and <em>Quake </em>together. I got together with a couple of my friends and built out an inexpensive networking solution for them, covering everything from cabling to routers to setting up configurations. I’m actually not that big of a gamer myself, but building out a network was really exciting for me. I love to understand things on a practical level, because while theoretical understanding is important, I always find it most meaningful to see how things actually work.</p><h3>The power of engineering</h3><p>After graduation, I got a job in networking. There were several interesting companies in the space at the time, and I ended up joining one run by a group of IIT (Indian Institute of Technology) professors. It was a great opportunity to learn from some of the brightest minds in the field. I always tried to make sure I was learning, so I sought out whatever would help me continue to grow. Eventually, I moved on to opportunities at larger internet companies where I could use my networking knowledge but also expand into topics like large-scale, distributed systems.</p><p>Being a software engineer has always been exciting to me because it gives you the power to solve so many problems. When my daughter was born, my wife and I were looking for names that had to fit multiple constraints–e.g. it had to start with a <em>specific </em>letter–and it was a struggle to come up with viable options. As an engineer, I realized there was an easier way to find all our choices. I wrote a quick program that downloaded a list of millions of names and then ran them through our criteria. From that, I was able to narrow it down to a list of a few thousand options. My wife was amazed that I could generate so many names with just a couple hours of work.</p><p>The challenges of engineering are also interesting. You have to work hard to keep yourself informed. The industry moves so fast. When I started, I was using Java 4, and now we’re on Java 18. The way you would solve a problem in either of these versions is so different. All these newer languages have also emerged, and you can apply each to different situations. It feels like every day new machine learning research pushes the boundaries in unimaginable ways. I don’t know what’s going to come next, but I know it’s going to evolve quickly.</p><h3>Finding impact at Airbnb</h3><p>After a while in my previous role, I began to feel like my learning curve was getting saturated, so I wanted to look for a new challenge somewhere I could have a larger impact. I heard Airbnb was opening a tech center in Bangalore, and I was excited by the opportunity to be one of the first engineers there.</p><p>I joined in September 2021 as the first engineer in the Hosting org in Bangalore. I focus on tools for compliance, which is a complex problem space. Every region has their own laws on short-term rentals that hosts have to follow, and the laws can vary at different levels — the US has their laws, and then California might customize some of them, and San Francisco might have their own on top of that, and so on. These laws can also change quickly, like they did during Covid, so our products need to be versatile and adapt to new conditions.</p><p>Airbnb has been a great place to work. It’s startup-y in that there are challenging technical problems to work on, but the job is stable and the company respects your work-life balance. As a technical leader, there’s a great opportunity to be part of the evolution of our technology roadmap. The architecture recently transitioned from a monorepo to a Service Oriented Architecture, so we’re still figuring out the best approaches for the problems we’re solving.</p><p>I’ve also appreciated Airbnb’s culture, especially the focus on inclusivity and belonging. My coworkers want to make everyone feel comfortable. When they introduced themselves to me, they all included their pronouns, making it easier for anyone else to share theirs. The people here live the culture and make everyone feel included.</p><h3>Building our office in Bangalore</h3><p>One of my favorite things about working as an engineer in the Bangalore office is the ownership and accountability. We’re not just a delivery center, where we’re being passed requirements from elsewhere and building that one piece of software. We like to call ourselves a capability center. Our team is tasked with the whole span of product development, from identifying what user problems exist all the way through to delivering a solution for them. We work on the same roadmap, codebase, and tech stack as Airbnb HQ.</p><p>Our team is growing quickly, both in Bangalore and remotely across India. With the team being spread out, trust and team-building have been important. We have a social meeting every Friday, and the whole team shows up so we can get to know one another. It’s great for connecting with teammates, and the trust we’re creating helps us build more successful products.</p><p>Airbnb leadership has a clear roadmap for the future of the Bangalore Tech Center, and the team is growing quickly. It’s been exciting to build our first tech center outside of headquarters. We’re hiring for a number of teams and we’d love to hear from you!</p><p>Check out these related roles based out of Bangalore:</p><p><a href="https://grnh.se/777f0dbd1us">Engineering Manager, Ambassador Platform Products</a></p><p><a href="https://grnh.se/9b78e7f21us">Staff Software Engineer, Ambassador Platforms</a></p><p><a href="https://grnh.se/e0c9d3761us">Manager, BizTech</a></p><p><a href="https://grnh.se/d43963981us">Senior Software Engineer, Cities</a></p><p><a href="https://grnh.se/6a500ddd1us">Sr. Analytics &amp; Insight Analyst: CSA</a></p><p><a href="https://grnh.se/b6de7b661us">Operations Engineer, Biztech</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=70468aa3bc06" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06">My Journey to Airbnb — Veerabahu Chandran</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sisyphus and the CVE Feed: Vulnerability Management at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/sisyphus-and-the-cve-feed-vulnerability-management-at-scale-e2749f86a7a4?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e2749f86a7a4</guid>
            <category><![CDATA[scalability]]></category>
            <category><![CDATA[vulnerability-management]]></category>
            <category><![CDATA[security]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[airbnb]]></category>
            <dc:creator><![CDATA[Keziah Plattner]]></dc:creator>
            <pubDate>Wed, 10 Aug 2022 17:05:15 GMT</pubDate>
            <atom:updated>2022-08-10T17:05:15.891Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*McONkEbdjlQOssDo" /></figure><p><strong>Authors<br></strong><a href="https://www.linkedin.com/in/keziahsonderplattner">Keziah Perez Sonder Plattner</a>, Senior Software Engineer<br><a href="https://www.linkedin.com/in/kadia-m-58a18328">Kadia Mashal</a>, Engineering Manager</p><h3>Introduction</h3><p>Every engineer knows that security is a never-ending problem. Until we delete all our code and move into a cottage in the woods, we have to accept that there is no such thing as 100% secure software. You could be doing everything perfectly, and a publicly known vulnerability (<a href="https://www.redhat.com/topics/security/what-is-cve">CVE</a>) could emerge for the most updated version of a third party library in your infrastructure. Things are secure until they are not. Like with Sisyphus, the boulder will never reach the top of the hill.</p><p>Rather than eliminating vulnerabilities, the goal of a vulnerability management program should be to quickly and effectively detect and respond to the barrage of threats that surface every day. There are many scanners and vendor tools that purport to solve the problem. But with the scanners comes the problem of a never-ending flood of CVE reports, thus slowing down our ability to remediate in a timely manner.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/416/0*m80-F_w_BxVFZo0_" /></figure><h3>Vulnerability Management Lifecycle</h3><p>If you are new to vulnerability management, here are the basics of the lifecycle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UxDN236N9MBuYvDh" /><figcaption><em>Fig. 1: The Vulnerability Management Lifecycle</em></figcaption></figure><p><strong>Detection</strong></p><p>Find potential vulnerabilities in our infrastructure, anywhere from CVEs to insecure misconfigurations.</p><p><strong>Risk Assessment</strong></p><p>Apply a risk framework to the findings to identify true positives and weed out non-applicable vulnerabilities.</p><p><strong>Reporting</strong></p><p>Find the team and/or person best suited to address it and track progress in a methodical way. In addition, centrally track all vulnerabilities in order to have a full view of our attack surface.</p><p><strong>Remediation and Prevention</strong></p><p>Promptly remediate the vulnerability and invest in work to prevent the vulnerability from being introduced in the first place.</p><h3>Objectives</h3><p>In building a vulnerability management program, we want to focus on the following:</p><ul><li><strong>Visualize Known Attack Surface</strong>: We can’t properly assess risk if we don’t know our vulnerability status in the first place.</li><li><strong>Speed</strong>: Detection, reporting, and remediation should be completed in a timely manner.</li><li><strong>Prioritization</strong>: Focus on the highest-priority vulnerabilities before tackling less important or harder to exploit ones.</li><li><strong>Scaling</strong>: Support a constantly evolving and expanding cloud infrastructure.</li></ul><h3>Gaps and Challenges with Standard Industry Solutions</h3><p>Standard industry advice leans on out-of-the-box vendor deployments, manual risk assessment, and operationally-heavy reporting processes. Automation, if it exists, relies on limited vendor functionality without the flexibility to adjust to unique attributes of our environment. This led to major challenges in accomplishing our objectives.</p><p><strong>Lack of Vendor Agnostic Solutions</strong></p><p>As our infrastructure expands, the number of vulnerability types we want to track grows along with it. A variety of scanning solutions are needed to cover our bases, but they come with different setups and reporting processes. And there may be future scanning solutions that will work better for us, so we don’t want to lock ourselves into a single vendor or solution.</p><p><strong>Noisy and Inaccurate Severity Ratings</strong></p><p>In our experience, the majority of scanners provide inaccurate risk scoring. Vulnerability bulletins and assessments like the basic Common Vulnerability Scoring System (CVSS) may describe a worst-case scenario that is difficult to exploit or sensationalize an issue, leading to inflated severity. And when a major zero-day vulnerability is found, it’s more likely to be identified by an anonymous Twitter user than a scanner.</p><p>Additionally, internal mitigations can lower the impact of a vulnerability, and generic scanners rarely have ways to add internal context to customize risk ratings. Asset information and the location of the vulnerability play a massive role in determining its severity.</p><p><strong>Operational Work</strong></p><p>Many vulnerability management solutions assume the need for human intervention in the process. However, humans make mistakes, and every manual step leads to slower remediation times. Spending time on onerous tasks like manually assessing risk severity or making tickets takes away from our time to focus on root-cause remediation.</p><h3>Guiding Principles</h3><p>Before developing our solution, we wanted to establish guiding principles. While there will always be exceptions, our goal is to keep this as our north star.</p><p><strong>Limit the need for human intervention by reducing false positives. </strong><br>It can be tempting to design a solution that catches close to 100% of true positives.</p><p>However, when maximizing true positives, it’s inevitable that the false positive rate will increase too. False positives create onerous manual work for both the security team and the owning engineering team. We don’t want to “cry wolf” on vulnerabilities that are not worth addressing. Doing so is both unscalable and breaks trust in the security org. Not to mention that relying on manual triage to verify alert severity slows down response time, leaving vulnerabilities open for longer.</p><p>In addition, the idea that manually checking vulnerabilities will result in higher accuracy doesn’t take into account human error and alert fatigue. No human or automated process will ever get 100% true positive accuracy, and that knowledge must be built into the solution instead of fighting a losing battle against the barrage of noisy alerts.</p><p><strong>Pair detection with preventative measures.</strong><br>Now that we have established that true positives will slip through the cracks, we need to address how to handle those cases. We pair our detection and reporting workflows with preventative solutions that address the root cause of the vulnerabilities. For example, if we make sure to have a regular patch cadence, then all vulnerabilities–including lower-priority ones–will be addressed within a reasonable timeframe. If we fix a flawed design, we will reduce the number of vulnerabilities in the first place.</p><p><strong>Build relationships.</strong></p><p>All the automation in the world won’t be useful if we antagonize other engineering teams, rather than empowering and incentivizing them to remediate problems. Developer productivity matters — we don’t want to create an onerous system that frustrates developers and makes security the enemy. We want to avoid blocking solutions unless the priority calls for it, and we want to focus engineering efforts on our highest priority gaps rather than spreading engineers thin across many low and medium level vulnerabilities. Of course, there are always exceptions here–sometimes a vulnerability is severe enough to require being paged in the middle of the night. But we want to be sparing with that approach.</p><p>It helps to get teams on board by pairing security fixes with other benefits, or working it into existing workflows so the process is essentially invisible to outside engineers.</p><p>We have tried top-down solutions in the past, but nothing has been as effective as treating engineering teams as our partners and seeking their input as stakeholders, making it a mutually beneficial process.</p><p><strong>Maintain Accountability</strong>.<br>Detecting vulnerabilities doesn’t help if there is no accountability to fix them. We want to make sure they are correctly attributed to business units so that leadership is held responsible for fixing vulnerabilities. Keeping business units accountable means that they will be incentivized to allocate resources towards the problem instead of keeping security issues on the backburner.</p><p>There should be a company-wide Service-Level Agreement (SLA) based on the severity of the vulnerability that is part of the success metrics for each org. If an SLA cannot be met, we offer automated SLA extension requests to allow teams to make adjustments. We avoid exceptions except when absolutely necessary and periodically review the reasoning.</p><h3>Building an Automated, Vendor-Agnostic Vulnerability Management Pipeline</h3><p>Given the challenges, the standard industry advice just didn’t work for our use case. So, we decided to create our own engineering solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*G2KcgicUKe7FhslT" /><figcaption><em>Fig. 2: Automated Vulnerability Pipeline</em></figcaption></figure><h4>Step 1: Aggregate and Process Vulnerabilities</h4><p>First, we turned the barrage of alerts from our scanners into a standardized format, centralized in a single place, instead of having each scanning tool siloed from the others.</p><p>In order to cleanly track vulnerabilities throughout the pipeline, we have developed a UUID generating process for every vulnerability type we encounter. This UUID is mappable to the vulnerability and the asset it is found in. This can change per vulnerability type–for example, for our third-party packages, we track vulnerabilities by <em>asset</em> + <em>package name</em> + <em>package version.</em> It is less important to individually track every CVE present in the asset, given that fixing a package version will address every related CVE. When we detect that an asset is no longer using a specific package or version, we can feel confident that the vulnerability is no longer present. And unifying multiple CVEs into one UUID is easier to manage.</p><p>This also helps when scanners have some overlap. Instead of using the limited vendor features directly, we leverage their APIs to ingest the results so that we can process the alerts according to our needs, and allow us to deduplicate repetitive alerts and verify fixes. We can then combine results as needed or add additional information.</p><h4>Step 2: Contextualize Risk</h4><p>Our next step was to automate risk assessment by taking the default severity calculation provided by the scanner and integrating additional context.</p><p>All companies have different infrastructure setups and mitigation strategies. For example, vulnerabilities involving DDOS aren’t as impactful if the load balancers have existing mitigations. On the other hand, a vulnerability in an application that handles PII can be much more severe than initially assessed by a scanner.</p><p>To improve the accuracy of our risk assessment, we take into account the following:</p><ul><li><strong>Internal mitigations</strong>: Certain vulnerability types may not be relevant in our infrastructure.</li><li><strong>Common Vulnerability Scoring System (CVSS) vector</strong>: The type of exploit is important. For example, if the attack requires local privileges and user interaction, the odds of exploitation are significantly lower, even if the impact is severe. Breaking down the CVSS base score by attack vector allowed us to gain a better understanding of the vulnerability risk.</li><li><strong>Asset risk</strong>: Is the asset public facing? Or is it an internal service that handles low-priority metadata? Does it work with PII? How critical is it to production flows?</li><li><strong>Multiple external scoring systems and metadata</strong>: Is there a difference between the National Vulnerability Database (NVD) rating, the Red Hat rating, and the vendor rating? Is this a package that hasn’t been updated in 3 years? Is this an old, unmaintained third-party package?</li></ul><p>It’s critical that this step be automated as much as possible. We don’t want to have engineers reading through every CVE guide to determine how severe a vulnerability is.</p><p>Depending on a company’s situation, tracking asset risk context may require engineering resourcing from both the security org and other engineering teams. This could involve engineering work to gather metadata from the codebase or cloud infra provider, or sorting through large existing datasets and compiling the most useful information for reference.</p><p>While that information may not always be available depending on the stage of the company, keeping track of asset information is a long-term investment that will pay out in dividends as a security program matures.</p><p>There’s no need to wait until all the relevant information is available–even a partial dataset is helpful to start, and the risk algorithm can be tuned as more data comes in. For example, in a microservice architecture, owners could fill out a yml file with attributes of the service on creation, and eventually graduate to more automated evaluation of service metadata.</p><h4>Step 3: Reporting and Remediation</h4><p>Once we’d standardized the format of the vulnerabilities and tracked them via UUIDs, we created a generic reporting service. Shared logic for ticket creation, closing, and metadata tracking is handled in the service so that we don’t need to constantly reinvent the wheel. We will go into more detail on the reporting service in the <em>Implementation</em> section.</p><h4>Step 4. Verification</h4><p>Once the vulnerability has been marked as fixed by the owner, we want to programmatically verify that it is truly gone. As our guiding principle states, humans are always the weak link in a process. Anyone can mistakenly close a ticket as fixed, and we can’t have 100% confidence unless we verify that the vulnerability is truly gone.</p><p>For scanners that report state daily, once a UUID is no longer present, we can mark the vulnerability as verified. For more complicated ones, we can write separate jobs that pass the UUID status to the reporting service so tickets can be closed and/or verified. For example, if we are tracking a vulnerability ad-hoc, we can collect information from deployment pipelines to ensure that the patch has been successfully deployed.</p><h3>Implementation and Scaling</h3><p>We want to share our specific implementation for our pipeline, however, it is worth noting that there are many different technologies that could be leveraged to handle similar logic.</p><h4>Detection</h4><p>Vulnerability scanners can be performance heavy. Moreover, deploying many vendor solutions and/or agents can increase an organization’s attack surface. It is key to understand what data the vulnerability scanner will provide and consider if an already rolled-out agent (i.e. Osquery, AWS Systems Manager Agent (SSM), etc) could provide similar output. So aside from a few necessary traditional vendor solutions, we primarily leveraged agents that were already being used for other purposes to identify security vulnerabilities.</p><h4>Airflow</h4><p>Our process is primarily implemented in <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">Airflow</a>, an open-source data processing tool released by Airbnb a few years ago. Airflow allows us to create scheduled jobs with upstream and downstream dependency management. Datasets can be tracked on any timeframe, and it is easy to backfill or rerun jobs on specific dates (in contrast to a standard cronjob).</p><p>The steps are implemented in <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html">Directed Acyclic Graphs (DAGs)</a> that can be chained together. We use this to create a data processing pipeline to ingest the alerts and process them as mentioned in our pipeline explanation. Shared data like internal risk context, ingested vulnerability feeds, and NVD/Red Hat scoring criteria can be easily accessed at any point of the pipeline for writing contextualized risk logic.</p><h4>Reporting Service</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*J5vHCbaIQTgs_5sN" /><figcaption><em>Fig. 3: Reporting Service Logic</em></figcaption></figure><p>Now that we had the process for a single type of vulnerability, we wanted to be able to easily scale for any new type of vulnerability that we start tracking.</p><p>This is where the importance of the reporting service comes in. Our reporting service doesn’t care about the details of the vulnerability. All it needs is the data stored in a table with the expected schema and the client-provided callbacks to create the ticket copy.</p><p>The modular nature of Airflow DAGs and the reporting service makes it simple to add a new vulnerability source into our systems. Our team doesn’t have to be responsible for writing every ingest / risk assessment process, and external teams using our pipeline don’t have to worry about managing the shared vulnerability tracking logic. Shared metadata can be reused over and over again, so things like risk assessment logic get easier every time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gqspSTfgjHG6PCSj" /><figcaption><em>Fig. 4: How we scale our reporting service for any number of alert types.</em></figcaption></figure><h3>Results</h3><h4>Scaling</h4><p>Before we standardized on this system, the vulnerability management team had to be much more involved in the vulnerability tracking process, creating a bottleneck. It was difficult to have a different team own a specific kind of scanner or vulnerability type, as we had to be deeply involved in how it was tracked.</p><p>After we rolled out this process, the number of vulnerabilities we were able to manage increased dramatically. <a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-3-34e592c45d46">Multiple security teams were able to integrate with our pipeline for their own purposes</a>, and unifying the functionality across our org allowed us to automatically get metrics that give us insight into our full attack surface.</p><h4>Operational Work</h4><p>Our work with contextualizing risk also made an enormous difference in managing the operational work around risk assessment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/455/0*rNTgLOG3QUSPktNn" /><figcaption><em>Fig. 5: False positive rate over time</em></figcaption></figure><p>For example, when we first deployed a new scanner, a large percentage of the alerts were false positives. We spent a lot of time going through the tickets daily to filter out the noise and identify the highest priority CVEs. Over the course of several months, we tuned our risk assessment algorithm to take in different kinds of criteria aside from the default severity score provided by our scanner. Now, we only occasionally have to manually review tickets to validate severity (primarily criticals, which can be difficult to distinguish from highs), and we trust that alerts are most likely accurate.</p><h4>Case Study: log4shell</h4><p>A side effect of having a scalable vulnerability management system is that it is significantly easier to react quickly during critical incidents.</p><p>Like the rest of the internet, we had to act fast when the log4j vulnerability occurred. Several years ago, the work would have been both operationally and programmatically difficult, and would require significant time and resources. However, our new pipeline allowed us to respond much more quickly. We simply wrote a new DAG to track all services running Java and passed it into the reporting service. While our engineers were busy patching the services, we wrote a second DAG programmatically detecting if a service had been patched. We then were able to confidently verify the status of a service, while reopening tickets for incorrectly fixed ones.</p><h3>Takeaways and Suggestions</h3><p>Vulnerability management is a hard problem to solve and getting to a solution that works best in a custom environment takes time. It is key for organizations to prioritize the problem space using automation to allow them to quickly address known attack surfaces.</p><p>Vulnerability management should be treated as an engineering problem, not as an operational problem. If you have not yet adopted this approach, hopefully the benefits we have described convince you to take steps towards this goal. And just like every engineering solution, you will learn to adjust your approach as you gather more datasets and metrics about your environment.</p><p>On top of detection and metrics tracking, it’s important to prioritize automation to address vulnerability root causes because metrics alone will not reduce attack surface. Prevention is always better than remediation.</p><p>Lastly, be creative with your solutions. Survey your existing tools, even ones not specifically geared towards security, to see if they can provide vulnerability insights. Your vulnerability management automation pipeline should be modular and vendor-agnostic to provide flexibility to incorporate all available data sources. The more you can reuse existing tools, the less additional attack surface you’ll need to maintain while still providing valuable signal.</p><h3>Acknowledgements</h3><p>Thanks to Deanna Bjorkquist who has helped drive the Vulnerability Management program and automation requirements. Thanks to Derek Wang for code excellence and feature expansion. Thanks to Christopher Barcellos for reviewing and providing feedback for our blog post. Thanks to Tina Nguyen for helping drive and make this blog post possible. Thanks to Mark Vlcek for his work on some of our scanning solutions. Thanks to the internal Airbnb Airflow team for their technology support.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2749f86a7a4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/sisyphus-and-the-cve-feed-vulnerability-management-at-scale-e2749f86a7a4">Sisyphus and the CVE Feed: Vulnerability Management at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Airbnb’s Approach to Access Management at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-approach-to-access-management-at-scale-cfa66c32f03c?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/cfa66c32f03c</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[access-control]]></category>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[authorization]]></category>
            <category><![CDATA[information-security]]></category>
            <dc:creator><![CDATA[Paul Bramsen]]></dc:creator>
            <pubDate>Mon, 08 Aug 2022 17:02:55 GMT</pubDate>
            <atom:updated>2022-08-18T17:00:00.023Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>How Airbnb securely manages permissions for our large team of employees, contractors, and call center staff.</strong></p><p><strong>By:</strong> <a href="https://www.linkedin.com/in/paul-bramsen-9a98638b/">Paul Bramsen</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l86VJi1iVOnE7ngb" /></figure><h3>Introduction</h3><p>Airbnb is a company that is built on trust. An important piece of this trust comes from protecting the data that our guests and hosts have shared with us. One of the ways we do this is by following the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least privilege</a>. Least privilege dictates that–in an ideal world–an employee has the exact permissions they need at the moment their job requires them. Nothing more, nothing less. Anything more introduces unnecessary risk–whether from a malicious employee, compromised laptop, or even just an honest mistake. Anything less inhibits productivity.</p><p>Not only has enforcing least privilege always been crucial for maintaining trust, it’s rapidly becoming a legal necessity. Airbnb operates in <a href="https://news.airbnb.com/about-us/">almost every country and region in the world</a> necessitating that we comply with an ever increasing set of data privacy regulations.</p><p>Administrators can effectively solve these problems with minimal tooling in small companies when an individual can track the work of all colleagues. But as a company grows, this approach does not scale. In this post, we will explain how Airbnb uses a novel software solution to maintain least privilege while enabling our large team of employees, contractors, and call center agents to do our jobs effectively and efficiently.</p><h3>Where We Started</h3><p>In Airbnb’s early days a combination of homegrown and vendor solutions were implemented, but the lack of a unifying architecture prevented us from scaling. The hodge-podge of systems used to control access made it difficult to hit either of our least privilege goals:</p><ul><li>It was often unclear where employees could get needed permissions, hampering productivity.</li><li>Projects aimed at reducing unnecessary access (i.e., drive least privilege) required significant effort across many systems. Integrating access control with a new system took months of engineering effort when it should have been one or two days.</li></ul><p>Ultimately, these factors led to growing operational burden, reduced security, and increased hours required for compliance efforts. This led us to the following conclusion: <strong>we need a single place to manage employee access</strong>.</p><h3>Clarifying Focus</h3><p>Having determined the need for centralized access control, we worked to set guiding principles for the solution we would implement. Ultimately, we boiled down the requirements for our system to two goals:</p><ol><li>The access control system should manage the entirety of the processes and logic around a permission’s lifecycle. This includes:<br>– Self-serve ways to request or revoke permissions.<br>– Settings to control who has to approve new permissions.<br>– Tools for managing groups of permissions.<br>– Settings for automatic permission expiration.<br>– Logging to meet operational and compliance requirements.<br>– Notifications about relevant permission updates like upcoming expirations or when an approval is required.<br>All of these features should be controlled declaratively for each available permission and the system should use these declarations to implement all necessary logic and actions.</li><li>We wanted to build a system that could easily and robustly integrate with any permission store (e.g., AWS IAM, LDAP, <a href="https://ranger.apache.org/">Apache Ranger</a>, MySQL, <a href="https://medium.com/airbnb-engineering/himeji-a-scalable-centralized-system-for-authorization-at-airbnb-341664924574">Himeji</a>, etc) without the need to modify it. To use a network analogy, the permission stores would be the data plane that enforces authorization while our access control system is the control plane that coordinates everything. This requirement led us to focus on providing the interface needed to efficiently synchronize permission changes from the central access control system into the permission stores. This would be accomplished using a little glue code for each store (allowing us to maintain the generality of the central system).</li></ol><p>We also clarified what the system would <em>not</em> be.</p><ul><li><strong>Not</strong> a hyper-reliable, hyper-low-latency way to answer online permission checks. The permission stores themselves would answer online authorization queries and, because we would sync with them, they could automatically act as a cache if the central access control system went down. While availability and performance are always important, our primary focus would be on the permission management logic.</li><li><strong>Not</strong> a place to dump one-off authorization code. Some of the prior permission management systems had evolved into authorization code dumping grounds incurring significant technical debt.</li><li><strong>Not</strong> a place to store permissions for our guests and hosts. Public product permission management requirements are generally quite different from permissions we grant to employees to access our internal tooling and data to do their jobs. The scales also generally differ by many orders of magnitude. Additionally, internal permissions are usually significantly more complex. So it makes sense to handle each case separately.</li></ul><p>If you could only take one thing away from this post, take away these goals. Clarifying our focus and using these two goals as our north star was the most critical step in building our centralized access control platform.</p><h3>Build Vs Buy</h3><p>We evaluated a number of products on the market but none of them solved for our specific goals. Generally, permissions were managed by a small group of knowledgeable administrators, operationalizing the approval process and failing our first goal. Additionally, integrations usually required modifying the client. While some of the permission stores already had plugins (e.g., LDAP plugin), others did not.</p><p>We hope that eventually a startup builds a solution that implements a centralized, self-serve, easy-to-plug-in model. We think this could provide a lot of value to other companies that don’t have the scale to justify building an in-house solution like ours.</p><h3>Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*L-FvRQ0fyPLiIS-LcYQiwg.png" /><figcaption>Each stage makes requests to the prior stage as updates flow through the system from left to right. Note that for the purposes of this post we are only considering stages 2 and 3. We can assume stages 1, 4, and 5 already exist.</figcaption></figure><p>We designed a system with a linear five-stage architecture. Changes flow from left to right. The architecture is linear in the sense that each stage can query the previous stage, but no others. For example the Access Control Platform can query Employee Data Systems and can be queried by Connectors, but never communicates directly with Permission Stores.</p><p>A stage can also have limited communication with the immediately following stage through loosely coupled channels like queues or callbacks. For example the Access Control Platform can enqueue an update message that will be consumed by a Connector to trigger a permission update.</p><ol><li><strong>Employee Data Systems<br></strong>These are the HR systems (e.g., LDAP) that contain employee data (e.g., title, location, status, management chain). The Access Control Platform ingests this data to enable features like dynamic groups based on title and approval flows based on management chain. These systems are owned by the IT team.</li><li><strong>Access Control Platform<br></strong>This is the core system. This includes all the business logic to manage permissions as well as the UI that employees use to make and/or approve changes. The Access Control Platform is highly configurable but does not directly interact with any permission stores that integrate with it. The security team owns this system.</li><li><strong>Connectors<br></strong>Connectors are the glue code that connects the Access Control Platform to the Permission Stores. They serve two purposes. First, connectors tell the Access Control Platform what permissions should be available for request. For example the data warehouse connector might make read access to the users and reservations Hive tables available for request. Secondly, if user bob received access to read reservations the data warehouse connector would synchronize this permission into the appropriate permission store — <a href="https://ranger.apache.org/">Apache Ranger</a> in this case. Since connectors are simply responding to messages on a queue by making the appropriate API calls, they can run in whatever environment their owner deems best (e.g., Kubernetes job, AWS Lambda, Airflow DAG). They are owned and operated by the team that owns the corresponding permission store. For example the storage team owns the MySQL connector.</li><li><strong>Permission Stores<br></strong>Permission stores are the systems that store the permissions and answer permission queries — for example, AWS IAM, LDAP, <a href="https://ranger.apache.org/">Apache Ranger</a>, MySQL’s built in permission system, <a href="https://medium.com/airbnb-engineering/himeji-a-scalable-centralized-system-for-authorization-at-airbnb-341664924574">Himeji</a>, or other internal systems. Note that in some cases Permission Stores may be built into clients in which case stages 4 and 5 would be combined, as is the case for MySQL.</li><li><strong>Clients<br></strong>The clients are all the systems that the end user needs — for example, SSH, Apache Superset, MySQL, internal customer support tools, Salesforce, etc.</li></ol><h3>Benefits Realized</h3><p>Two years ago we implemented this architecture and since then we’ve integrated many systems into this centralized Access Control Platform. Here we highlight a few of the benefits we’ve realized.</p><h4>Security</h4><p>One of the biggest wins for security is having a single place where we can implement new least privilege features and then apply them across the board (as opposed to implementing once for AWS, once for MySQL, once for SSH, etc). A great case study is usage-based expiration. Usage-based expiration is a feature where permissions that have not been used for a significant period of time are automatically revoked. This approach is good for security because unnecessary permissions are quickly cleaned up. But it is also good for the user experience because employees can rest assured that the permissions being removed aren’t the ones they use regularly. Before the revocations happen, the Access Control Platform notifies impacted employees about the upcoming change and provides instructions on what to do if they need to keep the permissions. The notifications also provide links to low-friction ways to get the permissions back after the revokes happen should they realize later that the permissions were needed.</p><p>The chart below shows relative change in users with access to a core production system we’ll call System X. After rolling out usage-based expiration at the end of April, users with access to System X hit a steady state of about one third peak. A significant least privilege win! We saw similar results in other systems where we rolled out usage-based expiration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/786/0*ahfDNc4H4XLmdXfF" /><figcaption>Users with System X access dropped by two thirds after enabling usage-based expiration in late April.</figcaption></figure><p>Another security benefit has been the ability to roll out consistent compliance changes across all systems as new regulations are introduced. For example, we could enable a rule that requires North American employees to get special approval from our European legal counsel in order to access certain protected data for European customers. This rule can be consistently applied across many systems such as online databases, offline datastores, and customer support tooling.</p><p>Another win has been having a centralized database, against which we can create cross-system least privilege metrics and track our progress over time. The chart above was generated using this database.</p><h4>Usability</h4><p>Having a centralized access control platform has been a big win for usability. By consolidating, users no longer need to be aware of the N different places they need to go to request access. Effectively we’ve been able to create a one-stop-shop for all access at Airbnb. Just search for what you need access to and we’ll guide you through the rest.</p><p>The self-serve features we’ve built into the platform have helped reduce operational overhead. Employees can request permissions without having to involve a support engineer. When a manager goes out of town they can delegate a peer to approve changes on their behalf. Users have self-serve revoke for their own permissions, their reports’ permissions, or permissions for systems they manage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/786/0*RItgoPuQHokQDkup" /><figcaption>Providing good self-serve access control tooling has significantly cut support costs.</figcaption></figure><h4>Developer Experience</h4><p>We’ve put significant effort into making it as easy as possible for developers to build the connectors that link the Access Control Platform with permission stores. A large portion of this effort has been building great tools.</p><p>As an example, a design decision we made that has proved extremely useful in providing a strong developer experience is notifying connectors about changes via an asynchronous message queue. Whenever a permission’s state changes, the Access Control Platform sends a message to the queue. The queue is processed by the connector that’s responsible for syncing the state of the updated permission into the permission store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*Yj_z9ZaQScTCHMKN" /><figcaption>The permission’s state (granted or revoked) has to be fetched from the Access Control Platform. It is not included in the enqueued message.</figcaption></figure><p>The contents of the message are a critical part of the design. The message contains what the permission is and who it is for, but <em>not</em> whether the permission was granted or revoked. To get the current state (granted / revoked), the connector must query the platform. You can think of the message as a trigger to cause the system to resync permission X for user Y.</p><p>This design has the following properties:</p><ul><li>Because the latest state is always fetched (granted / revoked), message processing is idempotent.</li><li>This allows us to use at-least-once delivery semantics, greatly simplifying the process of ensuring that the proper messages are sent every time a permission changes. If a permission changes but the process is killed (perhaps due to a deploy) before we’ve recorded that the platform triggered the necessary update message, we just trigger the message again in a clean-up process.</li><li>Replay attacks are nullified. So we let connector developers freely enqueue messages to aid in debugging. As a connector developer, this is quite useful when trying to determine why a permission sync is failing.</li><li>If updates do fail too many times, the message goes to a dead letter queue and the team responsible for the connector is alerted. Developers then use our tools to read the messages from the dead letter queue and debug the failing updates. Once issues are fixed, all failed messages can be re-enqueued which will bring all permissions back in sync.</li><li>We run regular offline jobs to do bulk permission diffs and identify any permission changes that need backfilling. Then we trigger resyncs by enqueuing update messages for these permissions. This means that connector developers only need to write code to support incremental sync rather than both backfill and incremental sync. The backfills are free!</li></ul><h3>Conclusion</h3><p>Managing permissions and ensuring least privilege is a challenge at any company and especially difficult in large companies. Many companies come up with operationally heavy solutions that are expensive, insecure, and provide a negative user experience. At Airbnb, we’ve solved this challenge by implementing a centralized, self-serve access control platform. What made our investments such a success was solving Airbnb’s unique goals in a cohesive and scalable way, and what is very rare is the degree to which we’ve actually rolled this out in production. The majority of permissions at Airbnb are managed by our Access Control Platform. Our approach has enabled us to make huge strides in ensuring that we’re doing everything we can to keep our community’s data safe while at the same time enabling Airbnb’s employees to do our best work.</p><p>We’ve made a lot of progress in the access management space, but there is still a lot to do! If you’re interested in working on this or other efforts to protect Airbnb’s community, check out security and software engineering jobs on <a href="https://careers.airbnb.com/">our careers page</a>.</p><h3>Acknowledgments</h3><p>The Access Control Platform we’ve built was the result of hard work from many collaborators at Airbnb. <a href="https://www.linkedin.com/in/zhusamuel/">Samuel Zhu</a>, <a href="https://www.linkedin.com/in/alissara-rojanapairat/">Alissara Rojanapairat</a>, <a href="https://www.linkedin.com/in/kyler-mejia-a9a323101/">Kyler Mejia</a>, <a href="https://www.linkedin.com/in/stephynancy/">Stephy Nancy</a>, and Maryna Butovych built significant portions of the system and contributed to the architecture. <a href="https://www.linkedin.com/in/alanyao/">Alan Yao</a> and <a href="https://www.linkedin.com/in/abhishek-parmar-924b529a/">Abhishek Parmar</a> provided invaluable feedback that influenced the architecture. <a href="https://www.linkedin.com/in/julia-k-cline/">Julia Cline</a> ensured that we were building a product that would meet the needs of our customers. <a href="https://www.linkedin.com/in/brettbukowski/">Brett Bukowski</a>, Jacqui Watts, <a href="https://www.linkedin.com/in/julia-k-cline/">Julia Cline</a>, <a href="https://www.linkedin.com/in/patmoynahan/">Pat Moynahan</a>, and <a href="https://www.linkedin.com/in/chris408/">Christopher B</a> provided valuable feedback on this blog post. <a href="https://www.linkedin.com/in/tinamn/">Tina Nguyen</a> and <a href="https://www.linkedin.com/in/laurenmackevich/">Lauren Mackevich</a> shepherded this blog post through the process. And many other colleagues contributed in small and large ways to make this possible.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cfa66c32f03c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-approach-to-access-management-at-scale-cfa66c32f03c">Airbnb’s Approach to Access Management at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Incident Management]]></title>
            <link>https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ae863dc5d47f</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[incident-management]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[reliability-engineering]]></category>
            <category><![CDATA[automation]]></category>
            <dc:creator><![CDATA[Vlad Vassiliouk]]></dc:creator>
            <pubDate>Wed, 27 Jul 2022 16:39:43 GMT</pubDate>
            <atom:updated>2022-07-27T16:39:43.843Z</atom:updated>
            <content:encoded><![CDATA[<h3>Automated Incident Management Through Slack</h3><p>How Airbnb automates incident management in a world of complex, rapidly evolving ensemble of microservices.</p><p><a href="https://www.linkedin.com/in/vladimir-vassiliouk">Vlad Vassiliouk</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hP8PSrLw_LTyLjhbR6LRxg.jpeg" /></figure><h3>Incident Management</h3><p>Incidents are unforeseeable events that disrupt normal business operations and are inevitable in complex systems that must be up and running 24/7. This is why it’s important to prepare and to train people to handle incidents in a timely and organized manner. Although each incident is unique, we follow the same procedure for detection, escalation, management, and resolution of incidents.</p><p>At Airbnb, we utilize a <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> which involves many interconnected services managed by small teams. Quickly figuring out what service is in trouble, and who to page is paramount to timely incident resolution. We found that our teams spent a lot of time switching between applications such as Slack, Pagerduty and Jira to raise an incident, page responders, and provide context. In order to have quick resolutions of incidents, we developed an incident management bot, a centralized automation tool for incident management.</p><h3>Incident Management Slack bot</h3><p>Our goal was to centralize incident management in Slack. Everyone at Airbnb is familiar with and has access to Slack, and it’s easy to bring people and resources together in an incident channel. In addition, the incident channel acts like a timeline of events which makes putting together a post mortem report easy.</p><p>Our requirements were as follows:</p><ul><li>Run in Airbnb’s <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> and have full support from our team.</li><li>Standardize incident-related communications in all tools such as Jira, Slack, PagerDuty.</li><li>Centralize incident management in Slack.</li><li>Single intake funnel for incidents with clearly defined steps.</li><li>Automate post-incident tasks such as setting up meetings and archiving channels.</li><li>Provide incident timelines and metrics.</li></ul><p>We decided to build our own app to meet our exact specifications and allow us to easily customize and develop further. We also chose to build the app in Golang, because of the great community, and their well documented <a href="https://pkg.go.dev/github.com/slack-go/slack">slack library</a>.</p><p>Finally, we decided to use chat commands instead of <a href="https://slack.com/help/articles/201259356-Slash-commands-in-Slack">slash commands</a> so that all commands sent to the bot would be visible to the members of the Slack channel.</p><p>Our incident management bot achieves incident response automation through four key commands:</p><ul><li><strong>new incident &lt;summary&gt;: </strong>Create a Jira ticket and page incident managers.</li><li><strong>new channel &lt;ticket&gt;: </strong>Create an incident Slack channel for an open incident ticket.</li><li><strong>page &lt;service|user&gt;: </strong>Page the on-call(s) for a PagerDuty Service or a user directly.</li><li><strong>get timeline:</strong> Compile a concise timeline of important chat events for post-incident analysis.</li></ul><h3>Incident Response Lifecycle</h3><p>We have defined four separate phases of an incident: detection, communication, escalation and resolution. Each of the bot’s commands automates tasks that would normally require coordination during these distinct phases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x2vbM-iepMqqsrH9" /></figure><h3>Detection</h3><p>Most of our incidents are detected by our <a href="https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f">monitoring and alerting tools</a>, although sometimes we learn about incidents from our team members or customers. No matter how an incident is detected, having a single intake funnel for all incidents is crucial for effective incident detection. Our bot solves this by providing the “new incident” command.</p><h4>New incident &lt;summary&gt;</h4><p>This command creates a blank JIRA ticket with default settings and asks the user if they’d like to page an incident manager.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*W275nEZINolYZ6pn" /></figure><p>Regardless of the user’s choice to page an incident manager, a popup appears to the user asking for additional information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/519/0*cwNt9XjvCIiVXNS2" /></figure><p>This allows us to escalate incidents quickly while still allowing the incident responder to provide valuable information for the incident managers. These fields are optional in the interests of urgency and can be filled out later if needed.</p><h3>Communication</h3><p>Another important first step is to set up communication channels and provide as much context as possible to responders.</p><h4>New channel [Jira ticket]</h4><p>This command takes an optional Jira ticket as a URL or key. If none is provided, it will show the last 5 recently opened incident tickets for the user to choose. A channel is then created using the Jira ticket key, the summary as the title, and all incident managers are invited.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/495/0*TfXcDuFds_fv8K-8" /></figure><p>To provide context to all users invited, the channel’s topic is set to the Jira ticket link along with the summary of the Jira ticket. In addition, we update the Jira ticket with a link to the newly created Slack channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/328/0*2WJcyBs7LocZ4yjP" /></figure><h3>Escalation</h3><p>You may have heard about the<a href="https://nvd.nist.gov/vuln/detail/CVE-2021-44228"> Log4j security vulnerability</a> which was characterized as the single biggest and most critical vulnerability of the last decade. Within 72 hours of vulnerability disclosure, there were reports of <a href="https://arstechnica.com/information-technology/2021/12/hackers-launch-over-840000-attacks-through-log4j-flaw">840,000 attacks</a> on companies globally, which turned into 100 internet wide attacks per minute over the following weekend.</p><p>At Airbnb, we have over a thousand micro services with hundreds of small teams managing them, which offered a unique challenge for us. We had to identify all vulnerable services, and quickly reach out to their respective owners for quick mitigation. This is where our Slack bot really shined, allowing our Incident Managers to quickly reach out to service owners and coordinate rolling out the fix much quicker than before. In a matter of minutes, the bot was used to page over 300 teams to assist with assessing impact and deploying patches. This equated to 4 hours saved compared to paging these teams manually, not to mention reducing the time spent in a vulnerable state.</p><h4>Page &lt;shortcut|service name|slack user&gt;</h4><p>The page command can be given a service shortcut, service name, or a slack user.</p><p>To get started, the user can view a list of shortcuts by typing in “page list”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/264/0*DVEqiuHCfuSqyruA" /></figure><p>Each shortcut corresponds to a PagerDuty service ID which will be used when creating a PagerDuty incident. The shortcuts are easily customizable by editing a YAML file.</p><p>If a user types in a service name which doesn’t match any shortcut, a search is done in the PagerDuty service directory and results are displayed for the user to choose.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/426/0*yym1kmhJGZTzsa4c" /></figure><p>Once a user chooses the service they want to page they’re asked to confirm and a new PagerDuty incident is created for that service.</p><p>We also allow paging Slack users directly for when additional responders are required outside of those on-call.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/0*pw130BpOe5IFD7CZ" /></figure><p>Once the page command is sent, the bot creates a new incident in PagerDuty with the Jira ticket, summary, and slack channel to provide context to the on-call person. After the on-call person is paged, the bot announces who was paged and invites them to the channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/457/0*f0cPvK_oG91okdgm" /></figure><h3>Resolution</h3><p>Once responders confirm there is no further user impact and a root cause is known, the incident is considered resolved and the team transitions to the post-incident phase. A robust timeline is required to have an effective post-incident review and an effective <a href="https://www.atlassian.com/incident-management/postmortem">post mortem</a> report.</p><h4>Get timeline</h4><p>This command will search the incident channel for all chat messages marked with a specific emoji which designates the message as a timeline event, and direct message the user a compiled timeline.</p><p>For example, we use the 📝 emoji to designate important events in the chat. As the incident is ongoing, anyone can add the emoji as a reaction to important chat events. Post-incident, the “get timeline” command will compile these chat events into an easy to copy paste timeline to be used in the post-incident report.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*vL094xETdA2Re2cR" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/472/0*Z2WAZ9sw8X_Fg0-J" /></figure><h4>Incident Review</h4><p>At Airbnb, we have after action review meetings (AAR) weekly where we review recent high severity incidents, post-incident reports, and ensure any corrective actions are called out and assigned. As soon as the Jira ticket tracking the incident is updated with the AAR meeting date, the bot will notify the person owning the Jira ticket when the meeting will be and what is expected of them.</p><h4>Followup Tracking</h4><p>Oftentimes, during our <a href="https://www.atlassian.com/incident-management/postmortem/blameless">blameless postmortem process</a>, tickets for corrective actions are created and assigned to teams to avoid similar incidents in the future. To encourage quick resolutions we set a strict deadline for these tickets. Our bot will send a warning message over Slack a couple of days before the deadline, and another message if the deadline has lapsed to the user assigned to the ticket.</p><h4>Archiving Incident Channels</h4><p>To keep our Slack workspace tidy, the bot automatically archives incident channels ten days after the incident’s Jira ticket has been closed.</p><h3>Results</h3><p>Since launch, our bot has saved our Incident Managers and responders many hours through its automation and centralization of incident management within Slack. By measuring the average amount of time each task takes to complete manually compared to the bot’s automation, we determined an estimated 44 hours of time saved so far in 2022.</p><h3>What’s Next?</h3><p>To further streamline our incident response from Slack, we plan to enhance our integration with PagerDuty.</p><p>Currently, every time the page command is used a new PagerDuty incident is created. Instead, we plan to unify all pages under a single PagerDuty incident to take advantage of PagerDuty’s incident metrics and to provide more context to responders.</p><p>Lastly, after a PagerDuty service is paged using the bot, we don’t have visibility of the status of the PagerDuty incident in Slack. Was the page acknowledged? Did the on-call not respond? Was it escalated and to who? We plan to build automation to follow the PagerDuty incident and report the current status to the incident’s channel. This will also allow us to record the timeline of actions taken in the PagerDuty incident after paging the service.</p><h3>Attribution and Thanks</h3><ul><li><a href="https://medium.com/u/af76cda83a53">Stephen</a>: for being a great partner on the Airbnb Incident Management team and helping to define the incident management bot’s feature roadmap</li></ul><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ae863dc5d47f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f">Incident Management</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Beti Gathegi]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/61c2db3d8546</guid>
            <category><![CDATA[program-management]]></category>
            <category><![CDATA[éducation]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[bootcamp]]></category>
            <category><![CDATA[onboarding]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 21 Jul 2022 17:28:47 GMT</pubDate>
            <atom:updated>2022-07-21T17:28:47.687Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Beti Gathegi</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mhJHc6Qqjet_mKa-bzwu-w.jpeg" /></figure><p>From exploring careers across continents to now helping others find their place at Airbnb.</p><p><em>After trying a series of careers ranging from television production to university communications and marketing, </em><a href="https://www.linkedin.com/in/betigathegi"><em>Beti Gathegi</em></a><em> works as a Senior Program Manager on the TechED (technical education) team at Airbnb. When she’s not lurking in the #bookworms Airbnb Slack channel, you can find Beti leading Bootcamp, our onboarding program for new technical hires, which takes engineers and data scientists through their first commit at Airbnb. Before this role, Beti was a recruiting program manager for Connect, Airbnb’s engineering apprenticeship program targeted at people from non-traditional technical backgrounds.</em></p><p><em>Beti herself has a non-traditional background, with a degree in journalism and several experiences outside the tech industry, including substantial time abroad. She is a major advocate for diversity and inclusion; part of her role in leading Bootcamp involves setting the company’s culture and encouraging new hires to shape the culture in their own unique ways.</em></p><h3>Setting my own direction</h3><p>I describe myself as half East Coast, half West Coast, with a bit of time abroad added in. I’m the child of Kenyan immigrants and I grew up in the San Francisco Bay Area, in a town called Albany, California. When I was 15, I moved to the East Coast, and it would be many years before I found myself back in the Bay Area.</p><p>For a long time, I wanted to be a journalist. To that end, I studied journalism in college as part of my communications degree. I was never fixated on a specific path and certainly explored a lot to reach where I am now. My father, who pivoted later in life by getting a law degree around age 40, was my guiding light in terms of being willing to try new things. I find personal exploration liberating — crafting my own, organic path gave me a chance to figure out my likes and dislikes, as well as my skills and growth opportunities.</p><p>Part of my outlook on life is that it’s okay to stop something that isn’t right for yourself. Sometimes there can be a lot of inertia that makes it hard to pause and change directions, but I think making a decision to pursue another path is really brave and can be worn as a badge of honor. In my case, I started a master’s in liberal arts in which I was studying the South Asian diaspora and the children of Indian immigrants in particular. Inspired by the stories of others, I was eager to discover more about my own background and history. I chose to leave my program to go live in Kenya and experience Kenyan culture for myself. Until that point, I’d only been to Kenya with my family, so this was a new lens to see the country on my own.</p><h3>Living and working in three continents</h3><p>Living in Kenya was a transformative experience and helped me understand my own identity more deeply. Having previously been told, by some, that I’m not Kenyan enough or not American enough, actually living in Kenya and encountering the sheer diversity of people made me realize there’s no singular way to be Kenyan, just like there’s no one way to be American or any culture for that matter. During my time abroad, I also realized I was ready to get more hands-on experience and enter the working world.</p><p>Ready to live a life of adventure, I moved to New York City but stumbled into a financial crisis when seemingly everyone was getting laid off. I worked retail for a little while but otherwise didn’t last too long in New York. This was just the first in a series of new experiences, my next being at a TV production firm where I was an assistant, and where to this day I have an IMDB credit for four episodes of the show <em>Swamp Men </em>on National Geographic. If that wasn’t enough, I also had jobs writing TV quizzes for Nielsen, doing marketing for the University of South Florida, and working at an Australian Aboriginal art gallery.</p><p>Eventually, I happened upon the tech industry when a friend recruited me to join Lyft in a customer support role. This was a completely new universe to me and I took every opportunity to get involved and apply my skills to a growing company. Practically by accident, my initiative to gather people via the company’s internal email list turned into Employee Resource Groups or ERGs. I helped form the Black ERG and Women’s luncheon, while also supporting others who wanted to create similar spaces for their communities. Very organically, I was taking a big part in the diversity and belonging conversation and making sure to educate myself to be a thoughtful contributor to these discussions. Later, this turned into an official job focused on Lyft’s culture, and afterward I moved to Pandora as a diversity and belonging program manager.</p><h3>Onboarding to Airbnb</h3><p>By the time I applied to join Airbnb, I had shifted to leading Pandora’s university recruiting program. I noticed a tremendous amount of potential in students, and found it really impactful to work on this key pipeline. That said, going directly from college to the tech industry isn’t the only way, my own career being a prime example. I jumped at the opportunity to join Airbnb as an apprenticeship program manager where I had the chance to revamp this pathway for engineers with unconventional backgrounds to join the company.</p><p>I ran the apprenticeship program for two years and while the role was primarily on the recruiting side, we worked very closely with engineering and I started to develop an interest in TechED, the team I’m on now that owns the onboarding process for Airbnb’s technical hires. I was already meeting regularly with my manager at the time, Leo, about growing in my career, so I started by sharing my goals in that setting. Leo was incredibly supportive and epitomizes how Airbnb has a culture of empowering people to explore their passions and what energizes them.</p><p>I reached out to the manager on the TechED team to express my interest in the space and not too long after, a role happened to open up. After interviewing, I got my current position leading Airbnb’s Bootcamp program, the onboarding process all software engineers and data scientists go through in their first weeks at the company. It took a ton of experiences across many other roles to arrive at my current spot, but that came with immeasurable learning and I wouldn’t have it any other way! I feel uniquely equipped to welcome people to a new experience or challenge, having gone through so many of my own.</p><h3>Leading Bootcamp and onboarding others to Airbnb</h3><p>My current role aligns with my passion for helping people acclimate and providing them with the resources they ended to be successful. It’s fulfilling to advocate for an incredible new hire experience, help new team members feel confident in their respective roles at Airbnb, and support them towards making their first commit or code change. I strive to be really respectful of people’s time and make Bootcamp as relevant, engaging, and valuable as possible to everyone who participates.</p><p>Onboarding is a challenging task because there are multiple variables. Typically you are onboarding people in various roles, at various levels, to various teams, which may use their own tools and process. There is a balancing act between providing general information and hyper-relevant but also highly specific information. Remote onboarding also adds its own set of challenges.</p><p>That being said, I love co-creating solutions. I get to work with incredibly smart people on the engineering and data science teams to identify and clarify our challenges, workshop ideas, execute solutions, monitor progress, and iterate. I get a ton of energy from that process and from our collaborations.</p><h3>How the first weeks at work can leave a lasting impact</h3><p>I’m also grateful to the many volunteers I partner with to shape the onboarding experience for our technical hires and set them up for success. For example, we pair each new hire with a buddy from their team. They serve both to scope the hire’s starter project as well as to answer the many questions that inevitably pop up in onboarding.</p><p>We have volunteers from various teams who raise their hands to host Bootcamp and lead sessions for each new hire cohort, and most of them are driven by providing a sense of belonging. Additionally, there’s a great community of collaborators across the industry to benchmark with and get mentorship from, since onboarding is a challenging problem that a lot of companies work on.</p><p>Beyond the technical parts of onboarding, Bootcamp plays a critical role in setting Airbnb’s culture. Especially in a remote work environment, the quality of onboarding can make or break whether new hires feel a sense of community and feel comfortable engaging with it themselves. We emphasize belonging and inclusivity as core values of our culture, and we welcome new hires to bring their own special qualities to integrate into our ever-evolving culture.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61c2db3d8546" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546">My Journey to Airbnb — Beti Gathegi</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Safeguards Changes in Production]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/9fc9024f3446</guid>
            <category><![CDATA[a-b-testing]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[continuous-delivery]]></category>
            <category><![CDATA[spinnaker]]></category>
            <dc:creator><![CDATA[Michael Lin]]></dc:creator>
            <pubDate>Mon, 11 Jul 2022 17:02:38 GMT</pubDate>
            <atom:updated>2022-07-11T18:13:11.762Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part I: Evolution of Airbnb’s experimentation platform</h4><p>By: <a href="https://www.linkedin.com/in/michaelcl/">Michael Lin</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/zack-loebel-begelman-85407698/">Zack Loebel-Begelman</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0J4whYTNqGPUdUme" /></figure><h3>Introduction</h3><p>As Airbnb has grown to a company with over 1,200 developers, the number of platforms and channels for pushing changes to our product — and the number of daily changes we push into production — has also grown tremendously. In the face of this growth, we constantly need to scale our ability to detect errors before they reach production. However, errors inevitably slip past pre-production validation, so we also invest heavily in mechanisms to detect errors quickly when they do make it to production. In this blog post we will cover the motivations and foundations for a system for safeguarding changes in production, which we call Safe Deploys. Two following posts will cover the technical architecture in detail for how we applied this to traditional A/B tests, and code deploys respectively.</p><h3>Continuous Delivery and Beyond</h3><p>Airbnb’s continuous delivery team recently wrote about <a href="https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876">our adoption of Spinnaker</a>, a modern CI/CD orchestrator. Spinnaker supports <a href="https://spinnaker.io/docs/guides/user/canary/">Automated Canary Analysis (ACA)</a> during deployment, splitting microservice traffic by request to compare versions of code to see if performance, error rates, or other key metrics are negatively impacted. If metrics for the new version regress, Spinnaker automatically rolls back the deployment, significantly reducing the time to remediate a bad push.</p><p>ACA at Airbnb has indeed caught a large number of errors early in the deployment process. However, it has a number of limitations:</p><ul><li><strong>Channels: </strong>Spinnaker’s ACA tests against changes to microservices. However, microservice updates are not the only source of errors that can be pushed into production. For instance, Android and iOS apps follow a release process through their respective app stores. Many “production pushes” at Airbnb may involve no new code at all, and are strictly applied through configuration changes. These changes include marketing campaigns or website content created with Airbnb’s <a href="https://medium.com/airbnb-engineering/airbnbs-promotions-and-communications-platform-6266f1ffe2bd">internal content management systems</a>. While seemingly benign, pushes through these systems can have dramatic effects. For example an incident was once caused when a marketing campaign was mistakenly applied to all countries except one, instead of the original intent of targeting one specific country. This simple mistake led to empty search results for nearly all users globally, and required over an hour to identify and revert.</li><li><strong>End-to-end business metrics: </strong>Spinnaker’s ACA is driven by local system metrics, such as a microservice’s local performance and error rates; not end-to-end business metrics, such as search click-through rates and booking rates. While roll-backs based on local system metrics are valuable, they aren’t sufficient, as some of our most costly bugs impact end-to-end business metrics but not local system metrics. For instance in 2020, a simple frontend change was deployed to production without being tested on a specific browser that did not support the CSS used, preventing users on that browser from booking trips. This had no impact on system metrics, but directly impacted business metrics. <br><br>Unfortunately, adding business metrics to Spinnaker’s ACA system is not possible because Spinnaker randomizes traffic by request, therefore the same user may be exposed to multiple variants. Business metrics, however, are generally user based and require each user to have a fixed variant assignment. More fundamentally, it’s not possible because business metrics need to be measured end-to-end and when two microservices undergo ACA at the same time, Spinnaker has no way of distinguishing the respective impact of those two services on end-to-end business metrics.</li><li><strong>Granularity: </strong>Spinnaker’s ACA tests at the level of the entire microservice. However, it’s often the case that two features are being worked on at the same time within a microservice. When ACA fails, it can be hard to tell which feature caused the failure.</li></ul><p>While we heavily depend upon Spinnaker’s ACA at Airbnb, it became clear there was an opportunity to complement it and address the above limitations where the circumstances call for it.</p><h3>Experimentation Reporting Framework (ERF)</h3><p>A/B testing has long been a fixture in product development at Airbnb. While sharing some qualities with ACA in counterfactual analysis, A/B testing has focused on determining whether a new feature improves business outcomes, versus determining whether that feature causes a system regression. Over the years Airbnb has developed our Experimentation Reporting Framework (ERF) to run hundreds of concurrent A/B experiments across a half dozen platforms to determine whether a new feature will have a positive impact.</p><p>ERF addresses the limitations of ACA listed above:</p><ul><li><strong>Channels: </strong>With each new platform, an ERF client has been introduced to support A/B testing on it. This includes mobile, web, and backend microservices. APIs were also introduced to provide config systems an avenue to treat config changes as A/B tests.</li><li><strong>End-to-end business metrics: </strong>ERF is driven <em>primarily</em> by end-to-end business metrics. On the technical side, it randomizes by user, not request, and it is able to distinguish the impact of hundreds of experiments running concurrently. ERF taps into Airbnb’s <a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">central metrics system</a> to access the thousands of business metrics and dimensions Product and Business teams have defined to measure what matters most to Airbnb overall.</li><li><strong>Granularity: </strong>Where Spinnaker’s ACA runs its experiments at the level of an entire microservice, ERF runs its experiments based on what are basically feature flags embedded into the code. Thus, if multiple features are being developed concurrently in the same microservice, ERF can determine which one is impacting the business metrics.</li></ul><p>The above characteristics of ERF address the limitations of ACA, but ERF also had a limitation of its own: it was a daily-batch system generating interactive reports intended to be consumed by human decision makers. To address the limitation of Spinnaker’s ACA, ERF needed to evolve into a near real-time system that can directly control the deployment process without human intervention.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rOQST4-KYXGe253y" /><figcaption>Figure 1: Areas of the ERF Platform augmented to support near real-time experimentation</figcaption></figure><p>This evolution had implications on both the data science behind ERF, and its software architecture. We describe the former in this post, and will describe the latter in the next post of this series.</p><h3>Realtime ERF — The Data Science</h3><p>The foundation of solid data science is solid data engineering. On the data engineering side, we needed to revisit the definitions of the business metrics to be computed in real-time. The metrics computed by the batch ERF system were designed for accuracy, and could take advantage of complex joins and pre-processing to achieve this. Near real-time metrics did not have this luxury, and required simplification to meet low latency requirements.</p><p>Not only did we have to build new metrics, but we knew we would have to build new statistical tests as well. It is imperative for safe deployment systems to not be noisy, otherwise people will stop using it. Traditional methods like T-Test suffer from a variety of issues that would be extremely problematic when implemented in a real-time system. Two issues in particular are false positives due to (1) <a href="http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1517.pdf">peeking</a> (looking before a predetermined amount of time) and (2) heavily skewed data.</p><p>When monitoring whether or not a metric has changed in real-time, users want to be notified as soon as the model has the confidence that this is true. However, doing so naively results in the first issue, peeking. In traditional A/B testing, the statistical test is only applied once after a predetermined time, because there is a chance that a significant result is due to randomness and not an actual effect. For real-time ERF, we aren’t making just one test, since, depending on how long we wait to take the test, we’re at risk for either taking too long to detect some errors, or missing other errors that take longer to surface. Instead, we want to check (peek at) the model every 5 minutes so that we can react quickly. With a p-value of 0.05 running 100 A/A comparisons, one could expect to have ~5 significant results that are actually false positives. We can transfer this issue to computing p-values on the same data set multiple times. Each evaluation results in a 5% chance of a false positive and so over multiple evaluations, the chance of having 1 or more false positives approaches 100%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/894/0*GHdgh0pCLSG87czD" /><figcaption>Figure 2: Increasing evaluations inevitably lead to false positives</figcaption></figure><p>To balance early detection without noisiness, we utilize <a href="https://en.wikipedia.org/wiki/Sequential_analysis">sequential analysis</a>. Sequential methods do not assume a fixed sample size (i.e., checking the model once) and allow us to continually monitor a metric without worrying about false positives incurred due to peeking. One way to correct for false positives (<a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">Type 1 Errors</a>) is by applying a <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>. If you check your model for statistical significance four times and want to guarantee a 5% overall false positive rate, you need to divide your p-value by four, meaning only results with p-value at or under 1.25% are valid. However, doing so is too conservative since each check is dependent. They are dependent because each check has the same base of data only adding additional observations as time goes on. Sequential models take this dependence into account while guaranteeing false positives rates more efficiently than Bonferroni. We use two different sequential models, SSRM (<a href="https://arxiv.org/abs/2011.03567">Sequential Sample Ratio Mismatch</a>) for count metrics, and <a href="https://arxiv.org/abs/1906.09712">Sequential Quantiles</a> (Howard, Ramdas) for quantile metrics.</p><p>The second issue that we needed to solve in order to be robust is handling skewed data. Performance metrics like latency can have extremely heavy tails. Models that assume a normal distribution won’t be effective because the Central Limit Theorem does not come into effect. By applying Sequential Quantiles, we can ignore assumptions about the metric’s distribution and directly measure the difference between arbitrary quantiles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/892/0*_UUUc7wmcL3jUKei" /><figcaption>Figure 3: Metrics may have non-normal distributions</figcaption></figure><p>Lastly, many important measures are not independent. Metrics like latency and impressions have within-user correlation, so each event in the data cannot be treated as an independent unit. In order to counteract skew, we aggregate all measures into user metrics first before evaluating statistical models.</p><h3>Conclusion</h3><p>With the statistical methods in place to evaluate business metrics in near real-time, we could now detect problems that were invisible to Spinnaker, or required too much lead time to rely on traditional ERF experiments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nmC6mnUBfe98hcxo" /><figcaption>Figure 4: How Real-time ERF fits between Spinnaker and Traditional ERF</figcaption></figure><p>Operationalizing the newly created near real-time metrics and statistical methods required further engineering, but more challenging, it required changing the experimentation culture at Airbnb. In the following post we will detail how our near real-time metrics pipeline was built, how these metrics powered automated decision making, and how we drove adoption across the company.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4262326/">Senior Software Engineer, Metric Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4216371/">Staff Software Engineer, Real-time Stream Processing Platform</a></p><p><a href="https://careers.airbnb.com/positions/2403782/">Staff Software Engineer — ML Ops Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><h3>Appreciations</h3><p>Thanks to <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/alex-shaojie-deng-b572347/">Alex Deng</a>, <a href="https://www.linkedin.com/in/antoinecreux/">Antoine Creux</a>, <a href="https://www.linkedin.com/in/erikriverson/">Erik Iverson</a>, <a href="https://www.linkedin.com/in/george-l-9b946655/">George Li</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/preetiramasamy/">Preeti Ramasamy</a>, <a href="https://www.linkedin.com/in/rstata/">Raymie Stata</a>, Reid Andersen, <a href="https://www.linkedin.com/in/ronnyk/">Ronny Kohavi</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/tatiana-xifara/">Tatiana Xifara</a>, <a href="https://www.linkedin.com/in/vincent-chan-70080423/">Vincent Chan</a>, <a href="https://www.linkedin.com/in/xin-tu/">Xin Tu</a> and the OMNI team.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fc9024f3446" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446">How Airbnb Safeguards Changes in Production</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[T-LEAF: Taxonomy Learning and EvaluAtion Framework]]></title>
            <link>https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/30ae19ce8c52</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[taxonomy]]></category>
            <dc:creator><![CDATA[Cen(Mia) Zhao]]></dc:creator>
            <pubDate>Thu, 23 Jun 2022 17:20:01 GMT</pubDate>
            <atom:updated>2022-06-28T21:16:44.392Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>How we applied qualitative learning, human labeling and machine learning to iteratively develop Airbnb’s Community Support Taxonomy.</strong></p><p><strong>By:</strong> <a href="https://medium.com/@cenzhao06">Mia Zhao,</a> <a href="https://www.linkedin.com/in/peggyshao">Peggy Shao</a>, <a href="https://www.linkedin.com/in/maggiekhanson/">Maggie Hanson</a>, <a href="https://medium.com/@wangpengcqb">Peng Wang</a>, <a href="https://www.linkedin.com/in/bo-zeng-71915624">Bo Zeng</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X863W9ZDWmr7SlKW" /></figure><h3>Background</h3><p>Taxonomies are knowledge organization systems used to classify and organize information. Taxonomies use words to describe things — as opposed to numbers or symbols — and hierarchies to group things into categories. The structure of a taxonomy expresses how those things relate to each other. For instance, a <em>Superhost</em> is a type of <em>Host</em> and a <em>Host</em> is a type of Airbnb <em>User</em>. Taxonomies provide vital terminology control and enable downstream systems to navigate information and analyze consistent, structured data.</p><p>Airbnb uses taxonomies in front-end products to help guests and hosts discover exciting stays or experiences, as well as inspirational content and customer support offerings. Airbnb also uses taxonomies in backstage tooling to structure data, organize internal information, and support machine learning applications.</p><p>Classifying the types of issues Airbnb community members face is vital for several reasons:</p><ul><li><strong>Hosts and guests</strong> need to be able to describe issues to Airbnb in order to receive relevant help suggestions or get connected with the best support.</li><li><strong>Support Ambassadors</strong> (Airbnb’s Community Support specialists) need quick and easy access to workflows that help them resolve issues for guests and Hosts.</li><li><strong>Airbnb business units</strong> need to understand where and why guests and Hosts encounter problems so that we can improve our product and make the Airbnb experience better.</li></ul><p>The Contact Reasons taxonomy is a new, consolidated issue taxonomy that supports all of these use cases. Before Contact Reasons, Community Support had siloed taxonomies for guests and Hosts, Support Ambassadors, and machine learning models that each used different words and structures to classify the same issues and relied on manual mapping efforts to keep in sync.</p><p>The consolidation of disjointed issue taxonomies into Contact Reasons was the first project of its kind at Airbnb. The development of such a new taxonomy requires iterative learning: create/revise the taxonomy by taxonomists; roll out to train ML model, product and services; evaluate the quality of the taxonomy and identify areas for improvement. Before this work, there was no systematic process in place to evaluate taxonomy development or performance and the iteration was mostly subjective and qualitative. To accelerate the iterative development with more quantitative and objective evaluation of the quality of the taxonomy, we created T-LEAF, a <strong><em>T</em></strong><em>axonomy </em><strong><em>L</em></strong><em>earning and </em><strong><em>E</em></strong><em>valu</em><strong><em>A</em></strong><em>tion </em><strong><em>F</em></strong><em>ramework</em>, to quantitatively evaluate taxonomy from three perspectives: coverage, usefulness, and agreement.</p><h3>Challenges in Evaluating the New Taxonomy</h3><p>In the Airbnb Community Support domain, new taxonomies or taxonomy nodes often need to be created before we have either real-world data or clear downstream workflow applications. Without a consistent quantitative evaluation framework to generate input metrics, it’s difficult to gauge the quality of a new taxonomy (or a taxonomy version) when directly applying it to downstream applications.</p><h3>Lack of quantitative evaluation framework</h3><p>Taxonomies are typically developed by qualitative-centric approaches¹. When we started prototyping the new taxonomy, we evaluated feedback from existing users, and recruited guests and Hosts for several rounds of user research to generate insights. While qualitative evaluation like domain expert review is helpful in identifying high-level challenges and opportunities, it is insufficient for providing evaluation at scale, due to small sample sizes and potential sample bias from users participating in the research.</p><h3>Lengthy and iterative product cycle for taxonomy launches</h3><p>Developing and launching a taxonomy can be a lengthy and iterative process that requires several quarters of use to get substantive and reliable quantitative feedback. A typical process includes:</p><ul><li><strong>Taxonomy discovery and development</strong> based on product requirement or data-driven analysis</li><li><strong>Production changes</strong> to integrate backend environments and frontend surfaces, including necessary design and content updates</li><li><strong>ML model</strong> (re)label training data, retraining, and deployment</li><li><strong>Logging and data analysis on user feedback</strong></li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2xY0JS_w7ZTjcm4Q" /><figcaption>Figure 1. Typical taxonomy development iteration cycle.</figcaption></figure><p>Before T-LEAF, the taxonomy development process relied solely on output metrics to measure the effectiveness of a new taxonomy, which means that: 1) major changes take a long time to experiment and test; and 2) minor changes like adding or updating new nodes aren’t tested. These two pain points can be addressed with the T-LEAF framework by consistent and periodic scoring.</p><p>T-LEAF has been developed to include more quantitative evaluation in the taxonomy development and address the above mentioned two pain points to accelerate the taxonomy development iteration.</p><h3>Taxonomy Learning and EvaluAtion Framework (T-LEAF)</h3><h3>Quality of a Taxonomy</h3><p>T-LEAF framework measures the quality of a taxonomy in three aspects: 1) coverage, 2) usefulness and 3) agreement.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/866/1*Sab-WsEGFK1FXkDTM4AbEA.png" /><figcaption>Figure 2. T-LEAF Structure</figcaption></figure><h3>Coverage</h3><p>Coverage indicates how well a taxonomy can classify the scope of real-world objects. In Contact Reasons, coverage score evaluates how well the taxonomy captures the reasons guests and Hosts contact Airbnb’s Community Support team. When ‘coverage’ is low, a lot of user issues (data objects) will not be covered by the taxonomy and become ‘Other’ or ‘Unknown’.</p><blockquote>Coverage Score = 1 - percentage of data classified as “other” or “undefined.”</blockquote><h3>Usefulness</h3><p>Usefulness shows how evenly objects distribute across the structure of the taxonomy into meaningful categories. If a taxonomy is too coarse, i.e., has too few nodes or categories, the limited number of options may not adequately distinguish between the objects that are being described. On the other hand, if a taxonomy is too granular, it may fail to explain similarities between objects.</p><p>In T-LEAF, for a benchmark dataset with n examples (e.g., distinct user issues), we hypothesize that a taxonomy with sqrt(n) number of nodes² gives a good balance between ‘too coarse’ and ‘too granular’. For any input <em>x</em>, we compute a split score from (0,1] to evaluate the ‘usefulness’:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*z45U60-CPSN1R5jdN76BIQ.png" /></figure><p>We want to evaluate the data deviation by assuming the normal distribution. For example, with 100 distinct user issues, if we split into 1 (‘too coarse’) or 100 categories (‘too granular’), the usefulness score would be close to 0; if we split into 10 categories, the usefulness score would be 1.</p><h3>Agreement</h3><p>Agreement captures the inter-rater reliability given the taxonomy. We propose two ways to evaluate agreement.</p><h4>Human Label Inter-rater Agreement</h4><p>Multiple human annotators annotate the same data according to the taxonomy definition and we calculate the inter-rater reliabilities using Cohen’s Keppa in the range of [-1, 1]:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GacnudDKA1g-QVKLeGsiKw.png" /></figure><h4>ML Model Training Accuracy</h4><p>Having multiple human raters annotate one data set can be expensive. In reality, most data is annotated by just one human. In Airbnb’s Community Support, each customer issue/ticket is processed by one agent and agents label the ticket’s issue type based on the taxonomy. We train a ML model based on this single-rater labeled training data and then apply the model over the training data to measure the training accuracy. If the taxonomy is well defined (i.e., with high ‘agreement’), then similar issues (data points) should have similar labels even though these labels come from different agents. ML models trained over highly agreed(consistent) training dataset should have high training accuracy.</p><p>We have done experiments comparing the multi-label inter-rater agreement approach and ML training accuracy over single-rated training data.</p><p>Results are shown in Table 1. We observed that for both methods: 1) accuracies were similar for the top two levels of the taxonomy (L1 and L2 issues are defined in the next section) and; 2) there were similar areas of confusion in both approaches. If taxonomy nodes are clear enough for humans to perform tagging, the consistency rate increases and the model can better capture human intent. The opposite is also true; model training accuracy is negatively impacted if end users are confused by options or unable to choose proper categories.</p><p>It took 1 analyst and 9 annotators about a month to create the multi-rater dataset. In contrast, it took one ML engineer a day to train a ML model over the single-rated data and calculate the training accuracy. As shown in Table 1, ML Training accuracy provides a similar evaluation of taxonomy’s ‘agreement’ quality.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YgpIzpXbhVKc9-GwRrljSQ.png" /><figcaption>Table 1. Comparison between multi-rater labeling approach and ML-model over single-rater training data.</figcaption></figure><h3>Developing the Contact Reason Taxonomy using T-LEAF</h3><p>The Contact Reasons taxonomy consists of nearly 200 nodes, spread across a hierarchy that goes from broad categories in Level 1 (L1) to narrower categories in Level 2 (L2) to specific issues in Level 3 (L3). For example:</p><ul><li>Problems with your reservation (L1)</li><li>Cleanliness and health concerns (L2)</li><li>Smoke or other odors in listing (L3)</li></ul><p>While the old taxonomy had unpredictable levels of granularity, depending on the section, Contact Reasons has a consistent, three-level structure that better supports our continuous evaluation framework. We utilized T-LEAF in the transition from the old taxonomy to the new taxonomy (Contact Reasons) to enable a faster feedback loop and provide a quantified quality control before launching the new taxonomy into production environments (Figure 3).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jT-HinGVLNec_9sG" /><figcaption>Figure 3. Iterative process of taxonomy development, evaluation, and deployment with T-LEAF.</figcaption></figure><p>First, we sent a real-world dataset to Airbnb Community Support Labs (CS Labs) — a group of skilled and tenured Support Ambassadors — for human annotation. Then, we used T-LEAF scores as an input to the taxonomy development process. Using that input,the Core Machine Learning (CoreML) Engineering team and the Taxonomy team collaborated to significantly improve T-LEAF scores before running experiments in production.</p><p>To evaluate the Contact Reasons taxonomy in one of these production environments, we reviewed its performance in Airbnb bot³. Airbnb bot is one of Community Support’s core products that helps guests and Hosts self-solve issues and connect to Support Ambassadors when necessary. We found that the improvements to the Contact Reason taxonomy as measured by T-LEAF’s metrics of coverage, usefulness, and agreement also translated to actual improvements in issue coverage, self-solve effectiveness, and issue prediction accuracy.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JbLYJHtVd1E3AWCCtMPDnQ.png" /><figcaption>Table 2. T-LEAF scores between old and new taxonomies</figcaption></figure><h3>A higher T-LEAF coverage score leads to greater issue coverage in production</h3><p>After launching the Contact Reasons taxonomy, we examined 4-months of production data and found that 1.45% of issues were labeled “It’s something else,” which is 5.8% less than the old taxonomy. This is consistent with T-LEAF coverage score improvement (5.3% more coverage than the previous version).</p><h3>A higher usefulness score leads to more issues being resolved through self-service</h3><p>For example, in the new taxonomy, there are two new nodes called “<em>Cancellations and refunds &gt; Canceling a reservation you booked &gt; Helping a Host with a cancellation</em>” and “<em>Cancellations and refunds &gt; Canceling a reservation you’re hosting &gt; Helping a guest with a cancellation.</em>” The old taxonomy only have nodes for “<em>Reservations &gt; Cancellations &gt; Host-initiated</em>” and “<em>Reservations &gt; Cancellations &gt;Guest-initiated</em>”, which did not have granularity to determine when the guest or Host seeking support is not the one requesting the cancellation.</p><p>With the new nodes, we developed a machine learning model that drives traffic to tailored cancellation workflows⁴. This ensures that guests receive the appropriate refund and Host cancellation penalties are applied only when relevant, all without needing to contact Airbnb Support Ambassadors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*h9c4FN7fDUPhXet6R-B4ow.png" /><figcaption>Figure 4. Airbnb Chatbot self-solve solutions</figcaption></figure><h3>A higher T-LEAF agreement score results in more accurate issue prediction</h3><p>Compared to issue prediction models built on the old taxonomy, the model built on the new taxonomy has improved accuracy by<strong> 9%.</strong> This means that the category the ML model predicts for an issue is more likely to match the category selected by the Support Ambassador.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/722/0*2FMAyCXyP75We2ku" /><figcaption>Figure 5. User/Agent and ML Model Agreement</figcaption></figure><h3>Conclusion</h3><p>A quantitative framework to evaluate taxonomy supports faster iterations and reduces the risk of launching major taxonomy transformations, which has positive impacts for all of our audiences: guests, Hosts, Support Ambassadors, and Airbnb businesses. The T-LEAF framework that scores the quality of taxonomy in the aspects of coverage, usefulness, agreement, has now been applied to a production taxonomy in Community Support and results show that using this methodology for quantitative taxonomy evaluation can lead to better model performance and larger issue coverage.</p><p>Developing, piloting, and establishing T-LEAF as part of our continuous improvement framework for taxonomy evolution has been a collaborative effort across teams. The CoreML team partnered closely with Taxonomy, Product, and CS Labs to create this new model for iterative development of issue categorization and prediction. Having piloted this new way of working on Contact Reasons, we’re confident we’ll see more positive results as we continue to apply the T-LEAF methodology to future taxonomy initiatives</p><p>[1]: Szopinski, D., Schoormann, T., &amp; Kundisch, D. (2019). Because Your Taxonomy is Worth IT: towards a Framework for Taxonomy Evaluation. <em>ECIS</em>. <a href="https://aisel.aisnet.org/ecis2019_rp/104/">https://aisel.aisnet.org/ecis2019_rp/104/</a></p><p>[2]: Carlis, J., &amp; Bruso, K. (2012). RSQRT: AN HEURISTIC FOR ESTIMATING THE NUMBER OF CLUSTERS TO REPORT. Electronic commerce research and applications, 11(2), 152–158. <a href="https://doi.org/10.1016/j.elerap.2011.12.006">https://doi.org/10.1016/j.elerap.2011.12.006</a></p><p>[3]: Intelligent Automation Platform: Empowering Conversational AI and Beyond at Airbnb. <a href="https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2">https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2</a></p><p>[4]: Task-Oriented Conversational AI in Airbnb Customer Support. <a href="https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa">https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa</a></p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/3919138/">Senior Staff Data Architect, Community Support Platform</a></p><p><a href="https://careers.airbnb.com/positions/4044524/">Sr. Product Manager, Community Experience Products</a></p><p><a href="https://careers.airbnb.com/positions/3887689/">Product Manager, Claims Experience</a></p><h3>Acknowledgments</h3><p>Thanks to CS Labs for labeling support on existing and new taxonomies!</p><p>Thanks to Pratik Shah, Rachel Lang, Dexter Dilla, Shuo Zhang, Zhiheng Xu, Alex Zhou, Wayne Zhang, Zhenyu Zhao, Jerry Hong, Gavin Li, Kristen Jaber, Aliza Hochsztein, Naixin Zhang, Gina Groom, Robin Foyle, Parag Hardas, Zhiying Gu, Kevin Jungmeisteris, Jonathan Li-On Wing, Danielle Martin, Bill Selman, Hwanghah Jeong, Stanley Wong, Lindsey Oben, Chris Enzaldo, Jijo George, Ravish Gadhwal, and Ben Ma for supporting our successful CS taxonomy launch and workflow related applications!</p><p>Thank Joy Zhang, Andy Yasutake, Jerry Hong, Lianghao Li, Susan Stevens, Evelyn Shen, Axelle Vivien, Lauren Mackevich, Cynthia Garda, for reviewing, editing and making great suggestions to the blog post!</p><p>Last but not least, we appreciate Joy Zhang, Andy Yasutake, Raj Rajagopal, Tina Su and Cynthia Garda for leadership support!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30ae19ce8c52" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52">T-LEAF: Taxonomy Learning and EvaluAtion Framework</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Airbnb’s Trip to Linaria]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/dc169230bd12</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[web-performance]]></category>
            <category><![CDATA[atomic-css]]></category>
            <category><![CDATA[css-in-js]]></category>
            <category><![CDATA[css]]></category>
            <dc:creator><![CDATA[Joe Lencioni]]></dc:creator>
            <pubDate>Thu, 16 Jun 2022 17:41:49 GMT</pubDate>
            <atom:updated>2022-06-16T17:41:48.753Z</atom:updated>
            <content:encoded><![CDATA[<h4>Learn how Linaria, Airbnb’s newest choice for web styling, improved both developer experience and web performance</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-qT4pQIPIsxHBZj22sQtag.jpeg" /></figure><p>CSS is a critical component of every web application, and many solutions have evolved for how styles are written by developers and delivered to visitors. In this post we’ll take you through Airbnb’s journey from Sass to CSS-in-JS and show you why we landed on <a href="https://github.com/callstack/linaria">Linaria, a zero-runtime CSS-in-JS library</a>, and the impact it has had on the developer experience and performance of Airbnb’s web app.</p><h3>From Sass to CSS-in-JS</h3><p>In 2016, our web frontend was in a monolithic <a href="https://rubyonrails.org/">Ruby on Rails</a> app using a combination of <a href="https://github.com/rails/sprockets">Sprockets</a>, <a href="https://browserify.org/">Browserify</a>, and <a href="https://sass-lang.com/">Sass</a>. We had a <a href="https://getbootstrap.com/">Bootstrap</a>-inspired internal toolkit for styling, but we weren’t using anything like <a href="https://github.com/css-modules/css-modules">CSS Modules</a> or <a href="http://getbem.com/">BEM</a>.</p><p>Production bugs were often caused by our styling — sometimes the correct stylesheet was missing from some pages and other times styles from different stylesheets conflicted unexpectedly.</p><style>body[data-twttr-rendered="true"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true"><p>&#x200a;&mdash;&#x200a;<a href="https://twitter.com/thomasfuchs/status/493790680397803521">@thomasfuchs</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height);resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute("width")); if ( 500  < maxWidth) {window.frameElement.setAttribute("width", "500");}}</script><p>Additionally, developers <a href="https://css-tricks.com/how-do-you-remove-unused-css-from-a-site/">rarely removed styles once added since it was hard to know whether they were still needed</a>. These issues compounded as our product surface area rapidly expanded.</p><p>As we began to build our <a href="https://www.youtube.com/watch?v=fHQ1WSx41CA">Design System</a> in React, we landed on CSS-in-JS as an exciting new option. At the time, CSS-in-JS was still in its infancy–only a few libraries existed and <a href="https://styled-components.com/">Styled Components</a> had not been invented yet. We chose <a href="https://github.com/khan/aphrodite">Aphrodite</a>, but didn’t want to be directly coupled to Aphrodite’s implementation for two reasons: since CSS-in-JS was a nascent space we wanted to have the flexibility to switch implementations at a later date, and we also wanted something that would work for open source projects where people might not want Aphrodite. So we created an abstraction layer called <a href="https://github.com/airbnb/react-with-styles">react-with-styles</a>, which gave us a <a href="https://reactjs.org/docs/higher-order-components.html">higher-order component (HOC)</a> to define themeable styles.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href">https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href</a></iframe><p>This allowed components to be styled in the same file, making repo organization more convenient. More importantly, <strong>moving from a globally-aware styling system to a component-based styling system gave us guarantees around how styles would be applied and what files were needed to render every component correctly on every page</strong>. This enabled us to rely on <a href="https://happo.io/">Happo, our screenshot testing tool of choice</a>, and as a result visual regressions plummeted (disclosure: I am the co-creator of Happo).</p><p>Though react-with-styles has served us well for years, it comes with <strong>performance</strong> and <strong>developer experience</strong> tradeoffs. The styles and runtime libraries increase critical path JS bundle size, and applying styles at render-time comes with a CPU cost (10–20% of our component mount times). While we get the aforementioned guarantees about styles, actually writing styles in JavaScript objects feels awkward compared to regular CSS syntax. These tradeoffs led us to reconsider how we style the web at Airbnb.</p><h3>Considering Our Options</h3><p>To address the problems with react-with-styles, we formed a working group of engineers from various teams. We considered a number of directions, which fit into the following high-level categories:</p><ul><li>Static extraction of CSS from react-with-styles at build time</li><li>Write our own framework</li><li>Investigate and adopt an existing framework</li></ul><p>We decided against <strong>static extraction</strong> from react-with-styles at build time because it would require a lot of effort. Additionally, it would be home-grown and therefore lack benefits of a community. Finally, it does not address developer ergonomics issues.</p><p>Similarly, <strong>writing our own framework</strong> would have had a high cost of initial implementation, maintenance, and support. Additionally, there were existing solutions for this problem that we wanted to leverage and contribute back to.</p><figure><img alt="An xkcd comic titled “How Standards Proliferate: (see: A/C chargers, character encodings, instant messaging, etc). Panel 1: Situation: There are 14 competing standards. Panel 2: “14?! Ridiculous! We need to develop one universal standard that covers everyone’s use cases.” “Yeah!” Panel 3: Soon: Situation: There are 15 competing standards." src="https://cdn-images-1.medium.com/max/500/0*7Kwk-MuLZOIUhgnv" /><figcaption>Comic from <a href="https://xkcd.com/927/">https://xkcd.com/927/</a> by Randall Munroe and is used under a CC-BY-NC 2.5 license.</figcaption></figure><p>After evaluating several <strong>existing frameworks</strong> against our requirements, we narrowed down candidates for building a proof of concept:</p><ul><li><a href="https://emotion.sh/docs/introduction">Emotion</a>: CSS-in-JS, with a low runtime cost</li><li><a href="https://github.com/callstack/linaria">Linaria</a>: zero-runtime CSS-in-JS (static CSS extraction)</li><li><a href="https://github.com/seek-oss/treat">Treat</a>: near zero-runtime CSS-in-JS (static CSS extraction)</li></ul><p>The proof-of-concepting work was done in a new repo that implemented a server-rendered client-hydrated unstyled version of Airbnb’s logged in homepage. For each framework, this allowed us to:</p><ul><li>Understand what changes might need to be made to our build system</li><li>Try out framework APIs and get a feel for developer ergonomics</li><li>Assess how each framework supports our web styling requirements</li><li>Gather performance metrics</li><li>Serve as a starting point for a migration plan</li></ul><p>Frameworks were evaluated against each other based on the following ranked list of criteria:</p><ol><li><strong>Performance</strong></li><li><strong>Community</strong> (i.e. support and adoption)</li><li><strong>Developer experience</strong></li></ol><h3>Performance Analysis</h3><p>Using <a href="https://www.speedcurve.com/">SpeedCurve</a>, local benchmarking, and the <a href="https://reactjs.org/docs/profiler.html">React &lt;Profiler /&gt;</a>, we ran performance benchmarking tests for each framework. All results were calculated as the median of 200 runs on a throttled MacBook Pro, and are statistically significantly different from control with a p-value of &lt;= 0.05.</p><p>Informed by <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb’s Page Performance Score</a> (similar to <a href="https://web.dev/performance-scoring/">Lighthouse’s performance score</a>), we focused on the following metrics to give us an idea of how each framework performed and would impact the user experience:</p><ul><li><a href="https://web.dev/tbt/">Total blocking time (TBT)</a></li><li>Bundle size</li><li>Update layout tree count and duration</li><li>Composite layers count and duration</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href">https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href</a></iframe><p>It is clear that the frameworks are divided into two groups: <strong>runtime frameworks</strong> (react-with-styles, Emotion) and <strong>build-time frameworks</strong> (Linaria, Treat).</p><p>Benchmarks of the server-rendered and client-hydrated version of our homepage showed Treat and Linaria performing 36% and 22% better than Emotion on <strong>Total Blocking Time</strong>, respectively. All frameworks performed significantly better than react-with-styles, ranging from a 32–56% improvement. <em>(Note that these numbers should not be used to estimate expected improvements in production, as this is a very specific benchmark designed to test differences between frameworks, not expected savings in production.)</em></p><p><strong>Bundle size</strong> differences also fall into these two categories — with savings on the order of 80 KiB (~12%) for the Linaria/Treat group.</p><p>The CSS metrics (<strong>update layout tree</strong> and <strong>composite layers</strong>) show that, on average, there is roughly one more layout tree update and layer composition event for react-with-styles/Emotion. This is likely due to the insertion and hydration of stylesheets with JavaScript that is not necessary with a CSS extraction library like Linaria or Treat.</p><p>This performance investigation shows that either Linaria or Treat would be promising options to adopt, and that all frameworks considered are a statistically significant improvement over react-with-styles with Aphrodite.</p><h3>What We Liked About Linaria</h3><p>The above <strong>performance</strong> improvements were largely thanks to Linaria extracting the styles from JS to static CSS files at build time, so there is no JS bundle or runtime CPU overhead — giving it a slight edge over the near-zero runtime Treat. Also, this brings caching benefits since these static CSS files may change at a different cadence than the JS files. Since the styles are extracted at build time, Linaria has the opportunity to automatically remove unused styles — this also opens the door to the possibility of deduplicating styles (i.e. <a href="https://css-tricks.com/lets-define-exactly-atomic-css/">Atomic CSS</a>). Additionally, Linaria supports injecting the critical CSS for server-side rendering, which we had wanted to preserve from our react-with-styles integration.</p><p><a href="https://snyk.io/advisor/npm-package/linaria">Linaria also seemed to be a healthy project</a> that saw a good amount of activity, <strong>community</strong> involvement, documentation, and adoption. Its good trajectory gave us confidence that it would continue to improve and that we would be able to contribute back.</p><p>We found Linaria’s <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> API that enables developers to use CSS syntax to be an attractive improvement over the JS object HOC API that we built for react-with-styles. Additionally, off-the-shelf integrations were available for stylelint, CSS autocompletion, and syntax highlighting, which enriched the <strong>developer experience</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hyXFKX-bB-ixvHsE" /><figcaption>Off-the-shelf integrations for stylelint, CSS autocompletion, and syntax highlighting working with Linaria in action.</figcaption></figure><p>We also found value in the similarities between Linaria and our existing solution. The co-location of styles within the component file was a big feature that tipped the scales in favor of Linaria over Treat for us, and the familiar API smoothed the transition for developers and gave us confidence that migration efforts could be eased with automation.</p><h3>Migration Strategy</h3><p>To roll out this big change, we adopted an incremental migration strategy that is largely automated by <a href="https://medium.com/airbnb-engineering/turbocharged-javascript-refactoring-with-codemods-b0cae8b326b9">codemods</a> we’ve written. We are leaning heavily on our <a href="https://happo.io/">Happo screenshot tests</a> to ensure that our components look the same after they are migrated. This allows sections of our codebase to be migrated by running a script and following up with any necessary tweaks, similar to <a href="https://medium.com/airbnb-engineering/ts-migrate-a-tool-for-migrating-to-typescript-at-scale-cd23bfeb5cc">the approach we took when adopting TypeScript</a>.</p><p>The first phase of the migration was handled by the web styling working group and targeted converting a subset of components on a few select pages with varying performance characteristics. This phase was gated on A/B tests which ensured that our initial understanding of the performance held up under the specifics of our app and assured us that there were no hidden problems.</p><p>Once we were confident about the performance and correctness of our Linaria integration, we allowed teams to start using Linaria in new code. We also encouraged teams to migrate their existing code using our codemods. Although the migration has proceeded at a good pace organically, we plan to ensure that all code has moved off of react-with-styles so that we can eventually remove the runtime dependencies from the bundles entirely. This consistency will give us an additional performance boost and reduce the cost of <a href="https://en.wikipedia.org/wiki/Decision_fatigue">decision fatigue</a>.</p><h3>Contributing Back</h3><p>Once we started using Linaria, we discovered that automatic style deduplication (i.e. Atomic CSS) would give us not just a performance boost, but also would fix some non-performance-related hiccups we ran into.</p><p>The selectors that Linaria generates are all of the same <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Specificity">specificity</a>. Since CSS selectors of the same specificity depend on their declaration order, the order that the bundler builds these files becomes important. This is problematic when sharing styles between files, since we cannot predict or maintain the order of the styles as the shape of the dependency graph changes.</p><p>We initially approached this problem by creating a new <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> for CSS fragments which allows for the styles to be interpolated into Linaria’s CSS tagged template literals. This works okay, but it is unintuitive, defeats <a href="https://github.com/prettier/prettier/blob/d13feed42b6478710bebbcd3225ab6f203a914c1/src/language-js/embed.js#L90-L121">tooling that expects styles to be defined in CSS tagged template literals</a>, and leads to the styles being included several times in the CSS bundles (which is suboptimal for performance).</p><p>Josh Nelson, a member of our web styling working group, <a href="https://github.com/callstack/linaria/pull/867">contributed Atomic CSS support back to Linaria</a> and the Linaria community has been very supportive. The change adds a new <a href="https://npmjs.com/@linaria/atomic">@linaria/atomic</a> package that when imported instead of <a href="https://www.npmjs.com/package/@linaria/core">@linaria/core</a> will generate Atomic CSS at build time. This means that if you write your code like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href">https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href</a></iframe><p>Instead of generating output like this (without Atomic CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href">https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href</a></iframe><p>The generated output will look something like this (with Atomic CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href">https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href</a></iframe><p>The order of appearance problem is solved by build time analysis that chains class names based on the order they are passed in to the cx function to increase specificity when necessary.</p><h3>Reception</h3><p>Our engineers have reacted positively to Linaria. Here are some quotes:</p><blockquote>“Linaria opens up a world where we can code like it’s 1999, in old school pure on CSS. It advises against bad patterns, but gives us the flexibility to build amazing experiences. We’re not fighting the platform anymore, we’re harnessing it and it feels incredibly powerful.” — Callie Riggins</blockquote><blockquote>“Compared to react-with-styles, I care more about what I’m creating now. Linaria is so good.” — Ian Demattei-Selby</blockquote><blockquote>“I really liked being able to write CSS again. It gives you so much more control over what you can style in the component.” — Brie Bunge</blockquote><blockquote>“It’s great to be writing actual CSS again.” — Victor Lin</blockquote><p>Thanks to its familiar CSS syntax, style extraction into static stylesheets, and application of styles using class names, Linaria <strong>increases product development speed</strong> and <strong>unlocks new styling capabilities not possible with react-with-styles and Aphrodite</strong>.</p><h3>Performance Impact</h3><p>Though we are still at the beginning of our migration, we have run some A/B tests that give us an encouraging look at the real world performance impact of switching to Linaria for a large group of visitors in the wild.</p><p>In one experiment, we converted about 10% of the components rendered on the airbnb.com homepage from react-with-styles to Linaria, and saw Homepage <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Page Performance Score</a> improve by 0.26%. <a href="https://web.dev/fcp/">Time to First Contentful Paint (TTFCP)</a> improved by 0.54% (mean of 790ms), while <a href="https://web.dev/tbt/">Total Blocking Time (TBT)</a> also had a strong improvement of 1.6% (mean of 1200ms). To put this in perspective, hydrating the homepage with React takes around 200ms for most people, so improvements of this order of magnitude are significant. We believe these performance improvements with Linaria are attributable to no longer generating CSS styles at render-time, which improves render times on both server and client.</p><p>Assuming the performance improvements will scale linearly (which is a big assumption), converting the remaining 90% of the components <em>might</em> result in a 2.6% improvement to Page Performance Score, 5.4% improvement to Time to First Contentful Paint (TTFCP), and 16% improvement to Total Blocking Time (TBT).</p><p>Note that direct comparisons with other industry numbers are a little tricky here, given the different ways we define pages especially with regard to client routing.</p><h3>What Does This Mean for react-with-styles?</h3><p>Given that we still have many components that still depend on react-with-styles and that it will take a while for us to complete our migration, <strong>we will put react-with-styles in maintenance mode</strong> until we approach the end of our migration. At that point, <strong>we intend to sunset react-with-styles</strong> and the related packages.</p><p>By removing an option from the marketplace we hope to help the community coalesce towards a common solution and invest in better frameworks. If you are looking for a new tool, we think Linaria is a great choice!</p><h3>Conclusion</h3><p>Styling infrastructure is still an exciting space, rich with opportunities. At Airbnb, we’ve found big improvements to the <strong>developer experience</strong> by adopting a framework that allows regular CSS syntax to be used alongside our React component code. And by replacing a runtime styling library with one that compiles to static CSS files at build time, we are able to continue driving toward faster <strong>performance</strong>. Thanks to the Linaria <strong>community</strong> and our collaboration, we expect this library to continue to improve for many years.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://grnh.se/ebfa55151us">Frontend Infrastructure Engineer, Web Platform</a><br><a href="https://grnh.se/b5afa9151us">Staff Software Engineer, Data Governance </a><br><a href="https://grnh.se/92c32fed1us">Staff Software Engineer, Cloud Infrastructure </a><br><a href="https://grnh.se/bbe55fe81us">Staff Database Engineer </a><br><a href="https://grnh.se/21e5c2011us">Staff Software Engineer — ML Ops Platform </a><br><a href="https://grnh.se/ee114dfc1us">Senior/Staff Software Engineer, Service Capabilities</a></p><h3>Acknowledgments</h3><p>We have a lot of appreciation for the folks at <a href="https://www.callstack.com/">callstack</a> and the <a href="https://github.com/callstack/linaria#contributors">Linaria community</a> for building such a great tool and for collaborating with us to make it even better. Also for <a href="https://www.khanacademy.org/">Khan Academy</a> for giving us Aphrodite which served us well for many years. This has been a huge effort at Airbnb that would not have been possible without all the work put in by so many people at Airbnb, including Mars Jullian, Josh Nelson, Nora Tarano, Alan Wright, Jimmy Guo, Ian Demattei-Selby, Victor Lin, Nnenna John, Adrianne Soike, Garrett Berg, Andrew Huth, Austin Wood, Chris Sorenson, and Miles Johnson. Finally, thank you to Surashree Kulkarni for help editing this blog post. Thank you all!</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dc169230bd12" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12">Airbnb’s Trip to Linaria</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Graph Machine Learning at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f868d65f36ee</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Devin Soni]]></dc:creator>
            <pubDate>Tue, 14 Jun 2022 15:59:22 GMT</pubDate>
            <atom:updated>2022-06-14T15:59:22.004Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>How Airbnb is leveraging graph neural networks to up-level our machine learning</strong></h4><p>By:<a href="https://www.linkedin.com/in/devinsoni/"> Devin Soni</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bEZU2cupMt44ke6mkK-low.jpeg" /></figure><h3>Introduction</h3><p>Many real-world machine learning problems can be framed as graph problems. On online platforms, users often share assets (e.g. photos) and interact with each other (e.g. messages, bookings, reviews). These connections between users naturally form edges that can be used to create a graph.</p><p>However, in many cases, machine learning practitioners do not leverage these connections when building machine learning models, and instead treat nodes (in this case, users) as completely independent entities. While this does simplify things, leaving out information around a node’s connections may reduce model performance by ignoring where this node is in the context of the overall graph.</p><p>In this blog post, we will explain the benefits of using graphs for machine learning, and show how leveraging graph information allows us to learn more about our users, in addition to building more contextual representations of them [4]. We will then cover specific graph machine learning methods, such as Graph Convolutional Networks, that are being used at Airbnb to improve upon existing machine learning models.</p><p>The motivating use-case for this work is to build machine learning models that protect our community from harm, but many of the points being made and systems being built are quite generic and could be applied to other tasks as well.</p><h3>Challenges</h3><h4><strong>The problem</strong></h4><p>When building trust &amp; safety machine learning models around entities such as users or listings, we generally begin by reaching for features that directly describe the entity. For example, in the case of users, we may use features such as their location, account age, or number of bookings. However, these simple features do not adequately describe the user in the context of the overall Airbnb platform and their interactions with other users.</p><p>Consider the hypothetical scenario in which a new host joins Airbnb. A week into their hosting journey, we likely do not have a lot of information about them other than what they have directly told us. This could include their listing’s location or their phone number. These direct attributes given to us by the host are relatively surface level and do not necessarily help us understand their trustworthiness or reputation.</p><p>In this state, it is hard for Airbnb to provide this new host with the best possible experience since we do not know what their usage pattern of the platform will be. Lacking information, we might then make this new host go through a slower onboarding process or request a lot of information up-front. Understanding this user’s relationships to the rest of the platform is data we can leverage to provide them with an improved experience.</p><h4><strong>An illustration of the usefulness of graphs</strong></h4><p>While we do not have much direct information about the new host, what we can do is leverage their surroundings to try and learn more. One example of this is the connections that they have to other users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/1*BAJt6yhBPetG4HwwUylGiQ.png" /></figure><p>We can first take a look at their one-hop neighborhood, or in other words, the set of users whom this host has a direct connection with. In this example, we can see that this new host shares a listing photo with an existing, tenured host. We can also see that the new host’s listing is in the same house as listings from three other hosts. With this additional knowledge, we now know more about the new host; they might be working with other hosts who have rooms in the same house. However, we can’t be completely sure how all of the connected hosts relate to each other without looking at more of the graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/971/1*9u8zd4_qXAA46RGCUWa0kw.png" /></figure><p>Let’s further expand our view and consider the two-hop neighborhood of the new host. This expands our view to users who are not necessarily directly connected to the new host. In this expanded network we now see that many of the hosts with listings in the location are connected to each other through a shared business name. It now becomes very likely that this new host is part of an existing group of hosts that rent out rooms in the same house, and has not yet updated their profile to reflect this.</p><p>Using the power of graphs, we were able to learn about a new host simply by inspecting their connections to other users within Airbnb. This additional knowledge extracted from the graph provides us with an improved glimpse into who our new host is. We are subsequently able to deliver an improved experience to this new host, all without requiring them to provide any more information to Airbnb.</p><p>Supplementing our models with graph information is one way to bootstrap our models. Using graphs, we can construct a detailed understanding of our users in scenarios where we have little historical data or observations pertaining to a user. While the semantic information we gain from the graph is often inferred and not directly told to us by the user, it can give us a strong baseline level of knowledge until we have more factual information about a user.</p><h3><strong>Graph Machine Learning</strong></h3><p>We have established that we want our machine learning models to be able to ingest graph information. The main challenge is figuring out how best to condense everything a graph can represent into a format that our models can use. Let’s dig into some of the options and explore the solution that we ultimately implemented.</p><p>One simple option is to calculate statistics about nodes and use them as numeric features. For example, we can calculate the number of users a user is connected to or how many listing photos they share with other hosts. These metrics are straightforward to calculate and give us a basic sense of the node’s role in the overall structure of the graph. These metrics are valuable but do not leverage the node’s features. As such, simple statistics cannot go beyond representing graph structure.</p><p>What we really want is to be able to produce an aggregation of a node’s neighborhood in the graph that captures both the node’s structural role in the graph and its node features. For example, we want to know more than how many users a user is connected to; we also want to understand the type of users they are connected to (e.g. their account tenure, or past booking counts) because that gives us more hints about the original user than simple edge counts.</p><h4><strong>Graph Convolutional Networks</strong></h4><p>To capture both graph structure and node features, we can use a type of graph neural network architecture called a graph convolutional network. Graph convolutional networks (GCN) are neural networks that generally take as input a matrix of node features in addition to an adjacency matrix of the graph and outputs a node-level output. This type of network architecture is preferred over simply concatenating pre-computed structural features with node features because it is able to jointly represent the two types of information, likely producing richer embeddings.</p><p>Graph convolutional networks consist of multiple layers. A single GCN layer aims to learn a representation of the node that aggregates information from its neighborhood (and in most cases, combines its neighborhood information with its own features). Using the example of our newly created host account, one GCN layer would be the host’s one-hop neighborhood. A second GCN layer representing the host’s two-hop neighborhood can be introduced to capture additional information. Since the output of GCN layer N is used to produce the representations used in GCN layer N+1, adding layers increases the span of the aggregation used to generate node representations [1].</p><p>Drawing from the example in the previous section, we would need a GCN with two layers in order to produce a graph embedding that captures the illustrated subgraph. We could go even deeper and expand further to third-order, fourth-order, and so on. In practice, however, a small number of layers (e.g. 2–4) is sufficient, as the connections beyond that point are likely to be very noisy and unlikely to be relevant to the original user.</p><h4><strong>Model architecture &amp; training</strong></h4><p>Having decided to use GCNs, we must now consider how complex we want each layers’ method of aggregating neighboring nodes’ features to be. There are a wide variety of aggregation methods which can be used. These include mean pooling, sum pooling, as well as more complex aggregators involving attention mechanisms [5].</p><p>When it comes to trust &amp; safety, we often work in adversarial problem domains where frequent model retraining is required due to concept drift. Limiting model complexity, and limiting the number of models that must be retrained is important for reducing maintenance complexity.</p><p>One might assume that GCNs with more complex, expressive aggregation functions are always better. This is not necessarily the case. In fact, several papers have shown that in many cases very simple graph convolutional networks are all that is needed for state-of-the-art performance in practical tasks [2, 3]. The Simplified GCN (SGC) architecture showed that we can achieve performance comparable to more complex aggregators using GCN layers that do not have trainable weights [2]. The Scalable Graph Inception Network (SIGN) architecture showed that, in general, we can precompute multiple aggregations without trainable weights, and use them in parallel as inputs into a downstream model [3]. SIGN and SGC are very related; SIGN provides a general framework for precomputing graph aggregations, and SGC provides the most straightforward aggregator to use within the SIGN framework.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5h9Tp1c8MkyEJAE5ZR2hBA.png" /></figure><p>Using SIGN and SGC, the GCN is purely a fixed feature extractor that does not need to learn anything itself — it has no weights that must be tuned during training. In this setting we are able to fundamentally treat the GCN as a fixed mathematical formula applied to its inputs. This aspect is very convenient because we do not need to worry about supervised training or pre-training of the GCN itself.</p><h4><strong>Model serving</strong></h4><p>When serving a graph neural network, the main considerations are around freshness of the data and how to obtain the inputs for the model in a production setting. Our primary concern is the trade-offs related to data freshness. The decision between real-time or batch methods has an impact on how up to date the information is.</p><p>Real-time methods can provide downstream models with the most up-to-date information. This increased freshness does, however, require more effort to serve the embeddings. In addition, it often relies on a downsampled version of the graph to handle nodes with many edges, such as in the GraphSAGE algorithm [4].</p><p>Offline batch methods are able to calculate all node embeddings at once. This provides a distinct advantage over real-time methods by reducing implementation complexity. Unfortunately, the tradeoff does come at a cost. We will not necessarily be able to serve the most recent node embedding as we will only be able to leverage information present in the last run of the pipeline.</p><h3><strong>Chosen solution</strong></h3><p>Given all the tradeoffs and our requirements, we ultimately decided to use a periodic offline pipeline which leverages the SIGN method for our initial implementation. The easy maintenance of batch pipelines and relative simplicity of SIGN allows us to optimize for learning instead of performance initially.</p><p>Despite the fact that many of our trust &amp; safety models are run online in real-time, we decided to start with an offline graph model. Features are computed using a snapshot of the graph and node features. When fetching these features online, the downstream model simply looks up the previous run’s output from our feature store, rather than having to compute the embedding in real-time. The alternative of a real-time graph embedding solution would involve a significant amount of additional implementation complexity.</p><h3>Benefits Realized</h3><p>With the batch pipeline implemented, we can now have access to new information as features in downstream models. Our existing feature sets did not capture this information and it has resulted in significant gains in our models. Components of the embedding are often among the top 10 features in downstream models based on feature importance computed using <a href="https://github.com/slundberg/shap">the SHAP approach</a>.</p><p>These positive results encourage further investment in the area of graph embeddings and graph signals, and we plan to explore other types of graphs &amp; graph edges. Investigating how to make our embedding more powerful either through improving the freshness of the data or using other algorithms has become a priority for us based on the success of augmenting our existing models with graph knowledge.</p><h3>Conclusion</h3><p>In this blog post, we showed how leveraging graph information can be broadly useful and discussed our approach to implementing graph machine learning. We ultimately decided to use a SIGN architecture that leverages a batch pipeline to calculate graph embeddings. These are subsequently fed into downstream models as features. Many of the new features have led to notable performance gains in the downstream models.</p><p>We hope that this information helps others understand how to leverage graph information to improve their models. Our various considerations provide insight into what one must be aware of when deciding to implement a graph machine learning system.</p><p>Graph machine learning is an exciting area of research in Airbnb, and this is only the beginning. If this type of work interests you, check out some of our related positions:</p><p><a href="https://careers.airbnb.com/positions/3910069/">Senior Machine Learning Engineer</a></p><p><a href="https://careers.airbnb.com/positions/4113532/">Senior Software Engineer, Trust</a></p><h3>Acknowledgments</h3><p>This project couldn’t have been done without the great work of many people and teams. We would like to thank:</p><ul><li>Owen Sconzo for working on this project and reviewing all of the code.</li><li>The Trust Foundational Modeling team for providing the foundational data for graph modeling.</li><li>Members of the Fraud &amp; Abuse Working Group for supporting this project, reviewing this blog post, and providing suggestions.</li></ul><h3>References</h3><p>[1] <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks.</a></p><p>[2] <a href="https://arxiv.org/abs/1902.07153">Simplifying Graph Convolutional Networks</a></p><p>[3] <a href="https://arxiv.org/abs/2004.11198">SIGN: Scalable Inception Graph Neural Networks</a></p><p>[4] <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a></p><p>[5] <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></p><p>****************</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f868d65f36ee" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee">Graph Machine Learning at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unified Payments Data Read at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e613e7af1a39</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[payments]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Alican GÖKSEL]]></dc:creator>
            <pubDate>Thu, 09 Jun 2022 22:23:30 GMT</pubDate>
            <atom:updated>2022-06-10T21:47:52.619Z</atom:updated>
            <content:encoded><![CDATA[<p>How we redesigned payments data read flow to optimize client integrations, while achieving up to 150x performance gains.</p><p>By: <a href="https://www.linkedin.com/in/ali-can-g%C3%B6ksel-7189214a">Ali Goksel,</a> <a href="https://www.linkedin.com/in/linglongzhu">Linglong Zhu</a>, <a href="https://www.linkedin.com/in/yixiamao">Yixia Mao</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fWhQRxAT0vMQwTEW" /></figure><h3>Introduction</h3><p>In recent years, Airbnb migrated most of its backend services from a monolith to a service-oriented architecture (SOA). This industry standard architecture brings countless benefits to a company that is at the scale of Airbnb; however, it is not free of challenges. With data scattered across many services, it’s difficult to provide all the information clients need in a simple and performant way, especially for complex domains such as payments. As Airbnb grew, this problem started to crop up for many new initiatives such as host earnings, tax form generation, and payout notifications, all of which required data to be read from the payments system.</p><p>In this blog post, we introduce Airbnb’s unified payments data read layer. This read layer was custom built to reduce the friction and complexity for client integrations, while greatly improving query performance and reliability. With this re-architecture, we were able to provide a greatly optimized experience to our host and guest communities, as well as for internal teams in the trust, compliance, and customer support domains.</p><h3>Evolution of Airbnb’s Payments Platform</h3><p>Payments is one of the earliest functionalities of the Airbnb app. Since our co-founder Nate’s first commit, Payments Platform has grown and evolved tremendously, and it continues to evolve at an even faster pace given our expanding global presence.</p><p>Similar to other companies, Airbnb started its journey with a monolithic application architecture. Since the feature set was initially limited, both write and read payment flows were “relatively” simple.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*dsBzT9rDVCNoG8MM" /><figcaption>Overly simplified diagram of Airbnb’s old monolithic architecture. Payments schemas were not very complex, and the feature set was limited.</figcaption></figure><p>Predictably, this architecture couldn’t scale well with the rapid growth and expansion of our company. Payments, along with most other parts of the tech stack, started to migrate to the SOA architecture. This brought a significant overhaul of the existing architecture and provided many advantages, including:</p><ul><li>We had clear boundaries between different services, which enabled better domain ownership and faster iterations.</li><li>Data was separated into domains in a very normalized shape, resulting in better correctness and consistency.</li></ul><p>For more, take a peek at our <a href="https://medium.com/airbnb-engineering/rebuilding-payment-orchestration-at-airbnb-341d194a781b">blog post</a> detailing the payments SOA migration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6lC8Bu1u3DA0WAzV" /><figcaption>After the SOA migration, every payments subdomain has its own service(s) and tables with clear boundaries, but more features leads to more complex and normalized data.</figcaption></figure><h3>New Architecture Introduces New Challenges</h3><p>Payments SOA provided us with a more resilient, scalable, and maintainable payments system. During this long and complex migration, correctness of the system was our top priority. Data was normalized and scattered across many payments domains according to each team’s responsibilities. This subdivision of labor had an important side effect: presentation layers now often needed to integrate with multiple payments services to fetch all the required data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*itWZQ8kaxNjoZ-7f" /><figcaption>How payments data read flows looked after the SOA migration. Presentation services called one or more payments services and aggregated data at the application layer.</figcaption></figure><p>At Airbnb, we believe in being transparent with our host and guest communities. Our surfaces related to payments and earnings display a range of details including fees, transaction dates, currencies, amounts, and total earnings. After the SOA migration, we needed to look into multiple services and read from even more tables than prior to the migration to get all the requested information. Naturally, this foundation brought challenges when we wanted to add new surfaces with payments data, or when we wanted to extend the existing surfaces to provide additional details. There were three main challenges that we needed to solve.</p><p>The first challenge was that <em>clients now needed to understand the payments domain well enough</em><strong> </strong>to pick the correct services and APIs. For client engineers from other teams, this required a non-trivial amount of time investment and slowed down overall time to market. On the payments side, engineers needed to provide continuous consultation and guidance, occupying a significant portion of their work time.</p><p>The second challenge was that there were many instances in which we had to change multiple payments APIs at the same time in order to meet the client requirements. When there are<em> too many touchpoints</em>, it becomes <em>hard to prioritize requests</em> since many teams have to be involved. This problem also caused significant negative impact to time to market. We had to slow down or push back feature releases when the alignment and prioritization meetings did not go smoothly. Similarly, when payments teams had to update their APIs, they had to make sure that all presentation services adopted these changes, which slowed down progress on the payments system.</p><p>Last but not least, the technical quality of the complex read flows was not where we wanted it to be. Application-level aggregations worked fine for the average use case, but we had space for improvement when it came to our large hosts and especially for our prohosts, who might have thousands of yearly bookings on our platform. To have confidence in our system over the long term, we needed to find a solution that provided inherently better <strong><em>performance, reliability, and scalability</em>.</strong></p><h3>Introducing the Payments Unified Data Read Layer</h3><p>To achieve our ambitious goals for payments, we needed to re-think how clients integrate with our payments platform.</p><h3>Unified Entry Points</h3><p>Our first task was to unify the payments data read entry points. To accomplish this, we leveraged <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, Airbnb’s data-oriented service mesh, where clients query for the “entity” instead of needing to identify dozens of services and their APIs. This new architecture required our clients to worry only about the requisite data entity rather than having to communicate with individual payments services.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rtNAGGJFpwHfnZ5V" /><figcaption>Instead of communicating with individual payments services, presentation services just use the read layer.</figcaption></figure><p>In these entry points, we provided as many filtering options as possible so each API could hide filtering and aggregation complexity from its clients. This also greatly reduced the numbers of APIs we needed to expose.</p><h3>Unified Higher-Level Data Entities</h3><p>Having a single entry point is a good start, but it does not resolve all the complexity. In payments, we have 100+ data models, and it requires a decent amount of domain knowledge to understand their responsibilities clearly. If we just expose all of these models from a single entry point, there would still be too much context required for client engineers.</p><p>Instead of making our clients deal with this complexity, we opted to hide payments internal details as much as possible by coming up with <strong>higher-level domain entities</strong>. Through this process, we were able to reduce the core payments data to fewer than ten<strong> </strong>high level entities, which greatly reduced the amount of exposed payments internal details. These new entities also allowed us to guard clients against changes made in Payments platform. When we internally update the business logic, we keep the entity schema the same without requiring any migrations on the client side. Our principles for the new architecture were the following:</p><ul><li><strong>Simple</strong>: Design for non-payments engineers, and use common terminology.</li><li><strong>Extensible</strong>: Maintain loose coupling with storage schema, and encapsulate concepts to protect from payments internal changes while allowing quick iterations.</li><li><strong>Rich</strong>: Hide away the complexity but not the data. If clients need to fetch data, they should be able to find it in <strong><em>one</em></strong> of the entities.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hPMYFXmawEXCM8tf" /><figcaption>Expose cleaner higher-level domain entities to hide payments internal details while guarding clients from frequent API migrations.</figcaption></figure><h3>Materialize Denormalized Data</h3><p>With unified entry points and entities, we greatly reduced the complexity for client onboardings. However, the “<strong><em>how</em></strong>” of fetching the data, combined with expensive application layer aggregations, was still a big challenge. While it’s important that clients are able to integrate with the payments system smoothly, our valued community should also enjoy the experience on our platform.</p><p>The core problem we identified was <strong><em>dependency on many tables and services during client queries</em></strong>. One of the promising solutions was denormalization–essentially, moving these expensive operations from query time to ingestion time. We explored different ways of pre-denormalizing payments data and materializing it reliably with less than 10 seconds replication lag. To our great luck, our friends in the Homes Foundation team were piloting a Read-Optimized Store Framework, which takes an event-driven lambda approach to materializing secondary indices. Using this framework, teams are able to get both near real-time data via database change capture mechanisms and historical data leveraging our daily database dumps stored in Hive. In addition, the maintenance requirements of this framework (e.g., single code for online and offline ingestion, written in Java) were much lower compared to other existing internal solutions..</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aEEGisG92n5f9mc8" /><figcaption>A high-level look at the read-optimized store framework usage by payments. It provides ingestion flows for both offline and near real-time data with shared business logic between them.</figcaption></figure><p>After combining all of above improvements, our new payments read flow looked like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kWOXKjHjm7Jvskze" /><figcaption>The final shape of the payments data read architecture. Clients do not need to know any payments services or internals.</figcaption></figure><p>We provide data in a reliable and performant way via denormalized read-optimized store indices.</p><h3>Results</h3><h3>Migrate and Elevate: Transaction History</h3><p>The first test surface for the new unified data read architecture was Transaction History (TH). Hosts on our platform use the <a href="https://www.airbnb.com/users/transaction_history">Transaction History page</a> to view their past and future payouts along with top-level earning metrics (e.g., total paid out amount).</p><p>On the technical side, this was one of the most complex payments flows we had. There were many different details required, and the data was coming from <strong>10+</strong> payments tables. This had caused issues in the past, including timeouts, slow loading times, downtime due to hard dependencies, and slow iteration speed as a result of complex implementations. While doing the initial technical design for TH migration from Airbnb monolith to SOA, we took the hard path of re-architecting this flow instead of applying band-aids. This helped to ensure long-term success and provide the best possible experience to our host community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*homIPsg24j6ZA1FT" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8SKW7SUEmrbGMFO1" /><figcaption>Transaction History page and simplified high level architecture. Airbnb monolith app behaves like a presentation service and fetches data from multiple payment services and also from legacy databases.</figcaption></figure><p>This use case was a great fit for our unified read layer. Using the data used by TH as a starting point, we came up with a new API and high-level entity to serve all data read use cases from similar domains.</p><p>After locking down the entity and its schema, we started to denormalize the data. Thanks to the read-optimized store framework, we were able to denormalize all the data from 10+ tables into a couple of Elasticsearch indices. Not only did we greatly reduce the touchpoints of the query, we were also able to paginate and aggregate much more efficiently by leveraging the storage layer instead of doing the same operations on the application layer. After close to two years of work, we migrated 100% of traffic and achieved up to <strong><em>150x</em></strong> latency improvements, while improving the reliability of the flow from ~96% to <strong><em>99.9+%</em></strong><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NvD6A6Q0AswwXib-" /><figcaption>After the re-architecture, payments data needed by Transaction History is provided by payments read-optimized store and accessed by clients using a well-defined and extensible payout schema over the unified data read layer.</figcaption></figure><h3>Unlocking New Experiences: Guest Payment History</h3><p>Our next use case, called Guest Payment History, came out of Airbnb’s annual company-wide hackathon. This hackathon project aimed to provide a detailed and easy way for our guest community to track their payments and refunds. Similar to Transaction History, this scenario also required information from multiple payments services and databases, including many legacy databases.</p><p>Guest Payment History (GPH) also helped to showcase many benefits brought by the unified read layer: a new unified entity to serve GPH and future similar use cases, along with an extensible API which supported many different filters. We denormalized and stored data from legacy and SOA payment tables using the read-optimized store framework into a single Elasticsearch index, which reduced the complexity and cost of queries greatly.</p><p>We released this new page to our community with our <a href="https://news.airbnb.com/2021-winter-release/">2021 Winter launch</a> and achieved a huge reduction on customer support tickets related to questions about guest payments; which resulted in close to $1.5M cost savings for 2021. It also illustrated our move towards a stronger technical foundation with high reliability and low latency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KNPtvQ1odi8dp2Xw" /><figcaption>Guests can track their payments and refunds using Guest Payment History.</figcaption></figure><p>The architecture is very similar to TH, where data is provided to clients via unified API and schema, backed by a secondary store.</p><p>After exposing these new entities via TH and GPH, we started to onboard many other critical use cases to leverage the same flow in order to efficiently serve and surface payments data.</p><h3>Conclusion</h3><p>Microservice/SOA architectures greatly help backend teams to independently scale and develop various domains with minimal impact to each other. It’s equally important to make sure the clients of these services and their data will not be subject to additional challenges under this new industry-standard architecture.</p><p>In this blog post, we illustrated some potential solutions, such as unified APIs and higher-level entities to hide away the internal service and architectural complexities from the callers. We also recommend leveraging denormalized secondary data stores to perform expensive join and transformation operations during ingestion time to ensure client queries can stay simple and performant. As we demonstrated with multiple initiatives, complex domains such as payments can significantly benefit from these approaches.</p><p>If this type of work interests you, take a look at the following related positions:</p><p><strong>US</strong>:</p><p><a href="https://careers.airbnb.com/positions/3393185/">Staff Software Engineer, Payments</a></p><p><strong>India</strong>:</p><p><a href="https://careers.airbnb.com/positions/3153981/">Senior Software Engineer, Cities Bangalore</a></p><p><a href="https://careers.airbnb.com/positions/3842855/">Engineering Manager, Ambassador Platform Products</a></p><p><a href="https://careers.airbnb.com/positions/2768475/">Manager, Engineering Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2773515/">Staff Software Engineer, Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2925359/">Senior Software Engineer, Payments Compliance</a></p><h3>Acknowledgments</h3><p>We had many people at Airbnb contributing to this big re-architecture, but countless thanks to Mini Atwal, Yong Rhyu, Musaab At-Taras, Michel Weksler, Linmin Yang, Linglong Zhu, Yixiao Peng, Bo Shi, Huayan Sun, Wentao Qi, Adam Wang, Erika Stott, Will Koh, Ethan Schaffer, Khurram Khan, David Monti, Colleen Graneto, Lukasz Mrowka, Bernardo Alvarez, Blazej Adamczyk, Dawid Czech, Marcin Radecki, Tomasz Laskarzewski, Jessica Tai, Krish Chainani, Victor Chen, Will Moss, Zheng Liu, Eva Feng, Justin Dragos, Ran Liu, Yanwei Bai, Shannon Pawloski, Jerroid Marks, Yi He, Hang Yuan, Xuemei Bao, Wenguo Liu, Serena Li, Theresa Johnson, Yanbo Bai, Ruize Lu, Dechuan Xu, Sam Tang, Chiao-Yu Tuan, Xiaochen He, Gautam Prajapati, Yash Gulani, Abdul Shakir, Uphar Goyal, Fanchen Kong, Claire Thompson, Pavel Lahutski, Patrick Connors, Ben Bowler, Gabriel Siqueira, Jing Hao, Manish Singhal, Sushu Zhang, Jingyi Ni, Yi Lang Mok, Abhinav Saini, and Ajmal Pullambi. We couldn’t have accomplished this without your invaluable contributions.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e613e7af1a39" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39">Unified Payments Data Read at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>