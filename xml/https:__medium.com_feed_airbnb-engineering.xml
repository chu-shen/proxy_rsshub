<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 07 Jan 2023 01:09:39 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[When a Picture Is Worth More Than Words]]></title>
            <link>https://medium.com/airbnb-engineering/when-a-picture-is-worth-more-than-words-17718860dcc2?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/17718860dcc2</guid>
            <category><![CDATA[similarity-search]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[aesthetics]]></category>
            <category><![CDATA[computer-vision]]></category>
            <dc:creator><![CDATA[Yuanpei Cao]]></dc:creator>
            <pubDate>Thu, 08 Dec 2022 18:03:50 GMT</pubDate>
            <atom:updated>2022-12-08T18:03:50.795Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb uses visual attributes to enhance the Guest and Host experience</p><p><em>By </em><a href="https://www.linkedin.com/in/yuanpei-cao-792b103b/"><em>Yuanpei Cao</em></a><em>, </em><a href="https://www.linkedin.com/in/bulam/"><em>Bill Ulammandakh</em></a><em>, </em><a href="https://www.linkedin.com/in/hao-wang-2661553/"><em>Hao Wang</em></a><em>, and </em><a href="https://www.linkedin.com/in/hwangtt/"><em>Tony Hwang</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qlUYnDrTVDAYZfqE" /></figure><h3><strong>Introduction</strong></h3><p>On Airbnb, our hosts share unique listings all over the world. There are hundreds of millions of accompanying listing photos on Airbnb. Listing photos contain crucial information about style and design aesthetics that are difficult to convey in words or a fixed list of amenities. Accordingly, multiple teams at Airbnb are now leveraging computer vision to extract and incorporate intangibles from our rich visual data to help guests easily find listings that suit their preferences.</p><p>In previous blog posts titled <a href="https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c"><em>WIDeText: A Multimodal Deep Learning Framework</em></a>,<em> </em><a href="https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3"><em>Categorizing Listing Photos at Airbnb</em></a> and <a href="https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e"><em>Amenity Detection and Beyond — New Frontiers of Computer Vision at Airbnb</em></a>, we explored how we utilize computer vision for room categorization and amenity detection to map listing photos to a taxonomy of discrete concepts. This post goes beyond discrete categories into how Airbnb leverages image aesthetics and embeddings to optimize across various product surfaces including ad content, listing presentation, and listing recommendations.</p><h3>Image aesthetics</h3><p>Attractive photos are as vital as price, reviews, and description during a guest’s Airbnb search journey. To quantify “attractiveness” of photos, we developed a deep learning-based image aesthetics assessment pipeline. The underlying model is a deep convolutional neural network (<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>) trained on human-labeled image aesthetic rating distributions. Each photo was rated on a scale from 1 to 5 by hundreds of photographers based on their personal aesthetic measurements (the higher the rating, the better the aesthetic). Unlike traditional classification tasks that classify the photo into low, medium and high-quality categories, the model was built upon the Earth Mover’s Distance (<a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">EMD</a>) as the loss function to predict photographers’ rating distributions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9uDKRsxap29D2jTg" /><figcaption><em>Figure 1. The model that predicts image aesthetics distribution is CNN-based and trained with the EMD loss function. Suppose the ground truth label of a photo is: 10% of users give ratings 1 and 2, respectively, 20% give rating 3, and 30% give ratings 4 and 5, respectively. The corresponding prediction is [0.1, 0.1, 0.2, 0.3, 0.3]</em></figcaption></figure><p>The predicted mean rating is highly correlated with image resolution and listing booking probability, as well as high-end Airbnb listing photo distribution. Rating thresholds are set based on use cases, such as ad photo recommendation on social media and photo order suggestion in the listing onboarding process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oX4zLXa1xuW5dZ1u" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kPzH6oeThUYOm-Lu" /><figcaption><em>Figure 2. Examples of Airbnb listing photos with aesthetics scores higher than the 90% percentile</em></figcaption></figure><h3>Image aesthetic-based ads quality improvement</h3><p>Airbnb uses advertising on social media to attract new customers and inspire our community. The social media platform chooses which ads to run based on millions of Airbnb-provided listing photos.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/0*Cs_Sp9Db3uebMDQA" /><figcaption><em>Figure 3. Airbnb Ads displayed on Facebook</em></figcaption></figure><p>Since a visually appealing Airbnb photo can effectively attract users to the platform and considerably increase the ad’s click-through rate (CTR), we utilized the image aesthetic score and room categorization to select the most attractive Airbnb photos of the living room, bedroom, kitchen, and exterior view. The criterion for “good quality” listing photos was set based on the top 50th percentile of the aesthetic score and tuned based on an internal manual aesthetic evaluation of 1K randomly selected listing cover photos. We performed A/B testing for this use case and found that the ad candidates with a higher aesthetic score generated a substantially higher CTR and booking rate.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qkiOeZNUrFiQ9hp0" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3wAi8A6I4pgZh2ti" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EAZedvb_PXHZT4j0" /><figcaption><em>Figure 4. Pre-selected Airbnb Creative Ads through image aesthetics and room type filters</em></figcaption></figure><h3>Automated photo ranking based on home design and room type</h3><p>When posting a new listing on Airbnb, hosts upload numerous photos. Optimally arranging these photos to highlight a home can be time-consuming and challenging. A host may also be uncertain about the ideal arrangement for their images because the work requires making trade-offs between photo attractiveness, photo diversity, and content relevance to guests. More specifically, the first five photos are the most important for listing success as they are the most frequently viewed and crucial to forming the initial guest impression. Accordingly, we developed an automated photo ranking algorithm that selects and orders the first five photos of a home leveraging two visual signals: home design evaluation and room categorization.</p><p>Home design evaluation estimates how well a home is designed from an interior design and architecture perspective. The CNN-based home design evaluation model is trained on Airbnb<em> Plus </em>and<em> Luxe</em> qualification data that assess the aesthetic appeal of each photo’s home design. Airbnb <em>Plus</em> and <em>Luxe</em> listings have passed strict home design evaluation criteria and so the data from their qualification process is well-suited to be used as training labels for a home design evaluation model. The photos are then classified into different room types, such as living room, bedroom, bathroom etc, through the room categorization model. Finally, an algorithm makes trade-offs between photo home design attractiveness, photo relevance, and photo diversity to maximize the booking probability of a home. Below is an example of how a new photo order is suggested. The photo auto-rank feature was launched in Host’s listing onboarding product in 2021, leading to significant lifts in new listing creation and booking success.</p><p><strong>Original ordering</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pnBb2R9FSRaKolwe" /></figure><p><strong>Auto-suggested ordering</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ycy7KI2DK_dhX98a" /><figcaption><em>Figure 5. The example of original photo order (top) uploaded by Airbnb Host and auto-suggested order (bottom) calculated by the proposed algorithm</em></figcaption></figure><h3>Image similarity</h3><p>Beyond aesthetics, photos also capture the general appearance and content. To efficiently represent this information, we encode and compress photos into image embeddings using computer vision models. Image embeddings are compact vector representations of images that represent visual features. These embeddings can be compared against each other with a distance metric that represents similarity in that feature space.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qJaLeY5a3rvkR87m" /><figcaption><em>Figure 6. Image embeddings can be compared by distance metrics like cosine similarity to represent their similarity in the encoded latent space</em></figcaption></figure><p>The features learned by the encoder are directly influenced by the training image data distribution and training objectives. Our labeled room type and amenity classification data allows us to train models on this data distribution to produce semantically meaningful embeddings for listing photo similarity use cases. However, as the quantity and diversity of images on Airbnb grow, it becomes increasingly untenable to rely solely on manually labeled data and supervised training techniques. Consequently, we are currently exploring self-supervised contrastive training to improve our image embedding models. This form of training does not require image labels; instead, it bootstraps contrastive learning with synthetically generated positive and negative pairs. Our image embedding models can then learn key visual features from listing photos without manual supervision.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hU4Ij4D2PUCVyN0V" /><figcaption><em>Figure 7. Introducing random image transformations to synthetically create positive and negative pairs helps refine our image encoders without additional labeling.</em></figcaption></figure><h3>Scalable embedding search</h3><p>It is often impractical to compute exhaustive pairwise embedding similarity, even within focused subsets of millions of items. To support real-time search use cases, such as (near) duplicate photo detection and visual similarity search, we instead perform an approximate nearest neighbor (<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor">ANN</a>) search. This functionality is largely enabled by an efficient embedding index preprocessing and construction algorithm called Hierarchical Navigable Small World (<a href="https://arxiv.org/abs/1603.09320">HNSW</a>). HNSW builds a hierarchical proximity graph structure that greatly constrains the search space at query time. We scale this horizontally with AWS OpenSearch, where each node contains its own HNSW embedding graphs and Lucene-backed indices that are hydrated periodically and can be queried in parallel. To add real-time embedding ANN search, we have implemented the following index hydration and index search design patterns enabled by existing Airbnb internal platforms.</p><p>To hydrate an embedding index on a periodic basis, all relevant embeddings computed by <a href="https://ieeexplore.ieee.org/document/8964147">Bighead</a>, Airbnb’s end-to-end machine learning platform, are aggregated and persisted into a Hive table. The encoder models producing the embeddings are deployed for both online inference and offline batch processing. Then, the incremental embedding update is synced to the embedding index on AWS OpenSearch through Airflow, our data pipeline orchestration service.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*701twLpGC7cAqooo" /><figcaption><em>Figure 8. Index hydration data pathway</em></figcaption></figure><p>To perform image search, a client service will first verify whether the image’s embedding exists in the OpenSearch index cache to avoid recomputing embeddings unnecessarily. If the embedding is already there, the OpenSearch cluster can return approximate nearest neighbor results to the client without further processing. If there is a cache miss, Bighead is called to compute the image embedding, followed by a request to query the OpenSearch cluster for approximate nearest neighbors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gPnnwDRV1gU6VJ9g" /><figcaption><em>Figure 9. Image similarity search for a previously unseen image</em></figcaption></figure><p>Following this embedding search framework, we are scaling real-time visual search in current production flows and upcoming releases.</p><h3>Expanding Airbnb categories</h3><p><a href="https://www.airbnb.com/2022-summer">Airbnb Categories</a> help our guests discover unique getaways. Some examples are “Amazing views”, “Historical homes”, and “Creative spaces”. These categories do not always share common amenities or discrete attributes, as they often represent an inspirational concept. We are exploring automatic category expansion by identifying similar listings based on their photos, which do capture design aesthetics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_m9Kv0hZdqYRDAZL" /><figcaption><em>Figure 10. Listing photos from the “Creative spaces” category</em></figcaption></figure><h3>Similar listing recommendations in rebooking assistance</h3><p>In the 2022 Summer Release, Airbnb introduced rebooking assistance to offer guests a smooth experience from Community Support ambassadors when a Host cancels on short notice. For the purpose of recommending comparable listings throughout the rebooking process, a two-tower reservation and listing embedding model ranks candidate listings, updated on a daily basis. As future work, we can consider augmenting the listing representation with image embeddings and enabling real-time search.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*at3MPM2B3Mj-etNu" /><figcaption><em>Figure 11. The example of a landing page that recommends similar listings to guests and Community Support ambassadors in the Rebooking assistance.</em></figcaption></figure><h3>Conclusion</h3><p>Photos contain aesthetic and style-related signals that are difficult to express in words or map to discrete attributes. Airbnb is increasingly leveraging these visual attributes to help our hosts highlight the unique character of their listings and to assist our guests in discovering listings that match their preferences.</p><p>Interested in working at Airbnb? Check out our <a href="https://careers.airbnb.com/">open roles</a>.</p><h3>Acknowledgements</h3><p>Thanks to Teng Wang, Regina Wu, Nan Li, Do-kyum Kim, Tiantian Zhang, Xiaohan Zeng, Mia Zhao, Wayne Zhang, Elaine Liu, Floria Wan, David Staub, Tong Jiang, Cheng Wan, Guillaume Guy, Wei Luo, Hanchen Su, Fan Wu, Pei Xiong, Aaron Yin, Jie Tang, Lifan Yang, Lu Zhang, Mihajlo Grbovic, Alejandro Virrueta, Brennan Polley, Jing Xia, Fanchen Kong, William Zhao, Caroline Leung, Meng Yu, Shijing Yao, Reid Andersen, Xianjun Zhang, Yuqi Zheng, Dapeng Li, and Juchuan Ma for the product collaborations. Also thanks Jenny Chen, Surashree Kulkarni, and Lauren Mackevich for editing.</p><p>Thanks to Ari Balogh, Tina Su, Andy Yasutake, Joy Zhang, Kelvin Xiong, Raj Rajagopal, and Zhong Ren’s leadership support on building computer vision products at Airbnb.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=17718860dcc2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/when-a-picture-is-worth-more-than-words-17718860dcc2">When a Picture Is Worth More Than Words</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Motion Engineering at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/motion-engineering-at-scale-5ffabfc878?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5ffabfc878</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[animation]]></category>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[swift]]></category>
            <dc:creator><![CDATA[Cal Stephens]]></dc:creator>
            <pubDate>Wed, 07 Dec 2022 18:00:35 GMT</pubDate>
            <atom:updated>2022-12-07T18:00:35.371Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb is applying declarative design patterns to rapidly build fluid transition animations</p><p><strong>By: </strong><a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fRCT2OZX8i42aJrDKPIrSg.jpeg" /></figure><p>Motion is a key part of what makes a digital experience both easy and delightful to use. Fluid transitions between states and screens are key for helping the user preserve context as they navigate throughout a feature. Quick flourishes of animation make an app come alive, and help give it a distinct personality.</p><p>At Airbnb we launch hundreds of features and experiments that have been developed by engineers across many teams. When building at this scale, it’s critical to consider efficiency and maintainability throughout our tech stack–and motion is no exception. Adding animations to a feature needs to be fast and easy. The tooling must compliment and fit naturally with other components of our feature architecture. If an animation takes too long to build or is too difficult to integrate with the overall feature architecture, then it’s often the first part of a product experience that gets dropped when translating from design to implementation.</p><p>In this post, we’ll discuss a new framework for iOS that we’ve created to help make this vision a reality.</p><h3>Imperative UIKit Transitions</h3><p>Let’s consider this transition on the Airbnb app’s homepage, which takes users from search results to an expanded search input screen:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*myWLTeEaraOgt8dMdzNWKQ.gif" /><figcaption>An example transition from Airbnb’s iOS app of expanding and collapsing the search input screen</figcaption></figure><p>The transition is a key part of the design, making the entire search experience feel cohesive and lightweight.</p><p>Within traditional UIKit patterns, there are two ways to build a transition like this. One is to create a single, massive view controller that contains both the search results and the search input screens, and orchestrates a transition between the two states using imperative <a href="https://developer.apple.com/documentation/uikit/uiview/1622418-animate"><em>UIView</em> animation blocks</a>. While this approach is easy to build, it has the downside of tightly coupling these two screens, making them far less maintainable and portable.</p><p>The other approach is to implement each screen as a separate view controller, and create a bespoke <a href="https://developer.apple.com/documentation/uikit/uiviewcontrolleranimatedtransitioning?language=objc"><em>UIViewControllerAnimatedTransitioning</em></a> implementation that extracts relevant views from each view hierarchy and then animates them. This is typically more complicated to implement, but has the key benefit of letting each individual screen be built as a separate <em>UIViewController</em> like you would for any other feature.</p><p>In the past, we’ve built transitions with both of these approaches, and found that they both typically require hundreds of lines of fragile, imperative code. This meant custom transitions were time consuming to build and difficult to maintain, so they were typically not included as part of a team’s main feature development flow.</p><p>A common trend has been to move away from this sort of <em>imperative</em> system design and towards <em>declarative</em> patterns. We use declarative systems extensively at Airbnb–we leverage frameworks like <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670">Epoxy</a> and SwiftUI to declaratively define the layout of each screen. Screens are combined into features and flows using <a href="https://github.com/airbnb/epoxy-ios#epoxypresentations">declarative</a> <a href="https://github.com/airbnb/epoxy-ios#epoxynavigationcontroller">navigation</a> APIs. We’ve found these declarative systems unlock substantial productivity gains, by letting engineers focus on defining how the app should behave and abstracting away the complex underlying implementation details.</p><h3>Declarative Transition Animations</h3><p>To simplify and speed-up the process of adding transitions to our app, we’ve created a <strong>new</strong> <strong>framework for building transitions declaratively</strong>, rather than imperatively as we did before. We’ve found that this new approach has made it much simpler to build custom transitions, and as a result far more engineers have been able to easily add rich and delightful transitions to their screens even on tight timelines<strong><em>.</em></strong></p><p>To perform a transition with this framework, you simply provide the <em>initial</em> state and <em>final</em> state (or in the case of a screen transition, the <em>source</em> and <em>destination</em> view controllers<em>)</em> along with a declarative <em>transition definition</em> of how each individual element on the screen should be animated. The framework’s generic <em>UIViewControllerAnimatedTransitioning</em> implementation handles everything else automatically.</p><p>This new framework has become instrumental to how we build features. It powers many of the new features included in Airbnb’s <a href="https://news.airbnb.com/2022-summer-release/">2022 Summer Release</a><em> </em>and <a href="https://www.airbnb.com/2022-winter">2022 Winter Release</a><em>,</em> helping make them easy and delightful to use:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*KW_2f2UzNCHBOdmXEdKIyw.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*LEVniwcfps4EQYWn4wvo3g.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*_fK31gs2yfPQPoewqmP_9A.gif" /><figcaption>Example transitions in Airbnb’s iOS app from new features introduced in 2022</figcaption></figure><p>As an introduction, let’s start with a example. Here’s a simple “search” interaction where a date picker in a bottom sheet slides up over a page of content:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*XbjUC8R9CnWR3x2qWbwBbw.gif" /><figcaption>An example transition for a simple “search” feature</figcaption></figure><p>In this example, there are two separate view controllers: the search results screen and the date picker screen. Each of the components we want to animate are tagged with an identifier to establish their identity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H9kFx3NvIlPDqTLirV1LbQ.png" /><figcaption><em>Diagram showing the search results screen and date picker screen annotated with component identifiers</em></figcaption></figure><p>These identifiers let us refer to each component semantically by name, rather than by directly referencing the <em>UIView </em>instance. For example, the <em>Explore.searchNavigationBarPill</em> component on each screen is a separate <em>UIView</em> instance,<em> </em>but since they’re tagged with the same identifier the two view instances are considered separate “states” of the same component.</p><p>Now that we’ve identified the components that we want to animate, we can define <em>how</em> they should animate. For this transition we want:</p><ol><li>The background to fade in</li><li>The bottom sheet to slide up from the bottom of the screen</li><li>The navigation bar to animate between the first state and second state (a “shared element” animation).</li></ol><p>We can express this as a simple transition definition:</p><pre>let transitionDefinition: TransitionDefinition = [<br>  BottomSheet.backgroundView: .crossfade,<br>  BottomSheet.foregroundView: .edgeTranslation(.bottom),<br>  Explore.searchNavigationBarPill: .sharedElement,<br>]</pre><p>Revisiting the example above for expanding and collapsing the search input screen, we want:</p><ol><li>The background to blur</li><li>The top bar and bottom bars to slide in</li><li>The home screen search bar to transition into the “where are you going?” card</li><li>The other two search cards to fade in while staying anchored relative to the “where are you going? card</li></ol><p>Here’s how that animation is defined using the declarative transition definition syntax:</p><pre>let transitionDefinition: TransitionDefinition = [<br>  SearchInput.background: .blur,<br>  SearchInput.topBar: .translateY(-40),<br>  SearchInput.bottomBar: .edgeTranslation(.bottom),<br>  <br>  SearchInput.whereCard: .sharedElement,<br>  SearchInput.whereCardContent: .crossfade,<br>  SearchInput.searchInput: .crossfade,<br>  <br>  SearchInput.whenCard: .anchorTranslation(relativeTo: SearchInput.whereCard),<br>  SearchInput.whoCard: .anchorTranslation(relativeTo: SearchInput.whereCard),<br>]</pre><h3>How It Works</h3><p>This declarative <em>transition definition </em>API is powerful and flexible, but it only tells half the story. To actually perform the animation, our framework provides a generic <em>UIViewControllerAnimatedTransitioning</em> implementation that takes the transition definition and orchestrates the transition animation. To explore how this implementation works, we’ll return to the simple “search” interaction.</p><p>First, the framework traverses the view hierarchy of both the <em>source</em> and <em>destination</em> screens to extract the <em>UIView</em> for each of the identifiers being animated. This determines whether or not a given identifier is present on each screen, and forms an <em>identifier hierarchy</em> (much like the view hierarchy of a screen).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2s84RSAuG2YFPJN94EIuiw.png" /><figcaption><em>The “identifier hierarchy” of the source and destination screens</em></figcaption></figure><p>The identifier hierarchies of the <em>source</em> and <em>destination</em> are diffed to determine whether an individual component was added, removed, or present in both. If the view was added or removed, the framework will use the animation specified in the transition definition. If the view was present in both states, the framework instead performs a “shared element animation” where the component animates from its initial position to its final position while its content is updated. These shared elements are animated recursively–each component can provide its own identifier hierarchy of child elements, which is diffed and animated as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-KCx3tfFxT3inrQss6yKHw.png" /><figcaption><em>The final identifier hierarchy after diffing the source and destination screens</em></figcaption></figure><p>To actually perform these animations, we need a single <em>view hierarchy</em> that matches the structure of our <em>identifier hierarchy</em>. We can’t just combine the source and destination screens into a single view hierarchy by layering them on top of each other, because the ordering would be wrong. In this case, if we just placed the destination screen over the source screen then the source <em>Explore.searchNavigationBarPill</em> view would be below the destination <em>BottomSheet.backgroundView</em> element, which doesn’t match the identifier hierarchy.</p><p>Instead, we have to create a separate view hierarchy that matches the structure of the identifier hierarchy. This requires making copies of the components being animated and adding them to the UIKit transition container. Most <em>UIView</em>s<em> </em>aren’t trivially copyable, so copies are typically made by “snapshotting” the view (rendering it as an image). We temporarily hide the “original view” while the animation is playing, so only the snapshot is visible.</p><p>Once the framework has set up the transition container’s view hierarchy and determined the specific animation to use for each component, the animations just have to be applied and played. This is where the underlying imperative <em>UIView</em> animations are performed.</p><h3>Conclusion</h3><p>Like with <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670">Epoxy</a> and other declarative systems, abstracting away the underlying complexity and providing a simple declarative interface makes it possible for engineers to focus on the <em>what</em> rather than the <em>how</em>. The declarative transition definition for these animations are only a few lines of code, which is by itself a <em>huge</em> improvement over any feasible imperative implementation. And since our declarative feature-building APIs have first-class support for UIKit <a href="https://developer.apple.com/documentation/uikit/uiviewcontrolleranimatedtransitioning?language=objc"><em>UIViewControllerAnimatedTransitioning</em></a> implementations, these declarative transitions can be integrated into existing features without making any architecture changes. This significantly accelerates feature development, making it easier than ever to create highly polished transitions, while also enabling long-term flexibility and maintainability.</p><p>We have a packed roadmap ahead. One area of active work is improving interoperability with SwiftUI. This lets us seamlessly transition between UIKit and SwiftUI-based screens, which unlocks incremental adoption of SwiftUI in our app without having to sacrifice motion. We’re also exploring making similar frameworks available on web and Android. Our long-term goal here is to make it as easy as possible to translate our designer’s great ideas into actual shipping products, on all platforms.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4693375/">Staff Software Engineer, Wishlists</a></p><p><a href="https://careers.airbnb.com/positions/4665949/">Staff Software Engineer, Guests &amp; Hosts</a></p><p><a href="https://careers.airbnb.com/positions/4590099/">Staff Android Software Engineer, Guest</a></p><h3>Acknowledgments</h3><p>Many thanks to Eric Horacek and Matthew Cheok for their major contributions to Airbnb’s motion architecture and our declarative transition framework.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5ffabfc878" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/motion-engineering-at-scale-5ffabfc878">Motion Engineering at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing Lottie 4.0 for iOS]]></title>
            <link>https://medium.com/airbnb-engineering/announcing-lottie-4-0-for-ios-d4d226862a54?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d4d226862a54</guid>
            <category><![CDATA[animation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[lottie]]></category>
            <dc:creator><![CDATA[Cal Stephens]]></dc:creator>
            <pubDate>Tue, 06 Dec 2022 20:01:37 GMT</pubDate>
            <atom:updated>2022-12-07T00:27:14.640Z</atom:updated>
            <content:encoded><![CDATA[<p>A new rendering engine with significant performance improvements powered by Core Animation</p><p><strong>By: </strong><a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y4aQ-u0Mnh0S5b71sKr-Fg.jpeg" /></figure><p><a href="https://airbnb.design/lottie/"><strong>Lottie</strong></a> is Airbnb’s <a href="https://airbnb.io/lottie/#/README">cross-platform</a>, <a href="https://github.com/airbnb/lottie-ios">open source</a> library for rendering vector motion graphics. We use Lottie extensively at Airbnb, and it also powers animations in thousands of other apps throughout the industry.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*YncHlVnpvqU3hnjwo4g7dg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*-69VqnYxgYdp8xOyiaDGSQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*Ie-4EfSyrNjA9sZnAo6ASA.gif" /><figcaption>Example Lottie animations included in Airbnb’s iOS app</figcaption></figure><p>Today we’re releasing <a href="https://github.com/airbnb/lottie-ios/releases/tag/4.0.0"><strong>Lottie 4.0</strong></a><strong> </strong>for iOS. This major new release brings <strong>significant performance improvements</strong> to all Lottie animations, with a brand new rendering engine powered by Core Animation.</p><p>Using Lottie at scale for many years, we’ve learned a lot about its performance characteristics in real-world use cases. We found that it was relatively common for Lottie animations to drop frames in some of our more complex screens. To understand why, we first have to take a look at how Lottie previously rendered animations.</p><p>Previous versions of Lottie played animations on the app’s main thread, effectively using a <a href="https://developer.apple.com/documentation/quartzcore/cadisplaylink?language=objc"><em>CADisplayLink</em></a>. Once per frame, Lottie would execute code on the main thread to advance the progress of the animation and re-render its content. This meant that animations would consume 5–20%+ of the CPU while playing, leaving fewer CPU cycles available for the rest of the app:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z4DbWMMV2dHCQoETuW0taQ.gif" /><figcaption><em>Playing an animation with Lottie 3.5.0, using the original main thread rendering engine</em></figcaption></figure><p>This also meant that animations would not update when the main thread was busy. This could cause animations to drop frames or freeze entirely, which results in a poor user experience:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nnlh9Fk_Im--KvAyL1v5KQ.gif" /><figcaption><em>Lottie animations dropping frames when the main thread is overloaded</em></figcaption></figure><p>These issues are inherent limitations of using a main-thread-bound rendering architecture.</p><p>On iOS, the most performant and power-efficient way to play animations is by using <a href="https://developer.apple.com/documentation/quartzcore?language=objc">Core Animation</a>. This system framework renders animations out-of-process with GPU hardware acceleration. Animation playback is managed by a separate system process called the “render server”. This means Core Animation-powered animations don’t contribute to the CPU utilization of the app process itself, and can continue even when its main thread is blocked or busy.</p><p>Throughout 2022, we’ve been working on a new rendering engine implementation for Lottie built on top of Core Animation. For each of the layers in the animation JSON file, the new engine builds a <em>CALayer</em> and applies <em>CAAnimation</em>s with keyframes for the layer’s animated properties. Lottie passes these animation keyframes off to Core Animation, which takes care of actually rendering them on-screen and updating the animation each frame.</p><p>This new engine eliminates the CPU overhead from playing a Lottie animation, and effectively guarantees that Lottie animations will animate smoothly at 60 or 120 fps regardless of the app’s CPU load.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QmNCLz3e_Zuez3nFBvu_LA.gif" /><figcaption><em>Playing an animation with Lottie 4.0, using the new Core Animation rendering engine</em></figcaption></figure><p>Since animations rendered by the new engine don’t execute any code on the app’s main thread, apps now have more resources available for other functionality. This is especially valuable when running tasks with high CPU load. As an example, the Airbnb app displays a Lottie animation when starting up for the first time. We ran an experiment here and found that switching to the new rendering engine <em>reduces</em> our app’s total launch time, while <em>also</em> improving the frame-rate and UX of the startup animation.</p><p>We <a href="https://github.com/airbnb/lottie-ios/discussions/1627">first introduced</a> the Core Animation rendering engine in Lottie 3.4.0 earlier this year, behind an opt-in feature flag. We’ve been using the new engine by default for all Lottie animations in the Airbnb app for over six months, and have been hard at work fixing issues reported by early-adopters in the community.</p><p>Starting in today’s Lottie 4.0 release for iOS, the Core Animation rendering engine is <strong>enabled by default</strong> for all apps using Lottie, with no additional work or migration required by app developers. This is a major milestone that we’ve been working towards for a long time, and we hope it helps raise the bar for animation quality and performance even higher throughout the industry!</p><p>Lottie 4.0 for iOS also includes several significant enhancements contributed by members of the community:</p><ul><li>Support for <a href="https://dotlottie.io/">dotLottie animation files</a>, which are much smaller in size than standard JSON files</li><li>A new animation decoding implementation that is ~2x faster than the previous <em>Codable</em>-based implementation</li></ul><p>You can learn more about Lottie, and our commitment to open source, in previous posts we’ve published:</p><ul><li><a href="https://airbnb.design/introducing-lottie/">Introducing Lottie</a>: Behind the scenes of our new open-source animation tool</li><li><a href="https://medium.com/airbnb-engineering/lottie-and-swift-at-airbnb-e0c85dc365e7">Moving Lottie Swiftly into the Future</a>:<strong> </strong>A personal story on how Airbnb rewrote the popular open source library Lottie in a new language</li></ul><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4693375/">Staff Software Engineer, Wishlists</a></p><p><a href="https://careers.airbnb.com/positions/4665949/">Staff Software Engineer, Guests &amp; Hosts</a></p><h3>Acknowledgments</h3><p>Many thanks to Eric Horacek for first proposing this project and reviewing 100+ pull requests over the past year. Also thanks to Brandon Withrow, the original author of Lottie, plus the <a href="https://github.com/airbnb/lottie-ios/graphs/contributors">many other contributors</a> who have helped out over the years.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d4d226862a54" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/announcing-lottie-4-0-for-ios-d4d226862a54">Announcing Lottie 4.0 for iOS</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How AI Text Generation Models Are Reshaping Customer Support at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/how-ai-text-generation-models-are-reshaping-customer-support-at-airbnb-a851db0b4fa3?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/a851db0b4fa3</guid>
            <category><![CDATA[customer-experience]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Gavin Li]]></dc:creator>
            <pubDate>Wed, 23 Nov 2022 17:38:57 GMT</pubDate>
            <atom:updated>2022-11-23T17:38:57.385Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>Leveraging text generation models to build more effective, scalable customer support products.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*74YI6FqsvcKKWNIDPhDVPw.jpeg" /></figure><p><a href="https://www.linkedin.com/in/gavin-li-64354117/">Gavin Li</a>, <a href="https://www.linkedin.com/in/mia-zhao-964a9213/">Mia Zhao</a> and <a href="https://www.linkedin.com/in/zhenyu-zhao-30b8632a/">Zhenyu Zhao</a></p><p>One of the fastest-growing areas in modern Artificial Intelligence (AI) is <a href="https://huggingface.co/tasks/text-generation">AI text generation models</a>. As the name suggests, these models generate natural language. Previously, most industrial natural language processing (NLP) models were classifiers, or what might be called discriminative models in machine learning (ML) literature. However, in recent years, generative models based on large-scale language models are rapidly gaining traction and fundamentally changing how ML problems are formulated. Generative models can now obtain some domain knowledge through large-scale pre-training and then produce high-quality text — for instance answering questions or paraphrasing a piece of content.</p><p>At Airbnb, we’ve heavily invested in AI text generation models in our community support (CS) products, which has enabled many new capabilities and use cases. This article will discuss three of these use cases in detail. However, first let’s talk about some of the beneficial traits of text generation models that make it a good fit for our products.</p><h3>About Text Generation Models</h3><p>Applying AI models in large-scale industrial applications like Airbnb customer support is not an easy challenge. Real-life applications have many long-tail corner cases, can be hard to scale, and often become costly to label the training data. There are several traits of text generation models that address these challenges and make this option particularly valuable.</p><h3>Encoding Knowledge</h3><p>The first attractive trait is the capability to encode domain knowledge into the language models. As illustrated by <a href="https://arxiv.org/abs/1909.01066">Petroni et al. (2019)</a>, we can encode domain knowledge through large-scale pre-training and transfer learning. In traditional ML paradigms, input matters a lot. The model is just a transformation function from the input to the output. The model training focuses mainly on preparing input, feature engineering, and training labels. While for generative models, the key is the knowledge encoding. How well we can design the pre-training and training to encode high-quality knowledge into the model — and how well we design prompts to induce this knowledge — is far more critical. This fundamentally changes how we solve traditional problems like classifications, rankings, candidate generations, etc.</p><p>Over the past several years, we have accumulated massive amounts of records of our human agents offering help to our guests and hosts at Airbnb. We’ve then used this data to design large-scale pre-training and training to encode knowledge about solving users’ travel problems. At inference time, we’ve designed prompt input to generate answers based directly on the encoded human knowledge. This approach produced significantly better results compared to traditional classification paradigms. A/B testing showed significant business metric improvement as well as significantly better user experience.</p><h3>Unsupervised Learning</h3><p>The second trait of the text generation model we’ve found attractive is its “unsupervised” nature. Large-scale industrial use cases like Airbnb often have large amounts of user data. How to mine helpful information and knowledge to train models becomes a challenge. First, labeling large amounts of data by human effort is very costly, significantly limiting the training data scale we could use. Second, designing good labeling guidelines and a comprehensive label taxonomy of user issues and intents is challenging because real-life problems often have long-tail distribution and lots of nuanced corner cases. It doesn’t scale to rely on human effort to exhaust all the possible user intent definitions.</p><p>The unsupervised nature of the text generation model allows us to train models without largely labeling the data. In the pre-training, in order to learn how to predict the target labels, the model is forced to first gain a certain understanding about the problem taxonomy. Essentially the model is doing some data labeling design for us internally and implicitly. This solves the scalability issues when it comes to intent taxonomy design and cost of labeling, and therefore opens up many new opportunities. We’ll see some examples of this when we dive into use cases later in this post.</p><h3>More Natural and Productive Language Models</h3><p>Finally, text generation models transcend the traditional boundaries of ML problem formulations Over the past few years, researchers have realized that the extra dense layers in autoencoding models may be unnatural, counterproductive, and restrictive. In fact, all of the typical machine learning tasks and problem formulations can be viewed as different manifestations of the single, unifying problem of language modeling. A classification can be formatted as a type of language model where the output text is the literal string representation of the classes.</p><p>In order to make the language model unification effective, a new but essential role is introduced: the <strong>prompt</strong>. A prompt is a short piece of textual instruction that informs the model of the task at hand and sets the expectation for what the format and content of the output should be. Along with the prompt, additional natural language annotations, or hints, are also highly beneficial in further contextualizing the ML problem as a language generation task. The incorporation of prompts has been demonstrated to significantly improve the quality of language models on a variety of tasks. The figure below illustrates the anatomy of a high-quality input text for universal generative modeling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fYmXfhyBq6uw7uKa" /><figcaption>Figure 1.1 An example of the prompt and input feature design of our text generation model</figcaption></figure><p>Now, let’s dive into a few ways that text generation models have been applied within Airbnb’s Community Support products. We’ll explore three use cases — content recommendation, real-time agent assistance, and chatbot paraphrasing.</p><h3>Content Recommendation Model</h3><p>Our content recommendation workflow, powering both Airbnb’s Help Center search and the support content recommendation in our <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">Helpbot</a>, utilizes pointwise ranking to determine the order of the documents users receive, as shown in Figure 2.1. This pointwise ranker takes the textual representation of two pieces of input — the current user’s issue description and the candidate document, in the form of its title, summary, and keywords. It then computes a relevance score between the description and the document, which is used for ranking. Prior to 2022, this pointwise ranker had been implemented using the XLMRoBERTa, however we’ll see shortly why we’ve switched to the MT5 model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9aM_OnaEBDRA5kzd" /><figcaption>Figure 2.1 How we utilized encoder-only architecture with an arbitrary classification head to perform pointwise document ranking</figcaption></figure><p>Following the design decision to introduce prompts, we transformed the classic binary classification problem into a prompt-based language generation problem. The input is still derived from both the issue description and the candidate document’s textual representation. However, we contextualize the input by prepending a prompt to the description that informs the model that we expect a binary answer, either “Yes” or “No”, of whether the document would be helpful in resolving the issue. We also added annotations to provide extra hints to the intended roles of the various parts of the input text, as illustrated in the figure below. To enable personalization, we expanded the issue description input with textual representations of the user and their reservation information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fnUD_3FEKSEJgzph" /><figcaption>Figure 2.2. How we leveraged an encoder-decoder architecture with a natural language output to serve as a pointwise ranker</figcaption></figure><p>We fine-tuned the MT5 model on the task described above. In order to evaluate the quality of the generative classifier, we used production traffic data sampled from the same distribution as the training data. The generative model demonstrated significant improvements in the key performance metric for support document ranking, as illustrated in the table below.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2Fcd16baf468&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2Fcd16baf468&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="294" frameborder="0" scrolling="no"><a href="https://medium.com/media/a63a53f8082e510013ab23e0bd9af2b4/href">https://medium.com/media/a63a53f8082e510013ab23e0bd9af2b4/href</a></iframe><p>In addition, we also tested the generative model in an online A/B experiment, integrating the model into Airbnb’s Help Center, which has millions of active users. The successful experimentation results led to the same conclusion — the generative model recommends documents with significantly higher relevance in comparison with the classification-based baseline model.</p><h3>‘Real-Time Agent Assistant’ Model</h3><p>Equipping agents with the right contextual knowledge and powerful tools leads to better experiences for our customers. So we provide our agents with just-in-time guidance, which directs them to the correct answers consistently and helps them resolve user issues efficiently.</p><p>For example, through agent-user conversations, suggested templates are displayed to assist agents in problem solving. To make sure our suggestions are enforced within CS policy, suggestion templates are gated by a combination of API checks and model intent checks. This model needs to answer questions to capture user intents such as:</p><ul><li>Is this message about a cancellation?</li><li>What cancellation reason did this user mention?</li><li>Is this user canceling due to a COVID sickness?</li><li>Did this user accidentally book a reservation?</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bL5t68bbHTkbVOAgrhx5hg.png" /><figcaption>Figure 3.1 AI-generated recommendation template</figcaption></figure><p>In order to support many granular intent checks, we developed a mastermind Question-Answering (QA) model, aiming to help answer all related questions. This QA model was developed using the generative model architecture mentioned above. We concatenate multiple rounds of user-agent conversations to leverage chat history as input text and then ask the prompt we care about at the point in time of serving.</p><p>Prompts are naturally aligned with the same questions we ask humans to annotate. Slightly different prompts would result in different answers as shown below. Based on the model’s answer, relevant templates are then recommended to agents.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2Fb70c8e7a4e&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2Fb70c8e7a4e&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="384" frameborder="0" scrolling="no"><a href="https://medium.com/media/fee7ae07a9080a402770f606b972ba44/href">https://medium.com/media/fee7ae07a9080a402770f606b972ba44/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*h5mMZvyhnEehuUxq" /><figcaption>Figure 2.2 Mastermind QA model architecture</figcaption></figure><p>We leveraged backbone models such as t5-base and Narrativa and did experimentations on various training dataset compositions including annotation-based data and logging-based data with additional post-processing. Annotation datasets usually have higher precision, lower coverage, and more consistent noise, while logging datasets have lower precision, higher case coverage, and more random noises. We found that combining these two datasets together yielded the best performance.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2F073ba3d3d1&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2F073ba3d3d1&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="248" frameborder="0" scrolling="no"><a href="https://medium.com/media/b4750e4a42e072d98c28b557fa80ab99/href">https://medium.com/media/b4750e4a42e072d98c28b557fa80ab99/href</a></iframe><p>Due to the large size of the parameters, we leverage a library, called <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, to train the generative model using multi GPU cores. DeepSpeed helps to speed up the training process from weeks to days. That being said, it typically requires longer for hyperparameter tunings. Therefore, experiments are required with smaller datasets to get a better direction on parameter settings. In production, online testing with real CS ambassadors showed a large engagement rate improvement.</p><h3>Paraphrase Model in Chatbot</h3><p>Accurate intent detection, slot filling, and effective solutions are not sufficient for building a successful AI chatbot. Users often choose not to engage with the chatbot, no matter how good the ML model is. Users want to solve problems quickly, so they are constantly trying to assess if the bot is understanding their problem and if it will resolve the issue faster than a human agent. Building a paraphrase model, which first rephrases the problem a user describes, can give users some confidence and confirm that the bot’s understanding is correct. This has significantly improved our bot’s engagement rate. Below is an example of our chatbot automatically paraphrasing the user’s description.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UBV5lgJYxCp9uxwi" /><figcaption>Figure 4.1 An actual example of the chatbot paraphrasing a user’s description of a payment issue</figcaption></figure><p>This method of paraphrasing a user’s problem is used often by human customer support agents. The most common pattern of this is “I understand that you…”. For example, if the user asks if they can cancel the reservation for free, the agent will reply with, “I understand that you want to cancel and would like to know if we can refund the payment in full.” We built a simple template to extract all the conversations where an agent’s reply starts with that key phrase. Because we have many years of agent-user communication data, this simple heuristic gives us millions of training labels for free.</p><p>We tested popular sequence-to-sequence transformer model backbones like <a href="https://arxiv.org/abs/1910.13461">BART</a>, <a href="https://doi.org/10.48550/ARXIV.1912.08777">PEGASUS</a>, <a href="http://arxiv.org/abs/1910.10683">T5</a>, etc, and autoregressive models like <a href="https://doi.org/10.48550/ARXIV.1907.05774">GPT2</a>, etc. For our use case, the T5 model produced the best performance.</p><p>As found by <a href="https://arxiv.org/abs/1905.05709">Huang et al. (2020)</a>, one of the most common issues of the text generation model is that it tends to generate bland, generic, uninformative replies. This was also the major challenge we faced.</p><p>For example, the model outputs the same reply for many different inputs: “I understand that you have some issues with your reservation.” Though correct, this is too generic to be useful.</p><p>We tried several different solutions. First, we tried to build a backward model to predict <em>P(Source|target)</em>, as introduced by <a href="https://arxiv.org/abs/1911.00536">Zhang et al. (2020)</a>, and use it as a reranking model to filter out results that were too generic. Second, we tried to use some rule-based or model-based filters.</p><p>In the end, we found the best solution was to tune the training data. To do this, we ran text clustering on the training target data based on pre-trained similarity models from <a href="https://www.sbert.net/">Sentence-Transformers</a>. As seen in the table below, the training data contained too many generic meaningless replies, which caused the model to do the same in its output.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2F4fc4479886&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2F4fc4479886&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="294" frameborder="0" scrolling="no"><a href="https://medium.com/media/87416f6754eb610f173dbe0c77e18976/href">https://medium.com/media/87416f6754eb610f173dbe0c77e18976/href</a></iframe><p>We labeled all clusters that are too generic and used Sentence-Transformers to filter them out from the training data. This approach worked significantly better and gave us a high-quality model to put into production.</p><h3>Conclusion</h3><p>With the fast growth of large-scale pre-training-based transformer models, the text generation models can now encode domain knowledge. This not only allows them to utilize the application data better, but allows us to train models in an unsupervised way that helps scale data labeling. This enables many innovative ways to tackle common challenges in building AI products. As demonstrated in the three use cases detailed in this post — content ranking, real-time agent assistance, and chatbot paraphrasing — the text generation models improve our user experiences effectively in customer support scenarios. We believe that text generation models are a crucial new direction in the NLP domain. They help Airbnb’s guests and hosts solve their issues more swiftly and assist Support Ambassadors in achieving better efficiency and a higher resolution of the issues at hand. We look forward to continuing to invest actively in this area.</p><h3>Acknowledgments</h3><p>Thank you <a href="https://www.linkedin.com/in/weipingpeng/">Weiping Pen</a>, <a href="https://www.linkedin.com/in/xin-liu-908b6b18/">Xin Liu</a>, <a href="https://www.linkedin.com/in/mukundn/">Mukund Narasimhan</a>, <a href="https://www.linkedin.com/in/cmujoy/">Joy Zhang</a>, <a href="https://www.linkedin.com/in/tina-su-saratoga/">Tina Su</a>, <a href="https://www.linkedin.com/in/ayasutake/">Andy Yasutake</a> for reviewing and polishing the blog post content and all the great suggestions. Thank you <a href="https://www.linkedin.com/in/cmujoy/">Joy Zhang</a>, <a href="https://www.linkedin.com/in/tina-su-saratoga/">Tina Su</a>, <a href="https://www.linkedin.com/in/ayasutake/">Andy Yasutake</a> for their leadership support! Thank you <a href="https://www.linkedin.com/in/elaineliu5/">Elaine Liu</a> for building the paraphrase end-to-end product, running the experiments, and launching. Thank you to our close PM partners, <a href="https://www.linkedin.com/in/shuangyi-cassie-cao/">Cassie Cao</a> and <a href="https://www.linkedin.com/in/jerryhong/">Jerry Hong</a>, for their PM expertise. This work could not have happened without their efforts.</p><p><em>Interested in working at Airbnb? Check out </em><a href="https://careers.airbnb.com/"><em>these</em></a><em> open roles.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a851db0b4fa3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-ai-text-generation-models-are-reshaping-customer-support-at-airbnb-a851db0b4fa3">How AI Text Generation Models Are Reshaping Customer Support at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building Airbnb Categories with ML and Human-in-the-Loop]]></title>
            <link>https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e97988e70ebb</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Mihajlo Grbovic]]></dc:creator>
            <pubDate>Mon, 21 Nov 2022 16:54:21 GMT</pubDate>
            <atom:updated>2022-11-22T18:14:20.267Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>Airbnb Categories Blog Series — Part I</strong></h4><p>By: <strong>Mihajlo Grbovic, Ying Xiao, Pratiksha Kadam, Aaron Yin, Pei Xiong, Dillon Davis, Aditya Mukherji, Kedar Bellare, Haowei Zhang, Shukun Yang, Chen Qian, Sebastien Dubois, Nate Ney, James Furnary, Mark Giangreco, Nate Rosenthal, Cole Baker, Bill Ulammandakh, Sid Reddy, Egor Pakhomov</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RrtVCKycvPPwPuukCLDT-Q.jpeg" /><figcaption>Figure 1. Browsing listings by categories: <strong>Castles</strong>, <strong>Desert</strong>, <strong>Design</strong>, <strong>Beach </strong>&amp;<strong> Countryside</strong></figcaption></figure><h3>25 Years of Online Travel Search</h3><p>Online travel search hasn’t changed much in the last 25 years. The traveler enters her destination, dates, and the number of guests into a search interface, which dutifully returns a list of options that best meet the criteria. Eventually, Airbnb and other travel sites made improvements to allow for better filtering, ranking, personalization and, more recently, to display results slightly outside of the specified search parameters–for example, by accommodating flexible dates or by suggesting nearby locations. Taking a page from the travel agency model, these websites also built more “inspirational” browsing experiences that recommend popular destinations, showcasing these destinations with captivating imagery and inventory (think digital “catalog”).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*atw4D5q1UfxGH6dj" /><figcaption>Figure 2. Airbnb Destination Recommendation Example</figcaption></figure><p>The biggest shortcoming of these approaches is that the traveler must have a specific destination in mind. Even travelers who are flexible get funneled to a similar set of well-known destinations, reinforcing the cycle of mass tourism.</p><h3>Introducing Airbnb Categories</h3><p>In our recent release, we flipped the travel search experience on its head by having the inventory dictate the destinations, not the other way around. In this way, we sought to inspire the traveler to book unique stays in places they might not think to search for. By leading with our unique places to stay, grouped together into cohesive “categories”, we inspired our guests to find some incredible places to stay off the beaten path.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*clTHm13EoXvGjmEt5PpUVQ.png" /><figcaption>Figure 3. Unique travel worthy inventory in lesser known destinations that users are unlikely to search for</figcaption></figure><p>Though our goal was an intuitive browsing experience, it required considerable work behind the scenes to pull this off. In this three-part series, we will pull back the curtain on the technical aspects of the <a href="https://news.airbnb.com/2022-summer-release/">Airbnb 2022 Summer Launch</a>.</p><ul><li><strong>Part I </strong>(this post) is designed to be a high-level introductory post about how we applied machine learning to build out the listing collections and to solve different tasks related to the browsing experience–specifically, quality estimation, photo selection and ranking.</li><li><strong>Part II </strong>of the series focuses on ML Categorization of listings into categories. It explains the approach in more detail, including signals and labels that we used, tradeoffs we made, and how we set up a human-in-the-loop feedback system.</li><li><strong>Part III</strong> focuses on ML Ranking of Categories depending on the search query. For example, we taught the model to show the Skiing category first for an Aspen, Colorado query versus Beach/Surfing for a Los Angeles query. That post will also cover our approach for ML Ranking of listings within each category.</li></ul><h3>Grouping Listings into Categories</h3><p>Airbnb has thousands of very unique, high quality listings, many of which received design and architecture awards or have been featured in travel magazines or movies. However, these listings are sometimes hard to discover because they are in a little-known town or because they are not ranked highly enough by the search algorithm, which optimizes for bookings. While these unique listings may not always be as bookable as others due to lower availability or higher price, they are great for inspiration and for helping guests discover hidden destinations where they may end up booking a stay influenced by the category.</p><p>To showcase these special listings we decided to group them into collections of homes organized by what makes them unique. The result was <strong>Airbnb Categories, </strong>collections of homes revolving around some common themes including the following:</p><ul><li><strong>Categories that revolve around a location or a place of interest (POI)</strong> such as Coastal, Lake, National Parks, Countryside, Tropical, Arctic, Desert, Islands, etc.</li><li><strong>Categories that revolve around an activity</strong> such as Skiing, Surfing, Golfing, Camping, Wine tasting, Scuba, etc.</li><li><strong>Categories that revolve around a home type</strong> such as Barns, Castles, Windmills, Houseboats, Cabins, Caves, Historical, etc.</li><li><strong>Categories that revolve around a home amenity</strong> such as Amazing Pools, Chef’s Kitchen, Grand Pianos, Creative Spaces, etc.</li></ul><p>We defined 56 categories and outlined the definition for each category. Now all that was left to do was to assign our entire catalog of listings to categories.</p><p>With the Summer launch just a few months away, we knew that we could not manually curate all the categories, as it would be very time consuming and costly. We also knew that we could not generate all the categories in a rule-based manner, as this approach would not be accurate enough. Finally, we knew we could not produce an accurate ML categorization model without a training set of human-generated labels. Given all of these limitations, we decided to combine the accuracy of human review with the scale of ML models to create a human-in-the-loop system for listing categorization and display.</p><h4>Rule-Based Candidate Generation</h4><p>Before we could build a trained ML model for assigning listings to categories, we had to rely on various listing- and geo-based signals to generate the initial set of candidates. We named this technique <strong><em>weighted sum of indicators</em></strong><em>. </em>It consists of building out a set of signals (indicators) that associate a listing with a specific category. The more indicators the listing has, the better the chances of it belonging to that category.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*l8v0iDnqsyguo85jgk75Lw.png" /><figcaption>Figure 4. Rule-based weighted sum of indicators approach to produce candidates for human review</figcaption></figure><p>For example, let’s consider a listing that is within 100 meters of a Lake POI, with keyword “lakefront” mentioned in listing title and guest reviews, lake views appearing in listing photos and several kayaking activities nearby. All this information together strongly indicates that the listing belongs to the <em>Lakefront</em> category. The weighted sum of these indicators totals to a high <strong><em>score</em></strong>, which means that this listing-category pair would be a strong candidate for human review. If a rule-based candidate generation created a large set of candidates we would use this score to prioritize listings for human review to maximize the initial yield.</p><h4>Human Review</h4><p>The manual review of candidates consists of several tasks. Given a listing candidate for a particular category or several categories, an agent would:</p><ul><li><strong>Confirm/reject the category or categories</strong> assigned to the listing by comparing it to the category definition.</li><li><strong>Pick the photo</strong> that best represents the category. Listings can belong to multiple categories, so it is sometimes appropriate to pick a different photo to serve as the cover image for different categories.</li><li><strong>Determine the quality tier </strong>of the selected photo. Specifically, we defined <strong>four quality tiers:</strong> <strong><em>Most Inspiring</em></strong>, <strong><em>High Quality</em></strong>, <strong><em>Acceptable Quality</em></strong>, and <strong><em>Low Quality. </em></strong>We use this information to rank the higher quality listings near the top of the results to achieve the “wow” effect with prospective guests.</li><li>Some of the categories rely on signals related to <strong>Places of Interest (POIs) data</strong> such as the locations of lakes or national parks, so the reviewers could add a POI that we were missing in our database.</li></ul><h4>Candidate Expansion</h4><p>Although the rule-based approach can generate many candidates for some categories, for others (e.g., Creative Spaces, Amazing Views) it may produce only a limited set of listings. In those cases, we turn to candidate expansion. One such technique leverages pre-trained listing embeddings. Once a human reviewer confirms that a listing belongs to a particular category, we can find similar listings via cosine similarity. Very often the 10 nearest neighbors are good candidates for the same category and can be sent for human review. We detailed one of the embedding approaches in our previous<a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"> blog post</a> and have developed new ones since then.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W3G7kW0PpbKJQ6wbWyZQ_Q.jpeg" /><figcaption>Figure 5. Listing similarity via embeddings can help find more listings that are from the same category</figcaption></figure><p>Other expansion techniques include keyword expansion, location-based expansion (i.e. considering neighboring homes for same POI category), etc.</p><h4>Training ML Models</h4><p>Once we collected enough human-generated labels, we trained a binary classification model that predicts whether or not a listing belongs to a specific category. We then used a holdout set to evaluate performance of the model using a precision-recall (PR) curve. Our goal here was to evaluate if the model was good enough to send highly confident listings directly to production.</p><p>Figure 6 shows a trained ML model for the Lakefront category. On the left we can see the feature importance graph, indicating which signals contribute most to the decision of whether or not a listing belongs to the Lakefront category. On the right we can see the hold out set PR curve of different model versions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n0HbLnOu9iqC4txk" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*viXUC31IIWQn13Wb" /><figcaption>Figure 6. Lakefront ML model feature importance and performance evaluation</figcaption></figure><p><strong>Sending confident listings to production: </strong>using a PR curve we can set a threshold that achieves 90% precision on a downsampled hold out set that mimics the true listing distribution. Then we can score all unlabeled listings and send ones above that threshold to production, with the expectation of 90% accuracy. In this particular case, we can achieve 76% recall at 90% precision, meaning that with this technique we can expect to capture 76% of the true Lakefront listings in production.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*M2RCGu7LEgn-x6_l" /><figcaption>Figure 7. Basic ML + Human in the Loop setup for tagging listings with categories</figcaption></figure><p><strong>Selecting listings for human review: </strong>given the expectation of 76% recall, to cover the rest of the Lakefront listings we also need to send listings below the threshold for human evaluation. When prioritizing the below-threshold listings, we considered the photo quality score for the listing and the current coverage of the category to which the listing was tagged, among other factors. Once a human reviewer confirmed a listing’s category assignment, that tag would be made available to production. Concurrently, we send the tags back to our ML models for retraining, so that the models improve over time.</p><p><strong>ML models for quality estimation and photo selection. </strong>In addition to the ML Categorization models described above, we also trained a Quality ML model that assigns one of the four quality tiers to the listing, as well as a Vision Transformer Cover Image ML model that chooses the listing photo that best represents the category. In the current implementation the Cover Image ML model takes the category information as the input signal, while the Quality ML model is a global model for all categories. The three ML models work together to assign category, quality and cover photo. Listings with these assigned attributes are sent directly into production under certain circumstances and also queued for review.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iwRE1sSgmYpJCuM0" /><figcaption>Figure 8. Human vs. ML flow to production</figcaption></figure><h4>Two New Ranking Algorithms</h4><p><a href="https://news.airbnb.com/2022-summer-release/">The Airbnb Summer release</a> introduced categories both to homepage (Figure 9 left), where we show categories that are popular near you, and to location searches (Figure 9 right), where we show categories that are related to the searched destination. For example, in the case of a Lake Tahoe location search we show <em>Skiing, Cabins, Lakefront, Lake House, etc.</em>, and <em>Skiing</em> should be shown first if searching in winter.</p><p>In both cases, this created a need for two new ranking algorithms:</p><ul><li><strong>Category ranking </strong>(green arrow in Figure 9 left): How to rank categories from left to right, by taking into account user origin, season, category popularity, inventory, bookings and user interests</li><li><strong>Listing Ranking</strong> (blue arrow in Figure 9 left): given all the listings assigned to the category, rank them from top to bottom by taking into account assigned listing quality tier and whether a given listing was sent to production by humans or by ML models.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vN4exaTinuCSGCHth6FtlA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Odmoseo1jwOCnRrTurqglA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Own3vspDyitaRu7GBpmIOA.png" /><figcaption>Figure 9. Listing Ranking Logic for Homepage and Location Category Experience</figcaption></figure><h3>Putting it all together</h3><p>To summarize, we presented how we create categories from scratch, first using rules that rely on listing signals and POIs and then with ML with humans in the loop to constantly improve the category. Figure 10 describes the end-to-end flow as it exists today.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o6lekCM1lHkZt8uNeTWKAw.png" /><figcaption>Figure 9: Logic for Category Creation and Improvement over time</figcaption></figure><p><em>Our approach was to </em><strong><em>define </em></strong><em>an acceptable delivery; </em><strong><em>prototype </em></strong><em>several categories to acceptable level; </em><strong><em>scale </em></strong><em>the rest of the categories to the same level;</em><strong><em> revisit </em></strong><em>the acceptable delivery and improve the product over time.</em></p><p>In Part II, we’ll explain in greater detail the models that categorize listings into categories.</p><h3>Acknowledgments</h3><p><em>We would like to thank everyone involved in the project. Building Airbnb Categories holds a special place in our careers as one of those rare projects where people with different backgrounds and roles came together to work jointly to build something unique.</em></p><p>Interested in working at Airbnb? Check out our open roles <a href="https://careers.airbnb.com/">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e97988e70ebb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb">Building Airbnb Categories with ML and Human-in-the-Loop</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mussel — Airbnb’s Key-Value Store for Derived Data]]></title>
            <link>https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/406b9fa1b296</guid>
            <category><![CDATA[storage]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Shouyan guo]]></dc:creator>
            <pubDate>Mon, 10 Oct 2022 17:40:04 GMT</pubDate>
            <atom:updated>2022-10-10T17:40:03.878Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>Mussel — Airbnb’s Key-Value Store for Derived Data</strong></h3><p><strong>How Airbnb built a persistent, high availability and low latency key-value storage engine for accessing derived data from offline and streaming events.</strong></p><p><strong>By:</strong> <a href="http://linkedin.com/in/chandramoulir">Chandramouli Rangarajan</a>, <a href="http://linkedin.com/in/shouyan-guo">Shouyan Guo</a>, <a href="http://linkedin.com/in/yuxijin">Yuxi Jin</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DviEp2cHiuC4NoH5" /></figure><h3>Introduction</h3><p>Within Airbnb, many online services need access to derived data, which is data computed with large scale data processing engines like Spark or streaming events like Kafka and stored offline. These services require a high quality derived data storage system, with strong reliability, availability, scalability, and latency guarantees for serving online traffic. For example, the user profiler service stores and accesses real-time and historical user activities on Airbnb to deliver a more personalized experience.</p><p>In this post, we will talk about how we leveraged a number of open source technologies, including <a href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/regionserver/HRegion.html">HRegion</a>, <a href="https://helix.apache.org/">Helix</a>, <a href="https://spark.apache.org/">Spark</a>, <a href="https://zookeeper.apache.org/">Zookeeper</a>,and <a href="https://kafka.apache.org/">Kafka</a> to build a scalable and low latency key-value store for hundreds of Airbnb product and platform use cases.</p><h3>Derived Data at Airbnb</h3><p>Over the past few years, Airbnb has evolved and enhanced our support for serving derived data, moving from teams rolling out custom solutions to a multi-tenant storage platform called Mussel. This evolution can be summarized into three stages:</p><p><strong>Stage 1 (01/2015): Unified read-only key-value store (HFileService)</strong></p><p>Before 2015, there was no unified key-value store solution inside Airbnb that met four key requirements:</p><ol><li>Scale to petabytes of data</li><li>Efficient bulk load (batch generation and uploading)</li><li>Low latency reads (&lt;50ms p99)</li><li>Multi-tenant storage service that can be used by multiple customers</li></ol><p>Also, none of the existing solutions were able to meet these requirements. <a href="https://www.mysql.com/">MySQL</a> doesn’t support bulk loading, <a href="https://hbase.apache.org/">Hbase</a>’s massive bulk loading (distcp) is not optimal and reliable, RocksDB had no built-in horizontal sharding, and we didn’t have enough C++ expertise to build a bulk load pipeline to support RocksDB file format.</p><p>So we built HFileService, which internally used <a href="http://devdoc.net/bigdata/hbase-0.98.7-hadoop1/book/hfilev2.html#:~:text=HFile%20is%20a%20low%2Dlevel,to%20write%20those%20inline%20blocks.">HFile</a> (the building block of Hadoop HBase, which is based on Google’s SSTable):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*38vsUsNg4FnouSzp" /><figcaption><em>Fig. 1: HFileService Architecture</em></figcaption></figure><ol><li>Servers were sharded and replicated to address scalability and reliability issues</li><li>The number of shards was fixed (equivalent to the number of Hadoop reducers in the bulk load jobs) and the mapping of servers to shards stored in Zookeeper. We configured the number of servers mapped to a specific shard by manually changing the mapping in Zookeeper</li><li>A daily Hadoop job transformed offline data to HFile format and uploaded it to S3. Each server downloaded the data of their own partitions to local disk and removed the old versions of data</li><li>Different data sources were partitioned by primary key. Clients determined the correct shard their requests should go to by calculating the hash of the primary key and modulo with the total number of shards. Then queried Zookeeper to get a list of servers that had those shards and sent the request to one of them</li></ol><p><strong>Stage 2 (10/2015): Store both real-time and derived data (Nebula)</strong></p><p>While we built a multi-tenant key-value store that supported efficient bulk load and low latency read, it had its drawbacks. For example, it didn’t support point, low-latency writes, and any update to the stored data had to go through the daily bulk load job. As Airbnb grew, there was an increased need to have low latency access to real-time data.</p><p>Therefore, Nebula was built to support both batch-update and real-time data in a single system. It internally used DynamoDB to store real-time data and S3/HFile to store batch-update data. Nebula introduced timestamp based versioning as a version control mechanism. For read requests, data would be read from both a list of dynamic tables and the static snapshot in HFileService, and the result merged based on timestamp.</p><p>To minimize online merge operations, Nebula also had scheduled spark jobs that ran daily and merged snapshots of DynamoDB data with the static snapshot of HFileService. Zookeeper was used to coordinate write availability of dynamic tables, snapshots being marked ready for read, and dropping of stale tables.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EGFSNgoYuewt2NptXXIdxA.png" /><figcaption>Fig. 2: Nebula Architecture</figcaption></figure><p><strong>Stage 3 (2018): Scalable and low latency key-value storage engine (Mussel)</strong></p><p>In Stage 3, we built a system that supported both read and write on real-time and batch-update data with timestamp-based conflict resolution. However, there were opportunities for improvement:</p><ol><li>Scale-out challenge: It was cumbersome to manually edit partition mappings inside Zookeeper with increasing data growth, or to horizontally scale the system for increasing traffic by adding additional nodes</li><li>Improve read performance under spiky write traffic</li><li>High maintenance overhead: We needed to maintain HFileService and DynamoDB at the same time</li><li>Inefficient merging process: The process of merging the delta update from DynamoDB and HFileService daily became very slow as our total data size became larger. The daily update data in DynamoDB was just 1–2% of the baseline data in HFileService. However, we re-published the full snapshot (102% of total data size) back to HFileService daily</li></ol><p>To solve the drawbacks, we came up with a new key-value store system called <strong>Mussel</strong>.</p><ol><li>We introduced Helix to manage the partition mapping within the cluster</li><li>We leveraged Kafka as a replication log to replicate the write to all of the replicas instead of writing directly to the Mussel store</li><li>We used HRegion as the only storage engine in the Mussel storage nodes</li><li>We built a Spark pipeline to load the data from the data warehouse into storage nodes directly</li></ol><p>Let’s go into more details in the following paragraphs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p_mMDfHNFVebMhGA3MXBvg.png" /><figcaption>Fig. 3: Mussel Architecture</figcaption></figure><p><strong>Manage partitions with Helix</strong></p><p>In Mussel, in order to make our cluster more scalable, we increased the number of shards from 8 in HFileService to 1024. In Mussel, data is partitioned into those shards by the hash of the primary keys, so we introduced Apache Helix to manage these many logical shards. Helix manages the mapping of logical shards to physical storage nodes automatically. Each Mussel storage node could hold multiple logical shards. Each logical shard is replicated across multiple Mussel storage nodes.</p><p><strong>Leaderless Replication with Kafka</strong></p><p>Since Mussel is a read-heavy store, we adopted a leaderless architecture. Read requests could be served by any of the Mussel storage nodes that have the same logical shard, which increases read scalability. In the write path, we needed to consider the following:</p><ol><li>We want to smooth the write traffic to avoid the impact on the read path</li><li>Since we don’t have the leader node in each shard, we need a way to make sure each Mussel storage node applies the write requests in the same order so the data is consistent across different nodes</li></ol><p>To solve these problems, we introduced Kafka as a write-ahead-log here. For write requests, instead of directly writing to the Mussel storage node, it’ll first write to Kafka asynchronously. We have 1024 partitions for the Kafka topic, each partition belonging to one logical shard in the Mussel. Each Mussel storage node will poll the events from Kafka and apply the change to its local store. Since there is no leader-follower relationship between the shards, this configuration allows the correct write ordering within a partition, ensuring consistent updates. The drawback here is that it can only provide eventual consistency. However, given the derived data use case, it is an acceptable tradeoff to compromise on consistency in the interest of ensuring availability and partition tolerance.</p><p><strong>Supporting both read, write, and compaction in one storage engine</strong></p><p>In order to reduce the hardware cost and operational load of managing DynamoDB, we decided to remove it and extend HFileService as the only storage engine to serve both real-time and offline data. To better support both read and write operations, we used <a href="https://hbase.apache.org/1.1/apidocs/org/apache/hadoop/hbase/regionserver/HRegion.html">HRegion</a> instead of <a href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html">Hfile</a>. HRegion is a fully functional key-value store with MemStore and BlockCache. Internally it uses a Log Structured Merged (LSM) Tree to store the data and supports both read and write operations.</p><p>An HRegion table contains column families, which are the logical and physical grouping of columns. There are column qualifiers inside of a column family, which are the columns. Column families contain columns with time stamped versions. Columns only exist when they are inserted, which makes HRegion a sparse database. We mapped our client data to HRegion as the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1wtQhbeeNTrCLsChR4BDLQ.png" /></figure><p>With this mapping, for read queries, we’re able to support:</p><ol><li>Point query by looking up the data with primary key</li><li>Prefix/range query by scanning data on secondary key</li><li>Queries for the latest data or data within a specific time range, as both real-time and offline data written to Mussel will have a timestamp</li></ol><p>Because we have over 4000 client tables in Mussel, each user table is mapped to a column family in HRegion instead of its own table to reduce scalability challenges at the metadata management layer. Also, as HRegion is a column-based storage engine, each column family is stored in a separate file so they can be read/written independently.</p><p>For write requests, it consumes the write request from Kafka and calls the HRegion put API to write the data directly. For each table, it can also support customizing the max version and TTL (time-to-live).</p><p>When we serve write requests with HRegion, another thing to consider is compaction. Compaction needs to be run in order to clean up data that is deleted or has reached max version or max TTL. Also when the MemStore in HRegion reaches a certain size, it is flushed to disk into a StoreFile. Compaction will merge those files together in order to reduce disk seek and improve read performance. However, on the other hand, when compaction is running, it causes higher cpu and memory usage and blocks writes to prevent JVM (Java Virtual Machine) heap exhaustion, which impacts the read and write performance of the cluster.</p><p>Here we use Helix to mark Mussel storage nodes for each logical shard into two types of resources: online nodes and batch nodes. For example, if we have 9 Mussel storage nodes for one logical shard, 6 of them are online nodes and 3 of them are batch nodes. The relationship between online and batch are:</p><ol><li>They both serve write requests</li><li>Only online nodes serve read requests and we rate limit the compaction on online nodes to have good read performance</li><li>Helix schedules a daily rotation between online nodes and batch nodes. In the example above, it moves 3 online nodes to batch and 3 batch nodes to online so those 3 new batch nodes can perform full speed major compaction to clean up old data</li></ol><p>With this change, now we’re able to support both read and write with a single storage engine.</p><p><strong>Supporting bulk load from data warehouse</strong></p><p>We support two types of bulk load pipelines from data warehouse to Mussel via <a href="https://airflow.apache.org/">Airflow</a> jobs: merge type and replace type. Merge type means merging the data from the data warehouse and the data from previous write with older timestamps in Mussel. Replace means importing the data from the data warehouse and deleting all the data with previous timestamps.</p><p>We utilize Spark to transform data from the data warehouse into HFile format and upload to S3. Each Mussel storage node downloads the files and uses HRegion bulkLoadHFiles API to load those HFiles into the column family.</p><p>With this bulk load pipeline, we can just load the delta data into the cluster instead of the full data snapshot every day. Before the migration, the user profile service needed to load about 4TB data into the cluster daily. After, it only needs to load about 40–80GB, drastically reducing the cost and improving the performance of the cluster.</p><h3>Conclusion and Next Steps</h3><p>In the last few years, Airbnb has come a long way in providing a high-quality derived data store for our engineers. The most recent key-value store Mussel is widely used within Airbnb and has become a foundational building block for any key-value based application with strong reliability, availability, scalability, and performance guarantees. Since its introduction, there have been ~4000 tables created in Mussel, storing ~130TB data in our production clusters without replication. Mussel has been working reliably to serve large amounts of read, write, and bulk load requests: For example, mussel-general, our largest cluster, has achieved &gt;99.9% availability, average read QPS &gt; 800k and write QPS &gt; 35k, with average P95 read latency less than 8ms.</p><p>Even though Mussel can serve our current use cases well, there are still many opportunities to improve. For example, we’re looking forward to providing the read-after-write consistency to our customers. We also want to enable auto-scale and repartition based on the traffic in the cluster. We’re looking forward to sharing more details about this soon.</p><h3>Acknowledgments</h3><p>Mussel is a collaborative effort of Airbnb’s storage team including: <a href="http://linkedin.com/in/calvinzou">Calvin Zou</a>, <a href="linkedin.com/in/dionitas">Dionitas Santos</a>, <a href="http://linkedin.com/in/ruan-maia-367281161">Ruan Maia</a>, <a href="http://linkedin.com/in/wonheec">Wonhee Cho</a>, <a href="linkedin.com/in/xiaomou-wang-5880b537">Xiaomou Wang</a>, <a href="linkedin.com/in/yanhan-zhang-724088a4">Yanhan Zhang</a>.</p><p>Interested in working on the Airbnb Storage team? Check out this role: <a href="https://careers.airbnb.com/positions/3029584/">Staff Software Engineer, Distributed Storage</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=406b9fa1b296" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296">Mussel — Airbnb’s Key-Value Store for Derived Data</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Beyond A/B test : Speeding up Airbnb Search Ranking Experimentation through Interleaving]]></title>
            <link>https://medium.com/airbnb-engineering/beyond-a-b-test-speeding-up-airbnb-search-ranking-experimentation-through-interleaving-7087afa09c8e?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/7087afa09c8e</guid>
            <category><![CDATA[evaluation]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[search-ranking]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Qing Zhang]]></dc:creator>
            <pubDate>Thu, 06 Oct 2022 15:52:18 GMT</pubDate>
            <atom:updated>2022-10-06T22:47:02.801Z</atom:updated>
            <content:encoded><![CDATA[<h3>Beyond A/B Test : Speeding up Airbnb Search Ranking Experimentation through Interleaving</h3><p>Introduction of Airbnb interleaving experimentation framework, usage and approaches to address challenges in our unique business</p><p><a href="https://medium.com/@zq.zhangqing">Qing Zhang</a>, <a href="https://medium.com/@michelle.du">Michelle Du</a>, Reid Andersen, <a href="https://medium.com/@liweihe">Liwei He</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4v8bM6rq3FsK7Zwa7UO7bA.jpeg" /></figure><h3>Introduction</h3><p>When a user searches for a place to stay on Airbnb, we aim to show them the best results possible. Airbnb’s relevance team actively works on improving search ranking experience and helps users to find and book listings that match their preference. A/B test is our approach for online assessment. Our business metrics are conversion-focused, and the frequency of guest travel transactions is lower than on other e-commerce platforms. These factors result in insufficient experiment bandwidth given the number of ideas that we want to test and there is considerable demand to develop a more efficient online testing approach.</p><p>Interleaving is an online ranking assessment approach [1–3]. In A/B tests, users are split into control and treatment groups. Those who are in each group will be consistently exposed to results from the corresponding ranker. Interleaving, on the other hand, blends the search results from both control and treatment and presents the “interleaved” results to the user (Figure 1). The mechanism enables direct comparison between the two groups by the same user, with which the impact of the treatment ranker can be evaluated by a collection of specifically designed metrics.</p><p>There are several challenges in building the framework on both engineering and data science fronts. On the engineering side, we needed to extend our current AB test framework to enable interleaving set up while adding minimum overhead to the ML engineers. Additionally, our search infrastructure is designed for single request search and required significant extension to support interleaving functionality. On the data science side, we designed user event attribution logic that’ key to the effectiveness of metrics.</p><p>In 2021, we built the interleaving experimentation framework and integrated it in our experiment process and reached a 50x sensitivity in the development of our search ranking algorithm. Further validation confirms high agreement with A/B tests. We have been using interleaving for a wide range of tasks such as ranker assessment, hyperparameter tuning as well as evaluating infra-level changes. The system design and learnings detailed in this blog post should benefit readers looking to improve their experimentation agility.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/815/0*UUyBfFnZWWa13Mxk" /></figure><p>Figure 1: An illustration of A/B testing v.s. Interleaving. In traditional A/B tests, users are split into two groups and exposed to two different rankers. In Interleaving, each user is presented with the blended results from two rankers.</p><h3>Search Ranking Experimentation Procedure</h3><p>With interleaving, Airbnb search ranking experimentation uses a three phase procedure for faster experimentation (Figure 2). First, we run standard offline evaluation on the ranker with NDCG (normalized discounted cumulative gain). Rankers with reasonable results move on to online evaluation with interleaving. The ones that get promising results go on for the A/B test.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LDlMakrih7JAiFkx" /></figure><p>Figure 2: Ranking experimentation procedure. We use interleaving to get preliminary online results in order to enable fast iteration</p><p>Currently, we split our search traffic into two portions, and use the vast majority for regular A/B tests and remaining for interleaving experiments. We divide the interleaving traffic into buckets (called interleaving lanes) and each lane is used for one interleaving experiment. Each interleaving experiment takes up about 6% of regular A/B test traffic, and one-third of running length. We achieve a 50x speedup over an A/B test given the same amount of traffic. The team now has the luxury to test out multiple variations of the idea in a short time frame and identify the promising routes to move forward.</p><h3>Airbnb Interleaving Framework</h3><p>The interleaving framework controls the experimentation traffic and generates interleaved results to return to the user as illustrated in Figure 3. Specifically, for users who are subject to interleaving, the system creates parallel search requests that correspond to control and treatment rankers and produce responses. The results generation component blends the two responses with team drafting algorithms, returns the final response to the user, and creates logging. A suite of metrics were designed to measure impact.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xrxh0CYHcHielAP2" /></figure><p>Figure 3: Interleaving system overview. The interleaving framework controls the experimentation traffic and generates interleaved results to return to the user</p><h3>Team Drafting and Competitive Pairs</h3><p>The framework employs the<em> team drafting algorithm</em> to “interleave” the results from control and treatment (we call them teams). For the purpose of generalizability, we demonstrate the drafting process with two teams A and B. The steps of the algorithm are as follows:</p><p>1 Flip a coin to determine if team A goes first</p><p>2 Start with an empty merged list. Repeat the following step until desired size is reached,</p><p>2. 1 From each of the two rankers A and B take the highest-ranked result that has not yet been selected (say listing a from ranker A and e from ranker B).</p><p>2.2 If the two listings are different, then select listings a and e, with assigned a to A and e assigned B. We will call (a, e) a <em>competitive pair</em>. Add the pair to the merged list with the order decided in Step 1</p><p>2.3 If the two listings are the same, then select that listing and do not assign it to either team. Figure 4 demonstrates the process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/710/0*pqLeLNEtmZEQ2D9o" /></figure><p>Figure 4: Team drafting example with competitive pair explained. Here we assume that team A goes first based on coin flip.</p><p>The<em> team drafting algorithm</em> enables us to measure user preference in a fair way. For each request we flip a coin to decide which team (control or treatment) has the priority in the ordering of a <em>competitive pair</em>. This means that position bias is minimized as listings from each team are ranked above the one from the other team in the competitive pair half of the time.</p><p>Creating <em>competitive pairs</em> makes <a href="https://alexdeng.github.io/causal/sensitivity.html#vrreg">variance reduction</a> (a procedure to speed up experimentation by increasing the precision of the point estimates) more intuitive, since it deduplicates items with the same rank and only assigns scores to the impression of competitive pairs instead of to each impression. In the example in Figure 4, the comparison between ranker A and ranker B reduces to a referendum on whether <em>a</em> is better than <em>e</em>. Leaving the other results unassigned improves the sensitivity in this case. In an extreme case where two rankers produce lists with exactly the same order, traditional interleaving would still associate clicks to teams and add noise to the result; while with competitive pairs, the entire search query can be ignored since the preference is exactly zero. This allows us to focus on the real difference with sensitivity improvement.</p><p>Furthermore, competitive pairs enable us to allocate credits to various user activities downstream much more easily. Again unlike traditional interleaving, which mostly assigns credits for clicks [3–5], we assign credits by bookings, which is a downstream activity. The flexibility in credit association has empowered us to design complicated metrics without having to rely on click signals. For example, we are able to define metrics that measure the booking wins over competition with certain types of listings (e.g. new listings) in the pairs. This enabled us to further understand whether changes to the ranking of a specific category of listings played its role in interleaving overall.</p><p>To determine a winning ranker in our interleaving approach, we compare the <em>preference margin</em> (margin of victory for the winning team) on target events and apply a 1-sample t-test over it to obtain the p-value. Validation studies confirmed that our framework produces results that are both reliable and robust — with a consistently low false positive rate, and minimum carryover effect between experiments.</p><h3>Attribution</h3><p><em>Attribution logic</em> is a key component of our measurement framework. As mentioned earlier, a typical scenario that is more unique to Airbnb compared to cases like Web search or streaming sites is that our guests can issue multiple search requests before booking, and the listing they book may have been viewed or clicked multiple times when owned by different interleaving teams, which is different from use cases where the primary goal is click-based conversion.</p><p>Let’s use a toy example to demonstrate the concept. As shown in Figure 5, the guest clicked the booked listing 3 times with each ranker having the listing on their team multiple times (2 times on team A, 1 time on team B) throughout the search journey. For this single guest alone, we see how the different attribution methods can end up with different conclusions:</p><ul><li>If we attribute the booking to the team when it was first clicked, we should assign it to team B and declare team B as the winner for this guest;</li><li>If we attribute the booking to the team when it was last clicked, we should assign it to team A and declare team A as the winner for the guest;</li><li>If we attribute the booking every time it was clicked, we should assign it twice to team A and once to team B, and end up declaring team A being the winner for the guest.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/898/0*7WbbbuEdUHlirDp3" /></figure><p>Figure 5: A simplified example of guest journey. The guest emits multiple searches and views the booked listing multiple times before finally making a booking.</p><p>We created multiple attribution logic variations and evaluated them on a wide collection of interleaving experiments that also had A/B runs as “ground truth”. We set our primary metric to be the one that has best alignment between interleaving and A/B tests.</p><h3>Alignment with A/B tests</h3><p>To further evaluate the consistency between interleaving and A/B tests, we tracked eligible interleaving and A/B pairs and confirmed that the two are consistent with each other 82% of the time (Figure 6). The experiments are also highly sensitive as noted in previous work from other companies like Netflix. To provide a concrete example, we have a ranker that randomly picks a listing in the top 300 results and inserts it to the top slot. It takes interleaving only 0.5% of the A/B running time and 4% of A/B traffic to get to the same conclusion as its corresponding A/B test.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/533/0*TeUKjwEQznkN9wUW" /></figure><p>Figure 6: Interleaving and A/B consistency. We tracked eligible interleaving and A/B pairs and the results demonstrate that the two are consistent with each other 82% of the time</p><p>In most cases where interleaving turned out to be inconsistent with traditional A/B testing, we found that the reason was set-level optimization. For example, one ranker relies on a model to determine how strongly it will demote listings with high host rejection probability and the model is the booking probability given the current page. Interleaving breaks this assumption and leads to inaccurate results. Based on our learnings, we advise that rankers that involve set-level optimization should use interleaving on a case by case basis.</p><h3>Conclusion</h3><p>Search ranking quality is key for an Airbnb user to find their desired accommodation and iterating on the algorithm efficiently is our top priority. The interleaving experimentation framework tackles our problem of limited A/B test bandwidth and provides up to 50x speed up on the search ranking algorithm iteration. We conducted comprehensive validation which demonstrated that interleaving is highly robust and has strong correlation with traditional A/B. Interleaving is currently part of our experimentation procedure, and is the main evaluation technique before the A/B test. The framework opens a new field of online experimentation for the company and can be applied to other product surfaces such as recommendations.</p><p>Interested in working at Airbnb? Check out our open roles <a href="https://careers.airbnb.com/">HERE</a>.</p><h3>Acknowledgments</h3><p>We would like to thank Aaron Yin for the guidance on the implementations of algorithms and metrics, Xin Liu for continuously advising us on optimizing and extending the framework to support more use cases, Chunhow Tan for valuable suggestions on improving the computational efficiency of interleaving metrics and Tatiana Xifara for advice on experiment delivery design.</p><p>The system won’t be possible without the support from our search backend team, especially Yangbo Zhu, Eric Wu, Varun Sharma and Soumyadip (Soumo) Banerjee. We benefit tremendously from their design advice and close collaboration on the operations.</p><p>We would also like to thank Alex Deng, Huiji Gao and Sanjeev Katariya for valuable feedback on the interleaving and this article.</p><h3>References</h3><p>[1] JOACHIMS, T. Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (KDD). ACM, New York, NY, 132–142. 2002.</p><p>[2] JOACHIMS, T. Evaluating Retrieval Performance using Clickthrough Data. In Text Mining, J. Franke, G. Nakhaeizadeh, and I. Renz, Eds., Physica/Springer Verlag, 79–96. 2003.</p><p>[3] RADLINSKI, F., KURUP, M., AND JOACHIMS, T. How does clickthrough data reflect retrieval quality. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM’08). ACM, New York, NY, 43–52. 2008.</p><p>[4] Radlinski, Filip, and Nick Craswell. “Optimized interleaving for online retrieval evaluation.” Proceedings of the sixth ACM international conference on Web search and data mining. 2013.</p><p>[5] Hofmann, Katja, Shimon Whiteson, and Maarten De Rijke. “A probabilistic method for inferring preferences from clicks.” Proceedings of the 20th ACM international conference on Information and knowledge management. 2011.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7087afa09c8e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/beyond-a-b-test-speeding-up-airbnb-search-ranking-experimentation-through-interleaving-7087afa09c8e">Beyond A/B test : Speeding up Airbnb Search Ranking Experimentation through Interleaving</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upgrading Data Warehouse Infrastructure at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/upgrading-data-warehouse-infrastructure-at-airbnb-a4e18f09b6d5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/a4e18f09b6d5</guid>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[airbnb]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ronnie Zhu]]></dc:creator>
            <pubDate>Mon, 26 Sep 2022 18:31:10 GMT</pubDate>
            <atom:updated>2022-09-26T18:31:10.909Z</atom:updated>
            <content:encoded><![CDATA[<p>This blog aims to introduce Airbnb’s experience upgrading Data Warehouse infrastructure to Spark and Iceberg.</p><p>By: <a href="https://www.linkedin.com/in/huirong-ronnie-zhu-97b0a980/">Ronnie Zhu</a>, <a href="https://www.linkedin.com/in/edgarrd/">Edgar Rodriguez</a>, <a href="https://www.linkedin.com/in/qiang-jason-xu-7101b025/">Jason Xu</a>, <a href="https://www.linkedin.com/in/gustavo-torres-torres/">Gustavo Torres</a>, <a href="https://www.linkedin.com/in/kerimoktay">Kerim Oktay</a>, <a href="https://www.linkedin.com/in/zhangxu325/">Xu Zhang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ky-obyCnt-A0R4qjnoY1_g.jpeg" /></figure><h3>Introduction</h3><p>In this blog, we will introduce our motivations for upgrading our Data Warehouse Infrastructure to Spark 3 and Iceberg. We will briefly describe the current state of Airbnb data warehouse infrastructure and the challenges. We will then share our learnings from upgrading one critical production workload: event data ingestion. Finally, we will share the results and the lessons learned.</p><h3>Context</h3><p>Airbnb’s Data Warehouse (DW) storage was previously migrated from legacy <a href="https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c">HDFS clusters</a> to S3 to provide better stability and scalability. While our team has continued to improve the reliability and stability of the workloads that operate on data in S3, certain characteristics of these workloads and the infrastructure they depend on introduce scalability and productivity limitations that our users encounter on a regular basis.</p><h4>Challenges</h4><h4>Hive Metastore</h4><p>With an increasing number of partitions, Hive’s backend DBMS’s load has become a bottleneck, as has the load on partition operations (e.g., querying thousands of partitions for a month’s worth of data). As a workaround, we usually add a stage of daily aggregation and keep two tables for queries of different time granularities (e.g., hourly and daily). To save on storage, we limit intraday Hive tables to short retention (three days), and keep daily tables for longer retention (several years).</p><h4>Hive/S3 Interactions</h4><p>Hive was not originally designed for object storage. Instead, many assumptions were made around HDFS when implementing features such as renames and file listings. When we migrated from HDFS to S3 it therefore required certain guarantees to ensure that datasets were consistent on list-after-write operations. We customized the way Hive writes to S3, first writing to an HDFS temporary cluster and then moving the data to S3 via an optimized distcp process that writes to unique locations during the commit phase, storing file-listing information in a separate store for fast access. This process has performed well over the past two years, but it requires additional cluster resources to run.</p><h4>Schema Evolution</h4><p>At Airbnb, we use three compute engines to access data in our Data Warehouse: Spark, Trino and Hive. Since each compute engine handles schema changes differently, changes to table schemas have almost always resulted in data quality issues or required engineers to perform costly rewrites.</p><h4>Partitioning</h4><p>Hive tables are partitioned by fixed columns, and partition columns cannot be easily changed. In case one needs to repartition a dataset, one has to create a new table and reload the entire dataset.</p><h3>New Data Stack</h3><p>These challenges have motivated us to upgrade our Data Warehouse infrastructure to a new stack based on Iceberg and Spark 3, which addresses these problems and also provides usability improvements.</p><h4>Iceberg</h4><p><a href="https://iceberg.apache.org/docs/latest/">Apache Iceberg</a> is a table format designed to address several of the shortcomings of traditional file system-based Data Warehousing storage formats such as Hive. Iceberg is designed to deliver high-performance reads for huge analytics tables, with features such as serializable isolation, snapshot-based time travel, and predictable schema evolution. Some important Iceberg features that help in some of the challenges mentioned early:</p><ul><li>Partition information is not stored in the Hive metastore, hence removing a large source of load to the metastore.</li><li>Iceberg tables do not require S3 listings, which removes the list-after-write consistency requirement, which can in turn eliminate the need for the extra discp job, and avoids entirely the latency of the list operation.</li><li>Consistent table schema is defined in <a href="https://iceberg.apache.org/spec/#schema-evolution">Iceberg spec</a>, which guarantees consistent behavior across compute engines avoiding unexpected behavior when changing columns.</li></ul><h4>Spark 3</h4><p><a href="https://spark.apache.org/">Apache Spark</a> has become the de facto standard for big data processing in the past 10 years. Spark 3 is a new major version released in 2020, it comes with a long list of features — new functionalities, bug fixes and performance improvements. We focus on introducing Adaptive Query Execution (AQE) here; you can find more info on the <a href="https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html">Databricks blog</a>.</p><p>AQE is a query optimization technique that uses runtime statistics to optimize the Spark query execution plan. This solves one of the greatest struggles of Spark cost-based optimization — inaccurate statistics collected before query starts often lead to suboptimal query plans. AQE will figure out data characteristics and improve query plans as the query runs, increasing query performance.</p><p>Spark 3 is also a prerequisite for Iceberg adoption. Iceberg table write and read support using Spark SQL is only available on Spark 3.</p><p>The diagram below shows the change we made:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cIYLTqi2XeFm3vxb" /><figcaption><strong><em>Figure 1.</em></strong><em> Evolution of data compute and storage tech stack</em></figcaption></figure><h3>Production Case Study — Data Ingestion</h3><p>At Airbnb, the Hive-based data ingestion framework processes &gt;35 billion Kafka event messages and 1,000+ tables per day, and lands datasets ranging from kilobytes to terabytes into hourly and daily partitions. The volume and coverage of datasets of different sizes, and time granularity requirement makes this framework a good candidate to benefit from our Spark+Iceberg tech stack.</p><h3>Spark 3</h3><p>The first step in migrating to the aforementioned Spark+Iceberg compute tech stack was to move our Hive queries to Spark. This introduced a new challenge: Spark tuning. Unlike Hive, which relies on data volume stats, Spark uses preset shuffle partition values to determine task split sizes. Thus, choosing the proper number of shuffle partitions became a big challenge in tuning the event data ingestion framework on Spark. Data volume of different events varies a lot, and the data size of one event also changes over time. Figure 2 shows the high variance of shuffle data size of Spark jobs processing a sampling of 100 different types of events.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*qAIItnnv9VtcG9nz" /><figcaption><strong>Figure 2.</strong> High variance of raw data size of 100 randomly sampled events; each bar represents a single dataset</figcaption></figure><p>There isn’t a fixed number of shuffle partitions that would work well for all events in the ingestion framework; if we pick a fixed number for all ingestion jobs, it might be too big for some jobs but too small for others, and both would result in low performance. While we were exploring different solutions to tune shuffle partition parameters, we found that Adaptive Query Execution could be a perfect solution.</p><h4>How does AQE help?</h4><p>In Spark 3.0, the AQE framework ships with several key features, including dynamically switching join strategies and dynamically optimizing skew joins. However, the most critical new feature for our use case is dynamically coalescing shuffle partitions, which ensures that each Spark task operates on roughly the same amount of data. It does this by combining adjacent small partitions into bigger partitions at runtime. Since shuffle data can dynamically grow or shrink between different stages of a job, AQE is continually re-optimizing the size of each partition through coalescing throughout a job’s lifetime. This brought a great performance boost.</p><p>AQE handles all cases in our data ingestion framework well, including edge cases of spiky events and new events. One note is that flattening of nested columns and compression of file storage format (in our case, Parquet GZIP) might generate fairly small output files for small task splits. To ensure output file sizes are large enough to be efficiently accessed, we can increase the AQE advisory shuffle partition size accordingly.</p><h4>AQE Tuning Experience</h4><p>Let’s walk through an example to get a better understanding of AQE and its tuning experience. Say we run the example query to load one dataset. The query has one Map stage to flatten events and another Reduce stage to handle deduplication. After adopting AQE and running the job in Spark, we can see two highlighted steps get added to the physical plan.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/739/1*XZWObyyGiQUBqgKNP7HSQw.png" /><figcaption><strong>Figure 3.</strong> Change of physical plan of the example Spark job</figcaption></figure><p>Now let’s take a closer look at our tuning phase. As shown in Table 1, we went through several iterations of param setting. From our experience, if the actual shuffle partition used is equal to the initial partition number we set, we should increase the initial partition number to split initial tasks more and get them coalesced. And if the average output file size is too small, we can increase the advisory partition size to generate larger shuffle partitions, and thus larger output files. Upon inspecting shuffle data of each task, we could also decrease executor memory and the max number of executors.</p><p>We also experimented with the tuned job parameters on datasets of different sizes, as shown in Table 2 and 3. From the results, we can see that once tuned, AQE performs well on datasets from zero bytes size to TB in size, all while using a single set of job parameters.¹</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eZu1Q9D0lyD5ZEVE" /><figcaption><strong>Table 1.</strong> Tuning AQE using example medium-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K6Ff5uiN90I_ci_o" /><figcaption><strong>Table 2.</strong> Job stats of example small-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-whu7KM8AK8w1N-f" /><figcaption><strong>Table 3.</strong> Job stats of example empty-size dataset</figcaption></figure><p>From our result, it’s clear that AQE can adjust the shuffle split size very close to our predefined value in the Reduce stage and thus generate outputs of target file size as we expect. Furthermore, since each shuffle split is close to predefined value, we can also lower executor memory from default values to ensure efficient resource allocation. As an additional big advantage to the framework, we do not need to do any special handling to onboard new datasets.</p><h3>Iceberg — Partition specs &amp; Compaction</h3><h4>How does Iceberg help?</h4><p>In our data ingestion framework, we found that we could take advantage of Iceberg’s flexibility to define multiple partition specs to consolidate ingested data over time. Each data file written in a partitioned Iceberg table belongs to exactly one partition, but we can control the granularity of the partition values over time. Ingested tables write new data with an hourly granularity (ds/hr), and a daily automated process compresses the files on a daily partition (ds), without losing the hourly granularity, which later can be applied to queries as a residual filter.</p><p>Our compaction process is smart enough to determine whether a data-rewrite is required to reach an optimal file size, otherwise just rewriting the metadata to assign the already existing data files to the daily partition. This has simplified the process for ingesting event data and provides a consolidated view of the data to the user within the same table. As an added benefit, we’ve realized cost savings in the overall process with this approach.</p><p>As shown in the diagram below, in the consolidated Iceberg table we switch the partition spec from ds/hr to ds at the end of day. In addition, now user queries are easier to write and able to access fresher data with full history. Keeping only one copy of data also helps improve both compute and storage efficiencies and ensures data consistency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Mrwb_94NNQWrs_mF" /><figcaption><strong>Figure 4.</strong> Change of table storage format for table consolidation</figcaption></figure><h4>Table Consolidation Experience</h4><p>Consolidating hourly and daily data into one Iceberg table requires changes in both the write and read path. For the write path, to mitigate the aforementioned issues caused by small files, we force run a compaction during the partition spec switch. Tables 4 and 5 compare the statistics from our intelligent compaction jobs with the cost of a full rewrite of all the data files associated with the daily partition. For some large tables we obtain resource savings of &gt; 90% by leveraging Iceberg’s ability to avoid data copying during compaction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/649/0*yNW6DneHRxWZdlwq" /><figcaption><strong>Table 4.</strong> Compaction job comparison of example small-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/672/0*R8NW6NHCpI2OUUxm" /><figcaption><strong>Table 5.</strong> Compaction job comparison of example large-size dataset</figcaption></figure><p>For the read path, since most data consumers use Airflow’s partition sensors, we updated the implementation of partition sensing. Specifically, we implemented a signal system to sense empty partitions in Iceberg tables, as opposed to the prior method of looking up each Hive partition as an actual row in Hive metastore.</p><h3>Results</h3><p>Comparing the prior TEZ and Hive stack, we see more than 50% compute resource saving and 40% job elapsed time reduction in our data ingestion framework with Spark 3 and Iceberg. From a usability standpoint, we made it simpler and faster to consume stored data by leveraging Iceberg’s capabilities for native schema and partition evolution.</p><h3>Conclusion</h3><p>In this post, we shared the upgrades we applied to Airbnb’s data compute and storage tech stack. We hope that readers enjoyed learning how our event data ingestion framework benefits from Adaptive Query Execution and Iceberg and that they consider applying similar tech stack changes to their use cases involving datasets of varying size and time granularity.</p><p>If this type of work interests you, please check out our open roles <a href="https://careers.airbnb.com/">here</a>!</p><h3>Acknowledgments</h3><p>Special thanks to Bruce Jin, Guang Yang, Adam Kocoloski and Jingwei Lu for their continued guidance and support!</p><p>Also countless thanks to Mark Giangreco, Surashree Kulkarni and Shylaja Ramachandra for providing edits and great suggestions to the post!</p><p>[1] One callout is that Spark AQE has a bug handling empty input (<a href="https://issues.apache.org/jira/browse/SPARK-35239">SPARK-35239</a>), and fixes are available in 3.2. Thus to take full advantage of AQE in lower Spark versions, we need to backport <a href="https://github.com/apache/spark/pull/32362">fix 1</a> and <a href="https://github.com/apache/spark/pull/31994">fix 2</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a4e18f09b6d5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/upgrading-data-warehouse-infrastructure-at-airbnb-a4e18f09b6d5">Upgrading Data Warehouse Infrastructure at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb safeguards changes in production]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-c83e94bfc52?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c83e94bfc52</guid>
            <category><![CDATA[experiment]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Zack Loebel-Begelman]]></dc:creator>
            <pubDate>Tue, 06 Sep 2022 17:16:01 GMT</pubDate>
            <atom:updated>2022-09-06T17:16:00.927Z</atom:updated>
            <content:encoded><![CDATA[<h3>Part II: Near Real-time Experiments</h3><p>By: <a href="https://www.linkedin.com/in/michaelcl/">Mike Lin</a>, <a href="https://www.linkedin.com/in/preetiramasamy/">Preeti Ramasamy</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/zack-loebel-begelman-85407698/">Zack Loebel-Begelman</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/796/1*TwziuVGxkiaD4XKu4A2-pA.jpeg" /></figure><p>In our <a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446">first post</a> we discussed the need for a near real time Safe Deploy system and some of the statistics that power its decisions. In this post we will cover the architecture and engineering choices behind the various components that Safe Deploys comprises.</p><p>Designing a near real-time experimentation system required making explicit tradeoffs among speed, precision, cost, and resiliency. An early decision was to limit near real-time results to only the first 24 hours of an experiment — enough time to catch any major issues and transition to using comprehensive results from the batch pipeline. The idea being once batch results were available, experimenters would no longer need real time results. The following sections describe the additional design decisions in each component of the Safe Deploys system.</p><h3>High Level Design</h3><p>There are 3 major components that make up the technical footprint of the Safe Deploys system:</p><ol><li><strong>Ramp Controller</strong>, a <a href="https://flink.apache.org/">Flink</a> job that acts as a centralized coordinator, providing experiment configuration to NRT via Kafka and invoking statistical computations by calling Measured via HTTP.</li><li><strong>Near Real Time (NRT) pipeline</strong>, another Flink job that extracts measures, joins and enriches those measures with assignment information (treatment and subject information), and stores the enriched measures into S3.</li><li><strong>Measured</strong>, a python library (invoked via a Python HTTP server and worker pool) that consumes enriched measures from S3, aggregates them, and runs stats to determine if any change is significant.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1002/0*dO4dDyRoREQgfsh9" /><figcaption>Fig 1: Architecture Diagram of the Safe Deploy system</figcaption></figure><h3>Ramp Controller</h3><p>The Ramp Controller performs automated experiment ramping based on the results from Measured. It increases experiment exposure in stages, slowly exposing more subjects and monitoring metric impacts at each stage. If any egregiously negative metric is observed, the Ramp Controller will immediately shut down the experiment to minimize the impacts of bad changes. It supports several ramping algorithms, but most users leverage a simple time based algorithm.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/876/0*Uu8Z0bAhhrDH9CSQ" /><figcaption>Figure 2: Ramping process</figcaption></figure><p>Ramp Controller was designed to be stateless, and resilient to any job failures. Within seconds of an experiment starting, it publishes metadata to Kafka, triggering NRT to start joining events for that experiment. The metadata includes a path in S3 that NRT will write to. At this point the Ramp Controller’s core loop will begin:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ypoetTDVqKYg0ID8_EpTNA.png" /></figure><p>Results are computed for the first 24 hours of the experiment, with new metrics consumed as new files are published to S3. A metric is marked egregious when the percent change is smaller than -20% with an adjusted p-value of less than or equal to 0.01. By leveraging the Measured framework for metric computation, we get custom aggregations, richer statistical models, and the ability to compute performance metrics, and dimensional cuts for metrics.</p><p>After overcoming these technical challenges in scaling the pipeline and tuning decision making, we were ready to vet the system with experiment owners and drive adoption.</p><h3>Near Real Time (NRT) Pipeline</h3><p>We built the new NRT pipeline in Java and Scala using Apache Flink. It reads from a multitude of Kafka streams: event streams containing raw user based events (impressions, booking requests etc.), a stream that the Ramp Controller emits containing experiment metadata, and the streaming that contains the raw assignment events emitted for all experiments which are also consumed by the batch pipeline.</p><p>Previously Airbnb had attempted to build an online data store for all experiment assignments, however this did not scale and was eventually shut down. By reducing the scope, specifically limiting the NRT pipeline to the first 24 hours of an experiment, we are able to store a bounded subset of assignments. Using a <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/">broadcast join</a> of experiment metadata lets us filter the assignment events and Flink makes <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/#state-time-to-live-ttl">aging out data</a> trivial.</p><p>The extraction is written in a stand-alone library so that the measure definitions can be re-used in both batch and streaming. In order to be highly performant, the measure extraction determines which events to extract first using an inverted index based on the existence and values of json fields then only running extraction on the relevant events. Not only do we extract measures, but also dimensions from each event. Because we want to limit the complexity in this job, we only support dimensions from the same event as a measure itself.</p><p>Our first difficulty was in how to handle measures and assignments coming in out of order. We want data to age out at different times when joining assignment events to measures and assignment data should be stored for the full 24 hours. Because the volume of measure events means we can’t keep them for 24 hours, we keep a short buffer dropping measures after 5 minutes. The outer join required to achieve this goal required building a custom join using the <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/co/KeyedCoProcessFunction.html">keyed co-process api</a>.</p><p>Once the data is joined we buffer it internally within Flink to reduce the total number of files for small experiments. We wrote a simple keyed process stage that hashes the events based on the timestamp against how many concurrent files we want to output. It’s important that we hash on the timestamp since Flink requires the keying mechanism to be deterministic. The events are buffered based on event counts and time, emitting the buffered list either once a partition hits an event based or time based threshold. This stage allows us to have more fine grained control over the number of files we output.</p><p>We leverage Flink’s <a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/filesystem/#parquet-format">built in support for parquet and S3 as a file sink</a> to write the files. In order to provide exactly-once semantics, Flink will only write files when checkpointing occurs. Files output by the NRT pipeline are consumed by the ramp controller to make decisions. To keep our latency low, we checkpoint every 5 minutes.</p><h3>Measured</h3><p>Measured is a framework for defining and computing metrics. It consists of a Scala library for extracting measures and dimensions from raw events that the NRT pipeline leverages, and a Python library for defining metrics (based off of those measures), statistical models, and visualizations. This section focuses on the Python library, and how it is used to compute metrics.</p><p>In order to provide consistent results across platforms we run the same Measured jobs that user’s run via a Python HTTP job server and worker pool. The NRT metric evaluation is one of those jobs, it downloads the event files from S3 using <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor">a Python worker pool</a>. Once the files are downloaded the job leverages <a href="https://duckdb.org/">duckdb</a>’s <a href="https://duckdb.org/docs/data/parquet">parquet reader functionality</a> to aggregate to the user level. Once we have a local user aggregate the job evaluates the various sequential models discussed in the first post. The results of these evaluations are stored in a MySQL database upon job completion to be retrieved by the UI or the Ramp Controller over HTTP.</p><h3>Adoption</h3><p>The full vision of Safe Deploys encompassed safeguarding any changes in production. However, to gain experience and trust, we initially focused our efforts on A/B tests. We knew that Safe Deploys, like any anomaly detection system, especially one that automates remediation steps, would face certain challenges in adoption, including:</p><ul><li>Trust in NRT metrics that were similar but not exactly the same as existing batch ones</li><li>Relinquishment of control in ramping and shutdown of experiments</li><li>False positives that could slow down experimentation by forcing restarts</li></ul><p>Before Safe Deploys, nearly a quarter of Airbnb teams had a manual process for ramping up experiments. This consisted of increasing the exposure of an experiment, manually verifying performance metrics, and repeating until reaching a target exposure. This often masked significant but not visually obvious negative impacts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/0*B_kTvecFqvuTWE17" /><figcaption>Figure 3: Illustration of how a controlled experiment provides greater sensitivity</figcaption></figure><p>We evangelized Safe Deploys as complementary to the existing process, providing increased sensitivity of detecting negative impacts, while still allowing experimenters to stop an experiment based on their own monitoring at any time. We also continually improved statistical methods used to decrease false positives and negatives. Since enabling Safe Deploys by default a year ago, it has been used for over 85% of experiment starts and helped prevent dozens of incidents, and flagged misconfigurations early, minimizing negative impacts to Airbnb’s business and wasted engineering resources on remediation.</p><h3>Tip of the Iceberg</h3><p>Safeguarding experiments was a significant step towards reducing incidents at Airbnb, however the full vision encompasses changes originating from other channels. The distribution of changes across different channels can be found in Figure 4.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/964/0*7Il9QJd_KJZcnBHp" /><figcaption>Figure 4: Distribution of changes pushed to production by channel</figcaption></figure><p>We tackled each remaining channel differently:</p><ul><li>Feature Flags were unified with Experiments to gain Safe Deploys capabilities</li><li>Content management systems were provided APIs to programmatically create experiments tied to content changes, and ramped with Safe Deploys</li><li>Code Deploys through Spinnaker ran Safe Deploys alongside pre-existing Automated Canary Analysis with each deploy</li></ul><p>(We considered infrastructure configs out of scope, since these lower level changes would require a fundamentally different approach to address.)</p><p>Code Deploys with Spinnaker account for an outsized majority of changes in production and required extensive work to enable. The next post in this series will cover how we achieved this through changes across traffic routing, service configs, and dynamically created configurations for Spinnaker.</p><p>Interested in working at Airbnb? Check out our <a href="https://careers.airbnb.com/">open roles</a>.</p><h3>Acknowledgements</h3><p>Safe Deploys was only made possible through the combined efforts across Airbnb’s infrastructure and data science teams. We would like to thank <a href="https://www.linkedin.com/in/jingwei-lu-5701222/">Jingwei Lu</a>, <a href="https://www.linkedin.com/in/wei-hou-93a069a4/">Wei Ho</a>, and the rest of the Stream Infrastructure team for helping implement, and subsequently scale the NRT pipeline. Also, thanks to Candace Zhang, <a href="https://www.linkedin.com/in/erikriverson/">Erik Iverson</a>, <a href="https://www.linkedin.com/in/minyong-lee-1a302466/">Minyong Lee</a>, Reid Andersen, <a href="https://www.linkedin.com/in/shant-torosean-606aa354/">Shant Toronsean</a>, <a href="https://www.linkedin.com/in/tatiana-xifara/">Tatiana Xifara</a>, and the many data scientists that helped build out metrics and verify their correctness. Also thanks to <a href="https://www.linkedin.com/in/kodnous/">Kate Odnus</a>, <a href="https://www.linkedin.com/in/kedar-bellare-3048128a/">Kedar Bellare</a> and <a href="https://www.linkedin.com/in/pmaccart/">Phil MacCart</a>, who were early adopters and provided us invaluable feedback. In addition <a href="https://www.linkedin.com/in/kocolosk/">Adam Kocoloski</a>, <a href="https://www.linkedin.com/in/rstata/">Raymie Stata</a> and <a href="https://www.linkedin.com/in/ronnyk/">Ronny Kohavi</a> for championing the effort across the company. We would also like to thank other members of the ERF team that contributed to Safe Deploys: <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/antoinecreux/">Antoine Creux</a>, <a href="https://www.linkedin.com/in/george-l-9b946655/">George Li</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, and <a href="https://www.linkedin.com/in/vincent-chan-70080423/">Vincent Chan</a>.</p><blockquote>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c83e94bfc52" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-c83e94bfc52">How Airbnb safeguards changes in production</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Veerabahu Chandran]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/70468aa3bc06</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[india]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Lauren Mackevich]]></dc:creator>
            <pubDate>Thu, 18 Aug 2022 18:50:01 GMT</pubDate>
            <atom:updated>2022-08-18T18:50:01.231Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Veerabahu Chandran</h3><p>Learning and growing in Airbnb’s new Bangalore Tech Center</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wwf3CMkjhKPlaxichQJd1g.jpeg" /></figure><p><em>Veera Chandran is an engineer in Airbnb’s new Bangalore Tech Center, where his team builds out technical systems to support hosts. As a lifelong learner, he has a passion for exploring new technologies and diving into practical problems. He’s excited to be tackling both the technical challenges of building new architecture and the organizational challenges of building out the capabilities of a new office.</em></p><p><em>Here’s Veera’s story:</em></p><h3>Learning and exploring</h3><p>I grew up in Tamil Nadu, in the South of India. I was always a curious kid, trying to understand how everything worked, so when it came to choosing a course of study, engineering was a natural fit. I feel lucky that I had a lot of education opportunities in front of me, and I was able to choose the path I wanted to take.</p><p>My first exposure to computers came when I was in 8th grade. These were still the relatively early days of the computer, and my dad brought one home so he could learn to use it. I found it fascinating and learned BASIC and Logo. These are simple languages, but I was excited that I could use them to make drawings and text appear on the screen. Those rudimentary programs opened me to the world of what’s possible with computer science.</p><h3>Studying and practical experience</h3><p>I went to the College of Engineering, Guindy to study Computer Science and Engineering. My studies covered a lot of subjects, but the one that excited me most was networking. I was really curious to understand how data moves from one place to another.</p><p>My first practical networking experience came while still in school. There were a bunch of gamers I knew, and they wanted to set up a LAN to play <em>Age of Empires</em> and <em>Quake </em>together. I got together with a couple of my friends and built out an inexpensive networking solution for them, covering everything from cabling to routers to setting up configurations. I’m actually not that big of a gamer myself, but building out a network was really exciting for me. I love to understand things on a practical level, because while theoretical understanding is important, I always find it most meaningful to see how things actually work.</p><h3>The power of engineering</h3><p>After graduation, I got a job in networking. There were several interesting companies in the space at the time, and I ended up joining one run by a group of IIT (Indian Institute of Technology) professors. It was a great opportunity to learn from some of the brightest minds in the field. I always tried to make sure I was learning, so I sought out whatever would help me continue to grow. Eventually, I moved on to opportunities at larger internet companies where I could use my networking knowledge but also expand into topics like large-scale, distributed systems.</p><p>Being a software engineer has always been exciting to me because it gives you the power to solve so many problems. When my daughter was born, my wife and I were looking for names that had to fit multiple constraints–e.g. it had to start with a <em>specific </em>letter–and it was a struggle to come up with viable options. As an engineer, I realized there was an easier way to find all our choices. I wrote a quick program that downloaded a list of millions of names and then ran them through our criteria. From that, I was able to narrow it down to a list of a few thousand options. My wife was amazed that I could generate so many names with just a couple hours of work.</p><p>The challenges of engineering are also interesting. You have to work hard to keep yourself informed. The industry moves so fast. When I started, I was using Java 4, and now we’re on Java 18. The way you would solve a problem in either of these versions is so different. All these newer languages have also emerged, and you can apply each to different situations. It feels like every day new machine learning research pushes the boundaries in unimaginable ways. I don’t know what’s going to come next, but I know it’s going to evolve quickly.</p><h3>Finding impact at Airbnb</h3><p>After a while in my previous role, I began to feel like my learning curve was getting saturated, so I wanted to look for a new challenge somewhere I could have a larger impact. I heard Airbnb was opening a tech center in Bangalore, and I was excited by the opportunity to be one of the first engineers there.</p><p>I joined in September 2021 as the first engineer in the Hosting org in Bangalore. I focus on tools for compliance, which is a complex problem space. Every region has their own laws on short-term rentals that hosts have to follow, and the laws can vary at different levels — the US has their laws, and then California might customize some of them, and San Francisco might have their own on top of that, and so on. These laws can also change quickly, like they did during Covid, so our products need to be versatile and adapt to new conditions.</p><p>Airbnb has been a great place to work. It’s startup-y in that there are challenging technical problems to work on, but the job is stable and the company respects your work-life balance. As a technical leader, there’s a great opportunity to be part of the evolution of our technology roadmap. The architecture recently transitioned from a monorepo to a Service Oriented Architecture, so we’re still figuring out the best approaches for the problems we’re solving.</p><p>I’ve also appreciated Airbnb’s culture, especially the focus on inclusivity and belonging. My coworkers want to make everyone feel comfortable. When they introduced themselves to me, they all included their pronouns, making it easier for anyone else to share theirs. The people here live the culture and make everyone feel included.</p><h3>Building our office in Bangalore</h3><p>One of my favorite things about working as an engineer in the Bangalore office is the ownership and accountability. We’re not just a delivery center, where we’re being passed requirements from elsewhere and building that one piece of software. We like to call ourselves a capability center. Our team is tasked with the whole span of product development, from identifying what user problems exist all the way through to delivering a solution for them. We work on the same roadmap, codebase, and tech stack as Airbnb HQ.</p><p>Our team is growing quickly, both in Bangalore and remotely across India. With the team being spread out, trust and team-building have been important. We have a social meeting every Friday, and the whole team shows up so we can get to know one another. It’s great for connecting with teammates, and the trust we’re creating helps us build more successful products.</p><p>Airbnb leadership has a clear roadmap for the future of the Bangalore Tech Center, and the team is growing quickly. It’s been exciting to build our first tech center outside of headquarters. We’re hiring for a number of teams and we’d love to hear from you!</p><p>Check out these related roles based out of Bangalore:</p><p><a href="https://grnh.se/777f0dbd1us">Engineering Manager, Ambassador Platform Products</a></p><p><a href="https://grnh.se/9b78e7f21us">Staff Software Engineer, Ambassador Platforms</a></p><p><a href="https://grnh.se/e0c9d3761us">Manager, BizTech</a></p><p><a href="https://grnh.se/d43963981us">Senior Software Engineer, Cities</a></p><p><a href="https://grnh.se/6a500ddd1us">Sr. Analytics &amp; Insight Analyst: CSA</a></p><p><a href="https://grnh.se/b6de7b661us">Operations Engineer, Biztech</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=70468aa3bc06" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06">My Journey to Airbnb — Veerabahu Chandran</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>