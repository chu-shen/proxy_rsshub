<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 22 Jun 2022 01:49:26 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Airbnb’s Trip to Linaria]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/dc169230bd12</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[web-performance]]></category>
            <category><![CDATA[atomic-css]]></category>
            <category><![CDATA[css-in-js]]></category>
            <category><![CDATA[css]]></category>
            <dc:creator><![CDATA[Joe Lencioni]]></dc:creator>
            <pubDate>Thu, 16 Jun 2022 17:41:49 GMT</pubDate>
            <atom:updated>2022-06-16T17:41:48.753Z</atom:updated>
            <content:encoded><![CDATA[<h4>Learn how Linaria, Airbnb’s newest choice for web styling, improved both developer experience and web performance</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-qT4pQIPIsxHBZj22sQtag.jpeg" /></figure><p>CSS is a critical component of every web application, and many solutions have evolved for how styles are written by developers and delivered to visitors. In this post we’ll take you through Airbnb’s journey from Sass to CSS-in-JS and show you why we landed on <a href="https://github.com/callstack/linaria">Linaria, a zero-runtime CSS-in-JS library</a>, and the impact it has had on the developer experience and performance of Airbnb’s web app.</p><h3>From Sass to CSS-in-JS</h3><p>In 2016, our web frontend was in a monolithic <a href="https://rubyonrails.org/">Ruby on Rails</a> app using a combination of <a href="https://github.com/rails/sprockets">Sprockets</a>, <a href="https://browserify.org/">Browserify</a>, and <a href="https://sass-lang.com/">Sass</a>. We had a <a href="https://getbootstrap.com/">Bootstrap</a>-inspired internal toolkit for styling, but we weren’t using anything like <a href="https://github.com/css-modules/css-modules">CSS Modules</a> or <a href="http://getbem.com/">BEM</a>.</p><p>Production bugs were often caused by our styling — sometimes the correct stylesheet was missing from some pages and other times styles from different stylesheets conflicted unexpectedly.</p><style>body[data-twttr-rendered="true"] {background-color: transparent;}.twitter-tweet {margin: auto !important;}</style><blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true"><p>&#x200a;&mdash;&#x200a;<a href="https://twitter.com/thomasfuchs/status/493790680397803521">@thomasfuchs</a></p></blockquote><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script><script>function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height);resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}twttr.events.bind('rendered', function (event) {notifyResize();}); twttr.events.bind('resize', function (event) {notifyResize();});</script><script>if (parent && parent._resizeIframe) {var maxWidth = parseInt(window.frameElement.getAttribute("width")); if ( 500  < maxWidth) {window.frameElement.setAttribute("width", "500");}}</script><p>Additionally, developers <a href="https://css-tricks.com/how-do-you-remove-unused-css-from-a-site/">rarely removed styles once added since it was hard to know whether they were still needed</a>. These issues compounded as our product surface area rapidly expanded.</p><p>As we began to build our <a href="https://www.youtube.com/watch?v=fHQ1WSx41CA">Design System</a> in React, we landed on CSS-in-JS as an exciting new option. At the time, CSS-in-JS was still in its infancy–only a few libraries existed and <a href="https://styled-components.com/">Styled Components</a> had not been invented yet. We chose <a href="https://github.com/khan/aphrodite">Aphrodite</a>, but didn’t want to be directly coupled to Aphrodite’s implementation for two reasons: since CSS-in-JS was a nascent space we wanted to have the flexibility to switch implementations at a later date, and we also wanted something that would work for open source projects where people might not want Aphrodite. So we created an abstraction layer called <a href="https://github.com/airbnb/react-with-styles">react-with-styles</a>, which gave us a <a href="https://reactjs.org/docs/higher-order-components.html">higher-order component (HOC)</a> to define themeable styles.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href">https://medium.com/media/0d5993a13c08a6ad1017623c4cbb4255/href</a></iframe><p>This allowed components to be styled in the same file, making repo organization more convenient. More importantly, <strong>moving from a globally-aware styling system to a component-based styling system gave us guarantees around how styles would be applied and what files were needed to render every component correctly on every page</strong>. This enabled us to rely on <a href="https://happo.io/">Happo, our screenshot testing tool of choice</a>, and as a result visual regressions plummeted (disclosure: I am the co-creator of Happo).</p><p>Though react-with-styles has served us well for years, it comes with <strong>performance</strong> and <strong>developer experience</strong> tradeoffs. The styles and runtime libraries increase critical path JS bundle size, and applying styles at render-time comes with a CPU cost (10–20% of our component mount times). While we get the aforementioned guarantees about styles, actually writing styles in JavaScript objects feels awkward compared to regular CSS syntax. These tradeoffs led us to reconsider how we style the web at Airbnb.</p><h3>Considering Our Options</h3><p>To address the problems with react-with-styles, we formed a working group of engineers from various teams. We considered a number of directions, which fit into the following high-level categories:</p><ul><li>Static extraction of CSS from react-with-styles at build time</li><li>Write our own framework</li><li>Investigate and adopt an existing framework</li></ul><p>We decided against <strong>static extraction</strong> from react-with-styles at build time because it would require a lot of effort. Additionally, it would be home-grown and therefore lack benefits of a community. Finally, it does not address developer ergonomics issues.</p><p>Similarly, <strong>writing our own framework</strong> would have had a high cost of initial implementation, maintenance, and support. Additionally, there were existing solutions for this problem that we wanted to leverage and contribute back to.</p><figure><img alt="An xkcd comic titled “How Standards Proliferate: (see: A/C chargers, character encodings, instant messaging, etc). Panel 1: Situation: There are 14 competing standards. Panel 2: “14?! Ridiculous! We need to develop one universal standard that covers everyone’s use cases.” “Yeah!” Panel 3: Soon: Situation: There are 15 competing standards." src="https://cdn-images-1.medium.com/max/500/0*7Kwk-MuLZOIUhgnv" /><figcaption>Comic from <a href="https://xkcd.com/927/">https://xkcd.com/927/</a> by Randall Munroe and is used under a CC-BY-NC 2.5 license.</figcaption></figure><p>After evaluating several <strong>existing frameworks</strong> against our requirements, we narrowed down candidates for building a proof of concept:</p><ul><li><a href="https://emotion.sh/docs/introduction">Emotion</a>: CSS-in-JS, with a low runtime cost</li><li><a href="https://github.com/callstack/linaria">Linaria</a>: zero-runtime CSS-in-JS (static CSS extraction)</li><li><a href="https://github.com/seek-oss/treat">Treat</a>: near zero-runtime CSS-in-JS (static CSS extraction)</li></ul><p>The proof-of-concepting work was done in a new repo that implemented a server-rendered client-hydrated unstyled version of Airbnb’s logged in homepage. For each framework, this allowed us to:</p><ul><li>Understand what changes might need to be made to our build system</li><li>Try out framework APIs and get a feel for developer ergonomics</li><li>Assess how each framework supports our web styling requirements</li><li>Gather performance metrics</li><li>Serve as a starting point for a migration plan</li></ul><p>Frameworks were evaluated against each other based on the following ranked list of criteria:</p><ol><li><strong>Performance</strong></li><li><strong>Community</strong> (i.e. support and adoption)</li><li><strong>Developer experience</strong></li></ol><h3>Performance Analysis</h3><p>Using <a href="https://www.speedcurve.com/">SpeedCurve</a>, local benchmarking, and the <a href="https://reactjs.org/docs/profiler.html">React &lt;Profiler /&gt;</a>, we ran performance benchmarking tests for each framework. All results were calculated as the median of 200 runs on a throttled MacBook Pro, and are statistically significantly different from control with a p-value of &lt;= 0.05.</p><p>Informed by <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb’s Page Performance Score</a> (similar to <a href="https://web.dev/performance-scoring/">Lighthouse’s performance score</a>), we focused on the following metrics to give us an idea of how each framework performed and would impact the user experience:</p><ul><li><a href="https://web.dev/tbt/">Total blocking time (TBT)</a></li><li>Bundle size</li><li>Update layout tree count and duration</li><li>Composite layers count and duration</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href">https://medium.com/media/8b77529c5df622792851c8c9884fb78f/href</a></iframe><p>It is clear that the frameworks are divided into two groups: <strong>runtime frameworks</strong> (react-with-styles, Emotion) and <strong>build-time frameworks</strong> (Linaria, Treat).</p><p>Benchmarks of the server-rendered and client-hydrated version of our homepage showed Treat and Linaria performing 36% and 22% better than Emotion on <strong>Total Blocking Time</strong>, respectively. All frameworks performed significantly better than react-with-styles, ranging from a 32–56% improvement. <em>(Note that these numbers should not be used to estimate expected improvements in production, as this is a very specific benchmark designed to test differences between frameworks, not expected savings in production.)</em></p><p><strong>Bundle size</strong> differences also fall into these two categories — with savings on the order of 80 KiB (~12%) for the Linaria/Treat group.</p><p>The CSS metrics (<strong>update layout tree</strong> and <strong>composite layers</strong>) show that, on average, there is roughly one more layout tree update and layer composition event for react-with-styles/Emotion. This is likely due to the insertion and hydration of stylesheets with JavaScript that is not necessary with a CSS extraction library like Linaria or Treat.</p><p>This performance investigation shows that either Linaria or Treat would be promising options to adopt, and that all frameworks considered are a statistically significant improvement over react-with-styles with Aphrodite.</p><h3>What We Liked About Linaria</h3><p>The above <strong>performance</strong> improvements were largely thanks to Linaria extracting the styles from JS to static CSS files at build time, so there is no JS bundle or runtime CPU overhead — giving it a slight edge over the near-zero runtime Treat. Also, this brings caching benefits since these static CSS files may change at a different cadence than the JS files. Since the styles are extracted at build time, Linaria has the opportunity to automatically remove unused styles — this also opens the door to the possibility of deduplicating styles (i.e. <a href="https://css-tricks.com/lets-define-exactly-atomic-css/">Atomic CSS</a>). Additionally, Linaria supports injecting the critical CSS for server-side rendering, which we had wanted to preserve from our react-with-styles integration.</p><p><a href="https://snyk.io/advisor/npm-package/linaria">Linaria also seemed to be a healthy project</a> that saw a good amount of activity, <strong>community</strong> involvement, documentation, and adoption. Its good trajectory gave us confidence that it would continue to improve and that we would be able to contribute back.</p><p>We found Linaria’s <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> API that enables developers to use CSS syntax to be an attractive improvement over the JS object HOC API that we built for react-with-styles. Additionally, off-the-shelf integrations were available for stylelint, CSS autocompletion, and syntax highlighting, which enriched the <strong>developer experience</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hyXFKX-bB-ixvHsE" /><figcaption>Off-the-shelf integrations for stylelint, CSS autocompletion, and syntax highlighting working with Linaria in action.</figcaption></figure><p>We also found value in the similarities between Linaria and our existing solution. The co-location of styles within the component file was a big feature that tipped the scales in favor of Linaria over Treat for us, and the familiar API smoothed the transition for developers and gave us confidence that migration efforts could be eased with automation.</p><h3>Migration Strategy</h3><p>To roll out this big change, we adopted an incremental migration strategy that is largely automated by <a href="https://medium.com/airbnb-engineering/turbocharged-javascript-refactoring-with-codemods-b0cae8b326b9">codemods</a> we’ve written. We are leaning heavily on our <a href="https://happo.io/">Happo screenshot tests</a> to ensure that our components look the same after they are migrated. This allows sections of our codebase to be migrated by running a script and following up with any necessary tweaks, similar to <a href="https://medium.com/airbnb-engineering/ts-migrate-a-tool-for-migrating-to-typescript-at-scale-cd23bfeb5cc">the approach we took when adopting TypeScript</a>.</p><p>The first phase of the migration was handled by the web styling working group and targeted converting a subset of components on a few select pages with varying performance characteristics. This phase was gated on A/B tests which ensured that our initial understanding of the performance held up under the specifics of our app and assured us that there were no hidden problems.</p><p>Once we were confident about the performance and correctness of our Linaria integration, we allowed teams to start using Linaria in new code. We also encouraged teams to migrate their existing code using our codemods. Although the migration has proceeded at a good pace organically, we plan to ensure that all code has moved off of react-with-styles so that we can eventually remove the runtime dependencies from the bundles entirely. This consistency will give us an additional performance boost and reduce the cost of <a href="https://en.wikipedia.org/wiki/Decision_fatigue">decision fatigue</a>.</p><h3>Contributing Back</h3><p>Once we started using Linaria, we discovered that automatic style deduplication (i.e. Atomic CSS) would give us not just a performance boost, but also would fix some non-performance-related hiccups we ran into.</p><p>The selectors that Linaria generates are all of the same <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/Specificity">specificity</a>. Since CSS selectors of the same specificity depend on their declaration order, the order that the bundler builds these files becomes important. This is problematic when sharing styles between files, since we cannot predict or maintain the order of the styles as the shape of the dependency graph changes.</p><p>We initially approached this problem by creating a new <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged template literal</a> for CSS fragments which allows for the styles to be interpolated into Linaria’s CSS tagged template literals. This works okay, but it is unintuitive, defeats <a href="https://github.com/prettier/prettier/blob/d13feed42b6478710bebbcd3225ab6f203a914c1/src/language-js/embed.js#L90-L121">tooling that expects styles to be defined in CSS tagged template literals</a>, and leads to the styles being included several times in the CSS bundles (which is suboptimal for performance).</p><p>Josh Nelson, a member of our web styling working group, <a href="https://github.com/callstack/linaria/pull/867">contributed Atomic CSS support back to Linaria</a> and the Linaria community has been very supportive. The change adds a new <a href="https://npmjs.com/@linaria/atomic">@linaria/atomic</a> package that when imported instead of <a href="https://www.npmjs.com/package/@linaria/core">@linaria/core</a> will generate Atomic CSS at build time. This means that if you write your code like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href">https://medium.com/media/3a3e079e4ec42de365e369ddcc9cfd77/href</a></iframe><p>Instead of generating output like this (without Atomic CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href">https://medium.com/media/520c2e335d3e36a2e93ad4d872342ea5/href</a></iframe><p>The generated output will look something like this (with Atomic CSS):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href">https://medium.com/media/f5126082ab3df8f6d84a5519635d0f96/href</a></iframe><p>The order of appearance problem is solved by build time analysis that chains class names based on the order they are passed in to the cx function to increase specificity when necessary.</p><h3>Reception</h3><p>Our engineers have reacted positively to Linaria. Here are some quotes:</p><blockquote>“Linaria opens up a world where we can code like it’s 1999, in old school pure on CSS. It advises against bad patterns, but gives us the flexibility to build amazing experiences. We’re not fighting the platform anymore, we’re harnessing it and it feels incredibly powerful.” — Callie Riggins</blockquote><blockquote>“Compared to react-with-styles, I care more about what I’m creating now. Linaria is so good.” — Ian Demattei-Selby</blockquote><blockquote>“I really liked being able to write CSS again. It gives you so much more control over what you can style in the component.” — Brie Bunge</blockquote><blockquote>“It’s great to be writing actual CSS again.” — Victor Lin</blockquote><p>Thanks to its familiar CSS syntax, style extraction into static stylesheets, and application of styles using class names, Linaria <strong>increases product development speed</strong> and <strong>unlocks new styling capabilities not possible with react-with-styles and Aphrodite</strong>.</p><h3>Performance Impact</h3><p>Though we are still at the beginning of our migration, we have run some A/B tests that give us an encouraging look at the real world performance impact of switching to Linaria for a large group of visitors in the wild.</p><p>In one experiment, we converted about 10% of the components rendered on the airbnb.com homepage from react-with-styles to Linaria, and saw Homepage <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Page Performance Score</a> improve by 0.26%. <a href="https://web.dev/fcp/">Time to First Contentful Paint (TTFCP)</a> improved by 0.54% (mean of 790ms), while <a href="https://web.dev/tbt/">Total Blocking Time (TBT)</a> also had a strong improvement of 1.6% (mean of 1200ms). To put this in perspective, hydrating the homepage with React takes around 200ms for most people, so improvements of this order of magnitude are significant. We believe these performance improvements with Linaria are attributable to no longer generating CSS styles at render-time, which improves render times on both server and client.</p><p>Assuming the performance improvements will scale linearly (which is a big assumption), converting the remaining 90% of the components <em>might</em> result in a 2.6% improvement to Page Performance Score, 5.4% improvement to Time to First Contentful Paint (TTFCP), and 16% improvement to Total Blocking Time (TBT).</p><p>Note that direct comparisons with other industry numbers are a little tricky here, given the different ways we define pages especially with regard to client routing.</p><h3>What Does This Mean for react-with-styles?</h3><p>Given that we still have many components that still depend on react-with-styles and that it will take a while for us to complete our migration, <strong>we will put react-with-styles in maintenance mode</strong> until we approach the end of our migration. At that point, <strong>we intend to sunset react-with-styles</strong> and the related packages.</p><p>By removing an option from the marketplace we hope to help the community coalesce towards a common solution and invest in better frameworks. If you are looking for a new tool, we think Linaria is a great choice!</p><h3>Conclusion</h3><p>Styling infrastructure is still an exciting space, rich with opportunities. At Airbnb, we’ve found big improvements to the <strong>developer experience</strong> by adopting a framework that allows regular CSS syntax to be used alongside our React component code. And by replacing a runtime styling library with one that compiles to static CSS files at build time, we are able to continue driving toward faster <strong>performance</strong>. Thanks to the Linaria <strong>community</strong> and our collaboration, we expect this library to continue to improve for many years.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://grnh.se/ebfa55151us">Frontend Infrastructure Engineer, Web Platform</a><br><a href="https://grnh.se/b5afa9151us">Staff Software Engineer, Data Governance </a><br><a href="https://grnh.se/92c32fed1us">Staff Software Engineer, Cloud Infrastructure </a><br><a href="https://grnh.se/bbe55fe81us">Staff Database Engineer </a><br><a href="https://grnh.se/21e5c2011us">Staff Software Engineer — ML Ops Platform </a><br><a href="https://grnh.se/ee114dfc1us">Senior/Staff Software Engineer, Service Capabilities</a></p><h3>Acknowledgments</h3><p>We have a lot of appreciation for the folks at <a href="https://www.callstack.com/">callstack</a> and the <a href="https://github.com/callstack/linaria#contributors">Linaria community</a> for building such a great tool and for collaborating with us to make it even better. Also for <a href="https://www.khanacademy.org/">Khan Academy</a> for giving us Aphrodite which served us well for many years. This has been a huge effort at Airbnb that would not have been possible without all the work put in by so many people at Airbnb, including Mars Jullian, Josh Nelson, Nora Tarano, Alan Wright, Jimmy Guo, Ian Demattei-Selby, Victor Lin, Nnenna John, Adrianne Soike, Garrett Berg, Andrew Huth, Austin Wood, Chris Sorenson, and Miles Johnson. Finally, thank you to Surashree Kulkarni for help editing this blog post. Thank you all!</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dc169230bd12" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-trip-to-linaria-dc169230bd12">Airbnb’s Trip to Linaria</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Graph Machine Learning at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f868d65f36ee</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[artificial-intelligence]]></category>
            <dc:creator><![CDATA[Devin Soni]]></dc:creator>
            <pubDate>Tue, 14 Jun 2022 15:59:22 GMT</pubDate>
            <atom:updated>2022-06-14T15:59:22.004Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>How Airbnb is leveraging graph neural networks to up-level our machine learning</strong></h4><p>By:<a href="https://www.linkedin.com/in/devinsoni/"> Devin Soni</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bEZU2cupMt44ke6mkK-low.jpeg" /></figure><h3>Introduction</h3><p>Many real-world machine learning problems can be framed as graph problems. On online platforms, users often share assets (e.g. photos) and interact with each other (e.g. messages, bookings, reviews). These connections between users naturally form edges that can be used to create a graph.</p><p>However, in many cases, machine learning practitioners do not leverage these connections when building machine learning models, and instead treat nodes (in this case, users) as completely independent entities. While this does simplify things, leaving out information around a node’s connections may reduce model performance by ignoring where this node is in the context of the overall graph.</p><p>In this blog post, we will explain the benefits of using graphs for machine learning, and show how leveraging graph information allows us to learn more about our users, in addition to building more contextual representations of them [4]. We will then cover specific graph machine learning methods, such as Graph Convolutional Networks, that are being used at Airbnb to improve upon existing machine learning models.</p><p>The motivating use-case for this work is to build machine learning models that protect our community from harm, but many of the points being made and systems being built are quite generic and could be applied to other tasks as well.</p><h3>Challenges</h3><h4><strong>The problem</strong></h4><p>When building trust &amp; safety machine learning models around entities such as users or listings, we generally begin by reaching for features that directly describe the entity. For example, in the case of users, we may use features such as their location, account age, or number of bookings. However, these simple features do not adequately describe the user in the context of the overall Airbnb platform and their interactions with other users.</p><p>Consider the hypothetical scenario in which a new host joins Airbnb. A week into their hosting journey, we likely do not have a lot of information about them other than what they have directly told us. This could include their listing’s location or their phone number. These direct attributes given to us by the host are relatively surface level and do not necessarily help us understand their trustworthiness or reputation.</p><p>In this state, it is hard for Airbnb to provide this new host with the best possible experience since we do not know what their usage pattern of the platform will be. Lacking information, we might then make this new host go through a slower onboarding process or request a lot of information up-front. Understanding this user’s relationships to the rest of the platform is data we can leverage to provide them with an improved experience.</p><h4><strong>An illustration of the usefulness of graphs</strong></h4><p>While we do not have much direct information about the new host, what we can do is leverage their surroundings to try and learn more. One example of this is the connections that they have to other users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/1*BAJt6yhBPetG4HwwUylGiQ.png" /></figure><p>We can first take a look at their one-hop neighborhood, or in other words, the set of users whom this host has a direct connection with. In this example, we can see that this new host shares a listing photo with an existing, tenured host. We can also see that the new host’s listing is in the same house as listings from three other hosts. With this additional knowledge, we now know more about the new host; they might be working with other hosts who have rooms in the same house. However, we can’t be completely sure how all of the connected hosts relate to each other without looking at more of the graph.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/971/1*9u8zd4_qXAA46RGCUWa0kw.png" /></figure><p>Let’s further expand our view and consider the two-hop neighborhood of the new host. This expands our view to users who are not necessarily directly connected to the new host. In this expanded network we now see that many of the hosts with listings in the location are connected to each other through a shared business name. It now becomes very likely that this new host is part of an existing group of hosts that rent out rooms in the same house, and has not yet updated their profile to reflect this.</p><p>Using the power of graphs, we were able to learn about a new host simply by inspecting their connections to other users within Airbnb. This additional knowledge extracted from the graph provides us with an improved glimpse into who our new host is. We are subsequently able to deliver an improved experience to this new host, all without requiring them to provide any more information to Airbnb.</p><p>Supplementing our models with graph information is one way to bootstrap our models. Using graphs, we can construct a detailed understanding of our users in scenarios where we have little historical data or observations pertaining to a user. While the semantic information we gain from the graph is often inferred and not directly told to us by the user, it can give us a strong baseline level of knowledge until we have more factual information about a user.</p><h3><strong>Graph Machine Learning</strong></h3><p>We have established that we want our machine learning models to be able to ingest graph information. The main challenge is figuring out how best to condense everything a graph can represent into a format that our models can use. Let’s dig into some of the options and explore the solution that we ultimately implemented.</p><p>One simple option is to calculate statistics about nodes and use them as numeric features. For example, we can calculate the number of users a user is connected to or how many listing photos they share with other hosts. These metrics are straightforward to calculate and give us a basic sense of the node’s role in the overall structure of the graph. These metrics are valuable but do not leverage the node’s features. As such, simple statistics cannot go beyond representing graph structure.</p><p>What we really want is to be able to produce an aggregation of a node’s neighborhood in the graph that captures both the node’s structural role in the graph and its node features. For example, we want to know more than how many users a user is connected to; we also want to understand the type of users they are connected to (e.g. their account tenure, or past booking counts) because that gives us more hints about the original user than simple edge counts.</p><h4><strong>Graph Convolutional Networks</strong></h4><p>To capture both graph structure and node features, we can use a type of graph neural network architecture called a graph convolutional network. Graph convolutional networks (GCN) are neural networks that generally take as input a matrix of node features in addition to an adjacency matrix of the graph and outputs a node-level output. This type of network architecture is preferred over simply concatenating pre-computed structural features with node features because it is able to jointly represent the two types of information, likely producing richer embeddings.</p><p>Graph convolutional networks consist of multiple layers. A single GCN layer aims to learn a representation of the node that aggregates information from its neighborhood (and in most cases, combines its neighborhood information with its own features). Using the example of our newly created host account, one GCN layer would be the host’s one-hop neighborhood. A second GCN layer representing the host’s two-hop neighborhood can be introduced to capture additional information. Since the output of GCN layer N is used to produce the representations used in GCN layer N+1, adding layers increases the span of the aggregation used to generate node representations [1].</p><p>Drawing from the example in the previous section, we would need a GCN with two layers in order to produce a graph embedding that captures the illustrated subgraph. We could go even deeper and expand further to third-order, fourth-order, and so on. In practice, however, a small number of layers (e.g. 2–4) is sufficient, as the connections beyond that point are likely to be very noisy and unlikely to be relevant to the original user.</p><h4><strong>Model architecture &amp; training</strong></h4><p>Having decided to use GCNs, we must now consider how complex we want each layers’ method of aggregating neighboring nodes’ features to be. There are a wide variety of aggregation methods which can be used. These include mean pooling, sum pooling, as well as more complex aggregators involving attention mechanisms [5].</p><p>When it comes to trust &amp; safety, we often work in adversarial problem domains where frequent model retraining is required due to concept drift. Limiting model complexity, and limiting the number of models that must be retrained is important for reducing maintenance complexity.</p><p>One might assume that GCNs with more complex, expressive aggregation functions are always better. This is not necessarily the case. In fact, several papers have shown that in many cases very simple graph convolutional networks are all that is needed for state-of-the-art performance in practical tasks [2, 3]. The Simplified GCN (SGC) architecture showed that we can achieve performance comparable to more complex aggregators using GCN layers that do not have trainable weights [2]. The Scalable Graph Inception Network (SIGN) architecture showed that, in general, we can precompute multiple aggregations without trainable weights, and use them in parallel as inputs into a downstream model [3]. SIGN and SGC are very related; SIGN provides a general framework for precomputing graph aggregations, and SGC provides the most straightforward aggregator to use within the SIGN framework.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5h9Tp1c8MkyEJAE5ZR2hBA.png" /></figure><p>Using SIGN and SGC, the GCN is purely a fixed feature extractor that does not need to learn anything itself — it has no weights that must be tuned during training. In this setting we are able to fundamentally treat the GCN as a fixed mathematical formula applied to its inputs. This aspect is very convenient because we do not need to worry about supervised training or pre-training of the GCN itself.</p><h4><strong>Model serving</strong></h4><p>When serving a graph neural network, the main considerations are around freshness of the data and how to obtain the inputs for the model in a production setting. Our primary concern is the trade-offs related to data freshness. The decision between real-time or batch methods has an impact on how up to date the information is.</p><p>Real-time methods can provide downstream models with the most up-to-date information. This increased freshness does, however, require more effort to serve the embeddings. In addition, it often relies on a downsampled version of the graph to handle nodes with many edges, such as in the GraphSAGE algorithm [4].</p><p>Offline batch methods are able to calculate all node embeddings at once. This provides a distinct advantage over real-time methods by reducing implementation complexity. Unfortunately, the tradeoff does come at a cost. We will not necessarily be able to serve the most recent node embedding as we will only be able to leverage information present in the last run of the pipeline.</p><h3><strong>Chosen solution</strong></h3><p>Given all the tradeoffs and our requirements, we ultimately decided to use a periodic offline pipeline which leverages the SIGN method for our initial implementation. The easy maintenance of batch pipelines and relative simplicity of SIGN allows us to optimize for learning instead of performance initially.</p><p>Despite the fact that many of our trust &amp; safety models are run online in real-time, we decided to start with an offline graph model. Features are computed using a snapshot of the graph and node features. When fetching these features online, the downstream model simply looks up the previous run’s output from our feature store, rather than having to compute the embedding in real-time. The alternative of a real-time graph embedding solution would involve a significant amount of additional implementation complexity.</p><h3>Benefits Realized</h3><p>With the batch pipeline implemented, we can now have access to new information as features in downstream models. Our existing feature sets did not capture this information and it has resulted in significant gains in our models. Components of the embedding are often among the top 10 features in downstream models based on feature importance computed using <a href="https://github.com/slundberg/shap">the SHAP approach</a>.</p><p>These positive results encourage further investment in the area of graph embeddings and graph signals, and we plan to explore other types of graphs &amp; graph edges. Investigating how to make our embedding more powerful either through improving the freshness of the data or using other algorithms has become a priority for us based on the success of augmenting our existing models with graph knowledge.</p><h3>Conclusion</h3><p>In this blog post, we showed how leveraging graph information can be broadly useful and discussed our approach to implementing graph machine learning. We ultimately decided to use a SIGN architecture that leverages a batch pipeline to calculate graph embeddings. These are subsequently fed into downstream models as features. Many of the new features have led to notable performance gains in the downstream models.</p><p>We hope that this information helps others understand how to leverage graph information to improve their models. Our various considerations provide insight into what one must be aware of when deciding to implement a graph machine learning system.</p><p>Graph machine learning is an exciting area of research in Airbnb, and this is only the beginning. If this type of work interests you, check out some of our related positions:</p><p><a href="https://careers.airbnb.com/positions/3910069/">Senior Machine Learning Engineer</a></p><p><a href="https://careers.airbnb.com/positions/4113532/">Senior Software Engineer, Trust</a></p><h3>Acknowledgments</h3><p>This project couldn’t have been done without the great work of many people and teams. We would like to thank:</p><ul><li>Owen Sconzo for working on this project and reviewing all of the code.</li><li>The Trust Foundational Modeling team for providing the foundational data for graph modeling.</li><li>Members of the Fraud &amp; Abuse Working Group for supporting this project, reviewing this blog post, and providing suggestions.</li></ul><h3>References</h3><p>[1] <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks.</a></p><p>[2] <a href="https://arxiv.org/abs/1902.07153">Simplifying Graph Convolutional Networks</a></p><p>[3] <a href="https://arxiv.org/abs/2004.11198">SIGN: Scalable Inception Graph Neural Networks</a></p><p>[4] <a href="https://arxiv.org/abs/1706.02216">Inductive Representation Learning on Large Graphs</a></p><p>[5] <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a></p><p>****************</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f868d65f36ee" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee">Graph Machine Learning at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unified Payments Data Read at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e613e7af1a39</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[payments]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Alican GÖKSEL]]></dc:creator>
            <pubDate>Thu, 09 Jun 2022 22:23:30 GMT</pubDate>
            <atom:updated>2022-06-10T21:47:52.619Z</atom:updated>
            <content:encoded><![CDATA[<p>How we redesigned payments data read flow to optimize client integrations, while achieving up to 150x performance gains.</p><p>By: <a href="https://www.linkedin.com/in/ali-can-g%C3%B6ksel-7189214a">Ali Goksel,</a> <a href="https://www.linkedin.com/in/linglongzhu">Linglong Zhu</a>, <a href="https://www.linkedin.com/in/yixiamao">Yixia Mao</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fWhQRxAT0vMQwTEW" /></figure><h3>Introduction</h3><p>In recent years, Airbnb migrated most of its backend services from a monolith to a service-oriented architecture (SOA). This industry standard architecture brings countless benefits to a company that is at the scale of Airbnb; however, it is not free of challenges. With data scattered across many services, it’s difficult to provide all the information clients need in a simple and performant way, especially for complex domains such as payments. As Airbnb grew, this problem started to crop up for many new initiatives such as host earnings, tax form generation, and payout notifications, all of which required data to be read from the payments system.</p><p>In this blog post, we introduce Airbnb’s unified payments data read layer. This read layer was custom built to reduce the friction and complexity for client integrations, while greatly improving query performance and reliability. With this re-architecture, we were able to provide a greatly optimized experience to our host and guest communities, as well as for internal teams in the trust, compliance, and customer support domains.</p><h3>Evolution of Airbnb’s Payments Platform</h3><p>Payments is one of the earliest functionalities of the Airbnb app. Since our co-founder Nate’s first commit, Payments Platform has grown and evolved tremendously, and it continues to evolve at an even faster pace given our expanding global presence.</p><p>Similar to other companies, Airbnb started its journey with a monolithic application architecture. Since the feature set was initially limited, both write and read payment flows were “relatively” simple.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*dsBzT9rDVCNoG8MM" /><figcaption>Overly simplified diagram of Airbnb’s old monolithic architecture. Payments schemas were not very complex, and the feature set was limited.</figcaption></figure><p>Predictably, this architecture couldn’t scale well with the rapid growth and expansion of our company. Payments, along with most other parts of the tech stack, started to migrate to the SOA architecture. This brought a significant overhaul of the existing architecture and provided many advantages, including:</p><ul><li>We had clear boundaries between different services, which enabled better domain ownership and faster iterations.</li><li>Data was separated into domains in a very normalized shape, resulting in better correctness and consistency.</li></ul><p>For more, take a peek at our <a href="https://medium.com/airbnb-engineering/rebuilding-payment-orchestration-at-airbnb-341d194a781b">blog post</a> detailing the payments SOA migration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6lC8Bu1u3DA0WAzV" /><figcaption>After the SOA migration, every payments subdomain has its own service(s) and tables with clear boundaries, but more features leads to more complex and normalized data.</figcaption></figure><h3>New Architecture Introduces New Challenges</h3><p>Payments SOA provided us with a more resilient, scalable, and maintainable payments system. During this long and complex migration, correctness of the system was our top priority. Data was normalized and scattered across many payments domains according to each team’s responsibilities. This subdivision of labor had an important side effect: presentation layers now often needed to integrate with multiple payments services to fetch all the required data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*itWZQ8kaxNjoZ-7f" /><figcaption>How payments data read flows looked after the SOA migration. Presentation services called one or more payments services and aggregated data at the application layer.</figcaption></figure><p>At Airbnb, we believe in being transparent with our host and guest communities. Our surfaces related to payments and earnings display a range of details including fees, transaction dates, currencies, amounts, and total earnings. After the SOA migration, we needed to look into multiple services and read from even more tables than prior to the migration to get all the requested information. Naturally, this foundation brought challenges when we wanted to add new surfaces with payments data, or when we wanted to extend the existing surfaces to provide additional details. There were three main challenges that we needed to solve.</p><p>The first challenge was that <em>clients now needed to understand the payments domain well enough</em><strong> </strong>to pick the correct services and APIs. For client engineers from other teams, this required a non-trivial amount of time investment and slowed down overall time to market. On the payments side, engineers needed to provide continuous consultation and guidance, occupying a significant portion of their work time.</p><p>The second challenge was that there were many instances in which we had to change multiple payments APIs at the same time in order to meet the client requirements. When there are<em> too many touchpoints</em>, it becomes <em>hard to prioritize requests</em> since many teams have to be involved. This problem also caused significant negative impact to time to market. We had to slow down or push back feature releases when the alignment and prioritization meetings did not go smoothly. Similarly, when payments teams had to update their APIs, they had to make sure that all presentation services adopted these changes, which slowed down progress on the payments system.</p><p>Last but not least, the technical quality of the complex read flows was not where we wanted it to be. Application-level aggregations worked fine for the average use case, but we had space for improvement when it came to our large hosts and especially for our prohosts, who might have thousands of yearly bookings on our platform. To have confidence in our system over the long term, we needed to find a solution that provided inherently better <strong><em>performance, reliability, and scalability</em>.</strong></p><h3>Introducing the Payments Unified Data Read Layer</h3><p>To achieve our ambitious goals for payments, we needed to re-think how clients integrate with our payments platform.</p><h3>Unified Entry Points</h3><p>Our first task was to unify the payments data read entry points. To accomplish this, we leveraged <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, Airbnb’s data-oriented service mesh, where clients query for the “entity” instead of needing to identify dozens of services and their APIs. This new architecture required our clients to worry only about the requisite data entity rather than having to communicate with individual payments services.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rtNAGGJFpwHfnZ5V" /><figcaption>Instead of communicating with individual payments services, presentation services just use the read layer.</figcaption></figure><p>In these entry points, we provided as many filtering options as possible so each API could hide filtering and aggregation complexity from its clients. This also greatly reduced the numbers of APIs we needed to expose.</p><h3>Unified Higher-Level Data Entities</h3><p>Having a single entry point is a good start, but it does not resolve all the complexity. In payments, we have 100+ data models, and it requires a decent amount of domain knowledge to understand their responsibilities clearly. If we just expose all of these models from a single entry point, there would still be too much context required for client engineers.</p><p>Instead of making our clients deal with this complexity, we opted to hide payments internal details as much as possible by coming up with <strong>higher-level domain entities</strong>. Through this process, we were able to reduce the core payments data to fewer than ten<strong> </strong>high level entities, which greatly reduced the amount of exposed payments internal details. These new entities also allowed us to guard clients against changes made in Payments platform. When we internally update the business logic, we keep the entity schema the same without requiring any migrations on the client side. Our principles for the new architecture were the following:</p><ul><li><strong>Simple</strong>: Design for non-payments engineers, and use common terminology.</li><li><strong>Extensible</strong>: Maintain loose coupling with storage schema, and encapsulate concepts to protect from payments internal changes while allowing quick iterations.</li><li><strong>Rich</strong>: Hide away the complexity but not the data. If clients need to fetch data, they should be able to find it in <strong><em>one</em></strong> of the entities.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hPMYFXmawEXCM8tf" /><figcaption>Expose cleaner higher-level domain entities to hide payments internal details while guarding clients from frequent API migrations.</figcaption></figure><h3>Materialize Denormalized Data</h3><p>With unified entry points and entities, we greatly reduced the complexity for client onboardings. However, the “<strong><em>how</em></strong>” of fetching the data, combined with expensive application layer aggregations, was still a big challenge. While it’s important that clients are able to integrate with the payments system smoothly, our valued community should also enjoy the experience on our platform.</p><p>The core problem we identified was <strong><em>dependency on many tables and services during client queries</em></strong>. One of the promising solutions was denormalization–essentially, moving these expensive operations from query time to ingestion time. We explored different ways of pre-denormalizing payments data and materializing it reliably with less than 10 seconds replication lag. To our great luck, our friends in the Homes Foundation team were piloting a Read-Optimized Store Framework, which takes an event-driven lambda approach to materializing secondary indices. Using this framework, teams are able to get both near real-time data via database change capture mechanisms and historical data leveraging our daily database dumps stored in Hive. In addition, the maintenance requirements of this framework (e.g., single code for online and offline ingestion, written in Java) were much lower compared to other existing internal solutions..</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aEEGisG92n5f9mc8" /><figcaption>A high-level look at the read-optimized store framework usage by payments. It provides ingestion flows for both offline and near real-time data with shared business logic between them.</figcaption></figure><p>After combining all of above improvements, our new payments read flow looked like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kWOXKjHjm7Jvskze" /><figcaption>The final shape of the payments data read architecture. Clients do not need to know any payments services or internals.</figcaption></figure><p>We provide data in a reliable and performant way via denormalized read-optimized store indices.</p><h3>Results</h3><h3>Migrate and Elevate: Transaction History</h3><p>The first test surface for the new unified data read architecture was Transaction History (TH). Hosts on our platform use the <a href="https://www.airbnb.com/users/transaction_history">Transaction History page</a> to view their past and future payouts along with top-level earning metrics (e.g., total paid out amount).</p><p>On the technical side, this was one of the most complex payments flows we had. There were many different details required, and the data was coming from <strong>10+</strong> payments tables. This had caused issues in the past, including timeouts, slow loading times, downtime due to hard dependencies, and slow iteration speed as a result of complex implementations. While doing the initial technical design for TH migration from Airbnb monolith to SOA, we took the hard path of re-architecting this flow instead of applying band-aids. This helped to ensure long-term success and provide the best possible experience to our host community.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*homIPsg24j6ZA1FT" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8SKW7SUEmrbGMFO1" /><figcaption>Transaction History page and simplified high level architecture. Airbnb monolith app behaves like a presentation service and fetches data from multiple payment services and also from legacy databases.</figcaption></figure><p>This use case was a great fit for our unified read layer. Using the data used by TH as a starting point, we came up with a new API and high-level entity to serve all data read use cases from similar domains.</p><p>After locking down the entity and its schema, we started to denormalize the data. Thanks to the read-optimized store framework, we were able to denormalize all the data from 10+ tables into a couple of Elasticsearch indices. Not only did we greatly reduce the touchpoints of the query, we were also able to paginate and aggregate much more efficiently by leveraging the storage layer instead of doing the same operations on the application layer. After close to two years of work, we migrated 100% of traffic and achieved up to <strong><em>150x</em></strong> latency improvements, while improving the reliability of the flow from ~96% to <strong><em>99.9+%</em></strong><em>.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NvD6A6Q0AswwXib-" /><figcaption>After the re-architecture, payments data needed by Transaction History is provided by payments read-optimized store and accessed by clients using a well-defined and extensible payout schema over the unified data read layer.</figcaption></figure><h3>Unlocking New Experiences: Guest Payment History</h3><p>Our next use case, called Guest Payment History, came out of Airbnb’s annual company-wide hackathon. This hackathon project aimed to provide a detailed and easy way for our guest community to track their payments and refunds. Similar to Transaction History, this scenario also required information from multiple payments services and databases, including many legacy databases.</p><p>Guest Payment History (GPH) also helped to showcase many benefits brought by the unified read layer: a new unified entity to serve GPH and future similar use cases, along with an extensible API which supported many different filters. We denormalized and stored data from legacy and SOA payment tables using the read-optimized store framework into a single Elasticsearch index, which reduced the complexity and cost of queries greatly.</p><p>We released this new page to our community with our <a href="https://news.airbnb.com/2021-winter-release/">2021 Winter launch</a> and achieved a huge reduction on customer support tickets related to questions about guest payments; which resulted in close to $1.5M cost savings for 2021. It also illustrated our move towards a stronger technical foundation with high reliability and low latency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KNPtvQ1odi8dp2Xw" /><figcaption>Guests can track their payments and refunds using Guest Payment History.</figcaption></figure><p>The architecture is very similar to TH, where data is provided to clients via unified API and schema, backed by a secondary store.</p><p>After exposing these new entities via TH and GPH, we started to onboard many other critical use cases to leverage the same flow in order to efficiently serve and surface payments data.</p><h3>Conclusion</h3><p>Microservice/SOA architectures greatly help backend teams to independently scale and develop various domains with minimal impact to each other. It’s equally important to make sure the clients of these services and their data will not be subject to additional challenges under this new industry-standard architecture.</p><p>In this blog post, we illustrated some potential solutions, such as unified APIs and higher-level entities to hide away the internal service and architectural complexities from the callers. We also recommend leveraging denormalized secondary data stores to perform expensive join and transformation operations during ingestion time to ensure client queries can stay simple and performant. As we demonstrated with multiple initiatives, complex domains such as payments can significantly benefit from these approaches.</p><p>If this type of work interests you, take a look at the following related positions:</p><p><strong>US</strong>:</p><p><a href="https://careers.airbnb.com/positions/3393185/">Staff Software Engineer, Payments</a></p><p><strong>India</strong>:</p><p><a href="https://careers.airbnb.com/positions/3153981/">Senior Software Engineer, Cities Bangalore</a></p><p><a href="https://careers.airbnb.com/positions/3842855/">Engineering Manager, Ambassador Platform Products</a></p><p><a href="https://careers.airbnb.com/positions/2768475/">Manager, Engineering Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2773515/">Staff Software Engineer, Payments Compliance</a></p><p><a href="https://careers.airbnb.com/positions/2925359/">Senior Software Engineer, Payments Compliance</a></p><h3>Acknowledgments</h3><p>We had many people at Airbnb contributing to this big re-architecture, but countless thanks to Mini Atwal, Yong Rhyu, Musaab At-Taras, Michel Weksler, Linmin Yang, Linglong Zhu, Yixiao Peng, Bo Shi, Huayan Sun, Wentao Qi, Adam Wang, Erika Stott, Will Koh, Ethan Schaffer, Khurram Khan, David Monti, Colleen Graneto, Lukasz Mrowka, Bernardo Alvarez, Blazej Adamczyk, Dawid Czech, Marcin Radecki, Tomasz Laskarzewski, Jessica Tai, Krish Chainani, Victor Chen, Will Moss, Zheng Liu, Eva Feng, Justin Dragos, Ran Liu, Yanwei Bai, Shannon Pawloski, Jerroid Marks, Yi He, Hang Yuan, Xuemei Bao, Wenguo Liu, Serena Li, Theresa Johnson, Yanbo Bai, Ruize Lu, Dechuan Xu, Sam Tang, Chiao-Yu Tuan, Xiaochen He, Gautam Prajapati, Yash Gulani, Abdul Shakir, Uphar Goyal, Fanchen Kong, Claire Thompson, Pavel Lahutski, Patrick Connors, Ben Bowler, Gabriel Siqueira, Jing Hao, Manish Singhal, Sushu Zhang, Jingyi Ni, Yi Lang Mok, Abhinav Saini, and Ajmal Pullambi. We couldn’t have accomplished this without your invaluable contributions.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e613e7af1a39" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/unified-payments-data-read-at-airbnb-e613e7af1a39">Unified Payments Data Read at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Faster JavaScript Builds with Metro]]></title>
            <link>https://medium.com/airbnb-engineering/faster-javascript-builds-with-metro-cfc46d617a1f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/cfc46d617a1f</guid>
            <category><![CDATA[module-bundler]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[metro-bundler]]></category>
            <category><![CDATA[webpack]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Rae Liu]]></dc:creator>
            <pubDate>Tue, 24 May 2022 17:39:39 GMT</pubDate>
            <atom:updated>2022-05-24T17:39:39.184Z</atom:updated>
            <content:encoded><![CDATA[<p><em>How Airbnb migrated from Webpack to Metro and made the development feedback loop nearly instantaneous, the largest production build 50% faster, with marginal end-user runtime improvements.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RZFWkaoezUfVzTxvpfm2gQ.jpeg" /></figure><p><strong>By:</strong> <a href="https://www.linkedin.com/in/raejin/">Rae Liu</a></p><h3>Introduction</h3><p>In 2018, the frontend Airbnb infrastructure relied on Webpack for JavaScript bundling which had served us well up until then; however, with our codebase almost having quadrupled in the previous year, the frontend team was noticing a significant impact on the development experience. Not only was build performance slow, but the average page refresh time for a trivial one-line code change was anywhere between 30 seconds and 2 minutes depending on the project size. In order to mitigate this, the team decided to migrate to <a href="https://facebook.github.io/metro/">Metro</a>.</p><p>Thanks to the switch to Metro, we’ve improved our build performance. In development, the time it takes for a simple UI change to be reflected and loaded (<a href="https://developer.mozilla.org/en-US/docs/Glossary/Time_to_interactive">Time to Interactive TTI metric</a>) is <strong>80% faster</strong>. The slowest production build compiling ~49k modules (JavaScript files) is <strong>55% faster</strong> (down from 30.5 minutes to 13.8 minutes). As an added bonus, we’ve observed improvements in the <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb Page Performance Scores</a> by ~<strong>1%</strong> for pages built with Metro.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/966/1*MlrQ2yEBbHj0OOHc_sYxtg.png" /></figure><p>Scaling issues with JavaScript bundlers certainly isn’t a unique problem to Airbnb. In this blog post, we want to highlight the key architectural differences between Webpack and Metro as well as some of the migration challenges we faced in both development and production builds. If you anticipate one of your own projects to scale up significantly in the future, we hope this post can provide useful insights on solving this problem.</p><h3>What is Metro?</h3><p><a href="https://facebook.github.io/metro/">Metro</a> is the open source JavaScript bundler for React Native. While <a href="https://medium.com/airbnb-engineering/sunsetting-react-native-1868ba28e30a">Airbnb no longer uses React Native</a>, we believed the infrastructure could be leveraged for the web as well. After numerous consultations with the Metro folks at Meta as well as some of our own modifications, we managed to build a flavor of Metro that now powers both development and production bundling for all Airbnb websites.</p><p>Conceptually, Metro breaks down bundling to three steps in the following order: <a href="https://facebook.github.io/metro/docs/concepts#resolution">resolution</a>, <a href="https://facebook.github.io/metro/docs/concepts#transformation">transformation</a> and <a href="http://serialization">serialization</a>.</p><ul><li>Resolution deals with how to resolve import/require statements.</li><li>Transformation is responsible for transpiling code (source-to-source compiler which converts modern TypeScript/JavaScript source code into functionally equivalent JavaScript code that’s more optimized and backwards compatible with older browsers), an example tool would be <a href="https://babeljs.io/">babel</a>.</li><li>Serialization combines the transformed files into JavaScript bundles.</li></ul><p>These three concepts are the fundamental building blocks to understand how Metro works. In the following sections, we highlight the key architectural differences between Metro and Webpack to provide deeper context into Metro’s strengths.</p><h3>Key architectural differences between Metro and Webpack</h3><h3>Process JS bundles on demand in development</h3><p>When we talk about bundles, a JavaScript bundle is technically just a serialized dependency graph, where an entry point is the root of the graph. At Airbnb, a web page maps to a single entry point. In development, Webpack (even the latest v5 version) requires knowing <a href="https://webpack.js.org/concepts/entry-points/">the entry points</a> for all pages before it can start bundling. On the other hand, the Metro development server processes the requested JavaScript bundles on the fly.</p><p>More specifically, at Airbnb, every frontend project has a Node server which matches a route to a specific entry point. When a web page is requested, the DOM includes script tags with the development JavaScript URLs. The browser loads the page, and makes requests to the Metro development server for the JavaScript bundles. In Figure 1, we illustrate the difference between our Metro &amp; Webpack development setup:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/813/0*d0S7RQA6IXt1YqAO" /></figure><p>Figure 1: Differences between the JS bundle development setups for Metro and Webpack</p><p>In this example, there is a web project with three entry points: entryPageA.js, entryPageB.js, and entryPageC.js. A developer makes changes to Page A, which includes only the entryPageA.js bundle. As you can see in Figure 1, in both scenarios, the browser loads Page A (1), then requests the entryPageA.js file from the bundler (2), and finally the bundler responds to the browser with the appropriate bundles (4). With the Webpack bundler (1a), even though the browser only requests entryPageA.js, Webpack compiles all entry points on start-up before it can respond to the entryPageA.js request from the browser. On the other hand, with the Metro bundler (1b), we see that the development server does not spend any time compiling entryPageB.js or entryPageC.js, instead only compiling entryPageA.js before responding to the browser request.</p><p>One of the biggest frontend projects at Airbnb has ~26k unique modules, with the median number of modules per page being ~7.2k modules. Because we also do server side rendering, the number of modules we ultimately have to process doubles to roughly ~48k. With Metro’s development model, we saved ~70% of work by compiling JavaScript on demand.</p><p>This key architectural difference improves the developer experience, as Metro only compiles what is needed (JavaScript bundles on the pages requested), whereas Webpack pre-compiles the entire project on start-up.</p><h3>Multi-layered cache</h3><p>Another powerful Metro feature we leverage is its <a href="https://facebook.github.io/metro/docs/caching">multi-layered caching</a> feature, which makes setting up both persistent and non-persistent caches straightforward. While Webpack 5 also comes with <a href="https://webpack.js.org/guides/build-performance/#persistent-cache">a disk persistent cache</a>, it isn’t as flexible as Metro’s multi-layered cache. Webpack offers two <a href="https://webpack.js.org/configuration/cache/#cachetype">distinct cache types</a>: “filesystem” or “memory”, which is limited to memory or disk cache, no remote cache capability is possible. In comparison, Metro provides more flexibility by allowing us to define the cache implementation, including mixing different types of cache layers. If a layer has a cache miss, Metro attempts to retrieve the cache from the next layer and so on.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/653/0*bvgMUBI6wm9xe0fZ" /></figure><p>Figure 2: How Airbnb configures the multi-cache layers with Metro</p><p>The ordering of the caches determines the cache priority. When retrieving a cache, the first cache layer with a result will be used. In the setup illustrated in Figure 2, the fastest in-memory cache layer is prioritized at the top, followed by the file/disk cache, and lastly the remote read-only cache. Compared with the default Metro implementation without a cache, hitting a remote read-only cache resulted in a 56% faster server build in a project compiling 22k files.</p><p>One contributing factor to Metro’s performance is its built-in worker support which amplifies the effect of the multi-layer cache. While Webpack requires careful configuration to leverage workers via a <a href="https://webpack.js.org/loaders/thread-loader/">third-party plugin</a>, Metro by default spins up workers to offload expensive transforms, allowing for increased parallelization without configuration.</p><p>But why use a remote read-only cache instead of a regular remote cache (read &amp; write)? We discovered that not writing to the remote cache saved an additional 17%<strong> </strong>build time in development for the same project with 22k files. Writing to the remote cache incurs network calls that can be costly, especially on a slower network. To populate the cache, instead of remote cache writes, we introduced a CI job that runs periodically on the default branch commit.</p><h3>Serialization</h3><p>In the bundler context, serialization means combining the transformed source files into one or multiple bundles. In Webpack, the concept of serialization is encapsulated in <a href="https://webpack.js.org/api/compilation-hooks/#root">the compilation hooks</a> (Webpack’s public APIs). In Metro, a serializer function is responsible for combining source files into bundles.</p><p>For one example of the importance of serialization, let’s take a look at Internationalization support. We currently support Airbnb websites in around 70 locales, and in 2020, our<a href="https://medium.com/airbnb-engineering/building-airbnbs-internationalization-platform-45cf0104b63c"> internationalization platform</a> served more than 1 million pieces of content. To support internationalization with JS bundles, we need to implement specific logic in the serialization step. Although we had to implement similar internationalization logic when serializing bundles for both Metro and Webpack, Webpack required lots of source code reading to find the appropriate compilation hooks for us to implement the support. On top of all that, it also required understanding the intricacies of concepts like what dependency templates are and how to write our own. Comparatively, it is a breath of fresh air to implement the same internationalization support with Metro. We only have to focus on how to serialize JS bundles with translation content and all the tasks are done in the single serializer function. The simplicity of Metro’s bundling concepts makes implementing any bespoke feature straightforward.</p><h3>Challenges of Adopting Metro at Airbnb</h3><p>Even though Metro has the architectural advantages described above, it also brought challenges to overcome in order to leverage it fully for the web. Because Metro is designed for use in a React Native environment, we needed to write <em>more code</em> to achieve feature parity with Webpack, so the decision to switch to Metro came at the expense of reinventing some wheels and learning the inner working of a JavaScript bundler that is usually abstracted away from us.</p><p>In development, we had to create a Metro server with custom endpoints to handle building dependency graphs, translation, bundling JS &amp; CSS files, and building source maps. For production builds, we ran Metro as a Node API to handle resolution, transformation, and serialization.</p><p>The surface area of the full migration was substantial, so we broke it down into two phases. Because the slow iteration speed of our Webpack setup incurred significant costs around developer productivity, we addressed the slow Webpack development experience with the Metro development server as our first priority. In the second phase, we brought Metro to feature parity with Webpack and ran an A/B test between Metro and Webpack in production. The two biggest challenges we faced along the way are outlined below.</p><h3>Bundle Splitting</h3><p>The out-of-the-box Metro setup for development produced giant ~5MiB bundles per entry point, since a single bundle is the intended use case for React Native. For the Web, this bundle size was taxing on browser resources and network latency. Every code change resulted in a 5MiB bundle being processed and downloaded, which was inefficient and could not be HTTP-cached. Even if the changed code recompiled instantly, we still needed to reduce the size and improve browser cacheability.</p><p>To improve the performance of Metro in the Web environment, we split the bundles by <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import#dynamic_import">dynamic import</a> boundaries, a technique also known as <a href="https://developer.mozilla.org/en-US/docs/Glossary/Code_splitting">code splitting</a>. The code splitting boundaries enabled us to leverage HTTP caching effectively.</p><p>In Figure 3, import(‘./file’) represents the dynamic import boundaries. The bundle on the left hand side (3a) is broken down to three smaller bundles on the right (3b). The additional bundles are requested when the import(‘./file’) statements are executed.</p><p>In Figure 3a, suppose fileA.js has changed, the entire bundle needs to be re-downloaded for the browser to pick up the change in fileA.js. With bundles split by dynamic import illustrated in Figure 3b, a change in fileA.js only results in re-downloading of the fileA.js bundle. The rest of the bundles can reuse browser cache.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/975/0*R2QCSRzc7ysunr3_" /></figure><p>Figure 3: Splitting bundles by dynamic import boundaries. A bundle is represented by the rectangular boxes with a pink background.</p><p>When we began to think about production bundles, we wanted to optimize a bit differently than in development. It takes time to run the bundle splitting algorithm, and we didn’t want to waste time on optimizing bundle sizes in development. Instead, we prioritized the page load performance over minimizing bundle sizes.</p><p>In production, we wanted to ship fewer and smaller JavaScript bundles to the end user so the page loads faster and the user experience is performant. There is no Metro development server in production, so all the bundles are pre-built. This makes bundle splitting the biggest blocking feature needed to make our Metro build production ready. With some inspiration from Webpack’s bundle splitting algorithm, we implemented a similar mechanism to split the Metro dependency graphs. The resulting bundle sizes decreased by ~20% (1549 KB –&gt; 1226 KB) on airbnb.com as compared to the development splitting by dynamic import boundaries.</p><p>On comparing the bundle splitting results between Metro and Webpack’s implementations, we realized both provided bundles of comparable sizes with a few pages shipping a slightly higher number of Javascript bundles with Metro. Despite the slightly heavier page weight, <a href="https://developer.mozilla.org/en-US/docs/Glossary/First_contentful_paint">TTFCP</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Largest_Contentful_Paint_API">largest contentful paint</a>, and <a href="https://web.dev/lighthouse-total-blocking-time/">Total Blocking Time</a> metrics are comparable between Metro and Webpack.</p><h3>Tree-shaking</h3><p>Bundle splitting alone decreased bundle sizes significantly, however we were able to make bundles even smaller by deleting dead code. However, it is not always obvious to identify what is considered dead code in a project, as some “dead code” in a project may be “used code” in the other projects. This is where tree-shaking came into play. It relied on the consistent usages of <a href="https://tc39.es/ecma262/#sec-modules">ECMAScript modules</a> (ESM) <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import">import</a>/<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/export">export</a> statements in the code base. Based on the import/export usages in a project, we analyzed what specific export statements were not imported by any file in the project. Finally, the bundler removes the unused export statements, making the overall bundle sizes smaller.</p><p>One challenge we faced while implementing the tree-shaking algorithm for Metro production builds was the risk of mistakenly removing code that is executed at runtime. For example, we ran into multiple bugs related to <a href="https://developer.mozilla.org/en-US/docs/web/javascript/reference/statements/export#re-exporting_aggregating">re-export statements</a>. Since Webpack handles ESM import/export statements in a different way, there was no comparable prior art for reference. After multiple iterations of tree-shaking algorithm implementation, the following table captures how much dead code we were finally able to drop given the project size.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WlsTTqWIGeJzk_ccUzFuBw.png" /></figure><h3>Conclusion</h3><p>The Metro migration brought forth some very significant improvements. The biggest Airbnb frontend project compiling ~48k modules (including server and browser compilations) saw a drop in the average build time by ~55% from 30.5 minutes to 13.8 minutes. Additionally, we saw improvements on the <a href="https://medium.com/airbnb-engineering/creating-airbnbs-page-performance-score-5f664be0936">Airbnb Page Performance Scores</a> with the pages built by Metro, ranging around +1%. The end user performance improvement was a nice surprise, as we initially aimed for achieving neutral experiment results.</p><p>The simplicity of Metro’s architecture has benefited us in many ways. Engineers from other teams have ramped up quickly to contribute to Airbnb’s Metro implementation, which means there is a lower barrier to entry for contributing to the bundling system. The <a href="https://facebook.github.io/metro/docs/caching">multi-layered cache system</a> is straightforward to work with, making experimentation with caching possible. The bespoke bundler feature integrations are made obvious and easier to implement.</p><p>We acknowledge that the landscape has changed since we evaluated <a href="https://parceljs.org/">Parcel</a>, <a href="https://webpack.js.org/">Webpack 4</a>, and <a href="https://facebook.github.io/metro/">Metro</a> back in 2018. There are other tools, such as <a href="https://rollupjs.org/guide/en/">rollup.js</a> and <a href="https://esbuild.github.io/">esbuild</a>, that we haven’t explored much, and we know that Metro isn’t a general-purpose JavaScript bundler when compared to Webpack. However, after a few years of working on Metro feature parity, the results we have seen have proven to us that it was a good decision to pursue Metro. Metro <strong>solved</strong> our most desperate scaling issues by dropping development and production build times. We are more productive than ever with instantaneous development feedback loops and faster production builds. If you would like to help us continue to improve our JavaScript tooling and build optimization, or tackle other web infrastructure challenges, check out these open roles at Airbnb:</p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=61d6ab411us">Senior Frontend Infrastructure Engineer, Web Platform</a></p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=61d6ab411us">Engineering Manager, Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/2623004/">Senior Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4168852/">Senior/Staff Software Engineer, Observability</a></p><h3>Acknowledgments</h3><p>Thank you everyone who has contributed to this multi-year project. We couldn’t have done it without any of you! Special shoutout to my lovely team <a href="mailto:michael.james@airbnb.com">Michael James</a> and <a href="mailto:noah.sugarman@airbnb.com">Noah Sugarman</a> for driving the Metro production migration to the finish line. Thank you <a href="mailto:breanna.bunge@airbnb.com">Brie Bunge</a>, <a href="mailto:dan.beam@airbnb.com">Dan Beam</a>, <a href="mailto:ian.myers@airbnb.com">Ian Myers</a>, <a href="mailto:ian.remmel@airbnb.com">Ian Remmel</a>, <a href="mailto:joe.lencioni@airbnb.com">Joe Lencioni</a>, <a href="mailto:madison.capps@airbnb.com">Madison Capps</a>, <a href="mailto:michael.james@airbnb.com">Michael James</a>, <a href="mailto:noah.sugarman@airbnb.com">Noah Sugarman</a> for reviewing and giving great feedback on this blog post.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cfc46d617a1f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/faster-javascript-builds-with-metro-cfc46d617a1f">Faster JavaScript Builds with Metro</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dynamic Kubernetes Cluster Scaling at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d79ae3afa132</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[cluster-autoscaler]]></category>
            <category><![CDATA[kubernetes]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[David Morrison]]></dc:creator>
            <pubDate>Mon, 23 May 2022 17:35:24 GMT</pubDate>
            <atom:updated>2022-05-23T17:35:24.292Z</atom:updated>
            <content:encoded><![CDATA[<p>Authors: <a href="https://www.linkedin.com/in/evansheng112/">Evan Sheng</a>, <a href="https://www.linkedin.com/in/david-morrison-9419b110/">David Morrison</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Elojmgc7Y06tItOaLdB0Cw.jpeg" /></figure><h3>Introduction</h3><p>An important part of running Airbnb’s infrastructure is ensuring our cloud spending automatically scales with demand, both up <strong>and </strong>down. Our traffic fluctuates heavily every day, and our cloud footprint should scale dynamically to support this.</p><p>To support this scaling, Airbnb utilizes Kubernetes, an open source container orchestration system. We also utilize OneTouch, a service configuration interface built on top of Kubernetes, and is described in more detail in a previous <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">post</a>.</p><p>In this post, we’ll talk about how we dynamically size our clusters using the Kubernetes Cluster Autoscaler, and highlight functionality we’ve contributed to the <a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">sig-autoscaling community</a>. These improvements add customizability and flexibility to meet Airbnb’s unique business requirements.</p><h3>Kubernetes Clusters at Airbnb</h3><p>Over the past few years, Airbnb has shifted almost all online services from manually orchestrated EC2 instances to Kubernetes. Today, we run thousands of nodes across nearly a hundred clusters to accommodate these workloads. However, this change didn’t happen overnight. During this migration, our underlying Kubernetes cluster setup evolved and became more sophisticated as more workloads and traffic shifted to our new technology stack. This evolution can be split into three stages.</p><p>Stage 1: Homogenous Clusters, Manual Scaling</p><p>Stage 2: Multiple Cluster Types, Independently Autoscaled</p><p>Stage 3: Heterogeneous Clusters, Autoscaled</p><h4>Stage 1: Homogenous Clusters, Manual Scaling</h4><p>Before using Kubernetes, each instance of a service was run on its own machine, and manually scaled to have the proper capacity to handle traffic increases. Capacity management varied per team and capacity would rarely be un-provisioned once load dropped.</p><p>Our initial Kubernetes cluster setup was relatively basic. We had a handful of clusters, each with a single underlying node type and configuration, which ran only stateless online services. As some of these services began shifting to Kubernetes, we started running containerized services in a multi-tenant environment (many pods on a node). This aggregation led to fewer wasted resources, and consolidated capacity management for these services to a single control point at the Kuberentes control plane. At this stage, we scaled our clusters manually, but this was still a marked improvement over the previous situation.</p><figure><img alt="An EC2 node running a single application vs. a Kubernetes node running 3 applications." src="https://cdn-images-1.medium.com/max/1024/0*xgJUXKfck5DuQOg1" /><figcaption>Figure 1: EC2 Nodes vs Kubernetes Nodes</figcaption></figure><h4>Stage 2: Multiple Cluster Types, Independently Autoscaled</h4><p>The second stage of our cluster configuration began when more diverse workload types, each with different requirements, sought to run on Kubernetes. To accommodate their needs, we created a cluster type abstraction. A “cluster type” defines the underlying configuration for a cluster, meaning that all clusters of a cluster type are identical, from node type to different cluster component settings.</p><p>More cluster types led to more clusters, and our initial strategy of manually managing capacity of each cluster quickly fell apart. To remedy this, we added the Kubernetes <a href="https://github.com/kubernetes/autoscaler">Cluster Autoscaler</a> to each of our clusters. This component automatically adjusts cluster size based on pod requests — if a cluster’s capacity is exhausted, and a pending pod’s request could be filled by adding a new node, Cluster Autoscaler launches one. Similarly, if there are nodes in a cluster that have been underutilized for an extended period of time, Cluster Autoscaler will remove these from the cluster. Adding this component worked beautifully for our setup, saved us roughly 5% of our total cloud spend, and the operational overhead of manually scaling clusters.</p><figure><img alt="Different Kubernetes clusters for different types of applications (CPU-bound or GPU-bound applications, for example)." src="https://cdn-images-1.medium.com/max/1024/0*XevtJSPUAo9vTpJn" /><figcaption>Figure 2: Kubernetes Cluster Types</figcaption></figure><h4>Stage 3: Heterogeneous Clusters, Autoscaled</h4><p>When nearly all online compute at Airbnb shifted to Kubernetes, the number of cluster types had grown to over 30, and the number of clusters to 100+. This expansion made Kubernetes cluster management tedious. For example, cluster upgrades had to be individually tested on each of our numerous cluster types.</p><p>In this third phase, we aimed to consolidate our cluster types by creating “heterogeneous” clusters that could accommodate many diverse workloads with a single Kubernetes control plane. First, this greatly reduces cluster management overhead, as having fewer, more general purpose clusters reduces the number configurations to test. Second, with the majority of Airbnb now running on our Kubernetes clusters, efficiency in each cluster provides a big lever to reduce cost. Consolidating cluster types allows us to run varied workloads in each cluster. This aggregation of workload types — some big and some small — can lead to better bin packing and efficiency, and thus higher utilization. With this additional workload flexibility, we had more room to implement sophisticated scaling strategies, outside of the default Cluster Autoscaler expansion logic. Specifically, we aimed to implement scaling logic that was tied to Airbnb specific business logic.</p><figure><img alt="A single Kubernetes cluster with multiple different node types: an Intel compute node, an AMD compute node, a high-memory node, and a GPU node." src="https://cdn-images-1.medium.com/max/1024/0*1GUcmg4jijWf_fdA" /><figcaption>Figure 3: A heterogeneous Kubernetes cluster</figcaption></figure><p>As we scaled and consolidated clusters so they were heterogeneous (multiple instance types per cluster), we began to implement specific business logic during expansion and realized some changes to the autoscaling behavior were necessary. The next section will describe some of the changes we’ve made to Cluster Autoscaler to make it more flexible.</p><h3>Cluster Autoscaler Improvements</h3><h4>Custom gRPC Expander</h4><p>The most significant improvement we made to Cluster Autoscaler was to provide a new method for determining node groups to scale. Internally, Cluster Autoscaler maintains a list of node groups which map to different candidates for scaling, and it filters out node groups that do not satisfy pod scheduling requirements by running a scheduling simulation against the current set of Pending (unschedulable) pods. If there are any Pending (unschedulable) pods, Cluster Autoscaler attempts to scale the cluster to accommodate these pods. Any node groups that satisfy all pod requirements are passed to a component called the <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-expanders">Expander</a>.</p><figure><img alt="A depiction of Cluster Autoscaler, which calls the Expander to determine which type of node to add in a heterogeneous Kubernetes cluster." src="https://cdn-images-1.medium.com/max/1024/0*ryQyolVdPY6bbSQy" /><figcaption>Figure 4: Cluster Autoscaler and Expander</figcaption></figure><p>The Expander is responsible for further filtering the node groups based on operational requirements. Cluster Autoscaler has a number of different built-in expander options, each with different logic. For example, the default is the random expander, which selects from available options uniformly at random. Another option,and the one that Airbnb has historically used, is the <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/priority">priority expander</a>, which chooses which node group to expand based on a user-specified tiered priority list.</p><p>As we moved toward our heterogeneous cluster logic, we found that the default expanders were not sophisticated enough to satisfy our more complex business requirements around cost and instance type selection.</p><p>As a contrived example, say we want to implement a weighted priority expander. Currently, the priority expander only lets users specify distinct tiers of node groups, meaning it will always expand tiers deterministically and in order. If there are multiple node groups in a tier, it will break ties randomly. A weighted priority strategy of setting two node groups in the same tier, but expanding one 80% of the time, and another 20% of the time, is not achievable with the default setup.</p><p>Outside of the limitations of the current supported expanders, there were a few operational concerns:</p><ol><li>Cluster Autoscaler’s release pipeline is rigorous and changes take time to review before being merged upstream. However, our business logic and desired scaling strategy is continuously changing. Developing an expander to fill our current needs today may not fulfill our needs in the future</li><li>Our business logic is specific to Airbnb and not necessarily other users. Any changes we implement specific to our logic would not be useful to contribute back upstream</li></ol><p>From these, we came up with a set of requirements for a new expander type in Cluster Autoscaler:</p><ol><li>We wanted something that was both extensible and usable by others. Others may run into similar limitations with the default Expanders at scale, and we would like to provide a generalized solution and contribute functionality back upstream</li><li>Our solution should be deployable out of band with Cluster Autoscaler, and allow us to respond more rapidly to changing business needs</li><li>Our solution should fit into the Kubernetes Cluster Autoscaler ecosystem, so that we do not have to maintain a fork of Cluster Autoscaler indefinitely</li></ol><p>With these requirements, we came up with a design that breaks out the expansion responsibility from the Cluster Autoscaler core logic. We designed a pluggable “<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/expander/grpcplugin">custom Expander</a>.” which is implemented as a gRPC client (similarly to the <a href="https://github.com/kubernetes/autoscaler/blob/68c984472acce69cba89d96d724d25b3c78fc4a0/cluster-autoscaler/proposals/plugable-provider-grpc.md">custom cloud provider</a>). This custom expander is broken into two components.</p><p>The first component is a gRPC client built into Cluster Autoscaler. This Expander conforms to the same interface as other Expanders in Cluster Autoscaler, and is responsible for transforming information about valid node groups from Cluster Autoscaler to a defined <a href="https://developers.google.com/protocol-buffers/docs/overview">protobuf</a> schema (shown below), and receives the output from the gRPC server to transform back to a final list of options for Cluster Autoscaler to scale up.</p><pre>service Expander {<br>  rpc BestOptions (BestOptionsRequest) returns (BestOptionsResponse) <br>}</pre><pre>message BestOptionsRequest {<br>  repeated Option options;<br>  map&lt;string, k8s.io.api.core.v1.Node&gt; nodeInfoMap;<br>}</pre><pre>message BestOptionsResponse {<br>  repeated Option options;<br>}</pre><pre>message Option {<br>  // ID of node to uniquely identify the nodeGroup<br>  string nodeGroupId;<br>  int32 nodeCount;<br>  string debug;<br>  repeated k8s.io.api.core.v1.Pod pod;<br>}</pre><p>The second component is the gRPC server, which is left up to the user to write. This server is intended to be run as a separate application or service, which can run arbitrarily complex expansion logic when selecting which node group to scale up, with the given information passed from the client. Currently, the protobuf messages passed over gRPC are slightly transformed versions of what is passed to the Expander in Cluster Autoscaler.</p><p>From our aforementioned example, a weighted random priority expander can be implemented fairly easily by having the server read from a priority tier list and weighted percentage configuration from a configmap, and choose accordingly.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MldTcDs1Df38IfHE" /><figcaption>Figure 5: Cluster Autoscaler and Custom gRPC Expander</figcaption></figure><p>Our implementation includes a failsafe option. It is recommended to use the option to pass in <a href="https://github.com/kubernetes/autoscaler/pull/4233">multiple expanders</a> as arguments to Cluster Autoscaler. With this option, if the server fails, Cluster Autoscaler is still able to expand using a fallback Expander.</p><p>Since it runs as a separate application, expansion logic can be developed out of band with Cluster Autoscaler, and since the gRPC server is customizable by the user based on their needs, this solution is extensible and useful to the wider community as a whole.</p><p>Internally, Airbnb has been using this new solution to scale all of our clusters without issues since the beginning of 2022. It has allowed us to dynamically choose when to expand certain node groups to meet Airbnb’s business needs, thus achieving our initial goal of developing an extensible custom expander.</p><p>Our custom expander was <a href="https://github.com/kubernetes/autoscaler/pull/4452">accepted</a> into the upstream Cluster Autoscaler earlier this year, and will be available to use in the next version (v1.24.0) release.</p><h3>Other Autoscaler Improvements</h3><p>Over the course of our migration to heterogeneous Kubernetes clusters, we identified a number of other bugs and improvements that could be made to Cluster Autoscaler. These are briefly described below:</p><ul><li><a href="https://github.com/kubernetes/autoscaler/pull/4489">Early abort for AWS ASGs with no capacity</a>: Short circuit the Cluster Autoscaler loop to wait for nodes it tries to spin up to see if they are ready by calling out to an AWS EC2 endpoint to check if the ASG has capacity. With this change enabled, users get much more rapid, yet correct scaling. Previously, users using a priority ladder would have to wait 15 minutes between each attempted ASG launch, before trying an ASG of lower priority.</li><li><a href="https://github.com/kubernetes/autoscaler/pull/4073">Caching launch templates to reduce AWS API calls</a>: Introduce a cache for AWS ASG Launch Templates. This change unlocks using large numbers of ASGs, which was critical for our generalized cluster strategy. Previously, for empty ASGs (no present nodes in a cluster), Cluster Autoscaler would repeatedly call an AWS endpoint to get launch templates, resulting in throttling from the AWS API.</li></ul><h3>Conclusion</h3><p>In the last four years, Airbnb has come a long way in our Kubernetes Cluster setup. Having the largest portion of compute at Airbnb on a single platform provided a strong, consolidated lever to improve efficiency, and we are now focused on generalizing our cluster setup (think <a href="http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/">“cattle, not pets”</a>). By developing and using a more sophisticated expander in Cluster Autoscaler (as well as fixing a number of other minor issues with the Autoscaler), we have been able to achieve our goals of developing our complex, business specific scaling strategy around cost and mixed instance types, while also contributing some useful features back to the community.</p><p>For more details on our heterogeneous cluster migration, watch our Kube-Con <a href="https://www.youtube.com/watch?v=GCCSY7ERXj4&amp;ab_channel=CNCF%5BCloudNativeComputingFoundation%5D">talk</a> and we’re also at KubeCon EU this year, come talk to us! If you’re interested in working on interesting problems like the ones we’ve described here, we’re hiring! Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/3949745/">Engineering Manager- Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3903900/">Senior Front End Engineer</a></p><p><a href="https://careers.airbnb.com/positions/2623004/">Senior Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4168852/">Software Engineer, Observability</a></p><p><a href="https://careers.airbnb.com/positions/3696687/">Software Engineer, Developer Infrastructure</a></p><h3>Acknowledgements</h3><p>The evolution of our Kubernetes Cluster setup is the work of many different collaborators. Special thanks to Stephen Chan, Jian Cheung, Ben Hughes, Ramya Krishnan, David Morrison, Sunil Shah, Jon Tai and Long Zhang, as this work would not have been possible without them.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d79ae3afa132" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132">Dynamic Kubernetes Cluster Scaling at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Kamini Dandapani]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/7f51f1fbb2bb</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[leadership]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[people]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Wed, 11 May 2022 19:35:36 GMT</pubDate>
            <atom:updated>2022-05-11T19:35:36.825Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Kamini Dandapani</h3><p>Airbnb’s VP of Engineering on why you don’t have to change your natural self to be a leader</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/0*t-dDS7QYW1gsBtG5" /></figure><p><a href="https://www.linkedin.com/in/kaminidandapani/">Kamini Dandapani</a>, VP of Engineering at Airbnb, leads the Infrastructure Engineering organization, which is in many ways the backbone of the company: responsible for powering the systems that keep Airbnb running smoothly and help new products reach millions of people. With a passion for how platforms can support and sustain the business and product, Kamini developed her considerate and welcoming leadership style at eBay and LinkedIn before joining Airbnb two years ago. In addition to her Infra role, she champions diversity and belonging in the workplace and is co-sponsor for Airbnb’s tech diversity council, which aims to create the most diverse and inclusive community in the tech industry.</p><p><em>Want to hear Kamini and other Infrastructure team members talk about some of the team’s latest projects? Check out the </em><a href="https://www.facebook.com/AirbnbTech/videos/635338454172729/"><em>“Powering Our Platform” Airbnb Tech Talk</em></a><em> from March 2022. You’ll hear about some of the major initiatives we’re working on in next-generation service mesh, observability, feature engineering, and scalable storage.</em></p><h3>From Chennai to Chicago</h3><p>Growing up in India, I was the youngest of three girls. Despite facing skepticism and criticism from others around them, my parents invested heavily in our education and gave us a very strong footing, without which I don’t think I would be where I am today.</p><p>I started familiarizing myself with the engineering world, and found that I immensely enjoyed it. I did my undergrad in electronics and communication, and with my dad’s encouragement — he camped out overnight in the line in front of the US Consulate to get a visa — I came to the US to pursue my master’s in computer science.</p><p>In Chicago, I had to adjust to a lot of new experiences (including the winter cold!). In India, I never did anything alone, but here I had to do everything independently, from managing my finances to driving a car. After I graduated, I felt very fortunate to get a job in Silicon Valley, and I’ve stayed here ever since.</p><h3>Leading at the intersection of platform and product</h3><p>Effective infrastructure can’t be built in a vacuum. Rather, it requires close partnerships with our product engineers to support both our product and overall business strategies. My professional sweet spot is where the platform architecture meets the end-user experience — plus scale!</p><p>In my engineering career, I worked at eBay for 12 years and grew into a director position, leading international expansion. After that, I was at LinkedIn for six years, leading infrastructure and tools for the consumer app, and that’s where I learned how to operate and develop a platform at scale. When Airbnb got in touch with me, I wasn’t looking for a change. But with every conversation that I had, there was something truly magical about the place — from the leadership, to the inclusivity, to the company’s mission — and I am so grateful that I made the leap.</p><p>What excited me most was bringing dozens of years of operating at scale to Airbnb. And one key component to operating at scale is working effectively and smoothly cross-functionally, and building close relationships with our product teams and key partners across the business. I’ve seen some truly incredible teamwork within my own team, and across all of Airbnb.</p><h3>Building the tech backbone of Airbnb</h3><p>Most of the technical foundation that powers Airbnb comes from the Infrastructure organization. The impact that this group has is so wide and profound.</p><p>The Infrastructure organization has several key pillars:</p><ul><li><strong>Search Infrastructure</strong>, which powers the backend systems for our guest search experience</li><li><strong>Data Platform</strong>, for storing, processing and managing all the data that powers every user experience</li><li><strong>Developer Platform</strong>, which helps make Airbnb engineers’ lives friction-free by building tools, services and environments to help them develop, build, test and deploy their code</li><li><strong>Cloud Infrastructure,</strong> which delivers and operates the cloud environment that powers Airbnb</li><li><strong>Reliability Engineering, </strong>which remedies and prevents site performance issues through tooling and automation</li></ul><p>Within each of these areas, we have many long-term, multi-year projects, all part of what we’re calling Tech Stack 2.0: an evolution and modernization of our technology. Some sample initiatives include <a href="https://news.airbnb.com/unique-stays-hosts-earn-more-than-300-million-since-start-of-pandemic/">flexible search for guests</a> and UDS, our pioneering next-generation storage system.</p><h3>My identity: female, South Asian, immigrant</h3><p>People often point out that I’m unique for being a female leader in tech. But in reality, there are three important aspects of my identity: yes, I’m a woman, but I’m also South Asian and an immigrant. All of these have shaped who I am today.</p><p>I grew up in a very different culture. We were discouraged from challenging the status quo, and for my parents and grandparents, the idea was that if you work extremely hard, recognition will follow. That’s not the way it works here: it sometimes seems like you need to have an opinion and advocate for yourself in order to be taken seriously.</p><p>In many ways, I think being different is an advantage as a leader. While I encourage everyone on the team to make sure their voice is being heard, I also believe in being your natural self. That’s how I’ve been able to build trust with my teams, by letting them see the real me. My philosophy is that no one can be an expert in everything. What you’ll see is varying degrees in people — and I want to fully support that diversity of thought and experience, because a team that’s well-rounded is more effective.</p><h3>Bringing people along</h3><p>When joining Airbnb, I asked to have the dedicated time and agency to do work around diversity and gender parity. I’m now the co-sponsor for the Tech Diversity Council alongside <a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-lucius-diphillips-79d1f0bc72a2">Lucius DiPhillips</a> (CIO), where we advocate for diversity-related projects around the Tech org. I’m also one of the advisors for our Asians@ employee resource group.</p><p>There’s something special about these employee resource groups here at Airbnb that I haven’t seen before. It’s a very small close-knit group, and we can relate to our similar upbringing and cultural norms. We genuinely look out for each other and amplify our Asian@ colleagues’ voices.</p><p>There’s a saying that “if you want to go fast, go alone, but if you want to go far, go together.” Whether it’s sharing context about our work, being vulnerable about my mistakes, or building a diverse organization, I very much believe in bringing people along. I couldn’t be at a better place than here at Airbnb, where our company’s mission is for anyone to belong anywhere.</p><p><em>Interested in working at Airbnb? We’re hiring! Check out these open roles:</em></p><p><a href="https://careers.airbnb.com/positions/3029584/">Staff Software Engineer, Distributed Storage</a></p><p><a href="https://careers.airbnb.com/positions/3903900/?gh_src=3da3a8881us">Senior Frontend Infrastructure Engineer, Web Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3747712/">Senior/Staff Backup and Recovery Engineer, Storage Infrastructure</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7f51f1fbb2bb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-kamini-dandapani-7f51f1fbb2bb">My Journey to Airbnb — Kamini Dandapani</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Continuous Delivery at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/6ac042bc7876</guid>
            <category><![CDATA[migration]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[deployment]]></category>
            <dc:creator><![CDATA[jens vanderhaeghe]]></dc:creator>
            <pubDate>Fri, 22 Apr 2022 17:09:17 GMT</pubDate>
            <atom:updated>2022-04-22T17:09:17.675Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/939/1*_bvb5WtcQRE3mL-32b0F4g@2x.png" /></figure><p><a href="https://www.linkedin.com/in/jensvanderhaeghe">Jens Vanderhaeghe</a>,<a href="https://www.linkedin.com/in/manishma"> </a><a href="mailto:manish.maheshwari@airbnb.com">Manish Maheshwari</a></p><h3>Introduction</h3><p>Over the years, Airbnb’s tech stack has <a href="https://medium.com/airbnb-engineering/building-services-at-airbnb-part-1-c4c1d8fa811b">shifted</a> from a monolith to 1,000+ services in our service-oriented architecture (SOA). While this migration solved our problems scaling our application architecture, it also introduced an array of new challenges.</p><p>In this blog post we’ll cover the deployment challenges faced on the road to our current architecture and how we’ve solved those problems by adopting Continuous Delivery best practices on top of <a href="https://spinnaker.io/">Spinnaker</a>. We’ll do a deep dive into how we’ve solved such a large scale migration in a short timespan while maintaining developer productivity along the way.</p><h3>From Deployboard to Spinnaker</h3><p><a href="https://medium.com/airbnb-engineering/introducing-deploy-pipelines-to-airbnb-fc804ac2a157">Deployboard</a>, Airbnb’s legacy deployment tool, was designed for a monolith having a few centrally managed pipelines. As we started moving to SOA, thousands of code changes across hundreds of service teams were being deployed. Deployboard was not designed for the SOA architecture, which is characterized by decentralized deployments. We needed something much more templated so that teams could quickly get a standard, best-practice pipeline, rather than start from scratch for every new service. Rather than continuing to build in-house solutions with siloed knowledge, it made the most sense for us to adopt open source solutions built from the ground-up for decentralized, SOA pipelines.</p><p>Spinnaker is proven at Airbnb’s scale, and beyond, by industry peers like Google and Netflix. We believe continuous delivery isn’t a problem unique to Airbnb, and decided we’d benefit from collaborating with the larger community. We chose Spinnaker as the replacement for Deployboard in part because we could bridge functionality gaps by plugging in custom logic easily, without forking the core code. Also, it was important to us that Spinnaker automated canary analysis (ACA), an extremely effective strategy in reducing the blast radius of buggy deployments.</p><h3>Migrating to Spinnaker</h3><p>When deciding to switch rather than evolve, we created a new problem: How do we get a globally distributed team of thousands of engineers working on thousands of services (each with their own deploy pipeline), working under business pressure to continuously improve their product and code base, to change one of the most important tools they depend on for day-to-day productivity.</p><p>We were particularly worried about the “long-tail migration problem,” where we successfully get 80% of the services migrated in the first year or so, but the remaining ones become stuck indefinitely on the old system. Having to operate in such a hybrid mode is costly, and it also is a reliability and even security risk, because the “legacy” systems (including the legacy deploy system) receive less and less attention over time.</p><p>Rather than forcing yet another new tool on our engineers, we came up with a migration strategy based on three pillars: focus on the benefits, automated onboarding, and data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/732/0*2fjFQ8VIEm7LD_Iz" /><figcaption>The 3 pillars of our migration strategy</figcaption></figure><h4>Focus on Benefits</h4><p>By focusing on the benefits of Spinnaker, we encouraged engineering teams to adopt Spinnaker voluntarily rather than forcing them.</p><p>We started out by manually onboarding a small group of early adopters. We identified a set of services that were prone to causing incidents or had a complicated deployment process. By migrating these services onto Spinnaker and automating their release process using a deployment pipeline with ACA, we were quickly able to demonstrate value. As we onboarded more teams, we iterated on the feature gaps between Deployboard and Spinnaker. These early services served as case studies, proving to both the rest of engineering as well as leadership that adopting an automated and standardized deployment process provides huge benefits.</p><p>These early adopters saw benefits so significant that they ended up becoming evangelists for continuous delivery and Spinnaker, spreading the word to other teams organically.</p><h4>Automated Onboarding</h4><p>As more and more services started adopting Spinnaker, the Continuous Delivery team could no longer keep up with demand. We switched gears and focused on building automated tooling to onboard services to Spinnaker.</p><p>At Airbnb, we store configuration as code <a href="https://www.infoq.com/presentations/airbnb-kubernetes-services/">using a framework called OneTouch</a>. This allows engineers to make changes to the code as well as the infrastructure running their code in a single commit and in the same folder. All infrastructure changes are version controlled.</p><figure><img alt="Example of a codified Spinnaker pipeline" src="https://cdn-images-1.medium.com/max/1024/0*1fytVc5gswBQQYwZ" /><figcaption>Example of a codified Spinnaker pipeline</figcaption></figure><p>Following the OneTouch philosophy, we created an abstraction layer on top of Spinnaker that enables all continuous delivery configuration to be source controlled and managed by our existing tools and processes.</p><p>Today, when new services are created they get Spinnaker integration, including ACA, for free out of the box.</p><h4>Data</h4><p>In addition to focusing on the benefits and making it easy to onboard, we wanted to clearly communicate the value-add of adopting Spinnaker in a data-driven way. We automatically instrumented <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Superset</a> dashboards for each service that adopted Spinnaker.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UaIkauaDzJcPZuOZ" /></figure><p>Example of an instrumented dashboard for a service that has adopted Spinnaker</p><p>Service owners get insight into deployment data like deploy frequency and number of regressions prevented by ACA. Most service owners saw a significant increase in deployment frequency and a marked decrease in production incidents by adopting our new tooling. By arming our users with the right data, they can more easily advocate for the benefits of adopting continuous delivery.</p><h3>Clearing the final hurdle</h3><p>As expected, we eventually hit an inflection point in adoption. Organic adoption slowed as we reached ~85% of deployments being done on Spinnaker.</p><p>Once we hit this point, it was time to switch our strategy again, to adopt the lagging services. Our plan consisted of the following steps.</p><ol><li><strong><em>Stop the bleeding</em></strong> <br>The first thing we did is stop any new service from being deployed with Deployboard. This kept our list of remaining services to adopt static. We did this by giving engineers ample heads-up that this change was coming.</li><li><strong><em>Announce deprecation date + increase friction </em></strong><br>We gradually increased friction when using Deployboard over Spinnaker by adding a banner and warning inside Deployboard. We also instituted an exemption process that would allow us to catch major blockers well before the actual deprecation date without hurting customer experience.</li><li><strong><em>Send out automated PRs for the remaining services. </em></strong><br>To ensure we could also help onboard services where owners are resource constrained we once again leveraged tools like our in-house refactor tool,Refactorator, to do the heavy lifting.</li><li><strong>Deprecation date and post-deprecation follow-up. </strong><br>On deprecation date, we had code in place that blocked any OneTouch deploy from Deployboard. We had some loopholes in place in case there were services that still needed to use Deployboard for emergency reasons. The exemption list allows them to temporary get access to Deployboard. Engineers on the CD team can also still deploy with Deployboard, a simple page to the on-call can quickly help service owners in this case. As of today, the number of those cases remains very minimal given the amount of preparation we’ve done.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vXsxUvTvvx0rP7gw" /><figcaption>By adding a banner to Deployboard recommending engineers to adopt Spinnaker, we were able to drive adoption more quickly.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-QuiF4QGkdQN1MV_" /><figcaption>Example of an automated Pull Request that migrates a service from Deployboard to Spinnaker with minimal engineering effort.</figcaption></figure><h3>Future Plans and Opportunities</h3><p>Now that we’ve standardized our deployment process, we’re excited to integrate various existing tools at Airbnb into our continuous delivery pipelines. In 2022 and beyond, we are investing resources into integrating automated load testing, providing a way to safely toggle feature flags, and enabling blue/green deployments to facilitate instant rollbacks. More broadly, we see Spinnaker not only as a tool for code deployments, but also for the automation of various manual processes, allowing engineers to orchestrate any arbitrary workload as a pipeline.</p><p>During our migration, we’ve made a ton of modifications, both large and small, to Spinnaker, which is a testament to how flexible the tool is. We will be focused on upgrading to the latest open-source version and are looking forward to contributing some of our changes back to the open-source community.</p><h3>Conclusion</h3><p>In our move from a monolithic architecture to SOA, we needed to rethink the way we do deployments at Airbnb.</p><p>By creating a Continuous Delivery team focused on delivering great tools to safely and easily deploy code, we were able to migrate from our in-house tool, Deployboard, to Spinnaker. This was a very carefully planned and crafted migration. To adopt the majority of services, we focused on the benefits using a data-driven and automated approach to migration.</p><p>As expected, there was a long tail of services that didn’t organically adopt our new tools. We were able to get to the 100% finish line by shifting our strategy towards adding more friction and eventually deprecating our old tool.</p><p>This migration now serves as a blueprint for other infrastructure related migrations at Airbnb and we look forward to continuing iterating on our strategies for bringing better tools to our engineers while maintaining existing productivity and reducing toil.</p><h3>Acknowledgments</h3><p>All of our achievements wouldn’t have been possible without support of the entire Continuous Delivery team: <a href="mailto:jerry.chung@airbnb.com">Jerry Chung</a>, Freddy Chen, Alper Kokmen, Brian Wolfe, <a href="mailto:dion.hagan@airbnb.com">Dion Hagan</a>, Ryan Zelen, Greg Foster, Jens Vanderhaeghe, Mohamed Mohamed, Jake Silver, Manish Maheshwari and Shylaja Ramachandra. The entire Developer Platform organization rallied behind this effort. We’re also grateful to the countless engineers at Airbnb that have adopted Spinnaker over the years and have provided us with valuable feedback. We’d also like to thank all of the people at our peer companies and volunteers who have spent countless hours working on the open source Spinnaker project.</p><p><strong><em>Interested in working at Airbnb? Check out these open roles:</em></strong></p><p><a href="https://careers.airbnb.com/positions/3696687/?gh_src=08eeee991us">Senior/Staff Software Engineer, Developer Infrastructure</a><br><a href="https://careers.airbnb.com/positions/3903900/?gh_src=e91bd0291us">Senior Frontend Infrastructure Engineer, Web Platform</a></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ac042bc7876" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876">Continuous Delivery at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Florian Andes]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-florian-andes-5080685262d3?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5080685262d3</guid>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[business-development]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[program-management]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Tue, 12 Apr 2022 17:56:39 GMT</pubDate>
            <atom:updated>2022-04-12T17:56:06.942Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Florian Andes</h3><p>From building airplanes to Staff Technical Program Manager at Airbnb</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gHm8C-tJ3j2bavkT" /></figure><p><a href="https://www.linkedin.com/in/floandes/"><em>Florian Andes</em></a><em> is a Staff Technical Program Manager at Airbnb. He has over 10 years of experience that spans the software, manufacturing, and strategy consulting industry. He studied in Frankfurt, London, Singapore, and Boston, where he received a bachelor’s and MBA degree in Business and Entrepreneurship.</em></p><p><em>Though it can be hard and intimidating to find your place in the “big tech” industry in Silicon Valley, Florian has relied on curiosity and openness to establish a successful career at Airbnb. Read on for Florian’s own words on working at the intersection of business and software engineering, transferring to the US and scaling tech programs from zero to 10x for Airbnb, and more.</em></p><h3>A global citizen finding a home for his career</h3><p>Many years ago, Airbnb had a tagline that really inspired me: “Don’t Go There. Live There.” I’ve tried to embrace that idea as much as I could. I’ve had the chance to live and work in several different countries — Germany, the UK, Singapore, China, and now the US, where I’m currently based in San Francisco.</p><p>I grew up in Southern Germany, specifically a smaller city called Ulm (which is famous for Albert Einstein, and for having the tallest church building in the world). Growing up, my dad was a great inspiration for me. In his twenties, he started his own business. His journey taught me a lot about perseverance, dedication, and visionary thinking. I’m still inspired by how he pioneered the intersection of hardware and software engineering in the very early days of computers.</p><p>In my career, I’ve broadly moved from hardware to software engineering across different industries. (My first job was folding pizza boxes at a friend’s restaurant.) In Germany, there’s a large hardware and manufacturing presence: cars, industrial machines, and appliances in general. I started off at Airbus, building airplanes with advanced plastics and fiber materials, and then moved into the automotive industry and strategy consulting in the mobility space.</p><p>Along the way, I became really interested in software companies and internet technology. In Singapore, where I studied for my MBA, I joined a fast-growing tech startup to manage their partnerships across APAC. I loved it. I was flying around Southeast Asia to speak at startup events, pitch to investors, explain the idea and what problem we were solving — it was all very exciting to me.</p><p>I ended up attending a fireside chat with Mike Curtis, Airbnb’s VP of Engineering at that time, hosted by a local startup co-working space, where he shared lessons he picked up on his journey. I was inspired and ended up connecting with Mike and some folks from Airbnb after the talk, and the opportunity for me to join Airbnb grew organically from there.</p><h3>Pioneering projects at the intersection of tech and business</h3><p>TPMs are involved in every major release at Airbnb. Some TPMs are more product and business-focused (that’s me), and others are more infrastructure and platform-oriented. The potential for impact is high because you’re often very close to engineering leaders. You have conversations with the CTO, and many senior leaders in engineering — and on our product teams, as well.</p><p>I’ve been with Airbnb now for more than five years, and over time, my work has expanded in technical depth and also in breadth. I started out with a focus on business operations and strategy in EMEA (based in Berlin), then transferred to the US to build out an entirely new API program from scratch, and I’m now overseeing all technical programs related to hosting products at Airbnb. It’s a really interesting area that bridges the gap between the tech (engineering, product, data science, and design) and commercial parts of our business.</p><p>There are two projects I have been involved in recently that demonstrate the breadth of our programs. One is the adoption of our <a href="https://airbnb.design/building-a-visual-language/">Design Language System</a> (DLS), a repository of prebuilt components that designers and engineers can reuse instead of building something new. So whenever an engineer at Airbnb has to implement a button, they don’t have to build this from scratch. Another is reducing our <a href="https://medium.com/airbnb-engineering/our-journey-towards-cloud-efficiency-9c02ba04ade8">AWS costs</a>. We introduced a new attribution model at Airbnb that directly associates AWS costs to different teams, and my role was monitoring our organization’s consumption and championing a lot of cultural change to think about AWS cost-efficiency whenever we build products.</p><h3>Strategies for remote program management</h3><p>Recently a big focus has been, naturally, keeping teams engaged and collaborative during remote work. Communicating a clear program vision and using frameworks to keep projects on track are two strategies that are more important than ever.</p><p>One other thing that I consider critical is celebrating wins and key milestones. Before, when we were in the office, it was much easier to celebrate big wins. At Airbnb, we have a tool where you can send appreciation to others. I use it all the time, because when I work with multiple stakeholders across design, product, engineering, and QA, people are generally contributing at different stages. When the project ends, you might not have this big forum anymore, because most people have moved on already. But it’s important to still give credit to the work that everyone contributed along the way and make sure their managers have visibility into their contributions.</p><h3>What makes the TPM role unique at Airbnb</h3><p>TPM is relatively new as a function itself, and every company approaches it slightly differently. Even at Airbnb, it’s continuously evolving and progressing. One thing that’s always the case, though, is that TPMs need to use influence to lead programs, teams, and products — without exercising direct authority. As a TPM, you have to champion ideas, be a great communicator, and work with your partners to align priorities and push projects forward.</p><p>You don’t have to study software engineering to be a technical program manager — I think you need to show that you have the ability to grow and learn on the technical side, and this can happen before the job or on the job. For instance, at first I didn’t have much context about AWS optimization, frontend design language systems, or open source, but people at Airbnb are super open to sharing knowledge which allowed me to quickly onboard and take on these programs. You just need to be curious and open to learn.</p><p>I love working at Airbnb because of the people and the mission-driven nature of the company. And I think what makes being a TPM at Airbnb extremely unique is that there’s still a lot of “greenfield” areas and unexplored territory, where you can really help define a new strategy and vision.</p><p><em>Interested in joining Airbnb as a TPM? Check out these open roles:</em></p><p><a href="https://careers.airbnb.com/positions/4024213/?gh_src=6b8b81e61us">Senior TPM, Guest and Host Technology</a></p><p><a href="https://careers.airbnb.com/positions/3955056/?gh_src=8a794a6e1us">Staff TPM, Insurance Platform</a></p><p><a href="https://careers.airbnb.com/positions/3651016/?gh_src=77ab28041us">Staff TPM, Infrastructure Regionalization</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5080685262d3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-florian-andes-5080685262d3">My Journey to Airbnb — Florian Andes</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hacking Human Connection: The Story of Awedience]]></title>
            <link>https://medium.com/airbnb-engineering/hacking-human-connection-the-story-of-awedience-ebf66ee6af0e?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ebf66ee6af0e</guid>
            <category><![CDATA[connection]]></category>
            <category><![CDATA[meetings]]></category>
            <category><![CDATA[pandemic]]></category>
            <category><![CDATA[virtual-events]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Avand Amiri]]></dc:creator>
            <pubDate>Tue, 05 Apr 2022 17:29:35 GMT</pubDate>
            <atom:updated>2022-04-11T22:22:11.531Z</atom:updated>
            <content:encoded><![CDATA[<h4>How a home-grown product helps Airbnb employees feel more connected during solitary times</h4><figure><img alt="A screenshot of Awedience during an all-company meeting. Brian Chesky, CEO, is in the center and is surrounded by thousands of employees represented by tiny squares. Each square is either a picture of the employee, a color, or a letter that people have arranged to spell words or draw shapes. Emojis are captured emerging from some of the seats." src="https://cdn-images-1.medium.com/max/1024/1*u9pnag1uWfgwnT8tv42I1g.jpeg" /></figure><h3>Introduction</h3><p>This is the story of how Airbnb employees stayed connected during a time they had never felt more apart. In this post, you’ll learn how an idea turned into an internal product that is now a core part of how Airbnb operates.</p><p>When you walk through the doors of an Airbnb office, you feel an energy that’s both inspiring and intimidating. After more than five years with the company, I explain this duality as Airbnb being both incredibly entrepreneurial and aspirational.</p><p>Airbnb company meetings are no different. Brian Chesky and his team keep our all-hands meetings exciting. I know what you’re thinking: “exciting meetings?!” But in all seriousness our all-hands are not just informative, they’re spectacular. Whether it’s drinking a smoothie of dehydrated bugs in solidarity with Engineering or eating spicy chicken wings in a Hot Ones-style Q&amp;A, our meetings are informative, energizing, and engaging. In somber moments, they’re human and heartfelt.</p><p>The pandemic changed that. Feeling connected to the presenter, feeling connected to our peers, or feeling the presenter’s connection to the audience all vanished. Instead, we each separately watched the presenter streaming through a 16:9 rectangle on our laptops. Other people were watching concurrently (one could assume based on the invite) but it couldn’t be <em>felt</em>. Inspired, I set out to solve this, wondering,“<a href="https://en.wikipedia.org/wiki/The_Six_Million_Dollar_Man">we have the technology</a>, why can’t we see and interact with everyone watching live?”</p><h3>Inspiration</h3><p>It’s impossible to know under which circumstances inspiration will find us, and in hindsight it seems perfectly planned.</p><p>Airbnb is a community based on connection and belonging, which is the reason so many of us came to work here. However, in March of 2020, as the spread of COVID-19 forced us to shelter in place, we struggled to find ways to preserve those traits within our company culture. What it meant to feel connected to the world and to each other was being redefined by emojis over video. Video chat helps us stay in communication, but it’s <a href="https://www.youtube.com/watch?v=DYu_bGbZiiQ">comically clunky</a>, dry, and lifeless. If these technologies feel contrived it’s because they are. We all know what an authentic human connection actually feels like, and it’s not that.</p><p>For almost two years before the pandemic hit, I volunteered to help produce Airbnb’s quarterly engineering all-hands. Unbeknownst to me, I was actually studying the question, “how does one make people feel more connected?” Whether it meant hosting an orca-themed relay race across our Portland, Seattle, and San Francisco offices, or projecting live chat in auditoriums to make the audience more engaged, I had been finding ways to foster human connection.</p><figure><img alt="Engineers dressed up in costumes working in pairs at laptops. Ping pong balls are being throw at them from the crowd, while they try to focus on solving engineering problems on the computers." src="https://cdn-images-1.medium.com/max/1024/1*SIXAHtcfgeFICxZ6B11UwA.gif" /><figcaption>Pairs of engineers complete programming challenges, while being pelted with ping pong balls by the audience. This was just one of the many ridiculous segments of Nerds@, the engineering all-hands meeting I voluntarily produced for two years.</figcaption></figure><p>Isolated at home with all these insights into event production, it’s not surprising that, while I watched our CEO, Brian Chesky, address thousands of employees via live stream, my attention drifted to the little number in the top right corner that showed how many other people were watching. These were my colleagues, my friends — some of the most talented people in the world. In our isolation, the only affordance that existed to capture our experience of watching this live stream together was an aggregate count. Instagram and YouTube allowed people to express themselves with emojis and comments during live broadcasts. Internally, we could not.</p><figure><img alt="Photos of various employees at-home workstations. Some photos are laptops on a desk or coffee table. Others are projected onto a TV in a living room. Two of the photos feature pets in the background." src="https://cdn-images-1.medium.com/max/1024/1*t-XAOqp7jvfTDsFhQpSejQ.png" /><figcaption>A look at life before Awedience: everyone watching the same stream at home alone, with the exception of some furry pals.</figcaption></figure><p>An idea started to emerge: supplement the live stream with small, thumbnail-sized videos of all the viewers’ webcams and capture audience sentiment with emojis. It seemed simple enough.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rat4XcYbupe-pho3KM1wqw.jpeg" /><figcaption>The original sketch. Simple, right?</figcaption></figure><p>Yet even this concept proved untenable. I had never built a fully real-time web application and would likely need months to figure out the webcam piece. I didn’t have months. I was only willing to dedicate a few days to see if this idea had merit.</p><p><a href="https://entrepreneurshandbook.co/how-airbnb-founders-sold-cereal-to-keep-their-dream-alive-d44223a9bdab">Cereal Entrepreneurs</a> will attest that ideas are cheap — nobody copies ideas. Only <em>proven</em> ideas get copied, and this idea was definitely not proven. So I started to collaborate with some of my peers and <a href="https://www.linkedin.com/in/stepan-parunashvili-65698932">Stepan Parunashvili</a>, helped us get the ball rolling. “Punt on video for now,” he said, “start with profile pictures from our company directory. Firebase can handle all the real-time stuff, and we already have an internal authentication service. Boom!”</p><p>Stepan continued to offer his support with the initial infrastructure. We needed a name and thought about it for all of a minute. This product was all about the “audience” and “aww” is one sound an audience makes when they’re experiencing something together. Inspiring “awe” was also part of the motivation of this work so we decided on “Awedience.”</p><p>Within a few hours, Stepan had the scaffolding complete. People could sign in and <a href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">“hello world”</a> with other users. Our authentication service played gatekeeper, guaranteeing that the application would have access to an employee’s LDAP username after they signed in. That username enabled me to load a profile picture from our company directory. <a href="https://firebase.google.com/docs/database/">Firebase</a> served as the realtime database, and React sat right on top of it, bound (almost directly) to Firebase events. I could finally focus on my specialty, UI and UX. With an iframe embedded front and center, the UI naturally formed a U-shaped auditorium with virtual seats. When you clicked a seat, your picture would appear, and as you reacted, emojis would float out of your seat for everyone to see. You could also write short messages and they would pop out of your seat too, simulating shouts to the crowd.</p><p>We had built something. Now, would anyone care?</p><h3>Growth</h3><p>Attention is an incredibly valuable resource and there are lots of ways to get it. Attention is commonly bought. Attention can be diverted from other channels. It can even be stolen.</p><p>But attention can also be <em>earned</em>. When people <em>love</em> a product, it not only has true staying power but will grow organically. Therefore, one way to see if people love something new is to say nothing and observe.</p><p>I invited close colleagues to try Awedience for an all-hands to see what would happen. Intuitively, they took a seat, started reacting, and used it throughout the meeting. The feedback was <a href="https://sive.rs/hellyeah">overwhelmingly positive</a>. Awedience didn’t do much but what it did do, it did well enough.</p><figure><img alt="A screenshot of Airbnb’s Slack while Audience was being used for the first time. Some messages say “we’re sitting together,” “love this feature it’s so cute,” “this is the coolest,” and “I noticed you sat next to me.”" src="https://cdn-images-1.medium.com/max/1024/1*DPnN31xW0LgGXbbysUO_fw.png" /><figcaption>A glimpse into our company Slack: first reactions to Awedience at Airbnb.</figcaption></figure><p>At the time, Brian hosted all-company Q&amp;As weekly. I created a calendar event alongside the all-company one with an alternative URL and only invited people that had previously used Awedience. Within a few months, Awedience was so popular that it was offered as a secondary option in the official calendar invites.</p><p>With the increased popularity, it was hard to support Awedience on nights and weekends. I asked my team for time to redirect my focus to Awedience for a few months, and they were supportive. The only request was that I figure out the product’s future by the end of that time and not leave things open-ended. Would it become a part of <a href="https://www.airbnb.com/s/experiences/online">Online Experiences</a>? Would it become a part of another team’s roadmap? We even speculated that it could be a completely new line of business.</p><p>While it was tempting to keep adding functionality, resourcefulness was now the name of the game. Awedience was crashing during peak moments so performance was and still is the most important feature. Before we implemented throttling, we were binding reactions directly to our app state, which triggered a deluge of re-renders:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9cc2af478f93a52dbd1e74dfafeb2e11/href">https://medium.com/media/9cc2af478f93a52dbd1e74dfafeb2e11/href</a></iframe><p>A crude <em>batchedThrottle</em> function reduced renders when users mashed an emoji button:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e7477b5316cc110ea5df6409c5d86694/href">https://medium.com/media/e7477b5316cc110ea5df6409c5d86694/href</a></iframe><p>Later, additional performance gains were found by detaching the React UI from Firebase real-time callbacks. Eventually, reactions would be managed natively without React at all:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/de71ca2d02402ba3df6f5db2228c4141/href">https://medium.com/media/de71ca2d02402ba3df6f5db2228c4141/href</a></iframe><p>There were more affordances I wanted to explore. At sporting events, attendees often hold up signs, flags, or even paint things on their bodies to spell out a message. Replicating this in Awedience was a huge hit. Rather than a profile picture, attendees could now pick a color, letter, or a portion of a graphic to display from their seat. People show up early and coordinate amongst themselves to spell out messages to represent their team, city, or the company. The result is magical. Awedience didn’t make it easy to tell your neighbor to change their seat picture. People were going out of their way to coordinate with one another. Connection between colleagues was happening organically and it was thrilling to see.</p><figure><img alt="A screenshot of Awedience during an all-company meeting. Brian Chesky, CEO, is in the center and is surrounded by thousands of employees represented by tiny squares. The squares spell out words like “insurance,” “human,” or “trust.” There are also a few Airbnb logos being drawn out in 5x5 tiles." src="https://cdn-images-1.medium.com/max/1024/1*DsAVOD_cZgUwAP39X6qQZQ.jpeg" /><figcaption>An earlier version of Awedience where people were spelling words, representing their teams, cities, and the company. Or the CEO’s face.</figcaption></figure><p>Making sure there were enough seats for everyone was also a challenge. Too few seats and some people can’t participate; too many and the auditorium feels empty. To handle this, Awedience does something its skeuomorphic counterpart can’t do: adding seats as needed. This feature felt vital for a product built at a company so focused on belonging. Later, we would improve on this feature by increasing seating density such that almost 1,000 people are visible “above the fold.”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kwLwo5MNpcT8HLk8xq3L7g.gif" /><figcaption>Adding rows of seats to the bottom worked for a long time but limited users from seeing everyone at once. It took the addition of virtual aisles to afford seats being added horizontally without compromising user-generated seat art.</figcaption></figure><p>Self-service features were also prioritized. Brian’s staff immediately wanted to know what content was spurring engagement — a question they hadn’t been able to answer since going remote. Cumulative data from each event was piped into a graphing library for quick and dirty analytics. Similarly, our video production team wanted to be able to create and edit auditoriums without relying on me so that self-service tooling came as well.</p><figure><img alt="A stacked line graph of emojis over time. One line, applause, for example goes up to over 300 reactions for a moment. Later, hearts, spike to nearly 150. Each emoji has its own usage graphed on the chart." src="https://cdn-images-1.medium.com/max/1024/1*ceUehJwN6__G7Q_OIQoZrA.png" /><figcaption>Awedience helps presenters understand exactly which parts of their presentation landed for their audience.</figcaption></figure><p>Later, during a hackathon, we even created applause sound effects that naturally scale up from the sound of a few hands clapping to an uproar based on audience engagement.</p><figure><a href="https://video.airbnb.com/media/t/1_o3egj5jk"><img alt="A video demo of Awedio, in which you can see Brian Chesky being interviewed on CBS while members of the audience applause around him. In addition to seeing the applause emojis rise from the audience, you can also hear the applause generated by the software." src="https://cdn-images-1.medium.com/max/1024/1*N_NlPojNdwUIK83bev6Rug.jpeg" /></a><figcaption>Awedio allows you to hear the audience’s applause reactions.</figcaption></figure><h3>Moments</h3><p>Awedience made Airbnb feel like Airbnb again. There’s now a place where you can see everyone and feel connected to them. It’s become home to celebratory moments and a place where we can sit alongside one another during somber ones.</p><p>When Airbnb announced a cut back to our workforce, there was an all-hands scheduled to honor and appreciate the employees who were leaving. However, with VPN access cut to roughly 2,000 soon-to-be alumni, Awedience was suddenly only accessible to the spared employees. Resident security guru, <a href="https://www.linkedin.com/in/keeleysam">Sam Keeley</a>, and I committed to making Awedience accessible outside of VPN and almost overnight switched authentication to Google IAP. When the founders addressed the company, they invited a standing ovation for our departing peers and Awedience obliged. It’s hard to imagine what kind of impersonal and solitary send-off we would have had without Awedience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*znVI-1UvXbpPEn9WClZzJw.gif" /><figcaption>Hundreds of employees joined our founders in a virtual standing ovation for the members of our team that were let go as a result of the pandemic cut backs.</figcaption></figure><p>In May, in the wake of social action around George Floyd’s murder, the company met to address the Black Lives Matter movement. At the end of this meeting, Brian invited the company to take an 8 minute and 46 second moment of silence.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-7x_Yi1B3DiKxQTq6yv_aA.gif" /><figcaption>Employees join in a somber yet moving virtual moment of silence for George Floyd.</figcaption></figure><h3>Conclusion</h3><p>At Airbnb, Awedience is here to stay and now receives ongoing support and maintenance. In collaboration with our Employee Experience team, we found a home where it would make sense long term. In fact, if this is the kind of work you find interesting, you may even consider joining our team to help us build internal tools to foster connection — <a href="https://careers.airbnb.com/positions/">we’re hiring</a>!</p><p>I feel fortunate to work for a company that creates space to bring these types of projects to life. Airbnb is an inspiring place — the combination and culmination of a rigorous entrepreneurial spirit and an ongoing commitment to outdo the status quo. That’s the type of environment you need for ideas like Awedience to flourish.</p><p>Awedience is more than just a triumph of passion and creativity. The spark of the idea was just that: a spark. In the words of <a href="https://twitter.com/richardbranson/status/264067714266587136">Richard Branson</a>, “opportunities are like the buses — there’s always one coming.” Without the help and support of many amazingly talented colleagues, there would literally be nothing to write about.</p><p>What makes Awedience awesome is the people. Big ideas are rarely the consequence of one person’s ideas or effort. It takes a lot of people to do incredible things.</p><h3>Acknowledgements</h3><p>To Stepan Parunashvili for fueling the fire and bootstrapping the infrastructure that got Awedience going. Without you, it would not have been possible. Thank you.</p><p>To Sam Keeley for enabling and evolving Awedience access for the entire company.</p><p>To Joe Gebbia for creating some air space for Awedience to grow and evolve.</p><p>To Byoung Bae, Allison Frelinger, Darrick Brown, and Judd Antin of my former team for taking a gamble on Awedience with me.</p><p>To Liz Kleinman and Beth Axelrod for creating a role for me to continue this work.</p><p>To Shawdi Ilbagian Hahn, Dave O’Neill, Kylie McQuain, Kelly Bechtel, Kate Walsh, Benny Etienne, Carrie Kissell, Alyce Thompson, John Lawrence, and Samantha Eaton for your collaboration and partnership in keeping the company engaged and connected.</p><p>To Cory Boldt, Steven McNellie, Garrett McGrath, Alex Lacayo, John Espey, and Scott Ethersmith for your help and creativity on the technical productions.</p><p>To Jenna Cushner, Ortal Yahdav, Lucille Hua, Christian Williams, Shawn Terasaki, Brian Wallerstein, Ben Muschol, Mike Fowler, Jason Goodman, Caty Kobe, Joe Lencioni, Nicolas Haunold, Christian Baker, Alan Sun, and Jacqui Watts for your early contributions and feedback.</p><p>To Kevin Swint, Danielle Zloto, Christine Berry, Federica Petruccio, and Consuelo Hernandez for going above and beyond to try Awedience with Online Experiences and the powerful insights that were created as a result.</p><p>To Nicholas Roth, Izzy Rattner, Jonathan Lieberman, Stephen Gikow, Steve Flanders, Lonya Breitel, Alan Shum, Brian Savage, Veronica Mariano, Allie Hastings, Alica Del Valle, Rajiv Patel, and Emily Bullis for your legal support in protecting Awedience’s intellectual property and making external partnerships possible.</p><p>To Sarah Baker for always rallying people together to create seat artwork.</p><p>To Gaurav Mathur, Hope Eckert, Sean Abraham, Jessie Li, Vaithiyanathan Sundaram, Andy Yasutake, Virginia Vickery, Jonathan Rahmani, Andrew Pariser, Sunakshi Kapoor, Diane Ko, Biki Berry, Francisco Diaz, Erik Ritter, Tony Gamboa, Mohsen Azimi, Bruce Paul, Omari Dixon, Sonia Anderson, CJ Cipriano, Chihwei Yeh, Arie Van Antwerp, Victor De Souza, Sam Shadwell, Deanna Bjorkquist, Jenna Cushner, Richard Kirk, Jake Silver, Alex Rosenblatt, David He, LA Logan, Ryan Booth, Pistachio Matt, Melanie Cebula, Brian Morearty, and Victor Caruso for your participation and support!</p><p>To Stephanie Wei, Micah Roumasset, Ryland Harris, Waylon Janowiak, and Ben Arnon for your willingness to try Awedience outside of Airbnb.</p><p>To Jerry Chabolla, Nicholas Schell, Ryan Jespersen, Sergio Garcia Murillo, Wes Dagget, and the entire team at Millicast for enabling real-time streaming.</p><p>To Brett Bukowski, Cara Moyer, Nicki Williams, Dylan Hurd, and Lauren Mackevich for encouragement and support in writing this blog post.</p><p>And lastly to Danee Chavez for powering the light bulb. 💡</p><p><strong>Interested in working at Airbnb? Check out these open roles:</strong></p><p><a href="https://careers.airbnb.com/positions/3714489/">Senior Software Engineer, Airfam Products</a></p><p><a href="https://careers.airbnb.com/positions/3955056/">Staff Technical Program Manager, Insurance Platform</a></p><p><a href="https://careers.airbnb.com/positions/3988445/">Staff Automation Engineer, BizTech Global Ops</a></p><p><a href="https://careers.airbnb.com/positions/4003012/">Operations Engineer</a></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ebf66ee6af0e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/hacking-human-connection-the-story-of-awedience-ebf66ee6af0e">Hacking Human Connection: The Story of Awedience</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Measuring Latency Overhead with Own Time]]></title>
            <link>https://medium.com/airbnb-engineering/measuring-latency-overhead-with-own-time-f4373f586ca?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f4373f586ca</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[graphql]]></category>
            <category><![CDATA[service-mesh]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Jimmy O’Neill]]></dc:creator>
            <pubDate>Mon, 21 Mar 2022 20:53:37 GMT</pubDate>
            <atom:updated>2022-03-21T23:05:47.028Z</atom:updated>
            <content:encoded><![CDATA[<p>by: <a href="https://www.linkedin.com/in/jimmyoneill">Jimmy O’Neill</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JQ6ZtmHuSu-86FXV1i7Lmg.jpeg" /></figure><h4>A new metric to quantify the latency overhead of our Viaduct framework</h4><p><a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>, a GraphQL-based data-oriented service mesh, is Airbnb’s paved road solution for fetching internal data and serving public-facing API requests. As a unified data access layer, the Viaduct framework handles high throughput and is capable of dynamically routing to hundreds of downstream destinations when executing arbitrary GraphQL queries.</p><h3>Performance Challenges in Viaduct</h3><p>Viaduct’s role as a data access layer puts it in the critical path of most activity on Airbnb. This makes runtime performance of utmost importance as overhead in the framework will apply universally and can have a multiplicative effect. At the same time, Viaduct accepts arbitrary queries against the unified data graph. In practice, this amounts to many thousands of heterogeneous queries in production, each of which is capable of making an arbitrary number of downstream and often concurrent calls during the course of query execution.</p><p>This presented a challenge for us. Runtime overhead in Viaduct is crucial for us to monitor and improve, but we did not have a good measure for it. Metrics on end-to-end query latencies are confounded by the performance of downstream services, making it difficult to accurately judge the effect of a performance intervention in Viaduct. We needed a metric that <em>isolates </em>the performance impact of Viaduct changes from the performance impact of downstream services.</p><h3>Defining Own Time</h3><p>To do this, we created a metric called “own time”. Own time measures the portion of a request’s wall-clock time that occurs when there are zero downstream requests in flight. The following is pseudocode to compute own time given a root request time span and a set of downstream fetch time spans:</p><pre>def calculateOwnTime(rootSpan, fetchSpans):<br>  ownTime = 0<br>  maxEndTimeSoFar = rootSpan.startTime<br>  sortedFetchSpans = fetchSpans sorted by increasing start-time<br>  for fetchSpan in sortedFetchSpans:<br>    if (maxEndTimeSoFar &lt; fetchSpan.startTime)<br>      ownTime += (fetchSpan.startTime - maxEndTimeSoFar)<br>    maxEndTimeSoFar = max(maxEndTimeSoFar, fetchSpan.endTime)<br>    ownTime += (rootSpan.endTime - maxEndTimeSoFar)<br>  return ownTime</pre><p>The own time metric allows us to focus on aspects of Viaduct’s overhead that are clearly unrelated to downstream service dependencies. While it does not capture <em>all </em>aspects of Viaduct’s overhead, we’ve found it captures enough to be a valuable indicator of overhead costs.</p><p><strong>Examples</strong></p><p>In the trivial case where all downstream calls are made serially, own time is a simple span difference of the root operation span and the sum of the downstream time spans.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-fwKbBCLNP-GkWJP" /></figure><p>When there are multiple downstream calls, they may be made fully or partially in parallel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DVUD3CPpix-Kt6h3" /></figure><p>In this example, the downstream calls happen partially in parallel, and the resulting own time value doesn’t include the time that any downstream request is in flight, parallel or not.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ezhwxOXWnSHMf1wj" /></figure><h3>Identifying and Reducing Runtime Latency</h3><p><strong>Measuring the influence of CPU vs. I/O on request latency</strong></p><p>Normalizing operation own time by overall operation latency gives us an estimate of how CPU-bound vs. I/O-bound an operation is. We call this the “own time ratio” of a query. For example, the Viaduct operation graphed below had an own time ratio of 20%, indicating that 20% of the request runtime in Viaduct was spent with no downstream request in flight. After deploying an internal Viaduct performance improvement, this operation’s own time ratio dropped to 17%, since Viaduct overhead improved while downstream performance remained constant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wy6PADUW1fd6-F-X" /><figcaption>This graph shows a day-over-day reduction in own time ratio for a Viaduct operation after a runtime overhead improvement was deployed.</figcaption></figure><p>A low own time ratio for an operation indicates that the biggest overall latency gains will likely be found by optimizing downstream services, not Viaduct. A high own time ratio indicates that the meaningful latency gains can come from optimizing internal Viaduct runtime for the operation. When making such optimizations for the sake of one operation, we can also use own time ratios across all operations, and especially low-ratio ones, to ensure we aren’t introducing a regression more broadly.</p><p><strong>Quantifying the impact of query size on runtime overhead</strong></p><p>Viaduct users reported that large queries were running slower than expected, attributing the slow execution to Viaduct overhead. Before own time, we had no metrics to assess such reports. After introducing own time, we had a starting point, but we needed to refine the metric further for this use case.</p><p>One would expect own time to increase as the number of fields returned by an operation increases. But was that a reasonable expectation? We found that normalizing own time by the count of fields returned by an operation yields a metric that more usefully indicates, across a heterogeneous set of operations, when own time is excessive. We defined field count to include both object fields and individual array elements.</p><p>The following graph shows that there is indeed an overall relationship between own time and field count across our set of operations, as well as some outliers that have unusually high own-time-to-field-count ratios.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MoOIoSyyD2fDoUog" /><figcaption>This chart plots the number of fields resolved against own time for unique GraphQL operations.</figcaption></figure><p>This relationship between field count and own time encouraged us to focus on framework logic that runs on every field for all operations, rather than other parts of the codebase. Through some CPU profiling, we were able to quickly identify bottlenecks. One resulting improvement was a change to our multithreading model for field execution, which decreased own time for all operations by 25%.</p><p><strong>Quantifying the impact of internal caching on runtime overhead</strong></p><p>Viaduct saw another performance issue. For some operations, latency appeared to vary an unusual amount, even between identical requests. Here again, we used own time to guide our investigation into root causes.</p><p>Viaduct relies on a number of internal caches to ensure that execution is fast, such as a cache for parsed and validated GraphQL documents. Own time metrics indicated that Viaduct runtime overhead, not downstream service dependencies, was causing the variance in latencies. We theorized that cache misses were the culprit. To test this theory, we instrumented our caches to report whether any lookup miss occurred during an operation execution and attached this hit/miss status to our own time metric output. This allowed us to report on own time by cache hit/miss status on a per-cache, per-operation basis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E3j_g3qCzFxhw_IN" /></figure><p>Adding this information to own time allowed us to both confirm our theory and quantify the potential benefit of implementing a solution, such as additional cache warming or moving in-memory caches to distributed caches, prior to committing actual engineering resources. Migrating the in-memory cache that stores the validation state of GraphQL documents to a distributed cache reduced miss rates. This had a significant impact on tail latencies, especially for low QPS operations that were more likely to encounter cold cache states.</p><h3>Setting Runtime Overhead Goals</h3><p>Establishing the own time metric normalized by field count ended up being a great way to account for changes in query patterns. Thus, we now use this metric, aggregated across all operations, to set framework-level performance targets that are isolated from changes in client query patterns. In particular, after measuring the base rate of normalized own time at the beginning of a quarter, we set a goal to improve normalized own time by a specific percentage quarter-over-quarter.</p><p>We also use this metric, aggregated on a per-operation basis, to let operation owners know how their operation overhead compares to the rest of the system.</p><h3>Integrating Own Time Into The Release Cycle</h3><p>To quantify the runtime performance impact of a change, we can set up experiments where two staged control and treatment applications receive identical production traffic replay. We can then graph the difference in own time between them. This allows us to quantify the impact of various framework interventions on runtime overhead and measure each intervention’s impact against our performance goals.</p><p>While replay experiments help us to assess the potential runtime improvements of a change on a limited set of use cases, narrowly-targeted optimizations can lead to broader performance regressions may still happen accidentally. To guard against such regressions, we leverage an automated canary analysis process before deployment. A canary instance and baseline instance receive identical production replay traffic for a period of time, and large discrepancies between them can automatically stop the deployment process. By inspecting the own time difference between the canary and baseline instances, we can identify unexpected performance regressions prior to the regression making it to production.</p><p>In addition to automated canary analysis, graphing day-over-day, week-over-week and month-over-month own time in production shows us long-term isolated performance trends and allows us to bisect any regressions that make it to production.</p><h3>Limitations and Future Work</h3><p>By ignoring what Viaduct does during all downstream calls, own time does not account for possible optimizations to a call pattern of the downstream requests themselves. For example, a request execution may be sped up by increasing concurrency of downstream calls or removing some calls altogether.</p><p>Although own time gives a measure of wall-clock runtime service overhead, it does not say <em>what</em><strong> </strong>is causing the overhead or how to best improve it, which will vary across operations in a GraphQL server. However, tracking downstream request spans in memory provides baseline data that can be enriched with other metadata and further filtered to measure the contribution of application-specific activity to own time.</p><p>Tracking down the root cause of unexpected own time changes or understanding why an operation is an own time outlier requires manual inspection and sometimes additional one-off measurements, which take valuable engineering time. We can automate the first steps in these investigations by measuring the contribution of various parts of the application to own time. This would speed up root cause analysis and limit time spent manually profiling CPU usage.</p><h3>Conclusion</h3><p>Own time has allowed us to isolate the runtime performance characteristics of Viaduct, our GraphQL-based data-oriented service mesh. Using own time, we can precisely measure the production runtime performance effects of application changes, set downstream-independent performance goals, and measure our long-term progress against those goals for an arbitrary underlying application. Enriching own time with application-specific data, such as fetched field counts and cache hit/miss states in Viaduct, gives us an overarching view of the relationship between an application’s state and its runtime performance characteristics.</p><h3>Acknowledgements</h3><p>Thanks to everyone who made this work possible by supporting the Viaduct framework, brainstorming ideas and providing feedback on this post, including Aileen Chen, Yuchun Chen, Zoran Dimitrijevic, Adam Miskiewicz, Parth Shah, Raymie Stata, and Kim Strauch.</p><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f4373f586ca" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/measuring-latency-overhead-with-own-time-f4373f586ca">Measuring Latency Overhead with Own Time</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>