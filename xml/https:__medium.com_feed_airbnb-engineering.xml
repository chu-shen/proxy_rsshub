<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 16 Sep 2021 16:16:31 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Automating Data Protection at Scale, Part 1]]></title>
            <link>https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c74909328e08</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[privacy]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[elizabeth nammour]]></dc:creator>
            <pubDate>Tue, 14 Sep 2021 17:00:16 GMT</pubDate>
            <atom:updated>2021-09-14T17:00:16.451Z</atom:updated>
            <content:encoded><![CDATA[<p>Part one of a series on how we provide powerful, automated, and scalable data privacy and security engineering capabilities at Airbnb.</p><p><a href="https://www.linkedin.com/in/elizabethnammour/">Elizabeth Nammour</a>, <a href="https://www.linkedin.com/in/wendy-jing-jin-81452921/">Wendy Jin</a>, <a href="https://www.linkedin.com/in/shengpu-liu/">Shengpu Liu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u5ErTNsWp-x1GE72" /></figure><p>Our community of hosts and guests trust that we will keep their data safe and honor their privacy rights. With frequent news reports of data security breaches, coupled with global regulations and security requirements, monitoring and protecting data has become an even more critical problem to solve.</p><p>At Airbnb, data is collected, stored, and propagated across different data stores and infrastructures, making it hard to rely on engineers to manually keep track of how user and sensitive data flows through our environment. This, in turn, makes it challenging for them to protect it. While many vendors exist for different aspects of data security, no one tool met all of our requirements when it came to data discovery and automated data protection, nor did they support all of the data stores and environments in our ecosystem.</p><p>In this three-part blog series, we’ll be sharing our experience building and operating a data protection platform at Airbnb to address these challenges. In this first post, we will give an overview of why we decided to build the Data Protection Platform (DPP), walk through its architecture, and dive into the data inventory component, Madoka.</p><h3>Data Protection Platform (DPP)</h3><p>Since no one tool was meeting our needs, we decided to build a data protection platform to enable and empower Airbnb to protect data in compliance with global regulations and security requirements. However, in order to protect the data, we first needed to understand it and its associated security and privacy risks.</p><h4>Understanding Airbnb’s Data</h4><p>At Airbnb, we store petabytes of data across different file formats and data stores, such as MySQL, Hive, and S3. Data is generated, replicated, and propagated daily throughout our entire ecosystem. In order to monitor and gain an understanding of the ever-changing data, we built a centralized inventory system that keeps track of all the data assets that exist. This inventory system also collects and stores metadata around the security and privacy properties of each asset, so that the relevant stakeholders at Airbnb can understand the associated risks.</p><p>Since some data assets may contain sensitive business secrets or public information, understanding what type of data is stored within a data asset is crucial to determining the level of protection needed. In addition, privacy laws, such as the European Union General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA), have granted users the right to access and delete their personal data. However, personal data is a less-than-precise term that represents many different data elements, including email addresses, messages sent on the platform, location info, etc. In order to comply with these laws, we need to pinpoint the exact location of all personal data. To do this, we built a scalable data classification system that continuously scans and classifies our data assets to determine what type of data is stored within them.</p><h4>Enabling Automated Data Protection</h4><p>Based on the understanding of the data, the DPP strives to automate its protection, or enables and notifies teams across the company to protect it. This automation focuses on a few key areas: data discovery, prevention of sensitive data leakages, and data encryption.</p><p>Discovering personal data is the first step to privacy compliance. This is especially true as personal data needs to be deleted or returned to a user upon request. Our platform enables us to automatically notify data owners when new personal data is detected in their data stores and integrate this data with our privacy orchestration service to ensure it gets deleted or returned if needed.</p><p>A common cause of data breaches is when sensitive secrets, such as API keys or credentials, are leaked internally and then make their way into the hands of an attacker. This can come from an engineer logging the secret within their service or committing the secret to code. Our data protection platform identifies potential leaks from various endpoints and notifies the engineer to mitigate the leakage by deleting the secret from the code or log, rotating the secret, and then hiding the new secret with our encryption tool sets.</p><p>One of the most popular and important methods of data protection is encryption, since even in case of an infiltration, attackers won’t be able to get their hands on sensitive data. However, breaches due to unencrypted sensitive data are unfortunately a common occurrence within the industry.</p><p>Why does it still happen? Secure encryption with proper key management is technically challenging, and organizations do not always know where sensitive data is stored. The DPP aims to abstract these challenges by providing a data encryption service and client library that engineers can use. It automatically discovers sensitive data, so that we don’t rely on manual identification.</p><h4>Platform Architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SUuqoIcTshHVhQQ_" /><figcaption>Figure 1: DPP Overview</figcaption></figure><p>The DPP aims to discover, understand, and protect our data. It integrates the services and tools we built to tackle different aspects of data protection. This end-to-end solution includes:</p><ul><li><strong>Inspekt</strong> is our data classification service. It continuously scans Airbnb’s data stores to determine what sensitive and personal data types are stored within them.</li><li><strong>Angmar</strong> is our secret detection pipeline that discovers secrets in our codebase.</li><li><a href="https://medium.com/airbnb-engineering/one-step-forward-in-data-protection-8071e2258d16"><strong>Cipher</strong></a> is our data encryption service that provides an easy and transparent framework for developers across Airbnb to easily encrypt and decrypt sensitive information.</li><li><strong>Obliviate</strong> is our orchestration service, which handles all privacy compliance requests. For example, when a user requests to be deleted from Airbnb, obliviate will forward this request to all necessary Airbnb services to delete the user’s personal data from their data stores.</li><li><strong>Minister</strong> is our third party risk and privacy compliance service that handles and forwards all privacy data subject rights requests to our external vendors.</li><li><strong>Madoka</strong> is our metadata service that collects security and privacy properties of our data assets from different sources.</li><li>Finally, we have our <strong>Data Protection Service</strong>,<strong> </strong>a presentation layer where we define jobs to enable automated data protection actions and notifications using information from Madoka (e.g., automate integrations with our privacy framework)</li></ul><h3>Madoka: A Metadata System</h3><p>Madoka is a metadata system for data protection that maintains the security and privacy related metadata for all data assets on the Airbnb platform. It provides a centralized repository that allows Airbnb engineers and other internal stakeholders to easily track and manage the metadata of their data assets. This enables us to maintain a global understanding of Airbnb’s data security and privacy posture, and provides an essential role in automating security and privacy across the company.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/920/0*OEag9YxDW3CT1iQy" /><figcaption>Figure 2: Madoka Architecture</figcaption></figure><p>Implemented by two different services, a crawler and a backend, Madoka has three major responsibilities: collecting metadata, storing metadata, and providing metadata to other services.The Madoka crawler is a daily crawling service that fetches metadata from other data sources, including Github, MySQL databases, S3 buckets, Inspekt (data classification service), etc. It then publishes the metadata onto an AWS Simple Queue Service (SQS) queue. The Madoka backend is a data service that ingests the metadata from the SQS queue, reconciles any conflicting information, and stores the metadata in its database. It provides APIs for other services to query the metadata findings.</p><p>The primary metadata collected by Madoka includes:</p><ul><li>Data assets list</li><li>Ownership</li><li>Data classification</li></ul><p>For each of the above we handle both MySQL and S3 formats.</p><h4>Data Assets List</h4><p>The first type of metadata that needs to be collected is the list of all data assets that exist at Airbnb, along with their basic metadata such as the schema, the location of the asset, and the asset type.</p><p>For MySQL, the crawler collects the list of all columns that exist within our production AWS account. It calls the AWS APIs to get the list of all clusters in our environment, along with their reader endpoint. The crawler then connects to that cluster using JDBI and lists all the databases, tables, and columns, along with the column data type.</p><p>The crawler retains the following metadata information and passes it along to the Madoka backend for storage:</p><ul><li>Cluster Name</li><li>Database Name</li><li>Table Name</li><li>Column Name</li><li>Column Data Type</li></ul><p>For S3, the crawler collects the list of all objects that exist within all of our AWS accounts.</p><p>At Airbnb, we use <a href="https://www.terraform.io/">Terraform</a> to configure AWS resources in code, including S3 buckets. The crawler parses the Terraform files to fetch the S3 metadata.</p><p>The crawler first fetches the list of all AWS account numbers and names, which are stored in a configuration file in our Terraform repository. It then fetches the list of all bucket names, since each bucket configuration is a file under the account’s subrepo.</p><p>In order to fetch the list of objects within a bucket, the crawler uses <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html">S3 inventory reports</a>, a tool provided by AWS. This tool produces and stores a daily or weekly CSV file of all the objects contained in the bucket, along with their metadata. This is a much faster and less costly way of getting the list compared to calling the List AWS API. We’ve enabled inventory reports on all production S3 buckets in Terraform, and the bucket configuration will specify the location of the inventory report.</p><p>The crawler retains the following information and passes it along to Madoka backend for storage:</p><ul><li>Account Number</li><li>Account Name</li><li>Assume Role Name</li><li>Bucket Name</li><li>Inventory Bucket Account Number</li><li>Inventory Assume Role Name</li><li>Inventory Bucket Prefix</li><li>Inventory Bucket Name</li><li>Object key</li></ul><h4>Ownership</h4><p>Ownership is a metadata property that describes who owns a specific data asset.</p><p>We decided to collect service ownership, which allows us to link a data asset to a specific codebase, and therefore automate any data protection action that requires code changes.</p><p>We also decided to collect team membership, which is crucial to perform any data protection action that requires an engineer to do some work or that requires a stamp of approval. We chose to collect team ownership and not user/employee ownership since team members constantly change, while the data asset remains with the team.</p><p>At Airbnb, since we migrated to a service-oriented architecture (SOA), most MySQL clusters belong to a single service and a single team. To determine service ownership, the crawler fetches the list of the services that connect to a MySQL cluster and will set the service with the most number of connections within the last 60 days as the owner of all the tables within that cluster. There are many services that connect to all clusters for monitoring, observability, and other common purposes, so we created a list of roles that should be filtered out when determining ownership.</p><p>There are still some legacy clusters in use that are shared amongst many services, where each service owns specific tables within the clusters. For those clusters, not all tables will have the correct service owner assigned, but we allow for a manual override to correct these mistakes.</p><p>The crawler uses service ownership to determine team ownership, since at Airbnb, team ownership is defined within the service’s codebase on Git.</p><p>At Airbnb, all S3 buckets have a project tag in their Terraform configuration file, which defines which service owns the bucket. The crawler fetches the service ownership from that file and uses it to determine the team ownership, as described above for MySQL.</p><h4>Data Classification</h4><p>Data classification is a metadata property that describes what type of data elements are stored within the asset — e.g., a MySQL column which stores email addresses or phone numbers would be classified as personal data. Gathering data classifications allows us to understand the riskiness of each data set so we can determine the level of protection needed.</p><p>The crawler fetches the data classification from two different sources. First, it fetches data classifications from our Git repositories, since data owners can manually set the classifications in their data schema. However, relying on manual classifications is insufficient. Data owners do not always know what an asset contains, or they may forget to change the classifications when the data asset is updated to store new data elements.</p><p>The crawler will then fetch data classifications from our automated data classification tool, called Inspekt, which we will describe in detail in a later blog post. Inspekt continuously scans and classifies all of our major data stores, such as MySQL and S3. It outputs what data elements were found in each data asset. This ensures that our data is constantly monitored, and classifications are updated as data changes. As with any automated detection tool, precision and recall are never 100%, so false positives and false negatives may occur.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uArO328-uBKl2WQ-" /><figcaption>Figure 3: Classification Reconciliation</figcaption></figure><p>Since the crawler fetches the data classifications from two different sources, some discrepancies may arise, where the manual classification contains data elements not found by Inspekt or vice versa. The crawler will forward all findings to the Madoka backend, which will resolve any conflicts. The status of the manual classification is marked as <em>new</em> by default and the status of the Inspekt classification is marked as suggested. If the manual classification aligns with the Inspekt result, the classification is automatically confirmed. If there is any discrepancy, we file tickets to the data owners through the data protection service. If the Inspekt classification is correct, the owners may update the data schema in the Git repository, or they can mark the Inspekt classification as incorrect to resolve the conflict.</p><h4>Other Security and Privacy Related Attributes</h4><p>Madoka also stores how data assets have integrated with our security and privacy tools. For example, we may store whether or not the data asset is encrypted using Cipher or is integrated with our privacy compliance service, Obliviate, for data subject rights requests. We built Madoka to be easily extensible and are constantly collecting and storing more security and privacy related attributes.</p><h3>Conclusion</h3><p>In this first post, we provided an overview of why we built the DPP, described the platform’s architecture, and dove into the data inventory component, Madoka. In our next post, we will focus on our data classification system that enables us to detect personal and sensitive data at scale. In our final post we will deep dive into how we’ve used the DPP to enable various security and privacy use cases.</p><h3>Acknowledgements</h3><p>The DPP was made possible thanks to many members of the data security team: Pinyao Guo, Julia Cline, Jamie Chong, Zi Liu, Jesse Rosenbloom, Serhi Pichkurov, and Gurer Kiratli. Thank you to the data governance team members for partnering and supporting our work: Andrew Luo, Shawn Chen, and Liyin Tang. Thank you Tina Nguyen for helping drive and make this blog post possible. Thank you to our leadership, Marc Blanchou, Brendon Lynch, Paul Nikhinson and Vijaya Kaza, for supporting our work. Thank you to previous members of the team who contributed greatly to the work: Lifeng Sang, Bin Zeng, Alex Leishman, and Julie Trias.</p><p>If this type of work interests you, see <a href="https://careers.airbnb.com/">our career page</a> for current openings.</p><p>Tags: data, security</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c74909328e08" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">Automating Data Protection at Scale, Part 1</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Task-Oriented Conversational AI in Airbnb Customer Support]]></title>
            <link>https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5ebf49169eaa</guid>
            <category><![CDATA[reinforcement-learning]]></category>
            <category><![CDATA[nlp]]></category>
            <category><![CDATA[customer-support]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Gavin Li]]></dc:creator>
            <pubDate>Tue, 10 Aug 2021 16:51:16 GMT</pubDate>
            <atom:updated>2021-08-10T17:45:29.187Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb is powering automated support to enhance the host and guest experience</p><p><a href="https://www.linkedin.com/in/gavin-li-64354117/">Gavin Li</a>, <a href="https://www.linkedin.com/in/mia-zhao-964a9213/">Mia Zhao</a></p><figure><img alt="mother and daughter sitting on a chair, scrolling through their phone" src="https://cdn-images-1.medium.com/max/1024/0*46ISdm7kRuJddwie" /></figure><p>Customer Support (CS) can make or break a guest’s travel experience. To support Airbnb’s community of guests and Hosts, we have been investing heavily in developing intelligent CS solutions leveraging state-of-the-art natural language processing (NLP), machine learning (ML), and artificial intelligence (AI) technologies.</p><p>In this blog post, we’ll introduce the automated support system at Airbnb, which employs the latest task-oriented conversational AI technology, through the lens of a recently launched feature called Mutual Cancellation. We will describe in detail how we framed the business problem as an AI problem, how we collected and labeled training data, how we designed and built the ML models, and how the models were deployed in the online system. Throughout each step, we’ll discuss some technical challenges we faced during this project and the solutions we innovated to address these challenges.</p><h4>A Case Study: Mutual Cancellation</h4><p>Prior to the development of the mutual cancellation model, guests needed to involve CS agents, even if they had already reached an agreement with the host for canceling a reservation. This meant that issues took longer to get resolved and precious CS agent hours were wasted. To solve this issue, we developed AI models that help guests and Hosts self-resolve cancellation and refund issues without involving a CS agent. This empowers hosts and guests to decide what is best for them, while allowing us to focus CS agent hours where they are needed most.</p><p>In the rest of the post, we will use the mutual cancellation feature as an example to describe the technical components of Airbnb’s task-oriented AI system.</p><h3>System Architecture</h3><p>Airbnb’s Intelligent Support Platform team develops cutting-edge AI technologies to help guests and hosts solve their issues in the most efficient manner. Based on the chatbot platform we built, <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">ATIS</a>, our AI models aim to learn and mimic how human agents provide warm and effective customer care. A warm and effective customer care experience starts with a personal and intelligent issue identification that aims to quickly understand the user’s situation, needs, questions and concerns with minimum friction.. Once the issue is clearly identified, we generate responses dynamically and guide users through various product workflows to solve their issues or route them to human agents.</p><p>Our intelligent customer support product is designed as a “task-oriented dialog system” (<a href="https://arxiv.org/abs/2007.12720">Zang et al. 2020</a>, <a href="https://arxiv.org/abs/2008.06239">Madotto et al. 2020</a>). Task-oriented dialog systems are gaining more and more interest in recent years, powering AI products ranging from virtual assistants to smart speakers. These models can understand the user’s intent (e.g., ‘play music’), extract needed parameters (e.g., ‘artist name and name of the song’) from the conversation, ask questions to clarify details (e.g., ‘there are two versions of this song, which one do you like to play?’), and complete the task — all while having a dialogue with the user that seems completely natural.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/851/1*LfN3IPLswc0LJcYVQCdmRQ.png" /><figcaption>Figure 1. Airbnb’s Chatbot as a task-oriented dialog system. It detects user intent and generates appropriate responses and completes the task through actions.</figcaption></figure><h3>Customer Support as a Task-Oriented Dialog Problem</h3><p>In real-world machine learning applications, the most crucial piece of the puzzle is how to formulate the problem. Problem formulation has a much more significant impact on the product’s long-term performance than the model itself. There are lots of decisions and trade-offs to be made before a single line of code is written. We designed a multi-layer issue detection and decision-making system to allow both extensibility and domain-specificity for the customer support problem, as demonstrated in Figure 2.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/1*TA0BXmY2hYhuswYJNOJrYA.png" /><figcaption>Figure 2. A multi-layer user issue detection and decision-making model structure.</figcaption></figure><p>When a user sends a message in the Airbnb chatbot, the message is processed by the first layer, a domain classification model. The domain classification model determines which domain the message belongs to, for example, a trip rebooking request, a cancellation refund request, or a question that can be answered with a help article recommendation. If the Mutual Cancellation domain is predicted to be the most likely domain, the system triggers the Mutual Cancellation flow and enters the second layer to further our understanding of the user’s intent and checks the eligibility of the Mutual Cancellation.</p><p>For Mutual Cancellation, there are two models in the second layer: the Q&amp;A-based intent understanding model and the “expected refund ratio prediction” model. The Q&amp;A intent model is trained on a manually labeled dataset. The “expected refund ratio prediction” model is trained on historical cancellation data and refund ratio decided by agents. Refund ratios capture many vital characteristics of the trip that are crucial for the AI system to make decisions on behalf of human agents.</p><p>The multi-layer structure has the benefit of:</p><ul><li><strong>Scalable</strong>: it allows the system to be extended to new domains and domain-specific models for existing domains won’t be affected by new domains.</li><li><strong>Effective</strong>: the top-level model is trained on manually labeled data which is usually high quality, but often difficult and expensive to collect. Domain-specific models are mostly trained from historical data, easy to collect but noisy and biased towards past user behavior. The multi-layer structure allows us to leverage human-labeled data to train the top-layer domain prediction model and historical data to train domain-specific models.</li></ul><h3>Collecting and Labeling Training Data</h3><p>A typical task-oriented dialog system builds an intent taxonomy tree where each node represents some intent, and the nodes are mutually exclusive. Airbnb’s customer support, similar to other shared-economy customer support, users’ issues contain complex issues that are less structural than a typical online marketplace. It is challenging, if possible at all, to define a clean taxonomy tree to capture ALL users’ issues and partition them in a hierarchical tree.</p><p>In addition, a taxonomy tree usually implies that we need to traverse from the root node following a path to the leaf node. Along the path, the system asks questions (e.g., “Do you want to cancel the reservation?”) or collects more information (e.g., “Is the user a Guest or a Host?”) to decide on which branch to continue. In Airbnb’s case, users’ issues are much more complicated and may require different sequences of questions to identify the issue efficiently. For Mutual Cancellation, the first question (“if the host and guest agree with each other”) and the second question(“who initiated the cancellation”) capture different aspects of the cancellation and refund process. It can be challenging to design a simple and clean tree structure taxonomy to cover all user issues and rely on the path down the tree to collect the needed information efficiently. Instead, we model intent understanding as a Question &amp; Answer (Q&amp;A) problem.</p><h4>A Q&amp;A Model for Understanding User Intent</h4><p>Given a user’s initial message to our CS platform, we ask a couple of questions about the user’s intent, and then have human agents/labelers answer those questions. Through this setup, we collect data and train a Q&amp;A model. The trained Q&amp;A model is able to answer those questions similarly. Users’ questions can have multiple answers and users often try to describe the problem from different angles. In some cases, the questions can be mutually exclusive, whereas in other cases the questions may contain redundant information.</p><p>Below are a few examples we ask our labeler team:</p><p><strong><em>User’s message to Airbnb:</em></strong></p><p><em>Hello! I made a reservation wrongly. Thinking it was a whole apartment rental when it was actually just a room. I didn’t pay attention. I immediately spoke to my host, she agreed to refund me and asked me to request the refund money from the app, but I can’t find the option.</em></p><p><strong><em>Question: Who initiated the cancellation?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>The host initiated the cancellation, or the host could not accommodate the guest</em></li><li><em>The guest initiated the cancellation</em></li><li><em>Not mentioned</em></li></ol><p><strong><em>Question: Do the host and guest agree on a refund?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Host agrees on offering a refund and the refund amount</em></li><li><em>Host and guest are having some differences on the refund amount</em></li><li><em>Host disagrees with issuing a refund or already declined it</em></li><li><em>Agreement not mentioned about refund</em></li><li><em>Refund not mentioned at all</em></li></ol><p><strong><em>Question: Is the guest asking how they can get what they want? (how to get refund, what to do, etc)</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Yes</em></li><li><em>No</em></li></ol><p><strong><em>Question: Is the guest asking how they can get a refund, if is it possible, or how much refund can they get?</em></strong></p><p><strong><em>Answer:</em></strong></p><ol><li><em>Yes</em></li><li><em>No</em></li></ol><p>Q&amp;A problems with multiple-choice answers are normally modeled as a multi-class classification problem, where each class maps to one question. However, <a href="https://arxiv.org/abs/2011.03292">Jiang et al. (2020)</a> proposed the idea of modeling Q&amp;A problems as single-choice binary classification problems. In modeling the problem this way, the difficulty of the problem increases. Picking the correct answer from multiple options is no longer sufficient － the model must predict the correct choice as positive and all other choices as negative. This approach makes consolidating multiple Q&amp;A problems easier, enabling us to increase the pre-training scale. <a href="https://arxiv.org/abs/2005.00700">Khashabi et al. (2020)</a> similarly found that unifying multiple pre-training datasets can help boost the model performance.</p><p>We follow the single-choice binary setup, which enables us to unify related user intent training labels from different domains to increase the scale of our training data and enhance the performance. As stated above, we continuously review the data labeling quality and refine the labeling questionnaire design. As a result, there are many versions of labeling questions and the answers for each version. A single-choice setup allows us to mix all the different versions of our training questions together in training.</p><p>Figures 3 and 4 show the difference between single-choice and multi-choice setups for an example message “<em>My host agreed to fully refund me, so if I cancel now can I get a full refund?</em>”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/915/1*iHFTwigsFWtMPuQ--aoy5w.png" /><figcaption>Figure 3. Single-choice Q&amp;A model setup</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/885/1*EmjZlJzZMdzpt3POAjMVrw.png" /><figcaption>Figure 4. Multi-choice Q&amp;A setup</figcaption></figure><p>Figure 5 shows the model performance difference in our experiment. Single-choice Q&amp;A setup outperforms traditional multi-class intent classification setup both on offline labeling prediction accuracy and on online conversion prediction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3GUwYj0tepuePtLJ0aBsmA.png" /><figcaption>Figure 5. Accuracy of single-choice vs. multi-class intent classification.</figcaption></figure><h4>Benefits and Challenges of Intent Prediction as Q&amp;A</h4><p>Compared with traditional multi-class classification, the Q&amp;A setup makes the data labeling much more manageable. We can continuously refine the questionnaire design and flexibly merge questions from different dimensions, different angles, or those with redundancy.</p><p>One of the biggest challenges of applying machine learning in real-world problems is the lack of high-quality training data. From a few-shot learning point of view, the single-choice Q&amp;A setup allows us to build many capabilities into the model, even with sparse training data. This setup trains the model to encode information in the user message, the question and the answer. The model can also learn from related questions from other domains. For this reason, it has the capability to understand both questions in training labels and some newly constructed, unseen questions.</p><p>A shortcoming of this setup is that it puts a lot of pressure on the serving latency. For example, if we want to use the model to answer five questions and then take actions based on the five questions, we have to run the model five times. Later in this post, we’ll discuss how we reduce the model latency including using GPU.</p><h3>Model Design and Implementation</h3><p>We use autoencoder transformers as model architecture. We tested all kinds of model choices as the backbone. The results are shown below:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JY2SnLt2CGbwHC7gYjQtGw.png" /><figcaption>Figure 6. Results on out-of-sample data from various intent classification models.</figcaption></figure><p>For most of our use cases, Roberta performs the best. However, the Roberta-base and Roberta-large’s performances vary depending on the scale of training labels. In our online product case, where we have around 20K labels, the Roberta-large model achieved the best performance and is the model that we deployed in production. However, with 335M parameters, it is very challenging to run this model online with a given latency budget.</p><p>To improve the performance of this model, we leveraged three key techniques:</p><ul><li>Pre-training our transformer model with transfer learning;</li><li>Translating training labels to utilize a multilingual model; and</li><li>Incorporating multi-turn intent predictions.</li></ul><h4>Pre-training</h4><p>Perhaps the most critical recent development in deep learning is transfer learning and pre-training. It dominates most state-of-the-art models in almost all kinds of NLP, computer vision(CV), and automatic speech recognition(ASR) domains.</p><p>We experimented with different pre-training methods extensively and found two pre-training methods to be particularly effective in boosting the model performance:</p><ul><li><strong>In-Domain Unsupervised Masked Language Model (MLM) Pre-training:</strong> Based on users’ conversations with our customer service platform, the listing descriptions, and the help articles, we construct a 1.08GB (152M word tokens) unsupervised training corpus. This corpus contains 14 different languages, with 56% in English. As shown through the experiment results in Figure 7, the in-domain MLM pre-training helps to boost the model performance for our tasks.</li><li><strong>Cross-Domain Task Finetune Pre-training: </strong>Pretraining a transformer model based on a cross-domain dataset is often helpful for many tasks. It’s also effective in boosting intent detection accuracy in our use cases. Experiments results can be found in Figures 8 and 9.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MxI1RhDvxilRZoll83yPBw.png" /><figcaption>Figure 7. In-domain pre-training performance</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ehncNQ9sU2J-_lySRTYAvA.png" /><figcaption>Figure 8. cross-domain task finetune pre-training performance.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HR9TzCqDhRxUF3lIvVwUHQ.png" /><figcaption>Figure 9. Multilingual task finetune pre-training performance.</figcaption></figure><p>Many challenging cases in our intent understanding problem require the model to have some logical reasoning capability. Similar to the finding in the logical reasoning public dataset in <a href="https://arxiv.org/abs/2002.04326">Yu et al. (2020)</a>, pre-training on the RACE dataset helps to boost the performance the most.</p><h4>Multilingual Model</h4><p>Airbnb customer support serves users from all around the world, currently supporting 14 languages. The top non-English languages, including French, Spanish, German, and Portuguese, represent around 30% of the requests. Since our model is targeted at users who speak all languages but labeled data are mainly in English, we leveraged a translated annotation dataset and multilingual model, XLM-RoBERTa, to boost model performance across all languages.</p><p>Translating the training labels to other languages is an unsupervised data augmentation technique proven effective in many deep learning training cases (<a href="https://arxiv.org/abs/1904.12848">Xie et al., 2020</a>). We translate the labeled English training corpus and the labeling questions and answers into other top languages and include them in the training data to train the XLM-RoBERTa model.</p><p>We also tried training monolingual models on translated text for comparison based on public pre-trained monolingual models. Results show that multilingual models trained on translated datasets significantly outperform the English-only training dataset. Model performance is comparable with monolingual models trained by translated annotation datasets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*usU-lK5fyOuDn5WWol6x6Q.png" /><figcaption>Figure 10. Multilingual vs. monolingual model performance.</figcaption></figure><h4>Incorporating Multi-Turn Intent Prediction</h4><p>When a user comes to chatbot with a Mutual Cancellation request, we pull all the text sequences from the user’s previous conversations and concatenate the text sequences of the previous messages and the current request message together as a new text sequence input to the transformer model. This works as a <strong>dialog state tracking</strong> (<a href="https://arxiv.org/abs/1908.01946">Gao et al., 2019</a>) module to incorporate the signals from user’s past interactions to better understand user intent. We experimented with two offline approaches to better consume this signal: 1) adding the last N round of messages as additional features to the current model, and 2) calculating multi-turn intent predictions on each message threshold and adding max intention score to the downstream model.</p><p>One challenge is that the computation complexity of transformer models is O(n⁴) of the sequence length, including all the previous conversions. The complexity makes it impossible to infer online in real-time. To solve this, we process the historical conversation asynchronously offline ahead of time and store pre-computer scores. During online serving, the model directly query the pre-computed scores associated to the user.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zpDgIz3WO9vsTyk1vrmcBg.png" /><figcaption>Figure 11. Multi-turn intent prediction performance and latency.</figcaption></figure><h3>Online Serving</h3><p>Deploying machine learning models online comes with a few major challenges that need to be managed differently than in the offline world.</p><h4>Online Inference GPU Serving</h4><p>One challenge in online serving is the latency of the model in production. We took two key steps to solve for latency requirements: 1) enabling GPU serving, and 2) leveraging transfer learning. Similar to the discussions in the section above, transfer learning techniques like teacher student model is used to reduce the amount of computation needed in online inference. In this section we mainly focus on how GPU serving helped us address this challenge.</p><p>To support GPU inference, we experimented with an offline benchmark on transformer models with 282M parameters on three different instance types — <em>g4dn.xlarge, p3.2xlarge and r5.2xlarge</em>. Figure 12 shows the latency results across these various instance types. The general trend of latency between CPU and GPU as our input messages grow in length can be seen in Figure 13. Shifting to GPU serving has a significant impact on the online latency and is more cost-efficient.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ihe4tlKCo_r9DGNqz4_Fuw.png" /><figcaption>Figure 12. GPU online serving latency for various instance types.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3EEzjo6bTnB_rZvS" /><figcaption>Figure 13. Latency using CPU vs. GPU with input message length increasing.</figcaption></figure><p>The results from our later online experiment (Figure 14) also show the improvement in latency from shifting to GPU inference on transformer models. With ~1.1B parameters and average input message length of 100 words, we were able to achieve ~60ms on p95, which is 3x faster on single transform and five times faster on batch transform.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hkQzw6r7Hk9K96_3" /><figcaption>Figure 14. Model latency in production before and after switching from CPU to GPU .</figcaption></figure><p>Switching to GPU not only improves the latency, it also allows us to run multiple model scoring in parallel. We leverage the PyTorch platform, which has built-in support for non-blocking model scoring, for better scalability.</p><h4>Contextual Bandit and Reinforcement Learning</h4><p>The second challenge in online serving is to adapt and optimize ML models based on new users’ online behavior. As we described in previous sections, the training data of the initial model is collected from historical user interaction over the product flow before the model is deployed. After the model is deployed, users interact with the system in a very different manner compared to the experience when the training data is collected. If the daily traffic is sufficiently large, we can always relabel the new data and update the model using the new data which reflects the updated user behavior, or directly perform multivariate testing on N policies. However, Airbnb’s CS chatbot traffic volume is relatively small compared to other ML systems such as search ranking. It will take a very long time to see the effect of any model change (either retrained model using new data or hyper parameter change).</p><p>To solve the challenge of low traffic volume, we use <strong>contextual bandit-based reinforcement learning</strong> (<a href="https://arxiv.org/abs/1802.04064">Bietti et al., 2019</a>; <a href="https://arxiv.org/abs/1606.03966">Agarwal et al., 2017</a>) to choose the best model and the most appropriate thresholds. Contextual Reinforcement Learning explores all the alternative problems by maximizing the rewards and minimizing the regrets. This allows us to learn from new behavior by dynamically balancing the exploration and exploitation.</p><p>We view this problem through three different actions in the product:</p><ul><li>a0: User is not directed through the mutual cancellation flow</li><li>a1: User is directed to the mutual cancellation UI for guests who have already agreed with the host on the refund</li><li>a2: User is directed to the mutual cancellation UI for cases where it was not clear if the host and guest have reached a mutual agreement</li></ul><p>Our reward function is <em>mutual cancellation flow entering rate</em> and <em>acceptance rate</em>. The reward at time step 𝑡 for any given action can be formulated as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*unZjK8QE_0njZb43-0yXMw.png" /></figure><p>where c denotes if a mutual cancellation flow is not entered/accepted.</p><p>We then leveraged greedy-epsilon as our first exploration strategy. If it’s in exploration mode, we compute the probabilities for each action based on policies’ preferences and select it based on the chances. If it’s in exploitation mode, we choose the best policy. We compute the models’ thresholds based on a set of logged (x, a, r, p) tuples. We use an self-normalized inverse propensity-scoring (IPS) estimator (<a href="https://papers.nips.cc/paper/2015/hash/39027dfad5138c9ca0c474d71db915c3-Abstract.html">Swaminathan and Joachims 2015</a>) to evaluate each policy:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/491/1*r7eHhtXDjK3Pbi3EfJY4nw.png" /></figure><p>In production, this approach successfully helped us explore many different models and parameter options and make the best use of the limited online traffic.</p><h3>Conclusion</h3><p>In this post, we introduced how we employ state-of-the-art machine learning and AI models to build support products that better serve the needs of our guests and hosts. We described how we leverage a single-choice Q&amp;A-based model, large-scale pretraining, multilingual models, multi-turn dialog state tracking, and GPU serving and successfully tackled the technical challenges.</p><p>Interested in tackling challenges in the machine learning and AI space?</p><p>We invite you to visit our <a href="https://careers.airbnb.com">careers page</a> or apply for these related opportunities:</p><p><a href="https://grnh.se/36f092141us"><strong>Staff Data Architect, Community Support Platform</strong></a></p><p><a href="https://grnh.se/ae09e08b1us"><strong>Staff Software Engineer — Machine Learning Modeling Platform</strong></a></p><p><a href="https://grnh.se/6648b3961us"><strong>Machine Learning Engineer, Search Ranking</strong></a></p><h3>Acknowledgements</h3><p>Thanks to Cassie Cao, Hao Wang, Bo Zeng, Ben Ma, Wayne Zhang, Mariel Young , Shahaf Abileah, Pratik Shah, Brian Wang, Hwanghah Jeong, Amy Guo, Vita Papernov, Courtney Nam, Aliza Hochsztein, Mike Hinckley, Yushuang Dong, Jan Castor, Ivy Cui, Lucia Ciccio for the great contributions to Mutual Cancellation workflow development, ERF analysis and product launches. Special thanks to Alex Deng for the help on contextual bandit and reinforcement learning work; many designs are originally Alex’s idea. We would also like to thank Atul Ktal, Bahador Nooraei, Shaowei Su, Alfredo Luque for the ML infrastructure support on GPU inference. In addition, we would like to thank the contributors of open source ML libraries such as PyTorch and HuggingFace Transformers, which benefited us a lot. Finally, we want to appreciate Ari Balogh, Tina Su, Andy Yasutake, and Joy Zhang’s leadership support in leveraging machine learning on Customer Support Platforms.</p><h3>References:</h3><ol><li>Zang X, Rastogi A, Sunkara S, Gupta R, Zhang J, Chen J (2020) MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines. CoRR abs/2007.12720</li><li>Jiang Y, Wu S, Gong J, Cheng Y, Meng P, Lin W, Chen Z, Li M (2020) Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning. CoRR abs/2011.03292</li><li>Khashabi D, Min S, Khot T, Sabharwal A, Tafjord O, Clark P, Hajishirzi H (2020) UnifiedQA: Crossing Format Boundaries With a Single QA System. In: Cohn T, He Y, Liu Y (eds) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16–20 November 2020. Association for Computational Linguistics, pp 1896–1907</li><li>Yu W, Jiang Z, Dong Y, Feng J (2020) ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26–30, 2020. OpenReview.net</li><li>Madotto A, Liu Z, Lin Z, Fung P (2020) Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems. CoRR abs/2008.06239</li><li>Xie Q, Dai Z, Hovy EH, Luong T, Le Q (2020) Unsupervised Data Augmentation for Consistency Training. In: Larochelle H, Ranzato M, Hadsell R, Balcan M-F, Lin H-T (eds) Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6–12, 2020, virtual</li><li>Bietti, Alberto, Alekh Agarwal, and John Langford. A Contextual Bandit Bake-off. Microsoft Research. 21 Mar. 2019</li><li>Agarwal, Alekh, Sarah Bird, Markus Cozowicz, Luong Hoang, John Langford, Stephen Lee, Jiaji Li, Dan Melamed, Gal Oshri, Oswaldo Ribas, Siddhartha Sen, and Alex Slivkins. Making Contextual Decisions with Low Technical Debt. ArXiv.org. 09 May 2017</li><li>Swaminathan A, Joachims T (2015) The Self-Normalized Estimator for Counterfactual Learning. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R (eds) Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7–12, 2015, Montreal, Quebec, Canada. pp 3231–3239</li><li>Gao S, Sethi A, Agarwal S, Chung T, Hakkani-Tür D (2019) Dialog State Tracking: A Neural Reading Comprehension Approach. In: Nakamura S, Gasic M, Zuckerman I, Skantze G, Nakano M, Papangelis A, Ultes S, Yoshino K (eds) Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, SIGdial 2019, Stockholm, Sweden, September 11–13, 2019. Association for Computational Linguistics, pp 264–273</li></ol><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5ebf49169eaa" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa">Task-Oriented Conversational AI in Airbnb Customer Support</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Built “Wall” to prevent data bugs]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ad1b081d6e8f</guid>
            <category><![CDATA[airflow]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-quality]]></category>
            <category><![CDATA[data-engineering]]></category>
            <dc:creator><![CDATA[Subrata Biswas]]></dc:creator>
            <pubDate>Wed, 04 Aug 2021 17:00:02 GMT</pubDate>
            <atom:updated>2021-08-04T17:00:01.993Z</atom:updated>
            <content:encoded><![CDATA[<p>Gaining trust in data with extensive data quality, accuracy and anomaly checks</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LkY1dXWTna9PbJNJ" /></figure><p>As shared in our Data Quality Initiative <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">post</a>, Airbnb has embarked on a project of massive scale to ensure trustworthy data across the company. To enable employees to make faster decisions with data and provide better support for business metric monitoring, we introduced <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469">Midas</a>, an analytical data certification process that certifies all important metrics and data sets. As part of that process, we made robust data quality checks and anomaly detection mandatory requirements to prevent data bugs propagating through the data warehouse. We also created guidelines on which specific data quality checks need to be implemented as part of the data model certification process. Adding data quality checks in the pipeline has become a standard practice in our data engineering workflow, and has helped us detect many critical data quality issues earlier in the pipelines.</p><p>In this blog post we will outline the challenges we faced while adding a massive number of data checks (i.e. data quality, accuracy, completeness and anomaly checks) to prevent data bugs company-wide, and how that motivated us to build a new framework to easily add data checks at scale.</p><h3>Challenges</h3><p>When we first introduced the <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469">Midas</a> analytical data certification process, we created recommendations on what kind of data quality checks need to be added, but we did not enforce how they were to be implemented. As a result, each data engineering team adopted their own approach, which presented the following challenges:</p><h4>1. Multiple approaches to add data checks</h4><p>In Airbnb’s analytical data ecosystem, we use Apache Airflow to schedule ETL jobs or data pipelines. Hive SQL, Spark SQL, Scala Spark, PySpark and Presto are widely used as different execution engines. However, because teams started building similar data quality checks in different execution engines, we encountered other inherent issues:</p><ul><li>We did not have any centralized way to view the data check coverage across teams.</li><li>A change in data check guidelines would require changes in multiple places in the codebase across the company.</li><li>Future-proof implementations were nearly impossible to scale. Teams kept re-inventing the wheel and duplicated code spread across the codebase.</li></ul><h4>2. Redundant efforts</h4><p>Different teams often needed to build tools to meet their own requirements for different data checks. Each Data Engineering (DE) team started to build data check tools in silos. Although each of these teams were building solid tools to meet their individual business needs, this approach was problematic for a few reasons:</p><ul><li>We started to build multiple frameworks in parallel.</li><li>Data check frameworks became costly to maintain and introduced operational overhead.</li><li>Missing features and lack of flexibility/extensibility made these frameworks difficult to reuse across the company.</li></ul><h4><strong>3. Complicated Airflow DAG code</strong></h4><p>Each check was added as a separate task in Airflow as part of the ETL pipeline. Airflow DAG files soon became massive. The operational overhead for these checks grew to the point that it became hard to maintain, because of a few different factors:</p><ul><li>There was no support for blocking vs non-blocking checks. Minor check failures or false alarms often blocked the SLA of critical data pipelines.</li><li>ETL logic and data checks became tightly coupled and not reusable.</li><li>Maintenance became operationally challenging, as we tracked the dependencies manually, which also made it difficult to add more checks.</li></ul><h3>Defining the Requirements</h3><p>To address these tooling gaps, we set out to build a unified data check framework that would meet the following requirements and ensure greater usability overtime:</p><ul><li>Extensible : Unify data check methodologies in use at Airbnb</li><li>Configuration-driven: Define the checks as YAML-formatted files for faster development</li><li>Easy to use: Provide a simplified interface to promote faster adoption company wide</li></ul><h3>Introducing Wall Framework</h3><p>Wall is the paved path for writing offline data quality checks. It is a framework designed to protect our analytical decisions from bad data bugs and ensure trustworthy data across Airbnb.</p><p>Wall Framework is written in Python on top of Apache Airflow. Users can add data quality checks to their Airflow DAGs by writing a simple config file and calling a helper function in their DAG.</p><ul><li>Wall provides most of the quality checks and anomaly detection mechanisms currently available in the company under a common framework, making data checks a lot easier to standardize.</li><li>It supports templated custom SQL-based business logic, accuracy checks, and an extensible library of predefined checks.</li><li>Wall is config driven — no code is required to add checks.</li><li>Checks can be used in the ETL pipeline in a <a href="https://airflow.apache.org/docs/apache-airflow/1.10.2/concepts.html?highlight=branch%20operator#subdags">Stage-Check-Exchange</a> pattern or as standalone checks.</li><li>The framework is extensible — any team can add their team-specific checks to Wall quite easily following the open source model (as per the Data Engineering Paved Path team’s approval).</li><li>Business users can easily add quality checks without creating any airflow DAG or tasks for each check.</li><li>Wall takes care of SQL-based checks and anomaly detection task creations. It also takes care of stage and exchange task creations and setting the appropriate dependency on the checks in a decoupled manner. Hence, after migrating to Wall, ETL pipelines were drastically simplified and we’ve seen cases where we were able to get rid of more than 70% of DAG code.</li></ul><h3>Wall Architecture</h3><p>Following our key requirements, this framework was designed to be extensible. It has three major components — WallApiManager, WallConfigManger and WallConfigModel..</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WkknwY6urhgMsEik" /><figcaption>Wall internal architecture</figcaption></figure><h4>WallApiManager</h4><p>The Wall Api Manager is the public interface to orchestrate checks and exchanges using Wall. Wall users only use this from their DAG files. It takes a config folder path as input and supports a wide variety of ETL operations such as Spark, Hive etc.</p><h4>WallConfigManager</h4><p>The Wall Config Manager parses and validates the check config files and then calls the relevant CheckConfigModels to generate a list of Airflow tasks. Wall primarily uses Presto checks to generate data checks.</p><h4>CheckConfigModel</h4><p>Each Wall check is a separate class that derives from BaseCheckConfigModel. CheckConfigModel classes are primarily responsible for validating check parameters and generating Airflow tasks for the check. CheckConfigModel makes the framework extensible. Different teams can add their own CheckConfigModel if existing models do not support their use cases.</p><h3>Key Features</h3><p>Wall framework provided the following key features to address the requirements we mentioned above.</p><h4>Flexibility</h4><ul><li>Wall configs can be located in the same repository where teams are already defining their data pipeline DAGs — teams or DAG owners can decide where they’re located. Teams can either use a separate YAML file for each table or a single YAML file for a group of tables to define checks.</li><li>Each check config model can define an arbitrary set of parameters and it can override parameters if needed. The same check configs can be orchestrated and run differently based on running context. i.e. as part of ETL’s stage-check-exchange or as pre/post checks.</li><li>A check property can be hierarchical (i.e. it can be defined at team level, file level, table level or at check level). Lower level property values override upper level values. Teams can define their team level defaults in a shared YAML file instead of duplicating the same configurations and checks in different YAML files.</li><li>In the case of stage-check-exchange checks, users can specify blocking and non-blocking checks. It makes Wall more flexible while onboarding new checks.</li></ul><h4>Extensibility</h4><ul><li>It’s easy to onboard a new type of check model. Wall is able to support commonly used data checks/validations mechanisms.</li><li>Each check config model is decoupled from each other and it can define its own set of params, validations, check generation logic, pre-processing etc.</li><li>Check config models can be developed by the data engineering community with the collaboration of the Data Engineering Paved Path team.</li></ul><h4>Simplicity</h4><ul><li>Easy to copy-paste to apply similar checks in different tables or contexts.</li><li>Check models are intuitive.</li><li>Checks are decoupled from DAG definition and ETL pipeline so that they can be updated without updating ETL.</li><li>Easy to test all the checks at once.</li></ul><h3>Adding a Wall check</h3><p>At the every high level, users need to write a yaml config and invoke Wall’s API from their DAG to orchestrate their ETL pipeline with data checks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R6UrxoyuLlaZ4dsI" /><figcaption>High level diagram of how users interact with Wall.</figcaption></figure><p>As an example of how easy it is to add a new data quality check, let’s assume you’d like to add a data quality check — verifying that a partition is not empty — to a table named foo.foo_bar in the wall_tutorials_00 DAG. It can be done by following these two steps:</p><ol><li>Decide on a folder to add your wall checks configs i.e. projects/tutorials/dags/wall_tutorials_00/wall_checks. Create a check config file (i.e. foo.foo_bar.yml ) with the following contents in your wall check config folder:</li></ol><pre>primary_table: foo.foo_bar<br>emails: [&#39;<a href="mailto:subrata.biswas@airbnb.com">subrata.biswas@airbnb.com</a>&#39;]<br>slack: [&#39;#subu-test&#39;]<br>quality_checks:<br>   - check_model: CheckEmptyTablePartition<br>     name: EmptyPartitionCheck</pre><p>Update the DAG file (i.e. wall_tutorials_00.py) to create checks based on the config file.</p><pre>from datetime import datetime</pre><pre>from airflow.models import DAG</pre><pre>from teams.wall_framework.lib.wall_api_manager.wall_api_manager import WallApiManager</pre><pre>args = {</pre><pre>&quot;depends_on_past&quot;: True,</pre><pre>&quot;wait_for_downstream&quot;: False,</pre><pre>&quot;start_date&quot;: datetime(2020, 4, 24),</pre><pre>&quot;email&quot;: [&quot;subrata.biswas@airbnb.com&quot;,],</pre><pre>&quot;adhoc&quot;: True,</pre><pre>&quot;email_on_failure&quot;: True,</pre><pre>&quot;email_on_retry&quot;: False,</pre><pre>&quot;retries&quot;: 2,</pre><pre>}</pre><pre>dag = DAG(&quot;wall_tutorials_00&quot;, default_args=args)</pre><pre>wall_api_manager = WallApiManager(config_path=&quot;projects/tutorials/dags/wall_tutorials_00/wall_checks&quot;)</pre><pre># Invoke Wall API to create a check for the table.</pre><pre>wall_api_manager.create_checks_for_table(full_table_name=&quot;foo.foo_bar&quot;, task_id=&quot;my_wall_task&quot;, dag=dag)</pre><p><strong>Validate and Test</strong></p><p>Now if you check the list of tasks of the wall_tutorials_00 you’ll see the following tasks created by the Wall Framework:</p><pre>&lt;Task(NamedHivePartitionSensor): ps_foo.foo_bar___gen&gt;</pre><pre>   &lt;Task(SubDagOperator): my_wall_task&gt;</pre><p>Wall created a SubDagOperator task and a NamedHivePartitionSensor task for the table in the primary DAG (i.e. wall_tutorials_00). Wall encapsulated all the checks inside the sub-dag. To get the check tasks list you would need to look at the sub-dag tasks i.e. run list_tasks for the wall_tutorials_00.my_wall_task dag. It returns the following list of tasks for this case:</p><pre>&lt;Task(WallPrestoCheckOperator): EmptyPartitionCheck_foo.foo_bar&gt;</pre><pre>   &lt;Task(DummyOperator): group_non_blocking_checks&gt;</pre><pre>      &lt;Task(DummyOperator): foo.foo_bar_exchange&gt;</pre><pre>&lt;Task(DummyOperator): group_blocking_checks&gt;</pre><pre>   &lt;Task(DummyOperator): foo.foo_bar_exchange&gt;</pre><pre>&lt;Task(PythonOperator): validate_dependencies&gt;</pre><p>Note: You probably noticed that Wall created a few DummyOperator tasks and a PythonOperator task in the sub-DAG. It was required to maintain control flows i.e. blocking vs non-blocking checks, dependencies, validation etc. You can ignore those tasks and don’t need to take dependencies on these tasks since they may change or can be deleted in future.</p><p>Now you can test your check tasks just like any airflow tasks i.e.</p><pre>airflow test wall_tutorials_00.my_wall_task EmptyPartitionCheck_foo.foo_bar {ds}</pre><h3>Wall in Airbnb’s Data Ecosystem.</h3><p>Integrating Wall with other tools in Airbnb’s data ecosystem was critical for its long term success. To allow other tools to integrate easily, we publish results from the “check” stage as Kafka events, to which other tools can subscribe. The following diagram shows how other tools are integrated with Wall:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*zPIHSSuv-7zdHmzJ" /><figcaption>Wall in Airbnb’s data ecosystem</figcaption></figure><h3>Conclusion</h3><p>Wall ensures a high standard for data quality at Airbnb and that the standard does not deteriorate over time.</p><p>Through enabling standardized but extensible data checks that can be easily propagated across our distributed data engineering organization, we continue to ensure trustworthy, reliable data across the company. As a result all of Airbnb’s critical business and financial data pipelines are using Wall and we have hundreds of data pipelines running thousands of Wall checks every day.</p><p>If this type of work interests you, check out some of our related positions:</p><p><a href="https://grnh.se/706bf4e01us">Senior Data Engineer</a></p><p><a href="https://grnh.se/a207325d1us">Staff Data Scientist- Algorithms, Payments</a></p><p>and more at <a href="https://careers.airbnb.com/">Careers at Airbnb</a>!</p><p>You can also learn more about our <em>Journey Toward High Quality</em> by watching our recent <a href="https://fb.watch/72Oumx3pJJ/">Airbnb Tech Talk</a>.</p><p>With special thanks to <a href="https://www.linkedin.com/in/nikuma/">Nitin Kumar</a>, <a href="https://www.linkedin.com/in/bharatrangan/">Bharat Rangan</a>, <a href="https://www.linkedin.com/in/kenneth-jung-71495256/">Ken Jung</a>, <a href="https://www.linkedin.com/in/victor-ionescu-3a029b5a/">Victor Ionescu</a>, <a href="https://www.linkedin.com/in/siyu-qiu-05b482a1/">Siyu Qiu</a> for being key partners while evangelizing this framework.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ad1b081d6e8f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f">How Airbnb Built “Wall” to prevent data bugs</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Sentiment Score to Assess Customer Service Quality]]></title>
            <link>https://medium.com/airbnb-engineering/using-sentiment-score-to-assess-customer-service-quality-43434dbe199b?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/43434dbe199b</guid>
            <category><![CDATA[ai-model]]></category>
            <category><![CDATA[a-b-testing]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[machine-learning-ai]]></category>
            <category><![CDATA[customer-experience]]></category>
            <dc:creator><![CDATA[Shuai Shao (Shawn)]]></dc:creator>
            <pubDate>Tue, 27 Jul 2021 15:48:00 GMT</pubDate>
            <atom:updated>2021-07-27T21:50:15.137Z</atom:updated>
            <content:encoded><![CDATA[<p>How AI-based Sentiment Models Complement Net Promoter Score</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oVSUzHkUykPwADBZ" /></figure><p>By <a href="https://medium.com/@shuai_shao">Shuai Shao</a>, <a href="https://medium.com/@cenzhao06">Mia Zhao</a>, <a href="https://medium.com/@chloe.yuanyuan.ni">Yuanyuan Ni</a></p><p>Net Promoter Score (NPS) is a well-accepted measurement of customer satisfaction in most customer-facing industries. We leverage NPS at Airbnb to help measure how well we serve our community of guests and hosts through our customer service. But NPS has two major drawbacks: 1) NPS is <em>sparse</em>, given only a fraction of users respond to the survey, and 2) NPS is <em>slow</em>. It takes at least a week for results to show up. Airbnb uses A/B testing heavily across our core products and customer service offerings. In the A/B testing world, the longer it takes to see results and interpret experiments, the longer it takes to iterate on the quality of our customer service. This is why we needed a much more <em>sensitive</em> and <em>robust</em> metric.</p><p>To address these limitations, Airbnb has developed an AI-based sentiment model to complement NPS. Sentiment models process messages users send to customer support (CS) representatives to extract signals reflecting users’ sentiment. Compared to NPS, the sentiment score has the following advantages:</p><ul><li>Higher <em>coverage</em>: we are not limited to those who submit a survey, and therefore more users in a given experiment register a value for this metric;</li><li>Better <em>sensitivity</em>: it takes much less time to reach statistical significance while running an experiment;</li><li><em>Causal relationship</em> with long term customer loyalty: we can ‘translate’ user sentiment scores into long term business values.</li></ul><p>This blog post provides insights on how we developed the sentiment model and the metric which aggregates the raw sentiment scores to measure customer sentiment. We leveraged Entropy Balancing (<a href="https://web.stanford.edu/~jhain/Paper/eb.pdf">Hainmueller, 2012</a>) to create a counterfactual group, in order to detect the relationship between the sentiment metric and future revenue. From our study, we show great results of sentiment metric compared to NPS.</p><h3>Sentiment Model Development</h3><p>Sentiment analysis is a great method to gauge consumers’ feeling of a particular product or service. In Airbnb’s customer support, sentiments from our guests and hosts are important signals for us to build better products and services, and ship changes with our community in mind. .</p><p>There are two main challenges we face when developing sentiment models in the customer support domain.</p><ul><li><strong>Skewed</strong><em> </em><strong>Data</strong><em>:</em> Most text inputs are negative in sentiment. Unlike when leaving reviews or messaging with hosts, guests typically contact customer support when they are experiencing an issue with Airbnb.</li><li><strong>Multilingual Input:</strong> More than 14 languages are supported by Airbnb’s customer service. Hosts and guests might be communicating in different languages in the same support ticket.</li></ul><p>To make a sentiment model tailored to our use case, we developed <strong>customized rating guidelines</strong> for customer support messages to make our model aware of domain-specific knowledge and contextual information. Examples below illustrate how the same messages are labelled differently when presented as a CS message versus a Social Media post or App Store review. In the CS domain, we focus on how well customers “think” the issue gets solved as a <em>positive</em> indication and how frustrating they “feel” the issue is as a <em>negative</em> indication.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WxB1wIGnyMsmpRnn" /></figure><p>We address data skewness via multiple iterations of sampling data for human annotations using ML model and retraining model using newly labelled data. The first round of annotation is performed based on random sampling, while subsequent annotation datasets are stratified on existing model predictions. This leads to a more balanced dataset for training.</p><p>We built and tested two deep learning architectures, both support multilingual inferences:</p><ul><li><a href="https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c">WIDeText</a> uses a CNN-based architecture to process text channels, while all categorical features are processed through the WIDe channel.</li><li><a href="https://arxiv.org/pdf/1911.02116.pdf">XLM-Roberta</a> uses a transformer-based architecture and leverages a pre-trained multilingual model to have CS messages trained in 14 languages. .</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*otPxo8B6qRoHOtv9" /><figcaption>WIDeText Architecture</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/366/0*95onJYQ62BgRs-gI" /><figcaption>Transformer Architecture</figcaption></figure><p>Transformer-based models achieve slightly better performance on English sentiment analysis and much better performance on less frequently used languages. We chose transformer-based classifier for production inference pipeline.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ld-Sd5q4Iznj9hqT-sVqmQ.png" /></figure><h3>Sentiment Metrics Development</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/946/0*r2iNOyss8cXLq35v" /></figure><p>From the raw sentiment scores, we developed the sentiment metric aiming to optimize the following criteria:</p><ul><li>Strong correlation with NPS</li><li>Sensitivity in experimentation</li><li>Demonstrable causal relationship with long-term business gains</li></ul><h3>Correlation with NPS</h3><p>Despite the limitations of NPS, it is still considered to be the gold-standard of users’ sentiment. It is desirable to make the sentiment metric, now more sensitive and robust, correlates well with NPS. We tested various ways to design the metric by aggregating the message-level raw sentiment scores (e.g., mean, cutoff, slope) to correlate with NPS.</p><p>The two charts below illustrate that sentiment scores and NPS correlate well for guest and host sentiment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/863/0*ULuPwKX14HSpn6F8" /></figure><p>NPS (green) vs Sentiment Metrics (orange) on guest sample</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/856/0*ECJNOsyJnhRMin8O" /></figure><p>NPS (green) vs Sentiment Metrics (orange) on host sample</p><h3>Sensitivity in Experimentation</h3><p>We revisited two types of past experiments (Scenario 1 and 2) to compare the sensitivity in experimentation between NPS and sentiment metric. The goal was to determine if sentiment metric can provide quicker or more accurate feedback in response to a shift in user sentiment.</p><h4>Scenario 1</h4><p>In the first type of experiments, a new product/service feature hurt the user experience from user research (e.g., the service required extra steps to contact a support agent), yet these features did not show any statistically significant changes in NPS.</p><p>For example, in one of our Interactive Voice Response (IVR) experiments, we successfully reduced contact rate by adding more questions to our automated phone messaging system. However, this also increased friction for users trying to reach customer support. At the end of the experiment, NPS trended negative but was not statistically significant after running for 30 days.</p><p>When we applied sentiment metrics to this experiment, we were able to detect that the change in new sentiment metrics reached statistical significance within 5 days.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MrcorRynIU3biIsnnkLHCg.png" /></figure><h4>Scenario 2</h4><p>The second type of experiments have features in product/service that hurt the user experience and did impact NPS in a statistically significant way. For example, one of our <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">chatbot </a>experiment decreased both NPS and sentiment metrics but NPS reached statistical significance at day 10, while sentiment metric converged much faster, detecting a shift by day 5.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X8WX3nicGLPVcCtsQX1Gtg.png" /></figure><h3>Relationship with Long-term Customer Loyalty</h3><p>As a low-frequency marketplace, one of the challenges in Airbnb’s experimentation framework is the difficulty of evaluating long-term customer loyalty such as user churn rate and future booking revenue in product iterations. For customer support teams, our products have an especially large impact on users’ experience. The experimentation should help the decision makers to answer the question “Should we launch a product/service feature if it reduces cost but hurts users’ satisfaction levels?”</p><p>Our third assessment quantifies the future booking impact of customer service using the sentiment score metric.</p><p>It would be very expensive, if possible at all, to run A/B tests with two distinct pools of agents who provide different standards of service to different groups of users. Instead, we use a novel causal inference technique to detect sentiment effects on a user’s future one-year booking revenue with observational data.</p><p>We divide the users into two groups: a <em>control</em> group, with comparatively lower sentiment scores, and a <em>treatment</em> group, with higher, more positive sentiment. We need to control for the fact that these two groups may be fundamentally different from each other in many ways, such as their tolerance to different levels of service quality, loyalty to our platform, and historical booking experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/882/0*Yrn1pWAswJu-JX3t" /></figure><p>Analysis workflow of establishing relationship between sentiment score and future revenue</p><p>In order to evaluate more reliable long-term effects of providing good customer service, we established a procedure to: 1) find confounding factors, 2) control these covariates using entropy balancing, and 3) evaluate treatment effects using weighted data.</p><h4>Confounding Variable Selection</h4><p>It took several rounds of iteration before we were able to narrow down the appropriate confounding variables and generate the covariate matrix. We listed all possible confounding variables that should be taken into account. This covered multiple disciplines including user account information, previous booking behaviors, customer contact habits, etc. We then selected related variables that correlated with both sentiment and future booking. For example, users with more previous bookings tend to book more and are more positive when communicating with customer support agents. Finally,<strong> </strong>we cross checked correlations among all variables to remove redundant ones. This helped us to select a short list of confounding variables.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6KfIB1PS0QaKby0K06fpsA.png" /></figure><h4>Entropy Balancing</h4><p>We use <a href="https://web.stanford.edu/~jhain/Paper/eb.pdf">Entropy Balancing</a> to achieve covariate balance. Entropy Balancing is a maximum entropy reweighting scheme to create balanced samples that satisfy a set of constraints. Here are two most important features in the scheme:</p><p><strong>1. Equalized moments of the covariate distributions. </strong>By assigning weight wi to each sample unit, we want the moments of the covariate distribution (e.g., mean, variance, and skewness) between the treatment and the reweighted control group to be equal (defined in equation 2). A typical balance constraint is formulated with mr containing the rthorder moment of a given variable Xj from the treatment group, whereas the moment functions are specified for the control group as cri(Xij)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aA3mjawjoqS17c6s6iZ7SA.png" /></figure><p><strong>2. Minimized distance from base weights</strong>. We also want to minimize the. distance between estimated weights wiand base weight qi(usually set as 1/n0 , uniformly distributed) to retain information as much as possible (defined in equation 3).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GuVv6BTriCaFHg5jEJpQWg.png" /></figure><p>Compared to more frequently used Propensity Score Matching, Entropy Balancing has several proven advantages:</p><ul><li><strong>It is good at balancing results even to high degrees of moments</strong>. In contrast to most other preprocessing methods that involve multiple rounds of manual adjustments on both model and matching until reaching balanced results (which often fails on high dimensional samples), entropy balancing directly searches for weights that can achieve exact covariate balance in finite samples. It significantly improves the balance that can be obtained by other methods, which are validated by an insurance use case <a href="https://www.thieme-connect.de/products/ejournals/abstract/10.1055/a-1009-6634">Matschinger (2019)</a>.</li><li><strong>It retains valuable information without discarding units</strong>. Entropy Balancing retains valuable information by allowing the unit weights to vary smoothly across units, so that we don’t have to throw away any unmatched data.</li><li><strong>It is versatile.</strong> The weights we get can be used to almost any standard estimation of treatment effects such as weighted mean and weighted regression.</li><li><strong>It is computationally inexpensive</strong>. It only takes a couple seconds to get balanced results for over 1M records.</li></ul><p>Evaluating Treatment Effects</p><p>We were able to reach balanced results for all confounding variables after using entropy reweighting:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/374/0*1JfKfSricjCrVLaf" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/378/0*rUOEG3pjW3Qtsbja" /></figure><p>With the weighted results, we found that <strong>guests on Airbnb with higher sentiment (potentially good CS experiences with sentiment metrics &gt;= 0.1) produce significantly more revenue in the subsequent 12 months</strong>. This result can be applied to trade-off analysis whenever we see an opposite result in cost and user CS sentiment score and help us make the right launch decision taking long-term revenue into consideration.</p><h3>Takeaways</h3><p>In this blog post, we provided details of sentiment model development and the framework of assessing sentiment metrics.</p><p>For ML practitioners, the success of a sentiment analysis depends on domain-specific data and annotation guidelines. Our experiments show transformer-based classifiers perform better than CNN-based architectures, especially in less frequently used languages.</p><p>For customer service providers who struggled with the pain of NPS, sentiment analysis has provided promising results to describe customers’ satisfaction levels. If you have user communication text, exploring sentiment analysis may solve the long lasting pain of NPS. However, if you only have phone call recordings, exploring audio to text transcription could be a good start before exploring emotion detection in audio.</p><p>For data analysts and data scientists, the framework of metrics development from a new signal (model output) is reusable: considering many user feedback metrics are either slow or sparse, data professionals can assess the new signals from coverage, sensitivity, and causal relationship with business values. For causal analysis challenges, it is worth spending some time to explore the new Entropy Balancing techniques, which may save you time from Propensity Score Matching.</p><p>If this type of work interests you, check out some of our related positions:</p><p><a href="https://grnh.se/049bfea61us">Senior Data Scientist — Analytics, Support Products</a></p><p><a href="https://grnh.se/36f092141us">Staff Data Architect, Community Support Platform</a></p><p>and more at <a href="https://careers.airbnb.com/">Careers at Airbnb</a>!</p><h3>Acknowledgements</h3><p>Thanks to Zhiying Gu and Lo-hua Yuan for providing important knowledge support on causal inference. Thanks to Mitral Akhtari and Jenny Chen for knowledge sharing on <a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">Airbnb’s Future Incremental Value system</a>. We would also like to thank Bo Zeng for sentiment modeling guidance, Mariel Young for the metrics iteration, and Aashima Paul, Evan Lin, and Keke Hu for their hard work on labelling the sentiment data. Last but not least, we appreciate Joy Zhang, Nathan Triplett, and Shijing Yao for their guidance.</p><h3>References:</h3><ol><li>Jens Hainmueller (2012) Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies, <em>Political Analysis</em>, 20:25−46 doi:10.1093/pan/mpr025</li><li>Herbert Matschinger, Dirk Heider, Hans-Helmut König (2020) A Comparison of Matching and Weighting Methods for Causal Inference Based on Routine Health Insurance Data, or: What to do If an RCT is Impossible,<em>Gesundheitswesen, </em>82(S 02): S139-S150 DOI: 10.1055/a-1009–6634</li></ol><p>Further Reading</p><p><a href="https://docs.google.com/document/d/1irNF89bKsTGgjNCmPoJXBVEaNPv00Fl3skwCAjBHK8M/edit">WIDeText: Multimodal Deep Learning Framework, and its application on Room Type Classification</a> goes into the details of Deep learning framework used in Airbnb</p><p><a href="https://ieeexplore.ieee.org/document/8964147">Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform</a> goes into the details of the Airbnb Machine Learning Infrastructure. <em>DSAA</em>’2019</p><p><a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">How Airbnb Measures Future Value to Standardize Tradeoffs</a> goes into details of how Airbnb optimizes for long-term decision-making through the propensity score matching model</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=43434dbe199b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/using-sentiment-score-to-assess-customer-service-quality-43434dbe199b">Using Sentiment Score to Assess Customer Service Quality</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Measures Future Value to Standardize Tradeoffs]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/3aa99a941ba5</guid>
            <category><![CDATA[data-platforms]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Jenny Chen]]></dc:creator>
            <pubDate>Tue, 13 Jul 2021 17:00:41 GMT</pubDate>
            <atom:updated>2021-08-27T13:55:53.714Z</atom:updated>
            <content:encoded><![CDATA[<h4>The propensity score matching model powering how we optimize for long-term decision-making</h4><p>By <a href="https://www.linkedin.com/in/mitra-akhtari/">Mitra Akhtari</a>, <a href="https://www.linkedin.com/in/jennychen96/">Jenny Chen</a>, <a href="https://www.linkedin.com/in/amelialemionet">Amelia Lemionet</a>, <a href="https://www.linkedin.com/in/dan-nguyen-b8817a34/">Dan Nguyen</a>, <a href="https://www.linkedin.com/in/hassan-obeid/">Hassan Obeid</a>, <a href="https://www.linkedin.com/in/yunshanz/">Yunshan Zhu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u3P2iMzEh4mLglRq" /></figure><p>At Airbnb, we have a <a href="https://news.airbnb.com/brian-cheskys-open-letter-to-the-airbnb-community-about-building-a-21st-century-company/">vision</a> to build a 21st century company by operating over an infinite time horizon and balancing the interests of all stakeholders. To do so effectively, we need to be able to compare, in a common currency, both the short and long-term value of actions and events that take place on our platform. These actions could be a guest making a booking or a host adding amenities to their listing, to name just two examples.</p><p>Though randomized experiments measure the initial impact of some of these actions, others, such as cancellations, are difficult to evaluate using experiments due to ethical, legal, or user experience concerns. Metrics in experiments can be hard to interpret as well, especially if the experiment affects opposing metrics (e.g., bookings increase but so do cancellations). Additionally, regardless of our ability to assess causal impact with A/B testing, <a href="https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7">experiments are often run only for a short period of time</a> and do not allow us to quantify impact over an extended period.</p><p>So what did we build to solve this problem?</p><h3><strong>Introducing Future Incremental Value (FIV)</strong></h3><p>We are interested in the long-term causal effect or “future incremental value” (FIV) of an action or event that occurs on Airbnb. We define “long-term” as 1 year, though our framework can adjust the time period to be as short as 30 days or as long as 2 years.</p><p>To use a concrete example, assume we would like to estimate the long-term impact of a guest making a booking. Denote the <em>n1</em> number of users who make a booking within a month as <em>i ∈a1</em>and the <em>n0</em> number of users who do not make a booking in that time period as <em>i∈a0</em>. In the following year, each of these users generates revenue (or any other outcome of interest) denoted by <em>y</em>. The naive approach to computing the impact of making a booking would be to simply look at the average differences between users who made a booking versus those that did not:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Vxqdv4sg3gdaHP5p" /></figure><p>However, these two groups of users are very different: those who made a booking “selected” into doing so. This selection bias obscures the true causal effect of the action, <em>FIV(a)</em>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/826/0*EiXkA-AcIEMhDMB2" /></figure><p>Our goal is to exclude the bias from the naive estimate to identify <em>FIV(a)</em>.</p><h3><strong>The Science Behind FIV</strong></h3><p>To minimize selection bias in estimating the FIV of an action, we need to compare observations from users or listings that are similar in every way except for whether or not they took or experienced an action. The well-documented, quasi-experimental methodology we have chosen for this problem is <a href="https://academic.oup.com/biomet/article/70/1/41/240879?login=true">propensity score matching</a> (PSM). We start by separating users or listings into two groups: observations from those that took the action (“focal”) during a given timeframe and observations from those that did not (“complement”). Using PSM, we construct a “counterfactual” group, a subset of the complement that matches the characteristics of the focal as much as possible, except that these users or listings did not take the action. The assumption is that “assignment” into focal versus counterfactual is as good as random.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*F43MRwKtSDShhbRi" /><figcaption><em>Figure 1. Overview of methodology behind FIV</em></figcaption></figure><p>The specific steps we take for eliminating bias from the naive method are:</p><ol><li><em>Generate the Propensity Score:</em> Using a set of pre-treatment or control features describing attributes of the user or listing (e.g., number of past searches), we build a binary, tree-based classifier to predict the probability that the user or listing took the action. The output here is a propensity score for each observation.</li><li><em>Trim for Common Support: </em>We remove from the dataset any observations that have no “matching twin” in terms of propensity score. After splitting the distribution of propensity scores into buckets, we discard observations in buckets where either the focal or complement have little representation.</li><li><em>Match Similar Observations: </em>To create the counterfactual, we use the propensity score to match each observation in the focal to a counterpart in the complement. Various <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">matching strategies</a> can be used, such as matching in bins or via nearest neighbors.</li><li><em>Results: </em>To get the FIV, we compute the average of the outcome or target feature in the focal minus the average in the counterfactual.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/952/0*P3Og2RJ1mLeasghj" /></figure><h4>Evaluation</h4><p>In a supervised machine learning problem, as more data becomes available and future outcomes are actualized, the model is either validated or revised. This is not the case for FIV. The steps above give us an estimate of the incremental impact of an action, but the “true” incremental impact is never revealed. In this world, how do we evaluate the success of our model?</p><p><em>Common Support: </em>One of the assumptions of using PSM for causal inference is “common support”.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*POao7OeMFaC3Sv5V" /></figure><p>where <em>D = 1 </em>denotes observations in the focal group and <em>X</em> are the controlling features. This assumption rules out the possibility of “perfect predictability” to guarantee that observations with the same <em>X</em> values have a positive probability of belonging to both groups and thus can be matched together to provide valid comparisons. Plotting the distribution of propensity scores for the focal and the complement group allows for a visual inspection of this assumption. Interestingly, in the case of causal inference with PSM, a high Area Under the Curve (AUC), a desirable feature for most prediction models, means that the model is able to distinguish between focal and complement observations too well, reducing our matching quality. In such cases, we assess whether those control features are confounders that affect the output metrics and eliminate them.</p><p><em>Matching Evaluation:</em> Observations are considered “similar” if the distributions of key features in the focal closely match the distributions of those in the counterfactual. But how close is close enough? To quantify this, we compute three metrics to assess the quality of the matching, as described in <a href="http://sekhon.berkeley.edu/papers/other/Rubin2001.pdf">Rubin (2001)</a>. These metrics identify whether the propensity score and key control features have similar distributions in the focal and counterfactual groups. Additionally, we are currently investigating whether to apply an additional regression adjustment to correct for any remaining imbalance in the key control features. For instance, after the matching stage, we could run a regression that directly controls for key features that we want an almost exact match for.</p><p><em>Past experiments:</em> Company-wide, we run experiments to test various hypotheses on how to improve the user experience, potentially leading to positive outcomes such as a significant increase in bookings. These experiments generate a source of variation in the likelihood of guests making a booking that does not suffer from selection bias, due to the randomization of treatment assignment in the experiment. By tracking and comparing the users in the control group to users in the treatment groups of these experiments, we observe the “long-term impact of making a booking”, which we can compare to our FIV estimate for “guest booking”. While the FIV estimate is a global average and experiments often estimate <a href="https://www.aeaweb.org/articles?id=10.1257/000282806776157641">local average treatment effects</a>, we can still use experimental benchmarks as an important gut check.</p><h3><strong>Adapting FIV for Airbnb</strong></h3><p>While PSM is a well-established method for causal inference, we must also address several additional challenges, including the fact that Airbnb operates in a two-sided marketplace. Accordingly, the FIV platform must support computation from both the guest and the listing perspective. Guest FIV estimates the impact of actions based on activity a guest generates on Airbnb after experiencing an action, while listing FIV is from the lens of a listing. We are still in the process of developing a “host-level” FIV. One challenge in doing so will be sample size: we have fewer unique hosts than listings.</p><p>To arrive at a “platform” or total FIV for an action, we cannot simply add guest and listing FIVs together because of double counting. We simplify the problem and only count the value from the guest-side or the listing-side depending on which mechanisms we believe drive the majority of the long-term impact.</p><p>Another feature of our two-sided market is cannibalization, especially on the supply-side: if a listing gets more bookings, some portion of this increase is taking away bookings from similar listings. In order to arrive at the true “incremental” value of an action, we apply cannibalization haircuts to listing FIV estimates based on our understanding of the magnitude of this cannibalization from experimental data.</p><h3><strong>The Platform Powering FIV</strong></h3><p>FIV is a data product and its clients are other teams within Airbnb. We provide an easy to use platform to organize, compute, and analyze actions and FIVs at scale. As part of this, we have built components that take in input from the client, construct and store necessary data, productionize the PSM model, compute FIVs, and output the results. The machinery, orchestrated through <a href="https://airflow.apache.org/docs/apache-airflow/2.0.1/">Airflow</a> and invisible to the client, looks as follows:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IqZbWBgn0wJFgXGd" /><figcaption><em>Figure 2. Overview of FIV Platform</em></figcaption></figure><h4>Client Input</h4><p>Use cases begin with a conversation with the client team to understand the business context and technical components of their desired estimate. An integral part of producing valid and useful FIV estimates is establishing well-defined focal and complement groups. Additionally, there are cases when the FIV tools are not applicable, such as when there is limited observational data (e.g., a new feature) or small group sizes (e.g., a specific funnel or lever).</p><p>The client submits a configuration file defining their focal and complement groups, which is essentially the only task the client does in order to use the FIV platform. Below is the config for the FIV of “guest booking”: a visitor who booked a home on our site (focal) versus one who did not book a home (complement).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/585c5f1c3b0f0789f6369b41006ad5c1/href">https://medium.com/media/585c5f1c3b0f0789f6369b41006ad5c1/href</a></iframe><p>The <em>cohort</em> identifies the maximum set of users to consider (in this case, all visitors to Airbnb’s platform), some of which are removed from consideration by the <em>filter_query</em> (in this case, users who also booked an Airbnb experience are removed). From the remaining set of users, the <em>action_event_query</em> allocates users to the focal with leftovers automatically assigned to the complement.</p><p>After the client’s config is reviewed, it is merged into the FIV repository and automatically ingested into our pipelines. We assign a version to each unique config to allow for iteration while storing historical results.</p><p>We have designed the platform to be as easy to use as possible. No special knowledge of modeling, inference, or complex coding is needed. The client only needs to provide a set of queries to define their groups and we take care of the rest!</p><h4>Data Pipeline</h4><p>The config triggers a pipeline to construct the focal and complement, join them with control and target features, and store this in the Data Warehouse. Control features will later serve as inputs into the propensity score model, whereas target features will be the outcomes that FIV is computed over. Target features are what allow us to convert actions from different contexts and parts of Airbnb into a “common currency”. This is one of FIV’s superpowers!</p><p>Leveraging <a href="https://databricks.com/session/zipline-airbnbs-machine-learning-data-management-platform">Zipline</a> as our feature management system, we currently have approximately 1,000 control features across both guests and listings, such as region, cancellations, or past searches. Though we have the capability to compute FIV in terms of numerous target features, we have a few target features that give us a standardized output, such as revenue, cost, and bookings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*keT6ycZkmSs4an8L" /><figcaption><em>Figure 4. Steps to compute the raw data needed for FIV, after taking in client input</em></figcaption></figure><p>The version of the config is also used here to automate backfills, significantly decreasing manual errors and intervention. There are multiple checks on the versioning to ensure that the data produced is always aligned with the latest config.</p><h4>Modeling Pipeline</h4><p>Because the focal and complement groups can be very large and costly to use in modeling, we downsample and use a subset of our total observations. To account for sampling noise, we take multiple samples from the output of our data pipeline and feed each sampling round into our modeling pipeline. Sampling improves our SLA, ensures each group has the same cardinality and allows us to get a sense of sampling noise. Outliers are also removed to limit the noisiness of our estimates.</p><p>The PSM model is built on top of <a href="https://ieeexplore.ieee.org/document/8964147">Bighead</a>, Airbnb’s machine learning platform. After fetching the sampled data, we perform feature selection, clean the features, and run PSM to produce FIVs in terms of each target feature before finally writing our results into the Data Warehouse. In addition to the FIVs themselves, we also collect and store evaluation metrics as well as metrics such as feature importance and runtime.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Hm6aj1TSpXLjd5kU" /><figcaption><em>Figure 5. Modeling steps needed to compute FIV, after the raw data has been generated</em></figcaption></figure><p>On top of the modeling pipeline we have built the ability to prioritize actions and rate limit the number of tasks we launch, giving us a big picture view of the resources being used.</p><h4>FIVs!</h4><p>Next we pull our FIVs into a <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Superset</a> dashboard for easy access by our clients. FIV point estimates and confidence intervals (estimated by <a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf">bootstrapping</a>) are based on the last 6 months of available data to smooth over seasonality or month-level fluctuations. We distinguish between the value generated by the action itself (tagged as “Present” below) and the residual downstream value (“Future”) of the action.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wM0yfjWPOP1NU9Yb" /><figcaption><em>Figure 6. Snapshot of the dashboard as seen by clients</em></figcaption></figure><h3><strong>FIV as a Product</strong></h3><p>Airbnb’s two-sided marketplace creates interesting but complicated tradeoffs. To quantify these tradeoffs in a common currency, especially when experimentation is not possible, we have built the FIV framework. This has allowed teams to make standardized, data-informed prioritization decisions that account for both immediate <em>and</em> long-term payoffs.</p><p>Currently, we have scaled to work with <em>all</em> teams from across the company (demand-side, supply-side, platform teams like Payments and Customer Support, and even the company’s most recent team, <a href="https://www.airbnb.org/">Airbnb.org</a>) and computed over 150 FIV action events from the guest and listing perspective. Use cases range from return on investment calculations (what is the monetary value of a “perfect” stay?) to determining the long-term value of guest outreach emails that may not always generate immediate output metrics. We have also used FIV to inform the <a href="https://www.linkedin.com/pulse/overall-evaluation-criterion-oec-ronny-kohavi/">overall evaluation criteria</a> in <a href="https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669">experiments</a> (what weights do we use when trading off increased bookings and cancellations?) and rank different listing levers to understand what to prioritize (what features or amenities are most useful for a host to adopt?).</p><p>In the absence of a centralized, scalable FIV platform, each individual team would need to create their own framework, methodology, and pipelines to assess and trade off long-term value, which would be inefficient and leave room for errors and inconsistencies. We have boiled down this complex problem into essentially writing two queries with everything else done behind the scenes by our machinery.</p><p>Yet, our work is not done–we plan to continue improving the workflow experience and explore new models in order to improve our estimates. The future of FIV at Airbnb is bright!</p><h3><strong>Acknowledgments</strong></h3><p>FIV has been an effort spanning multiple teams and years. We’d like to especially thank <a href="https://www.linkedin.com/in/diana-chen-42332939/">Diana Chen</a> and <a href="https://www.linkedin.com/in/xuyuhe/">Yuhe Xu</a> for contributing to the development of FIV and the teams who have onboarded and placed trust into FIV.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3aa99a941ba5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5">How Airbnb Measures Future Value to Standardize Tradeoffs</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Deep Dive into Airbnb’s Server-Driven UI System]]></title>
            <link>https://medium.com/airbnb-engineering/a-deep-dive-into-airbnbs-server-driven-ui-system-842244c5f5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/842244c5f5</guid>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[mobile-app-development]]></category>
            <category><![CDATA[server-driven-ui]]></category>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[web]]></category>
            <dc:creator><![CDATA[Ryan Brooks]]></dc:creator>
            <pubDate>Tue, 29 Jun 2021 17:00:44 GMT</pubDate>
            <atom:updated>2021-06-29T21:05:35.690Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb ships features faster across web, iOS, and Android using a server-driven UI system named Ghost Platform 👻.</p><p>By <a href="https://www.linkedin.com/in/rbro112/">Ryan Brooks</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CedYKpSYMIGEiX7m" /></figure><h4>Background: Server-Driven UI</h4><p>Before we dive into Airbnb’s implementation of server-driven UI (SDUI), it’s important to understand the general idea of SDUI and how it provides an advantage over traditional client-driven UI.</p><p>In a traditional world, data is driven by the backend and the UI is driven by each client (web, iOS, and Android). As an example, let’s take Airbnb’s listing page. To show our users a listing, we might request listing data from the backend. Upon receiving this listing data, the client transforms that data into UI.</p><p>This comes with a few issues. First, there’s listing-specific logic built on each client to transform and render the listing data. This logic becomes complicated quickly and is inflexible if we make changes to how listings are displayed down the road.</p><p>Second, each client has to maintain parity with each other. As mentioned, the logic for this screen gets complicated quickly and each client has their own intricacies and specific implementations for handling state, displaying UI, etc. It’s easy for clients to quickly diverge from one another.</p><p>Finally, mobile has a versioning problem. Each time we need to add new features to our listing page, we need to release a new version of our mobile apps for users to get the latest experience. Until users update, we have few ways to determine if users are using or responding well to these new features.</p><h4>The Case for SDUI</h4><p>What if clients didn’t need to know they were even displaying a listing? What if we could pass the UI directly to the client and skip the idea of listing data entirely? That’s essentially what SDUI does — we pass both the UI and the data together, and the client displays it agnostic of the data it contains.</p><p>Airbnb’s specific SDUI implementation enables our backend to control the data and how that data is displayed across all clients at the same time. Everything from the screen’s layout, how sections are arranged in that layout, the data displayed in each section, and even the actions taken when users interact with sections is controlled by a single backend response across our web, iOS, and Android apps.</p><h3>SDUI at Airbnb: The Ghost Platform 👻</h3><p>The Ghost Platform (GP) is a unified, opinionated, server-driven UI system that enables us to iterate rapidly and launch features safely across web, iOS, and Android. It is called Ghost because our primary focus is around ‘<strong>G</strong>uest’ and ‘<strong>Host</strong>’ features, the two sides to our Airbnb apps.</p><p>GP provides web, iOS, and Android frameworks in each client’s native languages (Typescript, Swift, and Kotlin, respectively) that enable developers to create server-driven features with minimal setup.</p><p>The core feature of GP is that features can share a library of generic sections, layouts, and actions, many backward compatible, enabling teams to ship faster and move complicated business logic to a central location on the backend.</p><h4>A Standardized Schema</h4><p>The backbone of the Ghost Platform is a standardized data model that clients can use to render UI. To make this possible, GP leverages a shared data layer across backend services using a unified data-service mesh, called <a href="https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344">Viaduct</a>.</p><p>The key decision that helped us make our server-driven UI system scalable was to use a single, shared GraphQL schema for Web, iOS, and Android apps — i.e., we’re using the same schema for handling responses and generating strongly typed data models across all of our platforms.</p><p>We’ve taken time to generalize the shared aspects of different features and to account for each page’s idiosyncrasies in a consistent, thoughtful way. The result is a universal schema that’s capable of rendering all features on Airbnb. This schema is powerful enough to account for reusable sections, dynamic layouts, subpages, actions, and more, and the corresponding GP frameworks in our client applications leverage this universal schema to standardize UI rendering.</p><h3>The GP Response</h3><p>The first fundamental aspect of GP is the structure of the overall response. There are two main concepts used to describe UI in a GP response: sections and screens.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Qx23_meafmhc7-xQ" /><figcaption>Figure 1. <em>How users see Airbnb features on GP vs. how GP sees those same features as screens and sections.</em></figcaption></figure><ul><li><strong>Sections:</strong> Sections are the most primitive building block of GP. A section describes the data of a cohesive group of UI components, containing the exact data to be displayed — already translated, localized, and formatted. Each client takes the section data and transforms it directly into UI.</li><li><strong>Screens:</strong> Any GP response can have an arbitrary number of screens. Each screen describes the layout of the screen and, in turn, where sections from the sections array will appear (called placements). It also defines other metadata, such as how to render sections — e.g., as a popover, modal, or full-screen — and logging data.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e7e7ae8430106ed099e41067cbbd626d/href">https://medium.com/media/e7e7ae8430106ed099e41067cbbd626d/href</a></iframe><p>A feature’s backend built with GP will implement this GPResponse (fig. 2) and populate the screens and sections depending on their use case. GP client frameworks on web, iOS, and Android provide developers standard handling to fetch a GPResponse implementation and translate that to UI with minimal work on their part.</p><h3>Sections</h3><p>Sections are the most basic building block of GP. The key feature of GP sections is that they are entirely independent of other sections and the screen on which they are displayed.</p><p>By decoupling sections from the context around them, we gain the ability to reuse and repurpose sections without worrying about a tight coupling of business logic to any specific feature.</p><h4>Section Schema</h4><p>In GraphQL schema, GP sections are a union of all possible section types. Each section type specifies the fields they provide to be rendered. Sections are received in a GPResponse implementation with some metadata and are provided through a SectionContainer wrapper, which contains details about the section’s status, logging data, and the actual section data model.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a95a97b2f0c56c0c99ba4117c91d17c2/href">https://medium.com/media/a95a97b2f0c56c0c99ba4117c91d17c2/href</a></iframe><p>One important concept to touch on is SectionComponentType. SectionComponentType controls <em>how</em> a section’s data model is rendered. This enables one data model to be rendered in many different ways if needed.</p><p>For example, the two SectionComponentTypes TITLE and PLUS_TITLE might use the same backing TitleSection data model, but the PLUS_TITLE implementation will use Airbnb’s Plus-specific logo and title style to render the TitleSection. This provides flexibility to features using GP, while still promoting schema and data reusability.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i-Mi5mngBjWXQkFk" /><figcaption>Figure 4. <em>Example of rendering a TitleSection data model differently using SectionComponentType.</em></figcaption></figure><h4>Section Components</h4><p>Section data is transformed into UI through “Section Components”. Each section component is responsible for transforming a data model and a SectionComponentType into UI components. Abstract section components are provided by GP on each platform in their native languages (i.e., Typescript, Swift, Kotlin) and can be extended by developers to create new sections.</p><p>Section components map a section data model to <strong>one</strong> unique rendering and therefore pertain only to one SectionComponentType. As mentioned previously, sections are rendered without any context from the screen they are on or the sections around them, so each section component has no feature-specific business logic provided to it.</p><p>I’m an Android developer, so let’s take an Android example (and because Kotlin is great 😄). To build a title section, we have the code snippet seen below (fig. 5). Web and iOS have similar implementations — in Typescript and Swift respectively — for building section components.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1fc6b8a69d9799529da8817ff11621f5/href">https://medium.com/media/1fc6b8a69d9799529da8817ff11621f5/href</a></iframe><p>GP provides many “core” section components, such as our example TitleSectionComponent above (fig. 5), meant to be configurable, styleable, and backward compatible from the backend so we can adapt to any feature’s use case. However, developers building new features on GP can add new section components as needed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8jjmsp_zOQfl0-Q8" /><figcaption>Figure 6. <em>GP takes section data, uses a section component to turn it into UI (TitleSectionComponent from fig. 5), and presents the built section UI to the user.</em></figcaption></figure><h3>Screens</h3><p>Screens are another building block of GP, but unlike sections, screens are mostly handled by GP client frameworks and are more opinionated in usage. GP screens are responsible for the layout and organization of sections.</p><h4>Screens schema</h4><p>Screens are received as a ScreenContainer type. Screens can be launched in a modal (popup), in a bottom sheet, or as a full screen, depending on values included in the screenProperties field.</p><p>Screens enable dynamic configuration of a screen’s layout and, in turn, arrangement of sections through a LayoutsPerFormFactor type. LayoutsPerFormFactor specifies the layout for compact and wide breakpoints using an interface called ILayout, which will be elaborated on below. The GP framework on each client then uses screen density, rotation, and other factors to determine which ILayout fromLayoutsPerFormFactor to render.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/183497776e1051791c38fb590888117c/href">https://medium.com/media/183497776e1051791c38fb590888117c/href</a></iframe><h4>ILayouts</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vSJJB0hlGHrzSlu6" /><figcaption>Figure 8. <em>A few examples of ILayout implementations, which are used to specify various placements.</em></figcaption></figure><p>ILayouts enable screens to change layouts depending on the response. In schema, ILayout is an interface with each ILayout implementation specifying various placements. Placements contain one or many SectionDetail types that point to sections in the response’s outermost sections array. We point to section data models rather than including them inline. This shrinks response sizes by reusing sections across layout configurations (LayoutsPerFormFactor from fig. 7).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7bfa81b59a9b172db7178c3c9db31aa2/href">https://medium.com/media/7bfa81b59a9b172db7178c3c9db31aa2/href</a></iframe><p>GP client frameworks inflate ILayouts for developers, as ILayout types are more opinionated than sections. EachILayout has a unique renderer in each client’s GP framework. The layout renderer takes each SectionDetail from each placement, finds the proper section component to render that section, builds the section’s UI using that section component, and finally, places the built UI into the layout.</p><h3>Actions</h3><p>The last concept of GP is our action and event handling infrastructure. One of the most game-changing aspects of GP is that in addition to defining the sections and layout of a screen from the network response, we also can define actions taken when users interact with UI on the screen, such as tapping a button or swiping a card. We do this through an IAction interface in our schema.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c66091e20e170cff94bc3bb43d63c078/href">https://medium.com/media/c66091e20e170cff94bc3bb43d63c078/href</a></iframe><p>Recall from earlier (fig. 6) that a section component is what translates our TitleSection to UI on each client. Let’s take a look at the same Android example of a TitleSectionComponent with a dynamic IAction fired on the click of the subtitle text.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cc64c3b4a32e77b593f830d6bcb080bf/href">https://medium.com/media/cc64c3b4a32e77b593f830d6bcb080bf/href</a></iframe><p>When a user taps the subtitle in this section, it fires the IAction passed for the onSubtitleClickAction field in TitleSection. GP is responsible for routing this action to an event handler defined for the feature, which will handle the IAction that was fired.</p><p>There is a standard set of generic actions that GP handles universally, such as navigating to a screen or scrolling to a section. Features can add their own IAction types and use those to handle their feature’s unique actions. Since the feature-specific event handlers are scoped to the feature, they can contain as much feature-specific business logic as they wish, enabling freedom to use custom actions and business logic when specific use cases arise.</p><h3>Bringing It All Together</h3><p>We’ve gone over several concepts, so let’s take an entire GP response and see how it’s rendered to tie everything together.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/dfe0f7b05badb9997ab1c0ab0d75bce2/href">https://medium.com/media/dfe0f7b05badb9997ab1c0ab0d75bce2/href</a></iframe><h4>Creating the Section Components</h4><p>Features using GP will need to fetch a response implementing GPResponse mentioned above (fig. 2). Upon receiving a GPResponse, GP infra handles parsing this response and building the sections for the developer.</p><p>Recall that each section in our sections array has a SectionComponentType and a section data model. Developers working on GP add section components, using SectionComponentType as the key for how to render the section data model.</p><p>GP finds each section component and passes it the corresponding data model. Each section component creates UI components for the section, which GP will insert into the proper placement in the layout below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2SLVYciGoKNbdau6" /><figcaption>Figure 13. <em>Transforming section data to UI.</em></figcaption></figure><h4>Handling Actions</h4><p>Now that each section component’s UI elements are set up, we need to handle users interacting with the sections. For example, if they tap a button, we need to handle the action taken on click.</p><p>Recall earlier that GP handles routing events to their proper handler. The example response above (fig. 12) contains two sections that can fire actions, toolbar_section and book_bar_footer. The section component for building both of these sections simply needs to take the IAction and specify when to fire it, which in both cases will be when a button is clicked.</p><p>We can do this through click handlers on each client, which will use GP infra to route the event on a click event.</p><pre>button(<br>  onClickListener = {<br>    GPActionHandler.handleIAction(section.button.onClickAction)<br>  }<br>)</pre><h4>Setting Up the Screen and Layout</h4><p>To arrange a fully interactive screen for our users, GP looks through the screens array to find a screen with the “ROOT” id (GP’s default screen id). GP will then find the proper ILayout type depending on the breakpoint and other factors relevant to the specific device the user is using. To keep things simple, we’ll use the layout from the compact field, a SingleColumnLayout.</p><p>GP will then find a layout renderer for SingleColumnLayout, where it’ll inflate a layout with a top container (the nav placement), a scrollable list (the main placement), and a floating footer (the footer placement).</p><p>This layout renderer will take the models for the placements, which contain SectionDetail objects. These SectionDetails contain some styling information as well as the sectionId of the section to inflate. GP will iterate through these SectionDetail objects and inflate sections into their respective placements using the section components we built earlier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*8mJWV_F1FPq0U5eI" /><figcaption>Figure 14. <em>GP Infra takes built sections with action handlers added, adds sections to ILayout placements.</em></figcaption></figure><h3>What’s Next for GP?</h3><p>GP has only existed for about a year, but a majority of Airbnb’s most used features (e.g., search, listing pages, checkout) are built on GP. Despite the critical mass of usage, GP is still in its infancy and there’s much more to be done.</p><p>We have plans for a more composable UI through “nested sections”, improving discoverability of elements that already exist through our design tools, such as Figma, and WYSIWYG editing of sections and placements, enabling no-code feature changes.</p><p>If you’re passionate about server-driven UI or building UI systems that scale, there’s so much more to be done. We encourage you to apply to the <a href="https://careers.airbnb.com/">open roles</a> on our engineering team.</p><h4>Re-engineering Travel Tech Talk</h4><p>Server-driven UI is complex. Countless hours have gone into creating a robust schema, client frameworks, and developer documentation that enables GP to be successful.</p><p>If you’d like a more high-level overview of SDUI and GP, I recently had the opportunity to speak at Airbnb’s <a href="https://www.facebook.com/AirbnbTech/videos/1445539065813160/">Re-engineering Travel tech talk</a> presenting GP. I’d encourage you to check it out for a general overview of server-driven UI and GP (skip to the 31-minute mark if you’re short on time).</p><h4>Special Thanks</h4><p>Special thanks to <a href="https://www.linkedin.com/in/abhinavvohra/">Abhi Vohra</a>, <a href="https://www.linkedin.com/in/wensheng-mao-76ab7142/">Wensheng Mao</a>, <a href="https://www.linkedin.com/in/jnvollmer/">Jean-Nicolas Vollmer</a>, <a href="https://www.linkedin.com/in/pranayairan/">Pranay Airan</a>, <a href="https://www.linkedin.com/in/stephen-herring-00381a6a/">Stephen Herring</a>, <a href="https://www.linkedin.com/in/jsperl/">Jasper Liu</a>, <a href="https://www.linkedin.com/in/kevinchrisweber/">Kevin Weber</a>, <a href="https://www.linkedin.com/in/rodolphe-courtier-97b32610/">Rodolphe Courtier</a>, <a href="https://www.linkedin.com/in/danielgarciacarrillo/">Daniel Garcia-Carrillo</a>, <a href="https://www.linkedin.com/in/fidelsosa/">Fidel Sosa</a>, <a href="https://www.linkedin.com/in/roshan-goli-03977a25/">Roshan Goli</a>, <a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a>, <a href="https://www.linkedin.com/in/chen-wu-a5677649/">Chen Wu</a>, <a href="https://www.linkedin.com/in/nickbryanmiller/">Nick Miller</a>, <a href="https://www.linkedin.com/in/yanlin-chen-58a41411/">Yanlin Chen</a>, <a href="https://www.linkedin.com/in/rsusandang/">Susan Dang</a>, and <a href="https://www.linkedin.com/in/amitywang/">Amity Wang</a>, as well as many more behind the scenes, for their tireless work building and supporting GP.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=842244c5f5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/a-deep-dive-into-airbnbs-server-driven-ui-system-842244c5f5">A Deep Dive into Airbnb’s Server-Driven UI System</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building an Inclusive Codebase]]></title>
            <link>https://medium.com/airbnb-engineering/building-an-inclusive-codebase-bbaa2315e5b8?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/bbaa2315e5b8</guid>
            <category><![CDATA[inclusion]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[diversity]]></category>
            <dc:creator><![CDATA[Michael Bachand]]></dc:creator>
            <pubDate>Tue, 15 Jun 2021 17:00:26 GMT</pubDate>
            <atom:updated>2021-06-15T17:00:26.369Z</atom:updated>
            <content:encoded><![CDATA[<p>Our playbook for driving down non-inclusive terminology</p><p><strong>By:</strong> <a href="https://www.linkedin.com/in/mbachand">Michael Bachand</a>, <a href="https://www.linkedin.com/in/amanda-vawter-pmp-she-her-4149a752">Amanda Vawter</a>, <a href="https://www.linkedin.com/in/dsfed">Dan Federman</a>, <a href="https://www.linkedin.com/in/silverjake">Jake Silver</a>, <a href="https://www.linkedin.com/in/juliahw">Julia Wang</a>, Mark Tai</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*aLIlIJy0RsbE0KI8xdVGKQ.jpeg" /></figure><p>Code is our craft. At Airbnb, we view our code and product as a reflection of our values. Each developer imbues their work with a piece of themselves. We want all engineers to be proud of the codebase in which they work and the systems that they use every day.</p><p>We want to share with you some of the work that we are doing at Airbnb to build an inclusive engineering culture. We hope that sharing our story may energize and bolster similar efforts to eliminate non-inclusive terminology throughout the industry.</p><h3>Bootstrapping Change</h3><p>Airbnb’s mission is to create a world where anyone can belong anywhere. Continued violence and prejudice against under-represented groups threaten belonging and equality. Increased diversity in Airbnb Engineering will allow our team to better empower Airbnb hosts to serve Airbnb guests. It is imperative that Airbnb engineers from all backgrounds feel like they belong when contributing to Airbnb.</p><p>In the summer of 2020, we identified terms in Airbnb’s technical stack that undermine our core value of belonging. Team members proactively organized a volunteer effort in a Slack channel, working with employees from affected communities to operationalize change. Together we co-authored a proposal to address non-inclusive terminology in our codebase. Within a month, we presented the proposal to our CTO. He then prioritized this work by staffing dedicated engineer hours to make our code and systems more inclusive.</p><p>Acknowledgement and resourcing from the highest levels of management legitimized this effort. The legitimacy of the effort was further reinforced by smaller actions undertaken by individuals. Every task, pull request comment, and Slack reaction sent a signal that this work is valued by the business.</p><h3>Building New Habits</h3><p>We consider non-inclusive terminology to be a legacy style of programming that we want to eliminate. To that end, we began by measuring the usage of non-inclusive terminology in our code. We then broke the problem into two pieces: preventing new additions of these terms and driving down existing instances of these terms.</p><p>To address the first problem, we introduced a <a href="https://en.wikipedia.org/wiki/Lint_(software)">linter</a> to flag pull requests that introduce terms that Airbnb has deemed non-inclusive. The linter comments on each pull request that adds lines containing one or more of these terms. The pull request comment suggests alternative terms.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HCBl2ll132O-VpRThOkUCw.png" /><figcaption>An example GitHub comment posted by the inclusive linter.</figcaption></figure><p>In order to build trust with engineers, we focused on making the linter actionable. We want the author to be able to resolve the linter’s comment in their existing pull request, without needing to edit code in another repository. Accordingly, we tend to exclude third-party code and code that is pulled in from upstream repositories when linting pull requests.</p><p>We began by introducing the linter to individual repositories, one at a time. We asked developers who were well-known in that codebase to drive the initial rollouts. These local experts were able to make judgements about which directories should be excluded from the linter and also had the trust of other developers in that codebase.</p><p>We have found success in raising awareness and limiting the addition of new terms with a non-blocking linter. We do not prevent an engineer from merging their pull request. Our belief is that with proper communication engineers will become aware that this is a business priority.</p><p>We now run the linter on all internal repositories at Airbnb. We have shifted our attention to driving down existing violations.</p><h3>Strengthening Our Approach</h3><p>For these initial terms, members of Employee Resource Groups (ERGs), such as Black@ and Able@, were involved in sourcing offensive word lists and implementing these improvements. For context, ERGs provide dedicated space for employees to join together around shared characteristics, interests, and life experiences. Black@ represents African-American and Black employees, and Able@ supports those with disabilities, chronic conditions, or mental health conditions. At Airbnb, we have 19 Employee Resource Groups.</p><p>We also reviewed similar initiatives across the industry for inspiration, including <a href="https://developers.google.com/style/inclusive-documentation">Google’s guide</a> on writing inclusive documentation and <a href="https://twitter.com/TwitterEng/status/1278733303508418560">Twitter’s post</a> on inclusive language. We focused on an initial list of terms that was large enough to drive meaningful impact and small enough to avoid overwhelming engineers.</p><p>The motivation for this effort is for everyone to feel like they belong when working on Airbnb code and systems. To that end, we created an anonymous feedback form which gives any engineer a chance to surface their thoughts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7p3uV3mg6QJ0mTsnMbNpyg.png" /><figcaption>Engineers can submit anonymous feedback via a Google Form.</figcaption></figure><p>The anonymous feedback has led to concrete improvements, most notably in our list of terms. As an example, we originally suggested that engineers replace “grandfathered” with “legacy status.” Anonymous feedback informed us that “legacy status” was not an alternative that worked in many contexts, so we’ve changed the suggested alternative to “exempted.”</p><p>Anonymous feedback also highlighted that it was not clear why we linted for each term. Specifically, some employees wondered why “dummy” was on our list. For each non-inclusive term, we now document our reasoning and the history of the term. We link to these resources in each pull request comment posted by our linter. We feel responsible for making it easy for engineers to understand why we are asking them to change their code.</p><p>We view this project as a long-term initiative that will never be complete. As the world evolves, we will need to update our list of non-inclusive terminology, and update our code to reflect our current values. We are establishing a transparent process for expanding our list of non-inclusive terms. A diverse group of individuals that includes representation from the impacted community will consider each proposed term individually in close contact with all stakeholders. The process considers the impact of the term on individuals and communities, alignment with company values, and the scope of the work required to replace the term across our codebase.</p><h3>Looking Ahead</h3><p>We cannot run away from history, but we can inform the future. Driving down non-inclusive terminology is a piece of a larger effort that we will undertake at Airbnb and in the technology community at large to make engineers from all backgrounds feel heard, welcome, and valued.</p><p>Eliminating language that does not reflect our values shows engineers at Airbnb that we care for them, that they do belong, and that they are seen and respected.</p><h3>Sharing Our Work</h3><p>Thank you for your interest in promoting inclusion within technology. We are optimistic about the impact that this work can have within Airbnb and as part of a larger effort among our peer companies.</p><p>In order to support similar efforts across the industry we have made public the <a href="https://github.com/airbnb/inclusion#infractions">list of terms</a> that we consider non-inclusive. We are publishing these terms as a simple data file that can be integrated with tools and systems at other companies, or forked and evolved based on varying contexts. We power our internal linter with this same data file.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bbaa2315e5b8" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-an-inclusive-codebase-bbaa2315e5b8">Building an Inclusive Codebase</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Standardized Metric Computation at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/9afe6695b486</guid>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[metrics]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Amit Pahwa]]></dc:creator>
            <pubDate>Tue, 01 Jun 2021 17:50:22 GMT</pubDate>
            <atom:updated>2021-06-02T00:14:47.247Z</atom:updated>
            <content:encoded><![CDATA[<h4>Metric Infrastructure with Minerva @ Airbnb</h4><h4>Part II: The six design principles of Minerva compute infrastructure</h4><p><strong>By: </strong><a href="https://www.linkedin.com/in/apahwa/">Amit Pahwa</a>, <a href="https://www.linkedin.com/in/cristianrfr/">Cristian Figueroa</a>, <a href="https://www.linkedin.com/in/donghan-zhang-670990135/">Donghan Zhang</a>, <a href="https://www.linkedin.com/in/haimgrosman/">Haim Grosman</a>, <a href="https://www.linkedin.com/in/john-bodley-a13327133/">John Bodley</a>, <a href="https://www.linkedin.com/in/jonathan-parks-15617820/">Jonathan Parks</a>, <a href="https://www.linkedin.com/in/shengnan-zhu-89403124/">Maggie Zhu</a>, <a href="https://www.linkedin.com/in/philip-weiss-391021b1/">Philip Weiss</a>, <a href="https://www.linkedin.com/in/robert-ih-chang/">Robert Chang</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/sylviatomiyama/">Sylvia Tomiyama</a>, <a href="https://www.linkedin.com/in/xiaohui-sun-24bb3017/">Xiaohui Sun</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vIcOQZ_CrbdC2lxOM3-zhg.png" /></figure><h3>Introduction</h3><p>As described in the <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">first post</a> of this series, Airbnb invested significantly in building Minerva, a single source of truth metric platform that standardizes the way business metrics are created, computed, served, and consumed. We spent years iterating toward the right metric infrastructure and designing the right user experience. Because of this multi-year investment, when Airbnb’s business was severely disrupted by COVID-19 last year, we were able to quickly turn data into actionable insights and strategies.</p><p>In this second post, we will deep dive into our compute infrastructure. Specifically, we will showcase how we standardize dataset definitions through declarative configurations, explain how data versioning enables us to ensure cross-dataset consistency, and illustrate how we backfill data efficiently with zero downtime. By the end of this post, readers will have a clear understanding of how we manage datasets at scale and create a strong foundation for metric computation.</p><h3>Minerva’s Design Principles</h3><p>As shared in <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">Part I</a>, Minerva was born from years of growing pains and related metric inconsistencies found within and across teams. Drawing on our experience managing a metric repository specific to <a href="https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166">experimentation</a>, we aligned on six design principles for Minerva.</p><p>We built Minerva to be:</p><ul><li><strong>Standardized</strong>: Data is defined unambiguously in a single place. Anyone can look up definitions without confusion.</li><li><strong>Declarative: </strong>Users define the “what” and not the “how”. The processes by which the metrics are calculated, stored, or served are entirely abstracted away from end users.</li><li><strong>Scalable</strong>: Minerva must be both computationally <em>and</em> operationally scalable.</li><li><strong>Consistent</strong>: Data is always consistent. If definition or business logic is changed, backfills occur automatically and data remains up-to-date.</li><li><strong>Highly available</strong>: Existing datasets are replaced by new datasets with zero downtime and minimal interruption to data consumption.</li><li><strong>Well tested</strong>: Users can prototype and validate their changes extensively well before they are merged into production.</li></ul><p>In the following sections, we will expand on each of the principles described here and highlight some of the infrastructure components that enable these principles. Finally, we will walk through the user experience as we bring it all together.</p><h3>Minerva is Standardized</h3><p>You may recall from <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">Part I</a> that the immense popularity of the core_data schema at Airbnb was actually a double-edged sword. On the one hand, core_data standardized table consumption and allowed users to quickly identify which tables to build upon. On the other hand, it burdened the centralized data engineering with the impossible task of gatekeeping and onboarding an endless stream of new datasets into new and existing core tables. Furthermore, pipelines built downstream of core_data created a proliferation of duplicative and diverging metrics. We learned from this experience that table standardization was not enough and that standardization at the metrics level is key to enabling trustworthy consumption. After all, users do not consume tables; they consume metrics, dimensions, and reports.</p><p>Minerva is focused around metrics and dimensions as opposed to tables and columns. When a metric is defined in Minerva, authors are required to provide important self-describing metadata. Information such as ownership, lineage, and metric description are all required in the configuration files. Prior to Minerva, all such metadata often existed only as undocumented institutional knowledge or in chart definitions scattered across various business intelligence tools. In Minerva, all definitions are treated as version-controlled code. Modification of these configuration files must go through a rigorous review process, just like any other code review.</p><p>At the heart of Minerva’s configuration system are event sources and dimension sources, which correspond to fact tables and dimension tables in a Star Schema design, respectively.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NTZAWIq7NJ-0rfqhI98cNA.png" /><figcaption>Figure 1: Event sources and dimension sources are the fundamental building blocks of Minerva.</figcaption></figure><p>Event sources define the atomic events from which metrics are constructed, and dimension sources contain attributes and cuts that can be used in conjunction with the metrics. Together, event sources and dimension sources are used to define, track, and document metrics and dimensions at Airbnb.</p><h3>Minerva is Declarative</h3><p>Prior to Minerva, the road to creating an insightful piece of analysis or a high-fidelity and responsive dashboard was a long one. Managing datasets to keep up with product changes, meet query performance requirements, and avoid metric divergence quickly became a significant operational burden for teams. One of Minerva’s key value propositions is to dramatically simplify this tedious and time consuming workflow so that users can quickly turn data into actionable insights.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uFs2-eHH3QuHtHarmx0TVA.png" /><figcaption>Figure 2: Data Science workflow improvement.</figcaption></figure><p>With Minerva, users can simply define a dimension set, an analysis-friendly dataset that is joined from Minerva metrics and dimensions. Unlike datasets created in an ad-hoc manner, dimension sets have several desirable properties:</p><ul><li>Users only define the what and need not concern about the how. All the implementation details and complexity are abstracted from users.</li><li>Datasets created this way are guaranteed to follow our best data engineering practices, from data quality checks, to joins, to backfills, everything is done efficiently and cost effectively.</li><li>Data is stored efficiently and is optimized to reduce query times and responsiveness of downstream dashboards.</li><li>Because datasets are defined transparently in Minerva, we encourage metric reuse and reduce dataset duplication.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zaIgcn5JVlMcNrlpHwVchQ.png" /><figcaption>Figure 3: Programmatic denormalization generates dimension sets which users can easily configure.</figcaption></figure><p>By focusing on the “what” and not on the “how”, Minerva improves user productivity and maximizes time spent on their primary objectives: studying trends, uncovering insights, and performing experiment deep dives. This value proposition has been the driving force behind Minerva’s steady and continual adoption.</p><h3>Minerva is Scalable</h3><p>Minerva was designed with scalability in mind from the outset. With Minerva now serving 5,000+ datasets across hundreds of users and 80+ teams, the cost and maintenance overhead is a top priority.</p><p>At its core, Minerva’s computation is built with the DRY (<strong>D</strong>o not <strong>R</strong>epeat <strong>Y</strong>ourself) principle in mind. This means that we attempt to re-use materialized data as much as possible in order to reduce wasted compute and ensure consistency. The computational flow can be broken down into several distinct stages:</p><ul><li><strong>Ingestion Stage</strong>: Partition sensors wait for upstream data and data is ingested into Minerva.</li><li><strong>Data Check Stage</strong>: Data quality checks are run to ensure that upstream data is not malformed.</li><li><strong>Join Stage</strong>: Data is joined programmatically based on join keys to generate dimension sets.</li><li><strong>Post-processing and Serving Stage</strong>: Joined outputs are further aggregated and derived data is virtualized for downstream use cases.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xKF2ixGotVJ97I7ZWBy2Sw.png" /><figcaption>Figure 4: High-level Minerva computation flow.</figcaption></figure><p>In the ingestion stage, Minerva waits for upstream tables to land and ingests the data into the static Minerva tables. This ingested data becomes the source-of-truth and requires modification to the Minerva configs in order to be changed.</p><p>The data check stage ensures the source data is formatted correctly before further processing is performed and dimension sets are created. Here are some typical checks that Minerva runs:</p><ul><li>sources should not be empty</li><li>timestamps should not be NULL and should meet ISO standards</li><li>primary keys should be unique</li><li>dimension values are consistent with what is expected</li></ul><p>For the join stage, the same data referenced in disparate dimension sets are sourced, computed, and joined from the same upstream tables with the same transformation logic. Intermediate datasets are also added in cases where it adds overall efficiency to the system. This centralized computation, as illustrated in the diagram above, is the key to how we ensure consistent and efficient dataset computation at scale.</p><p>Finally, in the post-processing and serving stage, data is optionally further optimized for end-user query performance and derived data is virtualized. We will dive into more details on this stage in the third post of this series.</p><p>In addition to being computationally scalable, we need our platform to be operationally efficient. We have included a few critical features that allow us to do this. Specifically, we will highlight smart self-healing, automated backfilling with batched backfills, and intelligent alerting.</p><p>Self-healing allows Minerva to gracefully and automatically recover from various transient issues such as:</p><ul><li>A bug in the pipeline or platform code</li><li>Infrastructure instability such as cluster or scheduler outages</li><li>Timeouts due to upstream data that has missed its SLA</li></ul><p>In order to achieve self-healing, Minerva must be data-aware. Each time a job starts, Minerva checks the existing data for any missing data. If missing data is identified, it automatically includes it in the current run. This means a single run can decide computation windows dynamically and backfill data. Users are not required to manually reset tasks when they fail due to transient issues.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9T9y0dDHl7-v389tpsyAYQ.png" /><figcaption>Figure 5: Missing data from failed runs are identified and computed as part of future runs.</figcaption></figure><p>This self-healing logic also leads to automated backfills. If Minerva identifies that no data exists for the relevant data version, it automatically begins generating the data from its upstream datasets. If the backfill window is very long (e.g., several years), it may generate a long-running query. While our underlying computation engine should be scalable enough to handle heavy queries, having one query running for a long time is still risky: long-running queries are sensitive to transient infrastructure failures, costly to recover if they fail, and create spikes in resource usage. At the other end of the spectrum, using a small backfilling window such as one day is too slow to work well over a longer time period. In order to improve scalability, reduce runtimes, and improve recoverability, we implemented batched backfills.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qv85uY1yGRCatPqe779tqQ.png" /><figcaption>Figure 6: A single job is broken up into several parallel monthly batches within the 2021–05–01 task.</figcaption></figure><p>With batched backfills, Minerva splits the job into several date ranges based on the scalability of that specific dataset. For example, Minerva can split a backfill of two years of data into 24 one-month batches, which run in parallel. Failed batches are retried automatically in the next run.</p><p>This automated dataset management also leads to split responsibilities across teams. Infrastructure issues are owned by the platform team, whereas data issues are owned by the respective product or data science teams. Different datasets require different levels of escalation. Minerva intelligently alerts the appropriate team based on the type of error and notifies downstream consumers of data delays. This effectively distributes operational load across the company while assigning responsibility to the team best suited to resolve the root cause. Through this alerting system design we avoid parallel triaging by multiple teams.</p><p>Self-healing, automated batched backfills, and intelligent alerting are three features that together enable Minerva to be a low-maintenance, operationally efficient, and resilient system.</p><h3>Minerva is Consistent</h3><p>Minerva’s metric repository is altered frequently by many users and evolves very rapidly. If we do not carefully coordinate these changes, it is very likely that metrics and dimensions will diverge. How can we ensure that datasets produced by Minerva are always consistent and up-to-date?</p><p>Our solution lies in what we call a data version, which is simply a hash of all the important fields that are specified in a configuration file. When we change any field that impacts what data is generated, the data version gets updated automatically. Each dataset has a unique data version, so when the version is updated a new dataset gets created and backfilled automatically. The example below illustrates how this mechanism works.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wn5ty30l5jvBV3nC9XbS9Q.png" /><figcaption>Figure 7: An update on a single dimension can trigger backfills across all datasets that use this dimension.</figcaption></figure><p>In the scenario seen in Figure 7, a certain dimension in dimension source <strong>1<em> </em></strong>is updated. Given that this dimension is being used by two dimension sets (i.e., <strong>A123</strong> and <strong>B123</strong>), data versions associated with these two dimension sets will get updated accordingly. Since the data versions are now updated, backfills for these two dimension sets will kick in automatically. In Minerva, the cycle of new changes resulting in new data versions, which in turn trigger new backfills, is what allows us to maintain data consistency across datasets. This mechanism ensures that upstream changes are propagated to all downstream datasets in a controlled manner and that no Minerva dataset will ever diverge from the single source of truth.</p><h3>Minerva is Highly Available</h3><p>Now that we have explained how Minerva uses data versioning to maintain data consistency, a keen user might already observe a dilemma: the rate of backfills competes with the rate of user changes. In practice, backfills often could not catch up with user changes, especially when updates affect many datasets. Given that Minerva only surfaces data that is consistent and up-to-date, a rapidly changing dataset could end up in backfill mode forever and cause significant data downtime.</p><p>To address this challenge, we created a parallel computation environment called the Staging environment. The Staging environment is a replica of the Production environment built from pending user configuration modifications. By performing the backfills automatically within a shared environment prior to replacing their Production counterparts, Minerva applies multiple unreleased changes to a single set of backfills. This has at least two advantages: 1) Users no longer need to coordinate changes and backfills across teams, and 2) Data consumers no longer experience data downtime.</p><p>The data flow for the Staging environment is as follows:</p><ol><li>Users create and test new changes in their local environment.</li><li>Users merge changes to the Staging environment.</li><li>The Staging environment loads the Staging configurations, supplements them with any necessary Production configurations, and begins to backfill any modified datasets.</li><li>After the backfills are complete, the Staging configurations are merged into Production.</li><li>The Production environment immediately picks up the new definitions and utilizes them for serving data to consumers.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*newXLCXFdPKl36P-EDZGXQ.png" /><figcaption><em>Figure 8: A configuration change is first loaded into Staging and then merged to Production when release-ready.</em></figcaption></figure><p>The Staging environment allows us to have both consistency <em>and</em> availability for critical business metrics, even when users update definitions frequently. This has been critical for the success of many mass data migrations projects within the company, and it has aided efforts to <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">revamp our data warehouse as we focused on data quality</a>.</p><h3>Minerva is Well Tested</h3><p>Defining metrics and dimensions is a highly iterative process. Users often uncover raw data irregularities or need to dig deeper to understand how their source data was generated. As the source of truth for metrics and dimensions built on top of automatically generated datasets, Minerva must help users validate data correctness, clearly explain what is happening, and speed up the iteration cycle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XbSDR7BngXfIkYqoBDM3kQ.png" /><figcaption><em>Figure 9: A user’s development flow using the Minerva prototyping tool.</em></figcaption></figure><p>To do this, we created a guided prototyping tool that reads from Production but writes to an isolated sandbox. Similar to the Staging Environment, this tool leverages the Minerva pipeline execution logic to generate sample data quickly on top of the user’s local modifications. This allows users to leverage new and existing data quality checks while also providing sample data to validate the outputs against their assumptions and/or existing data.</p><p>The tool clearly shows the step-by-step computation the Minerva pipeline will follow to generate the output. This peek behind-the-curtain provides visibility into Minerva computation logic, helps users debug issues independently, and also serves as an excellent testing environment for the Minerva platform development team.</p><p>Finally, the tool uses user-configured date ranges and sampling to limit the size of the data being tested. This dramatically speeds up execution time, reducing iteration from days to minutes, while allowing the datasets to retain many of the statistical properties needed for validation.</p><h3>Putting It Together: A COVID-19 Case Study</h3><p>To illustrate how everything fits together, let’s walk through an example of how Alice, an analyst, was able to turn data into shared company insights with Minerva. As described in our <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">first post</a>, COVID-19 has completely changed the way people travel on Airbnb. Historically, Airbnb has been pretty evenly split between demand for urban and non-urban destinations. At the onset of the pandemic, Alice hypothesized that travelers would avoid large cities in favor of destinations where they could keep social distance from other travelers.</p><p>To confirm this hypothesis, Alice decided to analyze the nights_booked metric, cut by the dim_listing_urban_category dimension. She knew that the nights_booked metric is already defined in Minerva because it is a top-line metric at the company. The listing dimension she cared about, however, was not readily available in Minerva. Alice worked with her team to leverage the <a href="https://sedac.ciesin.columbia.edu/data/collection/grump-v1">Global Rural-Urban Mapping Project </a>and <a href="https://sedac.ciesin.columbia.edu/data/collection/gpw-v4">GPW v4 World Population Density Data</a>¹ created by NASA to tag all listings with this new metadata. She then began to prototype a new Minerva dimension using this new dataset.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fMbzWJZ-4bJwhi8JHz0zhQ.png" /><figcaption>Figure 10: Alice configures the new dimension in a dimension source.</figcaption></figure><p>Alice also included the new dimension definition in several dimension sets used across the company for tracking the impact of COVID-19 on business operations.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cRv-V-KFHQscaP-lKTNDag.png" /><figcaption>Figure 11: Alice adds the new dimension to the COVID SLA dimension set owned by the Central Insights team.</figcaption></figure><p>To validate this new dimension in Minerva, Alice used the prototyping tool described above to compute a sample of data with this new dimension. Within minutes, she was able to confirm that her configuration was valid and that the data was being combined accurately.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*i9mvdpsjCb8_O_cT" /><figcaption>Figure 12: Alice was able to share sample data with her teammate within a few minutes.</figcaption></figure><p>After validating the data, Alice submitted a pull request for code review from the Core Host team, which owns the definition of all Listing metadata. This pull request included execution logs, computation cost estimates, as well as links to sample data for easy review. After receiving approvals, Alice merged the change into the shared Staging environment where, within a few hours, the entire history of the modified datasets were automatically backfilled and eventually merged into Production.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jQXWg92I7MlKVexe" /><figcaption>Figure 13: With Alice’s change, anyone in the company could clearly see the shift in guest demands as travel rebounds.</figcaption></figure><p>Using the newly created datasets, teams and leaders across the company began to highlight and track these shifts in user behavior in their dashboards. This change to our key performance indicators also led to new plans to revamp key product pages to suit users’ new travel patterns.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nXxrizQIhtHaBT7_SyOx9g.png" /><figcaption>Figure 14: Adoption of the new dimension source (red) across event sources (y-axis).</figcaption></figure><p>Through this process, Alice was able to define a new dimension, attach it to pre-existing metrics, obtain approvals from domain owners, and update numerous critical datasets across several teams within days. All of this was done with just a few dozen lines of YAML configuration.</p><h3>Closing</h3><p>In this post, we explored Minerva’s compute infrastructure and its design principles. We highlighted how users are able to define standardized data in Minerva with detailed metadata. By making the user interface declarative, users need only focus on the “what” and not the “how”. Because Minerva is well-tested and consistent, we were able to foster trust with our users. Finally, Minerva’s scalable design enabled us to expand our footprint to drive adoption and data standardization within the company.</p><p>As Minerva has become ubiquitous within the company, users have found it much easier to do their data work. We are dedicated to making Minerva even better in the future. Some items from our near-term roadmap include leveraging <a href="https://iceberg.apache.org/">Apache Iceberg</a> as our next-generation table format for storage, expanding to real-time data, and supporting more complex definitions, to name just a few!</p><p>In our next post in this series, we will switch gears to discuss how Minerva leveraged the above consistency and availability guarantees to abstract users away from the underlying datasets. We will outline the challenges, highlight our investment in the Minerva API, and explain how we integrated Minerva with the rest of the Airbnb data stack to provide a unified metric centric consumption experience in our metric layer. Stay tuned for our next post!</p><h3>Acknowledgements</h3><p>Minerva is made possible only because of the care and dedication from those who worked on it. We would like to thank <a href="https://www.linkedin.com/in/clintonkelly/">Clint Kelly</a> and <a href="https://www.linkedin.com/in/lchircus/">Lauren Chircus</a> for helping us build and maintain Minerva. We would also like to thank <a href="https://www.linkedin.com/in/aaronkeys/">Aaron Keys</a>, <a href="https://www.linkedin.com/in/michaelcl/">Mike Lin</a>, <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/michelleethomas/">Michelle Thomas</a>, <a href="https://www.linkedin.com/in/erikrit/">Erik Ritter</a>, <a href="https://www.linkedin.com/in/serena-jiang/">Serena Jiang</a>, <a href="https://www.linkedin.com/in/krist-wongsuphasawat-279b1617/">Krist Wongsuphasawat</a>, <a href="https://www.linkedin.com/in/chris-williams-1bb8b936/">Chris Williams</a>, <a href="https://www.linkedin.com/in/kenchendesign/">Ken Chen</a>, <a href="https://www.linkedin.com/in/yguang11/">Guang Yang</a>, <a href="https://www.linkedin.com/in/jinyang-li-607690143/">Jinyang Li</a>, <a href="https://www.linkedin.com/in/clark-wright-67955537/">Clark Wright</a>, <a href="https://www.linkedin.com/in/vquoss/">Vaughn Quoss</a>, <a href="https://www.linkedin.com/in/zi-jerry-chu/">Jerry Chu</a>, <a href="https://www.linkedin.com/in/palanieppan-muthiah-b7088924/">Pala Muthiah</a>, <a href="https://www.linkedin.com/in/ruiqinyang/">Kevin Yang</a>, <a href="https://www.linkedin.com/in/ellen-huynh/">Ellen Huynh</a>, and many more who partnered with us to make Minerva more accessible across the company. Finally, thank you <a href="https://www.linkedin.com/in/bulam/">Bill Ulammandakh</a> for providing a great case study to walk through!</p><p><em>¹ All data associated with the Global Rural-Urban Mapping Project and GPW v4 World Population Density Data datasets are the property of NASA’s Earth Science Data Systems program; Airbnb claims no ownership of that data and complies fully with all legal use restrictions associated with it.</em></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9afe6695b486" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">How Airbnb Standardized Metric Computation at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building a faster web experience with the postTask scheduler]]></title>
            <link>https://medium.com/airbnb-engineering/building-a-faster-web-experience-with-the-posttask-scheduler-276b83454e91?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/276b83454e91</guid>
            <category><![CDATA[web-performance]]></category>
            <category><![CDATA[frontend]]></category>
            <category><![CDATA[web]]></category>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[core-web-vitals]]></category>
            <dc:creator><![CDATA[Callie]]></dc:creator>
            <pubDate>Thu, 20 May 2021 17:13:59 GMT</pubDate>
            <atom:updated>2021-05-20T17:30:39.146Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>Building a Faster Web Experience with the postTask Scheduler</strong></h3><figure><img alt="An illustration of art supplies and sketches of a project being planned into tasks." src="https://cdn-images-1.medium.com/max/1024/1*jE9sERwYdWpv_nUoRDRXlg.jpeg" /></figure><p>When was the last time you opened a page on a website and tried to click on something multiple times for it to work? Or the last time you swiped on an image in a carousel and it stuttered and shifted around unnaturally?</p><p>While this type of experience happens far too often, we can use tools that can help us make better, more responsive experiences for users. Scheduling and prioritizing tasks efficiently can be the difference between a responsive experience and one that feels sluggish.</p><p>At Airbnb, we’ve been collaborating with the Chrome team on improving performance using a prioritized task scheduler to implement new patterns and improve the performance of existing ones.</p><h3>Meet the prioritized postTask scheduler</h3><p>The <a href="https://github.com/WICG/scheduling-apis/blob/main/explainers/prioritized-post-task.md">prioritized postTask API</a> is designed to give us more flexibility and power around scheduling tasks efficiently. Similar to <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/requestIdleCallback">requestIdleCallback</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/setTimeout">setTimeout</a>, using it effectively can help reduce <a href="https://web.dev/tbt/">Total Blocking Time</a>, <a href="https://web.dev/fcp/">First Contentful Paint</a>, <a href="https://web.dev/fid/">Input Delays</a> and <a href="https://web.dev/custom-metrics/#long-tasks-api">other key metrics</a>.</p><p>While many performance efforts focus on the initial page load, we wanted to improve the user experience <em>after</em> the page has loaded.<strong> </strong>We used the postTask scheduler in a number of way — from how we preload images in carousels to making our map more responsive.</p><p>To get a sense of the progress we’ve made since this effort began, <a href="https://www.youtube.com/watch?v=e215_uiU3LQ">we created new real user monitoring performance metrics</a> and leveraged existing lab-based metrics from tools such as <a href="https://www.webpagetest.org/easy">WebPageTest</a> and <a href="https://developers.google.com/web/tools/lighthouse">Lighthouse</a>.</p><p><strong>How it started</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GM30aPSBUMR78m0J" /><figcaption>Total Blocking Time was nearly 16 seconds loading our search results page</figcaption></figure><p><strong>How it’s going</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*sPuqPzLVckzRSj4z" /><figcaption>Total Blocking Time improved by nearly 10 seconds for a typical mobile web user on average hardware</figcaption></figure><h3>What is the postTask Scheduler?</h3><p>Much like requestAnimationFrame, setTimeout or requestIdleCallback, scheduler.postTask allows us to schedule a function on the browser’s event loop. That function then gets prioritized and run by the browser.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/508/1*qJMiqKGd5cas7iqRRz0JJQ.png" /></figure><p><em>“The first two — microtask and don’t yield — are generally antithetical to scheduling and the goal of improving responsiveness. They are implicit priorities that developers can and do use.”</em></p><h3>Breaking up long tasks</h3><p>We can and should break our long tasks up in order to improve responsiveness. Here’s an example of a long task that sets up basic error tracking and event logging. Notice how the browser has flagged the task as a <a href="https://w3c.github.io/longtasks/">Long task</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gCfCj2i4HRyprPFWHLYR4Q.png" /><figcaption>249ms long task early during the page lifecycle</figcaption></figure><p>Once we identify a long task, we can use postTask to break up the tasks into smaller ones.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a1409667223af9c92543865e00862b72/href">https://medium.com/media/a1409667223af9c92543865e00862b72/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iHMjICJM8p_4MJKXIoknKw.png" /><figcaption>After using scheduler.postTask, we no longer have any long tasks, only smaller ones below the Long Task threshold</figcaption></figure><h3>Hello, postTask</h3><p>Currently postTask is <a href="https://www.chromestatus.com/features/6031161734201344">implemented</a> in Chromium behind the #enable-experimental-web-platform-features flag under chrome://flags, and is planned to be fully supported in Chrome in an upcoming release. While a polyfill exists, at the time of this writing the polyfill hasn’t been open sourced yet. The official polyfill status can be <a href="https://github.com/WICG/scheduling-apis/issues/37">tracked on the WICG repo</a>, but until then we can try it out on Chrome Canary.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/be3aa9ac1095092b0d3ba7fff92f10da/href">https://medium.com/media/be3aa9ac1095092b0d3ba7fff92f10da/href</a></iframe><p>In the above example, we pass a new <strong>delay</strong> and <strong>priority</strong> argument to postTask, telling it we want to run our task in the background after waiting for 1 second. The postTask scheduler currently supports 3 different priorities.</p><figure><img alt="user-blocking is the highest priority and is meant to be used for tasks that are blocking the user’s ability to interact with the page, such as rendering the core experience or responding to user input. user-visible is the second highest priority and is meant to be used for tasks that are visible to the user but not necessarily blocking user actions, such as rendering secondary parts of the page. This is the default priority. background is the lowest priority." src="https://cdn-images-1.medium.com/max/1024/1*HqiJr6uePtE406jIAhXjKw.png" /></figure><p>One of the benefits of the postTask scheduler is that it’s built on top of <a href="https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal">Abort Signals</a>, allowing us to cancel a task that is queued but hasn’t yet executed. The API also defines a new <a href="https://wicg.github.io/scheduling-apis/#sec-task-controller">TaskController</a>, which allows for the priority to be used by the signal to control tasks and priorities.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2574fb9c4128fa531f738da379bd0c6d/href">https://medium.com/media/2574fb9c4128fa531f738da379bd0c6d/href</a></iframe><h3>Deferring non-critical tasks</h3><p>Most sites load a large amount of third-party libraries, such as Google Analytics, Tag Manager, logging libraries and so much more. It’s important to measure and understand the impact these have on a user’s initial loading experience.</p><p>Airbnb, for example, prioritizes getting to the point where a user can enter a search term, which requires us to load some JavaScript and “hydrate” our page with React. It might make sense to defer certain tasks until that key moment has completed.</p><h3>Introducing the experimental “scheduler.wait”</h3><p><a href="https://github.com/WICG/scheduling-apis#apis-and-status">scheduler.wait</a> is a proposed extension that allows for waiting for some milestone in the page, a custom DOM event in our case. Let’s see how to use it to load Google Tag Manager after we finish loading our page.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b52bf1d5616c3f98d766eb853892b000/href">https://medium.com/media/b52bf1d5616c3f98d766eb853892b000/href</a></iframe><p>The beauty of this is its simplicity — we get a promise back that we can block on until our custom event is triggered.</p><p>We can also specify this on any task as an option passed to postTask, allowing us to further simplify our Google Tag Manager registration.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/48df309a340971005e194853c3c38d5e/href">https://medium.com/media/48df309a340971005e194853c3c38d5e/href</a></iframe><p>It’s relatively painless to polyfill the wait function as it is defined today, since it takes the same options that postTask does but doesn’t require a task to be specified.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e91d794100d27ce279482d7608cb192f/href">https://medium.com/media/e91d794100d27ce279482d7608cb192f/href</a></iframe><p>We also needed to create a wrapper around the postTask method used in the shim above to support the event option when calling scheduler.postTask.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cede0987930719060894dcf540cc0102/href">https://medium.com/media/cede0987930719060894dcf540cc0102/href</a></iframe><p>The code for the waitForEvent call above is a wrapper mapping a DOM event to a promise that resolves when it fires, allowing us to wait for any event within our postTask wrapper. There’s no standards-defined way on how this might go yet. Our implementation is very naïve — for example it can’t reset the state of an event after it’s been fired if we were in a single page application and the page changed. In such a case we would likely want to fire a new loading event. There are a range of options being explored in the <a href="https://github.com/WICG/scheduling-apis#apis-and-status">proposed API addition</a>, so expect this to continue to evolve.</p><h3>Use case: Prefetching important resources</h3><p>Preloading the next image in an image carousel or the details of a page before the user loads it can dramatically increase the performance of a site and its perceived performance to users. We recently used the postTask scheduler to implement a delayed, staggered, and cancelable image preloader for our main search results image carousels. Let’s see how to build a simple version of it using postTask.</p><p><strong>Requirements for our image preloader on our listing carousels:</strong></p><ul><li>Wait until the listing is about 50% visible on the screen</li><li>Delay for one second; if the user is still viewing it, load the next image in the carousel</li><li>If the user swipes on an image, preload the next three images, <em>each staggered 100ms after the previous one begins</em></li><li>If the carousel leaves the viewport at any point prior to the one second timer finishing, we should cancel all of the preloading tasks that have not completed yet. If the user navigates to another page, also cancel all of the preloading tasks</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QLlVrXv404cqYzxOvDwP9Q.gif" /><figcaption>When the next slide scrolls into view, the 2nd image loads. Once we swipe, the next 3 load, each beginning 100ms after the previous one</figcaption></figure><p>Let’s start by looking at the first part of this, which is preloading the next image in the carousel if the user scrolls the card at least 50% into view for one second. While we use React for the next few examples, it is not required. All of the concepts here can also be achieved with other frameworks, or importantly, no frameworks at all.</p><p>Let’s assume we have a method called preloadImages that starts fetching the next images, and toggles a boolean field when it has completed preloading the images.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a41879ca5b1c946bd809a80ab466e6e5/href">https://medium.com/media/a41879ca5b1c946bd809a80ab466e6e5/href</a></iframe><p>We can combine that with an intersection observer and the postTask scheduler and accomplish loading the second image after being 50% in view for one second.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ca61e2aed561ae541643714af232d7c/href">https://medium.com/media/8ca61e2aed561ae541643714af232d7c/href</a></iframe><p>There’s a whole bunch of logic going on behind the scenes, so let’s break it down into smaller steps to understand what’s happening.</p><p>Let’s start by looking at how to determine if a user has come into view at least 50%. For this task we can use intersection observers. We use a small helper to set them up, but you can also use them with no library.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b8ebb9f984a5aa2b9e0a44f83d82c19e/href">https://medium.com/media/b8ebb9f984a5aa2b9e0a44f83d82c19e/href</a></iframe><p>Now that we know when we’ve scrolled into view 50%, we can wait for them to stay in view for one second with a useEffect hook combined with scheduler.postTask.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3361d52458c94d9f406fb61de3fe4536/href">https://medium.com/media/3361d52458c94d9f406fb61de3fe4536/href</a></iframe><p>Since we also passed an associated TaskSignal, the call to abort() will cancel any pending calls to preloadImages when a user scrolls out of view. We also take care to remove the reference to the controller to allow restarting the flow if they scroll back into view.</p><p><strong>Staggering network resources<br></strong>The last requirement we need to implement is staggering the next few image requests, each by 100ms after a user swipes on a carousel. Let’s see how we can modify our existing code to account for this scenario using the postTask scheduler. First let’s add a hook to call our preload logic with three images to preload when a user interacts with it. We’ll skip the first image since we’ve already loaded it.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6a8e3109698a0383dad395e1b618544b/href">https://medium.com/media/6a8e3109698a0383dad395e1b618544b/href</a></iframe><p>One of the goals of the postTask scheduler is to provide a low-level API to build on top of. We’ve been building up an integration that lets us perform a number of different patterns or strategies when used within React that we think are very useful.</p><h3>Using postTask effectively in React</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WG39QbJaL86XeWlY" /><figcaption>Opt-in debugging, showing total time taken and other useful information</figcaption></figure><p>While having a custom integration with React, Vue, Angular, Lit, etc. isn’t necessary, we can gain some major benefits by doing so. In React, for example, when a component unmounts, we typically want to cancel any tasks that are still queued up.</p><p>We can do that by passing a function as the return value to useEffect. Remembering to do that every time however is a challenge, and not doing it can lead to memory leaks. It’s also a challenge to remember to catch any AbortError the scheduler throws when we call abort(), as those are very much expected, but not something we can make a blanket exception for.</p><p>Let’s lay out some nice-to-haves for a usePostTaskScheduler hook that will make it easier for folks to consume.</p><ul><li>Pass an enabled flag, allowing the ability to bypass the scheduler to make A/B testing easier</li><li>Allow for easy cancellation, including auto canceling on unmount</li><li>Propagate the signal automatically to scheduler.postTask and scheduler.wait</li><li>Catch and suppress AbortErrors or anything that looks like them</li><li>Support robust debugging capabilities</li><li>Allow for specifying a strategy for common patterns such as the 2 we covered in this post</li><li>Add a hook for waiting for a delay to complete</li></ul><p>While going into the implementation of this hook is beyond the scope of this article, let’s see how this simplifies using the postTask scheduler within React. Let’s delay loading a high-cost, low-importance React component until after the load event fires, as well as cleanup some old localStorage states after load as well.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/36a617964c21a4e6d0a5c900e9ddbe41/href">https://medium.com/media/36a617964c21a4e6d0a5c900e9ddbe41/href</a></iframe><p>Here we can see that we have a boolean flag that will indicate when loading completes, as well as a traditional callback that lets us cleanup our localStorage keys. In this instance, if this component unloads prior to that event, we will cancel the task to cleanupLocalStorageKeys and never render the &lt;ExpensiveComponent /&gt;. In our case, the ExpensiveComponent loads asynchronously, so by deferring it, we reduce the cost of initial hydration significantly in both blocking time and bundle size costs.</p><p>Let’s see how we can defer loading our service worker until five seconds after the load event in the background.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c9c37f06d031e2ee31ffea2c08cfd643/href">https://medium.com/media/c9c37f06d031e2ee31ffea2c08cfd643/href</a></iframe><h3>The path ahead</h3><p><strong>Browser Compatibility<br></strong>Chromium is the first to implement and prototype this new API; however, the API is being developed openly in the <a href="https://github.com/WICG">WICG</a> with the goal of being standardized and adopted by all browsers. It’s also worth noting that even without native support, we’ve seen a number of performance improvements in browsers like Safari and Chrome by using the polyfill as it enforces good prioritization and scheduling patterns.</p><p>We’ve been delighted to get the chance to test the <a href="https://github.com/WICG/scheduling-apis/blob/main/explainers/prioritized-post-task.md">postTask scheduler</a>, and hopefully in this blog post you have a sense of the developer experience and performance improvements it enables. We’re looking forward to seeing it progress through the standards body and begin to land across web browser vendors.</p><h3>Acknowledgements ❤️</h3><p>This effort was made possible with the support of so many folks. We are grateful to Scott Haseley (Google), Shubhie Panicker (Google), Aditya Punjani, Josh Nelson, Elliott Sprehn, Casey Klimkowsky, Etienne Tripier, Victor Lin, and Kevin Weber.</p><h3>Improving the guest experience</h3><p>Over the last year, in addition to performance, we’ve worked on reengineering large parts of our tech stack to support new product use cases for our guests. Go behind the scenes with our Guest Experience tech team by registering for our upcoming Tech Talk, <a href="https://reengineeringtravel.splashthat.com/">REENGINEERING TRAVEL</a> on June 8th at 12pm PST. In this talk we’ll share more about how we’re building our platform and processes to fit the travel of today.</p><p><em>JavaScript is a registered trademark of Oracle America, Inc. React is a registered trademark of Facebook, Inc. Chrome, Google Analytics, Angular, and Chromium are registered trademarks of either Google LLC or Google, Inc. Airbnb claims no rights or ownership in any of those products beyond what has been legally agreed upon and documented in the various agreements between Airbnb and those companies.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=276b83454e91" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-a-faster-web-experience-with-the-posttask-scheduler-276b83454e91">Building a faster web experience with the postTask scheduler</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Airbnb’s Promotions and Communications Platform]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-promotions-and-communications-platform-6266f1ffe2bd?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/6266f1ffe2bd</guid>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[growth]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Paritosh Aggarwal]]></dc:creator>
            <pubDate>Tue, 11 May 2021 17:19:03 GMT</pubDate>
            <atom:updated>2021-05-11T17:19:03.589Z</atom:updated>
            <content:encoded><![CDATA[<p>OMNI is an intuitive, homegrown platform that supports message creation, processing, and distribution to engage our guests and hosts at the right time and on the right channels.</p><p>By <a href="https://medium.com/@paritosh.aggarwal">Paritosh Aggarwal</a> and <a href="https://medium.com/@henry_wu">Henry Wu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9-883pQVlSuNxSmX" /></figure><h3>Introduction</h3><p>Airbnb is a busy marketplace with millions of active users across the globe. Billions of messages are exchanged each year — both between Airbnb and our users, and between our host and guest communities. To facilitate the scaling of these messages, we invested in building a reliable, scalable, and cost-effective communication platform.</p><p>The Promotions and Communications Platform (internally called “OMNI”) forms a core pillar of Marketing Technology at Airbnb. Marketing Technology aims to provide a state of the art platform and measurement tools to enable marketing and product teams to engage with our customers effectively. This platform is the bedrock for all promotional, marketing, and transactional communications at Airbnb. We serve nearly <strong>50 billion</strong> communications every year to a growing community of guests and hosts, while bringing in millions of bookings through promotional campaigns.</p><p>In addition to supplying the systems and APIs that make these communications possible, we provide a self-service tool for creation and management of promotional campaigns. This tool enables product and marketing teams to deliver content to the Airbnb community and measure its efficacy. Our systems serve multiple product and business teams and use cases.</p><p>Over the course of the past few years, we have developed and battle-tested this technology platform to effectively meet our promotion and communication needs. In this blog post, we will introduce the business problems, discuss our overall platform architecture, and share some key learnings through the development process.</p><h3>Overview</h3><p>OMNI is the platform that we use to create, manage, and distribute content and messages to our community through multiple channels. The name OMNI refers to the ability of the platform to deliver omni-channel content. A few examples of content and messages supplied by OMNI are listed in Figure 1.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*56z1qEH2lA6CU9Nv" /><figcaption><em>Figure 1a. Onsite channel: Homepage outdoors wishlists powered via OMNI</em></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/445/0*gsbavK7k0FFMctYc" /><figcaption><em>Figure 1b. Offsite channel: Email configured and delivered via OMNI</em></figcaption></figure><h3>High-level Architecture</h3><p>OMNI delivers two types of content:</p><ol><li><strong>Transactional Content</strong>: Transactional content is usually automated, real-time messages that are triggered based on a customer’s action with the company’s website or application. Examples of transactional messages include order confirmation notifications, order status emails, password reset emails, and email receipts.</li><li><strong>Promotional Content</strong>: Promotional content involves sending a message (e.g., an email or push notification) solely to communicate a special offer or a product catalog item. It can also include messages mandated by legal or regulatory policies, such as Terms of Service updates. A campaign or promotion usually requires a set of declarative configurations, which specify the content and delivery time period.</li></ol><p>With this understanding, let’s dive a little deeper into the various components of our platform. Figure 2 is a modular diagram of platform features and dependencies. OMNI is built on top of the shared infrastructure at Airbnb, which leverages AWS as the primary cloud provider. It consists of two major subsystems: the promotion creation and management tool, called OMNI UI, and a set of backend services.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eTTJtCrc1-Ioe1su" /><figcaption><em>Figure 2. Overview of OMNI</em></figcaption></figure><p>OMNI UI is a web application built to provide full life-cycle support for content creation and distribution. It consists of multiple components that handle campaign management, content building, approval processes, translation, and analytics. Campaign data is created, updated, and deleted in a backend database through the Campaign Service, which is also responsible for managing versions, providing pre-made templates, and enforcing access control. Content created through the Campaign Service can also be enqueued for the Translation Service. After a campaign has been launched, the tool allows for visualizing and understanding campaign performance via a suite of analytics dashboards. This makes it easy for non-technical teams to gain actionable insights all within the same tool.</p><p>To best understand the services, let’s take a look at how someone would create a new promotion or communication. In order to create content, a campaign creator needs to define the “what”, “who”, “when”, and “where”. This is illustrated by an example email sent to an Airbnb guest, seen in Figure 3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/919/0*W1ijzCoZOkvrmKdL" /><figcaption>Figure 3. OMNI services used when sending an email</figcaption></figure><p>Each function in the figure above is powered by one of the following backend services:</p><ul><li><strong>Audience Service</strong>: This service defines the “who”, supporting functionality such as getting all eligible users in a specific channel or getting users based on a particular rule.</li><li><strong>Workflow</strong> <strong>Service</strong>: This service orchestrates the “when” by listening to delivery request events, evaluating delivery conditions, and eventually passing appropriate events to the Delivery Service.</li><li><strong>Optimization</strong> <strong>Service</strong>: This service performs message personalization via content ranking, user propensity-based personalization, and send time optimizations.</li><li><strong>Presentation</strong> <strong>Service</strong>: This service handles the “what” and “where” by enabling request validation, translation, and content generation.</li><li><strong>Rendering Service</strong>: This service provides rendering for content specific to different channels (e.g., emails or push notifications).</li><li><strong>Delivery</strong> <strong>Service</strong>: This service delivers the finalized message to Airbnb users through various vendor services (e.g., SendGrid, Twilio, FCM, etc.)</li></ul><p>One of our key design principles is to build with omni-channel content delivery in mind. Table 1 illustrates how we serve some of the common use cases across the Airbnb product ecosystem. In the subsequent sections, we will dive deeper into a few of the services.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*W5jdB_atd9KN-3Ha" /><figcaption>Table. 1. Example usages of OMNI in Airbnb products</figcaption></figure><h4><strong>Audience Service</strong></h4><p>Audience Builder, seen in Figure 4, is a rich user interface that sits within the OMNI UI and is powered by the Audience Service. Our internal customers can create and target different user audiences by filtering on user attributes. Currently, we support 100+ such user attributes (both static and machine learning-derived), enabling our marketing teams to create audience segments. It is also possible to upload a predefined set of users (either from Airbnb’s data warehouse or CSV files) for customized audiences. The size of user audience can often reach up to tens of millions for certain policy or promotional messages.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iyhIGWx9fYfIKKvZ" /><figcaption>Figure 4. Audience Builder user interface</figcaption></figure><p>The Audience Service, the backend service supporting Audience Builder, has the high-level architecture shown in Figure 5.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*vyLTbGOvXeR19Ps1" /><figcaption>Figure 5. Overview of Audience Service</figcaption></figure><p>ElasticSearch is used to serve queries aimed at getting target users — for example, a common use case is to find all users who are eligible for a nearby promotion. Audience Service also contains an internally built key-value storage of additional object information, such as user information based on a user ID. All user attributes are ingested into online storage through either batch pipelines from offline storage systems (e.g., Hive tables) or real-time streaming jobs from other online systems. Audience rules defined in Audience Builder are converted to queries against ElasticSearch and key-value storage to obtain information about target user audience.</p><h4><strong>Workflow Service</strong></h4><p>The Workflow Service, which orchestrates the “when”, is a queue-based delivery job processing system (see Figure 6).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*smnx_rO23E1_ablp" /><figcaption>Figure 6. Overview of Workflow Service</figcaption></figure><p>A cron job runs every hour in the Workflow Service and processes scheduling-based campaigns. This job includes the following steps:</p><ol><li>Fetch all active campaigns from campaign storage</li><li>Get reach estimation for each campaign from Audience Service</li><li>Split the delivery quota among campaigns based on reach estimation size</li><li>Fetch user IDs for active campaigns from Audience Service</li><li>Loop through each user to check user states and decide top-ranked campaigns</li><li>Post events to the Delivery Service to send messages at scheduled times</li></ol><p>An event-driven job tries to match an incoming real-time event (e.g., a new booking) to identify the corresponding user, then performs the following procedure:</p><ol><li>Evaluate and update the state of the user in storage</li><li>Call Audience Service to fetch all eligible campaigns for the user</li><li>For eligible campaigns, determine the top-ranked campaign</li><li>Update the job and user state storage with the latest information</li><li>Post an event to the Delivery Service to send a scheduled message to the user</li></ol><h4><strong>Delivery Service</strong></h4><p>The Delivery Service sends messages reliably and efficiently to Airbnb’s users through offline channels (e.g., email, SMS, push notification). It is on the critical path for crucial use cases at Airbnb such as messaging, login, signups and customer support. As shown in Figure 7, the Delivery Service takes delivery events as input, processes these events, and eventually sends the final message to Airbnb users through appropriate vendors. Event processing leverages Amazon’s Simple Queue Service (SQS) to enqueue jobs to delivery workers for various channels. The typical end-to-end delivery latency is less than 30 seconds through the Delivery Service, and all delivered messages are logged into a data store for analysis and debugging purposes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rga6_rkH3tPk4E75" /><figcaption>Figure 7. Overview of Delivery Service</figcaption></figure><p>The Delivery Service aims to provide a simple interface for internal users. It efficiently and robustly handles delivery-related issues, such as spam reputation, legal compliance, user experience, deliverability, reachability, optimization, and metrics. While the Delivery Service has been quite effective in supporting OMNI, there are still many areas for improvement. The areas that we’re currently focusing on include:</p><p><strong>Uptime and Deliverability</strong>: Important metrics for our team include uptime and delivery success rate. We are working towards a goal of 99.99% uptime and 90% delivery success rate across all channels at present.</p><p><strong>Cost</strong>: Each year millions are spent on maintaining the system and paying for vendor APIs, especially for SMS channels. We are investigating some optimization opportunities and channel shift ideas in an effort to bring down this spend.</p><p><strong>Reachability</strong>: There is limited geographic support for popular channels such as WhatsApp and WeChat. The system needs to be extended further for such channels so that Airbnb can better serve its customers in the future.</p><h3>Learnings</h3><p>There was a lot to learn throughout the journey of building the promotions and communications platform from scratch. We share some key learnings so you can utilize them in your promotional and marketing adventure.</p><p><strong>Include Content Governance Mechanisms</strong>: One of the biggest challenges we encountered when building the system was ensuring high quality content and appropriate review processes. Tooling made it easier to create and deliver a lot of content to users, but also increased the challenge of quality control. In hindsight, the approval processes, debug tooling, and systematic guardrails should have always been a mandatory part of a promotions and communications platform from the start.</p><p><strong>Design for Unpredictable Traffic Patterns</strong>: Promotional content delivery tends to be spiky in nature, which translates to unpredictable traffic patterns on our underlying services. By integrating elastic scaling of resources, rate limiting, and back pressure mechanisms in our design, we ensured that promotional traffic would be isolated from the regular user traffic on our website.</p><p><strong>Scaling ElasticSearch Requires Intention: </strong>As the platform gained traction, we learned that operating ElasticSearch requires specialized expertise. Our storage strained under the load of an ever-growing set of attributes. Some of these were added and used only once, but were continually processed. To manage this growth, we audited the data we store in ElasticSearch and added an allowlist process to limit indexed fields. We also refined our storage schema to make it sparse, and actively manage realtime and batch updated fields.</p><h3>Conclusion</h3><p>We built OMNI as the unified and shared communication platform across Airbnb. There are many benefits to building products and features with OMNI campaign life-cycle management, flexible audience selection, real-time traffic monitoring and analytics, and scalable delivery across multiple channels.</p><p>Developing a communication platform requires close collaboration with many cross-functional teams across the company, as well as strong support and commitment from leadership. An architecture based on distributed services and storage is critical to achieving the scalability and extensibility required for such a platform. We plan on authoring additional posts to deep-dive into the relevant technical details and features of major OMNI components (e.g., Optimization Service, Presentation Service, Rendering Service), as well as the challenges encountered through the design and implementation process. Stay tuned for follow-up posts.</p><p>If you are passionate about building distributed systems to solve communications-related technical challenges, then <a href="https://careers.airbnb.com/positions/">apply here</a> to join our team.</p><h3>Acknowledgements</h3><p>This work was only possible through a massive amount of hard work and great support across our entire organization. Special thanks to Arjun Raman, Bita Gorjiara, David Yang, Emre Ozdemir, Ganesh Venkataraman, Haiqing Deng, Irene Kai, Jasmine Price, Laurie Jin, Mengting Li, Michael Endelman, Michael Kinoti, Min Li, Mukund Narasimhan, Nnenna John, Priyank Singhal, Sharvari Apte, Vidhur Vohra, Xin Tu, and Zhentao Sun.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6266f1ffe2bd" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-promotions-and-communications-platform-6266f1ffe2bd">Airbnb’s Promotions and Communications Platform</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>