<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 21 Oct 2022 01:34:04 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Mussel — Airbnb’s Key-Value Store for Derived Data]]></title>
            <link>https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/406b9fa1b296</guid>
            <category><![CDATA[storage]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Shouyan guo]]></dc:creator>
            <pubDate>Mon, 10 Oct 2022 17:40:04 GMT</pubDate>
            <atom:updated>2022-10-10T17:40:03.878Z</atom:updated>
            <content:encoded><![CDATA[<h3><strong>Mussel — Airbnb’s Key-Value Store for Derived Data</strong></h3><p><strong>How Airbnb built a persistent, high availability and low latency key-value storage engine for accessing derived data from offline and streaming events.</strong></p><p><strong>By:</strong> <a href="http://linkedin.com/in/chandramoulir">Chandramouli Rangarajan</a>, <a href="http://linkedin.com/in/shouyan-guo">Shouyan Guo</a>, <a href="http://linkedin.com/in/yuxijin">Yuxi Jin</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DviEp2cHiuC4NoH5" /></figure><h3>Introduction</h3><p>Within Airbnb, many online services need access to derived data, which is data computed with large scale data processing engines like Spark or streaming events like Kafka and stored offline. These services require a high quality derived data storage system, with strong reliability, availability, scalability, and latency guarantees for serving online traffic. For example, the user profiler service stores and accesses real-time and historical user activities on Airbnb to deliver a more personalized experience.</p><p>In this post, we will talk about how we leveraged a number of open source technologies, including <a href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/regionserver/HRegion.html">HRegion</a>, <a href="https://helix.apache.org/">Helix</a>, <a href="https://spark.apache.org/">Spark</a>, <a href="https://zookeeper.apache.org/">Zookeeper</a>,and <a href="https://kafka.apache.org/">Kafka</a> to build a scalable and low latency key-value store for hundreds of Airbnb product and platform use cases.</p><h3>Derived Data at Airbnb</h3><p>Over the past few years, Airbnb has evolved and enhanced our support for serving derived data, moving from teams rolling out custom solutions to a multi-tenant storage platform called Mussel. This evolution can be summarized into three stages:</p><p><strong>Stage 1 (01/2015): Unified read-only key-value store (HFileService)</strong></p><p>Before 2015, there was no unified key-value store solution inside Airbnb that met four key requirements:</p><ol><li>Scale to petabytes of data</li><li>Efficient bulk load (batch generation and uploading)</li><li>Low latency reads (&lt;50ms p99)</li><li>Multi-tenant storage service that can be used by multiple customers</li></ol><p>Also, none of the existing solutions were able to meet these requirements. <a href="https://www.mysql.com/">MySQL</a> doesn’t support bulk loading, <a href="https://hbase.apache.org/">Hbase</a>’s massive bulk loading (distcp) is not optimal and reliable, RocksDB had no built-in horizontal sharding, and we didn’t have enough C++ expertise to build a bulk load pipeline to support RocksDB file format.</p><p>So we built HFileService, which internally used <a href="http://devdoc.net/bigdata/hbase-0.98.7-hadoop1/book/hfilev2.html#:~:text=HFile%20is%20a%20low%2Dlevel,to%20write%20those%20inline%20blocks.">HFile</a> (the building block of Hadoop HBase, which is based on Google’s SSTable):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*38vsUsNg4FnouSzp" /><figcaption><em>Fig. 1: HFileService Architecture</em></figcaption></figure><ol><li>Servers were sharded and replicated to address scalability and reliability issues</li><li>The number of shards was fixed (equivalent to the number of Hadoop reducers in the bulk load jobs) and the mapping of servers to shards stored in Zookeeper. We configured the number of servers mapped to a specific shard by manually changing the mapping in Zookeeper</li><li>A daily Hadoop job transformed offline data to HFile format and uploaded it to S3. Each server downloaded the data of their own partitions to local disk and removed the old versions of data</li><li>Different data sources were partitioned by primary key. Clients determined the correct shard their requests should go to by calculating the hash of the primary key and modulo with the total number of shards. Then queried Zookeeper to get a list of servers that had those shards and sent the request to one of them</li></ol><p><strong>Stage 2 (10/2015): Store both real-time and derived data (Nebula)</strong></p><p>While we built a multi-tenant key-value store that supported efficient bulk load and low latency read, it had its drawbacks. For example, it didn’t support point, low-latency writes, and any update to the stored data had to go through the daily bulk load job. As Airbnb grew, there was an increased need to have low latency access to real-time data.</p><p>Therefore, Nebula was built to support both batch-update and real-time data in a single system. It internally used DynamoDB to store real-time data and S3/HFile to store batch-update data. Nebula introduced timestamp based versioning as a version control mechanism. For read requests, data would be read from both a list of dynamic tables and the static snapshot in HFileService, and the result merged based on timestamp.</p><p>To minimize online merge operations, Nebula also had scheduled spark jobs that ran daily and merged snapshots of DynamoDB data with the static snapshot of HFileService. Zookeeper was used to coordinate write availability of dynamic tables, snapshots being marked ready for read, and dropping of stale tables.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EGFSNgoYuewt2NptXXIdxA.png" /><figcaption>Fig. 2: Nebula Architecture</figcaption></figure><p><strong>Stage 3 (2018): Scalable and low latency key-value storage engine (Mussel)</strong></p><p>In Stage 3, we built a system that supported both read and write on real-time and batch-update data with timestamp-based conflict resolution. However, there were opportunities for improvement:</p><ol><li>Scale-out challenge: It was cumbersome to manually edit partition mappings inside Zookeeper with increasing data growth, or to horizontally scale the system for increasing traffic by adding additional nodes</li><li>Improve read performance under spiky write traffic</li><li>High maintenance overhead: We needed to maintain HFileService and DynamoDB at the same time</li><li>Inefficient merging process: The process of merging the delta update from DynamoDB and HFileService daily became very slow as our total data size became larger. The daily update data in DynamoDB was just 1–2% of the baseline data in HFileService. However, we re-published the full snapshot (102% of total data size) back to HFileService daily</li></ol><p>To solve the drawbacks, we came up with a new key-value store system called <strong>Mussel</strong>.</p><ol><li>We introduced Helix to manage the partition mapping within the cluster</li><li>We leveraged Kafka as a replication log to replicate the write to all of the replicas instead of writing directly to the Mussel store</li><li>We used HRegion as the only storage engine in the Mussel storage nodes</li><li>We built a Spark pipeline to load the data from the data warehouse into storage nodes directly</li></ol><p>Let’s go into more details in the following paragraphs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p_mMDfHNFVebMhGA3MXBvg.png" /><figcaption>Fig. 3: Mussel Architecture</figcaption></figure><p><strong>Manage partitions with Helix</strong></p><p>In Mussel, in order to make our cluster more scalable, we increased the number of shards from 8 in HFileService to 1024. In Mussel, data is partitioned into those shards by the hash of the primary keys, so we introduced Apache Helix to manage these many logical shards. Helix manages the mapping of logical shards to physical storage nodes automatically. Each Mussel storage node could hold multiple logical shards. Each logical shard is replicated across multiple Mussel storage nodes.</p><p><strong>Leaderless Replication with Kafka</strong></p><p>Since Mussel is a read-heavy store, we adopted a leaderless architecture. Read requests could be served by any of the Mussel storage nodes that have the same logical shard, which increases read scalability. In the write path, we needed to consider the following:</p><ol><li>We want to smooth the write traffic to avoid the impact on the read path</li><li>Since we don’t have the leader node in each shard, we need a way to make sure each Mussel storage node applies the write requests in the same order so the data is consistent across different nodes</li></ol><p>To solve these problems, we introduced Kafka as a write-ahead-log here. For write requests, instead of directly writing to the Mussel storage node, it’ll first write to Kafka asynchronously. We have 1024 partitions for the Kafka topic, each partition belonging to one logical shard in the Mussel. Each Mussel storage node will poll the events from Kafka and apply the change to its local store. Since there is no leader-follower relationship between the shards, this configuration allows the correct write ordering within a partition, ensuring consistent updates. The drawback here is that it can only provide eventual consistency. However, given the derived data use case, it is an acceptable tradeoff to compromise on consistency in the interest of ensuring availability and partition tolerance.</p><p><strong>Supporting both read, write, and compaction in one storage engine</strong></p><p>In order to reduce the hardware cost and operational load of managing DynamoDB, we decided to remove it and extend HFileService as the only storage engine to serve both real-time and offline data. To better support both read and write operations, we used <a href="https://hbase.apache.org/1.1/apidocs/org/apache/hadoop/hbase/regionserver/HRegion.html">HRegion</a> instead of <a href="https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/HFile.html">Hfile</a>. HRegion is a fully functional key-value store with MemStore and BlockCache. Internally it uses a Log Structured Merged (LSM) Tree to store the data and supports both read and write operations.</p><p>An HRegion table contains column families, which are the logical and physical grouping of columns. There are column qualifiers inside of a column family, which are the columns. Column families contain columns with time stamped versions. Columns only exist when they are inserted, which makes HRegion a sparse database. We mapped our client data to HRegion as the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1wtQhbeeNTrCLsChR4BDLQ.png" /></figure><p>With this mapping, for read queries, we’re able to support:</p><ol><li>Point query by looking up the data with primary key</li><li>Prefix/range query by scanning data on secondary key</li><li>Queries for the latest data or data within a specific time range, as both real-time and offline data written to Mussel will have a timestamp</li></ol><p>Because we have over 4000 client tables in Mussel, each user table is mapped to a column family in HRegion instead of its own table to reduce scalability challenges at the metadata management layer. Also, as HRegion is a column-based storage engine, each column family is stored in a separate file so they can be read/written independently.</p><p>For write requests, it consumes the write request from Kafka and calls the HRegion put API to write the data directly. For each table, it can also support customizing the max version and TTL (time-to-live).</p><p>When we serve write requests with HRegion, another thing to consider is compaction. Compaction needs to be run in order to clean up data that is deleted or has reached max version or max TTL. Also when the MemStore in HRegion reaches a certain size, it is flushed to disk into a StoreFile. Compaction will merge those files together in order to reduce disk seek and improve read performance. However, on the other hand, when compaction is running, it causes higher cpu and memory usage and blocks writes to prevent JVM (Java Virtual Machine) heap exhaustion, which impacts the read and write performance of the cluster.</p><p>Here we use Helix to mark Mussel storage nodes for each logical shard into two types of resources: online nodes and batch nodes. For example, if we have 9 Mussel storage nodes for one logical shard, 6 of them are online nodes and 3 of them are batch nodes. The relationship between online and batch are:</p><ol><li>They both serve write requests</li><li>Only online nodes serve read requests and we rate limit the compaction on online nodes to have good read performance</li><li>Helix schedules a daily rotation between online nodes and batch nodes. In the example above, it moves 3 online nodes to batch and 3 batch nodes to online so those 3 new batch nodes can perform full speed major compaction to clean up old data</li></ol><p>With this change, now we’re able to support both read and write with a single storage engine.</p><p><strong>Supporting bulk load from data warehouse</strong></p><p>We support two types of bulk load pipelines from data warehouse to Mussel via <a href="https://airflow.apache.org/">Airflow</a> jobs: merge type and replace type. Merge type means merging the data from the data warehouse and the data from previous write with older timestamps in Mussel. Replace means importing the data from the data warehouse and deleting all the data with previous timestamps.</p><p>We utilize Spark to transform data from the data warehouse into HFile format and upload to S3. Each Mussel storage node downloads the files and uses HRegion bulkLoadHFiles API to load those HFiles into the column family.</p><p>With this bulk load pipeline, we can just load the delta data into the cluster instead of the full data snapshot every day. Before the migration, the user profile service needed to load about 4TB data into the cluster daily. After, it only needs to load about 40–80GB, drastically reducing the cost and improving the performance of the cluster.</p><h3>Conclusion and Next Steps</h3><p>In the last few years, Airbnb has come a long way in providing a high-quality derived data store for our engineers. The most recent key-value store Mussel is widely used within Airbnb and has become a foundational building block for any key-value based application with strong reliability, availability, scalability, and performance guarantees. Since its introduction, there have been ~4000 tables created in Mussel, storing ~130TB data in our production clusters without replication. Mussel has been working reliably to serve large amounts of read, write, and bulk load requests: For example, mussel-general, our largest cluster, has achieved &gt;99.9% availability, average read QPS &gt; 800k and write QPS &gt; 35k, with average P95 read latency less than 8ms.</p><p>Even though Mussel can serve our current use cases well, there are still many opportunities to improve. For example, we’re looking forward to providing the read-after-write consistency to our customers. We also want to enable auto-scale and repartition based on the traffic in the cluster. We’re looking forward to sharing more details about this soon.</p><h3>Acknowledgments</h3><p>Mussel is a collaborative effort of Airbnb’s storage team including: <a href="http://linkedin.com/in/calvinzou">Calvin Zou</a>, <a href="linkedin.com/in/dionitas">Dionitas Santos</a>, <a href="http://linkedin.com/in/ruan-maia-367281161">Ruan Maia</a>, <a href="http://linkedin.com/in/wonheec">Wonhee Cho</a>, <a href="linkedin.com/in/xiaomou-wang-5880b537">Xiaomou Wang</a>, <a href="linkedin.com/in/yanhan-zhang-724088a4">Yanhan Zhang</a>.</p><p>Interested in working on the Airbnb Storage team? Check out this role: <a href="https://careers.airbnb.com/positions/3029584/">Staff Software Engineer, Distributed Storage</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=406b9fa1b296" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296">Mussel — Airbnb’s Key-Value Store for Derived Data</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Beyond A/B test : Speeding up Airbnb Search Ranking Experimentation through Interleaving]]></title>
            <link>https://medium.com/airbnb-engineering/beyond-a-b-test-speeding-up-airbnb-search-ranking-experimentation-through-interleaving-7087afa09c8e?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/7087afa09c8e</guid>
            <category><![CDATA[evaluation]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[search-ranking]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Qing Zhang]]></dc:creator>
            <pubDate>Thu, 06 Oct 2022 15:52:18 GMT</pubDate>
            <atom:updated>2022-10-06T22:47:02.801Z</atom:updated>
            <content:encoded><![CDATA[<h3>Beyond A/B Test : Speeding up Airbnb Search Ranking Experimentation through Interleaving</h3><p>Introduction of Airbnb interleaving experimentation framework, usage and approaches to address challenges in our unique business</p><p><a href="https://medium.com/@zq.zhangqing">Qing Zhang</a>, <a href="https://medium.com/@michelle.du">Michelle Du</a>, Reid Andersen, <a href="https://medium.com/@liweihe">Liwei He</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4v8bM6rq3FsK7Zwa7UO7bA.jpeg" /></figure><h3>Introduction</h3><p>When a user searches for a place to stay on Airbnb, we aim to show them the best results possible. Airbnb’s relevance team actively works on improving search ranking experience and helps users to find and book listings that match their preference. A/B test is our approach for online assessment. Our business metrics are conversion-focused, and the frequency of guest travel transactions is lower than on other e-commerce platforms. These factors result in insufficient experiment bandwidth given the number of ideas that we want to test and there is considerable demand to develop a more efficient online testing approach.</p><p>Interleaving is an online ranking assessment approach [1–3]. In A/B tests, users are split into control and treatment groups. Those who are in each group will be consistently exposed to results from the corresponding ranker. Interleaving, on the other hand, blends the search results from both control and treatment and presents the “interleaved” results to the user (Figure 1). The mechanism enables direct comparison between the two groups by the same user, with which the impact of the treatment ranker can be evaluated by a collection of specifically designed metrics.</p><p>There are several challenges in building the framework on both engineering and data science fronts. On the engineering side, we needed to extend our current AB test framework to enable interleaving set up while adding minimum overhead to the ML engineers. Additionally, our search infrastructure is designed for single request search and required significant extension to support interleaving functionality. On the data science side, we designed user event attribution logic that’ key to the effectiveness of metrics.</p><p>In 2021, we built the interleaving experimentation framework and integrated it in our experiment process and reached a 50x sensitivity in the development of our search ranking algorithm. Further validation confirms high agreement with A/B tests. We have been using interleaving for a wide range of tasks such as ranker assessment, hyperparameter tuning as well as evaluating infra-level changes. The system design and learnings detailed in this blog post should benefit readers looking to improve their experimentation agility.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/815/0*UUyBfFnZWWa13Mxk" /></figure><p>Figure 1: An illustration of A/B testing v.s. Interleaving. In traditional A/B tests, users are split into two groups and exposed to two different rankers. In Interleaving, each user is presented with the blended results from two rankers.</p><h3>Search Ranking Experimentation Procedure</h3><p>With interleaving, Airbnb search ranking experimentation uses a three phase procedure for faster experimentation (Figure 2). First, we run standard offline evaluation on the ranker with NDCG (normalized discounted cumulative gain). Rankers with reasonable results move on to online evaluation with interleaving. The ones that get promising results go on for the A/B test.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*LDlMakrih7JAiFkx" /></figure><p>Figure 2: Ranking experimentation procedure. We use interleaving to get preliminary online results in order to enable fast iteration</p><p>Currently, we split our search traffic into two portions, and use the vast majority for regular A/B tests and remaining for interleaving experiments. We divide the interleaving traffic into buckets (called interleaving lanes) and each lane is used for one interleaving experiment. Each interleaving experiment takes up about 6% of regular A/B test traffic, and one-third of running length. We achieve a 50x speedup over an A/B test given the same amount of traffic. The team now has the luxury to test out multiple variations of the idea in a short time frame and identify the promising routes to move forward.</p><h3>Airbnb Interleaving Framework</h3><p>The interleaving framework controls the experimentation traffic and generates interleaved results to return to the user as illustrated in Figure 3. Specifically, for users who are subject to interleaving, the system creates parallel search requests that correspond to control and treatment rankers and produce responses. The results generation component blends the two responses with team drafting algorithms, returns the final response to the user, and creates logging. A suite of metrics were designed to measure impact.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xrxh0CYHcHielAP2" /></figure><p>Figure 3: Interleaving system overview. The interleaving framework controls the experimentation traffic and generates interleaved results to return to the user</p><h3>Team Drafting and Competitive Pairs</h3><p>The framework employs the<em> team drafting algorithm</em> to “interleave” the results from control and treatment (we call them teams). For the purpose of generalizability, we demonstrate the drafting process with two teams A and B. The steps of the algorithm are as follows:</p><p>1 Flip a coin to determine if team A goes first</p><p>2 Start with an empty merged list. Repeat the following step until desired size is reached,</p><p>2. 1 From each of the two rankers A and B take the highest-ranked result that has not yet been selected (say listing a from ranker A and e from ranker B).</p><p>2.2 If the two listings are different, then select listings a and e, with assigned a to A and e assigned B. We will call (a, e) a <em>competitive pair</em>. Add the pair to the merged list with the order decided in Step 1</p><p>2.3 If the two listings are the same, then select that listing and do not assign it to either team. Figure 4 demonstrates the process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/710/0*pqLeLNEtmZEQ2D9o" /></figure><p>Figure 4: Team drafting example with competitive pair explained. Here we assume that team A goes first based on coin flip.</p><p>The<em> team drafting algorithm</em> enables us to measure user preference in a fair way. For each request we flip a coin to decide which team (control or treatment) has the priority in the ordering of a <em>competitive pair</em>. This means that position bias is minimized as listings from each team are ranked above the one from the other team in the competitive pair half of the time.</p><p>Creating <em>competitive pairs</em> makes <a href="https://alexdeng.github.io/causal/sensitivity.html#vrreg">variance reduction</a> (a procedure to speed up experimentation by increasing the precision of the point estimates) more intuitive, since it deduplicates items with the same rank and only assigns scores to the impression of competitive pairs instead of to each impression. In the example in Figure 4, the comparison between ranker A and ranker B reduces to a referendum on whether <em>a</em> is better than <em>e</em>. Leaving the other results unassigned improves the sensitivity in this case. In an extreme case where two rankers produce lists with exactly the same order, traditional interleaving would still associate clicks to teams and add noise to the result; while with competitive pairs, the entire search query can be ignored since the preference is exactly zero. This allows us to focus on the real difference with sensitivity improvement.</p><p>Furthermore, competitive pairs enable us to allocate credits to various user activities downstream much more easily. Again unlike traditional interleaving, which mostly assigns credits for clicks [3–5], we assign credits by bookings, which is a downstream activity. The flexibility in credit association has empowered us to design complicated metrics without having to rely on click signals. For example, we are able to define metrics that measure the booking wins over competition with certain types of listings (e.g. new listings) in the pairs. This enabled us to further understand whether changes to the ranking of a specific category of listings played its role in interleaving overall.</p><p>To determine a winning ranker in our interleaving approach, we compare the <em>preference margin</em> (margin of victory for the winning team) on target events and apply a 1-sample t-test over it to obtain the p-value. Validation studies confirmed that our framework produces results that are both reliable and robust — with a consistently low false positive rate, and minimum carryover effect between experiments.</p><h3>Attribution</h3><p><em>Attribution logic</em> is a key component of our measurement framework. As mentioned earlier, a typical scenario that is more unique to Airbnb compared to cases like Web search or streaming sites is that our guests can issue multiple search requests before booking, and the listing they book may have been viewed or clicked multiple times when owned by different interleaving teams, which is different from use cases where the primary goal is click-based conversion.</p><p>Let’s use a toy example to demonstrate the concept. As shown in Figure 5, the guest clicked the booked listing 3 times with each ranker having the listing on their team multiple times (2 times on team A, 1 time on team B) throughout the search journey. For this single guest alone, we see how the different attribution methods can end up with different conclusions:</p><ul><li>If we attribute the booking to the team when it was first clicked, we should assign it to team B and declare team B as the winner for this guest;</li><li>If we attribute the booking to the team when it was last clicked, we should assign it to team A and declare team A as the winner for the guest;</li><li>If we attribute the booking every time it was clicked, we should assign it twice to team A and once to team B, and end up declaring team A being the winner for the guest.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/898/0*7WbbbuEdUHlirDp3" /></figure><p>Figure 5: A simplified example of guest journey. The guest emits multiple searches and views the booked listing multiple times before finally making a booking.</p><p>We created multiple attribution logic variations and evaluated them on a wide collection of interleaving experiments that also had A/B runs as “ground truth”. We set our primary metric to be the one that has best alignment between interleaving and A/B tests.</p><h3>Alignment with A/B tests</h3><p>To further evaluate the consistency between interleaving and A/B tests, we tracked eligible interleaving and A/B pairs and confirmed that the two are consistent with each other 82% of the time (Figure 6). The experiments are also highly sensitive as noted in previous work from other companies like Netflix. To provide a concrete example, we have a ranker that randomly picks a listing in the top 300 results and inserts it to the top slot. It takes interleaving only 0.5% of the A/B running time and 4% of A/B traffic to get to the same conclusion as its corresponding A/B test.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/533/0*TeUKjwEQznkN9wUW" /></figure><p>Figure 6: Interleaving and A/B consistency. We tracked eligible interleaving and A/B pairs and the results demonstrate that the two are consistent with each other 82% of the time</p><p>In most cases where interleaving turned out to be inconsistent with traditional A/B testing, we found that the reason was set-level optimization. For example, one ranker relies on a model to determine how strongly it will demote listings with high host rejection probability and the model is the booking probability given the current page. Interleaving breaks this assumption and leads to inaccurate results. Based on our learnings, we advise that rankers that involve set-level optimization should use interleaving on a case by case basis.</p><h3>Conclusion</h3><p>Search ranking quality is key for an Airbnb user to find their desired accommodation and iterating on the algorithm efficiently is our top priority. The interleaving experimentation framework tackles our problem of limited A/B test bandwidth and provides up to 50x speed up on the search ranking algorithm iteration. We conducted comprehensive validation which demonstrated that interleaving is highly robust and has strong correlation with traditional A/B. Interleaving is currently part of our experimentation procedure, and is the main evaluation technique before the A/B test. The framework opens a new field of online experimentation for the company and can be applied to other product surfaces such as recommendations.</p><p>Interested in working at Airbnb? Check out our open roles <a href="https://careers.airbnb.com/">HERE</a>.</p><h3>Acknowledgments</h3><p>We would like to thank Aaron Yin for the guidance on the implementations of algorithms and metrics, Xin Liu for continuously advising us on optimizing and extending the framework to support more use cases, Chunhow Tan for valuable suggestions on improving the computational efficiency of interleaving metrics and Tatiana Xifara for advice on experiment delivery design.</p><p>The system won’t be possible without the support from our search backend team, especially Yangbo Zhu, Eric Wu, Varun Sharma and Soumyadip (Soumo) Banerjee. We benefit tremendously from their design advice and close collaboration on the operations.</p><p>We would also like to thank Alex Deng, Huiji Gao and Sanjeev Katariya for valuable feedback on the interleaving and this article.</p><h3>References</h3><p>[1] JOACHIMS, T. Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (KDD). ACM, New York, NY, 132–142. 2002.</p><p>[2] JOACHIMS, T. Evaluating Retrieval Performance using Clickthrough Data. In Text Mining, J. Franke, G. Nakhaeizadeh, and I. Renz, Eds., Physica/Springer Verlag, 79–96. 2003.</p><p>[3] RADLINSKI, F., KURUP, M., AND JOACHIMS, T. How does clickthrough data reflect retrieval quality. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM’08). ACM, New York, NY, 43–52. 2008.</p><p>[4] Radlinski, Filip, and Nick Craswell. “Optimized interleaving for online retrieval evaluation.” Proceedings of the sixth ACM international conference on Web search and data mining. 2013.</p><p>[5] Hofmann, Katja, Shimon Whiteson, and Maarten De Rijke. “A probabilistic method for inferring preferences from clicks.” Proceedings of the 20th ACM international conference on Information and knowledge management. 2011.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7087afa09c8e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/beyond-a-b-test-speeding-up-airbnb-search-ranking-experimentation-through-interleaving-7087afa09c8e">Beyond A/B test : Speeding up Airbnb Search Ranking Experimentation through Interleaving</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upgrading Data Warehouse Infrastructure at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/upgrading-data-warehouse-infrastructure-at-airbnb-a4e18f09b6d5?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/a4e18f09b6d5</guid>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[airbnb]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Ronnie Zhu]]></dc:creator>
            <pubDate>Mon, 26 Sep 2022 18:31:10 GMT</pubDate>
            <atom:updated>2022-09-26T18:31:10.909Z</atom:updated>
            <content:encoded><![CDATA[<p>This blog aims to introduce Airbnb’s experience upgrading Data Warehouse infrastructure to Spark and Iceberg.</p><p>By: <a href="https://www.linkedin.com/in/huirong-ronnie-zhu-97b0a980/">Ronnie Zhu</a>, <a href="https://www.linkedin.com/in/edgarrd/">Edgar Rodriguez</a>, <a href="https://www.linkedin.com/in/qiang-jason-xu-7101b025/">Jason Xu</a>, <a href="https://www.linkedin.com/in/gustavo-torres-torres/">Gustavo Torres</a>, <a href="https://www.linkedin.com/in/kerimoktay">Kerim Oktay</a>, <a href="https://www.linkedin.com/in/zhangxu325/">Xu Zhang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ky-obyCnt-A0R4qjnoY1_g.jpeg" /></figure><h3>Introduction</h3><p>In this blog, we will introduce our motivations for upgrading our Data Warehouse Infrastructure to Spark 3 and Iceberg. We will briefly describe the current state of Airbnb data warehouse infrastructure and the challenges. We will then share our learnings from upgrading one critical production workload: event data ingestion. Finally, we will share the results and the lessons learned.</p><h3>Context</h3><p>Airbnb’s Data Warehouse (DW) storage was previously migrated from legacy <a href="https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c">HDFS clusters</a> to S3 to provide better stability and scalability. While our team has continued to improve the reliability and stability of the workloads that operate on data in S3, certain characteristics of these workloads and the infrastructure they depend on introduce scalability and productivity limitations that our users encounter on a regular basis.</p><h4>Challenges</h4><h4>Hive Metastore</h4><p>With an increasing number of partitions, Hive’s backend DBMS’s load has become a bottleneck, as has the load on partition operations (e.g., querying thousands of partitions for a month’s worth of data). As a workaround, we usually add a stage of daily aggregation and keep two tables for queries of different time granularities (e.g., hourly and daily). To save on storage, we limit intraday Hive tables to short retention (three days), and keep daily tables for longer retention (several years).</p><h4>Hive/S3 Interactions</h4><p>Hive was not originally designed for object storage. Instead, many assumptions were made around HDFS when implementing features such as renames and file listings. When we migrated from HDFS to S3 it therefore required certain guarantees to ensure that datasets were consistent on list-after-write operations. We customized the way Hive writes to S3, first writing to an HDFS temporary cluster and then moving the data to S3 via an optimized distcp process that writes to unique locations during the commit phase, storing file-listing information in a separate store for fast access. This process has performed well over the past two years, but it requires additional cluster resources to run.</p><h4>Schema Evolution</h4><p>At Airbnb, we use three compute engines to access data in our Data Warehouse: Spark, Trino and Hive. Since each compute engine handles schema changes differently, changes to table schemas have almost always resulted in data quality issues or required engineers to perform costly rewrites.</p><h4>Partitioning</h4><p>Hive tables are partitioned by fixed columns, and partition columns cannot be easily changed. In case one needs to repartition a dataset, one has to create a new table and reload the entire dataset.</p><h3>New Data Stack</h3><p>These challenges have motivated us to upgrade our Data Warehouse infrastructure to a new stack based on Iceberg and Spark 3, which addresses these problems and also provides usability improvements.</p><h4>Iceberg</h4><p><a href="https://iceberg.apache.org/docs/latest/">Apache Iceberg</a> is a table format designed to address several of the shortcomings of traditional file system-based Data Warehousing storage formats such as Hive. Iceberg is designed to deliver high-performance reads for huge analytics tables, with features such as serializable isolation, snapshot-based time travel, and predictable schema evolution. Some important Iceberg features that help in some of the challenges mentioned early:</p><ul><li>Partition information is not stored in the Hive metastore, hence removing a large source of load to the metastore.</li><li>Iceberg tables do not require S3 listings, which removes the list-after-write consistency requirement, which can in turn eliminate the need for the extra discp job, and avoids entirely the latency of the list operation.</li><li>Consistent table schema is defined in <a href="https://iceberg.apache.org/spec/#schema-evolution">Iceberg spec</a>, which guarantees consistent behavior across compute engines avoiding unexpected behavior when changing columns.</li></ul><h4>Spark 3</h4><p><a href="https://spark.apache.org/">Apache Spark</a> has become the de facto standard for big data processing in the past 10 years. Spark 3 is a new major version released in 2020, it comes with a long list of features — new functionalities, bug fixes and performance improvements. We focus on introducing Adaptive Query Execution (AQE) here; you can find more info on the <a href="https://databricks.com/blog/2020/06/18/introducing-apache-spark-3-0-now-available-in-databricks-runtime-7-0.html">Databricks blog</a>.</p><p>AQE is a query optimization technique that uses runtime statistics to optimize the Spark query execution plan. This solves one of the greatest struggles of Spark cost-based optimization — inaccurate statistics collected before query starts often lead to suboptimal query plans. AQE will figure out data characteristics and improve query plans as the query runs, increasing query performance.</p><p>Spark 3 is also a prerequisite for Iceberg adoption. Iceberg table write and read support using Spark SQL is only available on Spark 3.</p><p>The diagram below shows the change we made:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cIYLTqi2XeFm3vxb" /><figcaption><strong><em>Figure 1.</em></strong><em> Evolution of data compute and storage tech stack</em></figcaption></figure><h3>Production Case Study — Data Ingestion</h3><p>At Airbnb, the Hive-based data ingestion framework processes &gt;35 billion Kafka event messages and 1,000+ tables per day, and lands datasets ranging from kilobytes to terabytes into hourly and daily partitions. The volume and coverage of datasets of different sizes, and time granularity requirement makes this framework a good candidate to benefit from our Spark+Iceberg tech stack.</p><h3>Spark 3</h3><p>The first step in migrating to the aforementioned Spark+Iceberg compute tech stack was to move our Hive queries to Spark. This introduced a new challenge: Spark tuning. Unlike Hive, which relies on data volume stats, Spark uses preset shuffle partition values to determine task split sizes. Thus, choosing the proper number of shuffle partitions became a big challenge in tuning the event data ingestion framework on Spark. Data volume of different events varies a lot, and the data size of one event also changes over time. Figure 2 shows the high variance of shuffle data size of Spark jobs processing a sampling of 100 different types of events.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*qAIItnnv9VtcG9nz" /><figcaption><strong>Figure 2.</strong> High variance of raw data size of 100 randomly sampled events; each bar represents a single dataset</figcaption></figure><p>There isn’t a fixed number of shuffle partitions that would work well for all events in the ingestion framework; if we pick a fixed number for all ingestion jobs, it might be too big for some jobs but too small for others, and both would result in low performance. While we were exploring different solutions to tune shuffle partition parameters, we found that Adaptive Query Execution could be a perfect solution.</p><h4>How does AQE help?</h4><p>In Spark 3.0, the AQE framework ships with several key features, including dynamically switching join strategies and dynamically optimizing skew joins. However, the most critical new feature for our use case is dynamically coalescing shuffle partitions, which ensures that each Spark task operates on roughly the same amount of data. It does this by combining adjacent small partitions into bigger partitions at runtime. Since shuffle data can dynamically grow or shrink between different stages of a job, AQE is continually re-optimizing the size of each partition through coalescing throughout a job’s lifetime. This brought a great performance boost.</p><p>AQE handles all cases in our data ingestion framework well, including edge cases of spiky events and new events. One note is that flattening of nested columns and compression of file storage format (in our case, Parquet GZIP) might generate fairly small output files for small task splits. To ensure output file sizes are large enough to be efficiently accessed, we can increase the AQE advisory shuffle partition size accordingly.</p><h4>AQE Tuning Experience</h4><p>Let’s walk through an example to get a better understanding of AQE and its tuning experience. Say we run the example query to load one dataset. The query has one Map stage to flatten events and another Reduce stage to handle deduplication. After adopting AQE and running the job in Spark, we can see two highlighted steps get added to the physical plan.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/739/1*XZWObyyGiQUBqgKNP7HSQw.png" /><figcaption><strong>Figure 3.</strong> Change of physical plan of the example Spark job</figcaption></figure><p>Now let’s take a closer look at our tuning phase. As shown in Table 1, we went through several iterations of param setting. From our experience, if the actual shuffle partition used is equal to the initial partition number we set, we should increase the initial partition number to split initial tasks more and get them coalesced. And if the average output file size is too small, we can increase the advisory partition size to generate larger shuffle partitions, and thus larger output files. Upon inspecting shuffle data of each task, we could also decrease executor memory and the max number of executors.</p><p>We also experimented with the tuned job parameters on datasets of different sizes, as shown in Table 2 and 3. From the results, we can see that once tuned, AQE performs well on datasets from zero bytes size to TB in size, all while using a single set of job parameters.¹</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eZu1Q9D0lyD5ZEVE" /><figcaption><strong>Table 1.</strong> Tuning AQE using example medium-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K6Ff5uiN90I_ci_o" /><figcaption><strong>Table 2.</strong> Job stats of example small-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-whu7KM8AK8w1N-f" /><figcaption><strong>Table 3.</strong> Job stats of example empty-size dataset</figcaption></figure><p>From our result, it’s clear that AQE can adjust the shuffle split size very close to our predefined value in the Reduce stage and thus generate outputs of target file size as we expect. Furthermore, since each shuffle split is close to predefined value, we can also lower executor memory from default values to ensure efficient resource allocation. As an additional big advantage to the framework, we do not need to do any special handling to onboard new datasets.</p><h3>Iceberg — Partition specs &amp; Compaction</h3><h4>How does Iceberg help?</h4><p>In our data ingestion framework, we found that we could take advantage of Iceberg’s flexibility to define multiple partition specs to consolidate ingested data over time. Each data file written in a partitioned Iceberg table belongs to exactly one partition, but we can control the granularity of the partition values over time. Ingested tables write new data with an hourly granularity (ds/hr), and a daily automated process compresses the files on a daily partition (ds), without losing the hourly granularity, which later can be applied to queries as a residual filter.</p><p>Our compaction process is smart enough to determine whether a data-rewrite is required to reach an optimal file size, otherwise just rewriting the metadata to assign the already existing data files to the daily partition. This has simplified the process for ingesting event data and provides a consolidated view of the data to the user within the same table. As an added benefit, we’ve realized cost savings in the overall process with this approach.</p><p>As shown in the diagram below, in the consolidated Iceberg table we switch the partition spec from ds/hr to ds at the end of day. In addition, now user queries are easier to write and able to access fresher data with full history. Keeping only one copy of data also helps improve both compute and storage efficiencies and ensures data consistency.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Mrwb_94NNQWrs_mF" /><figcaption><strong>Figure 4.</strong> Change of table storage format for table consolidation</figcaption></figure><h4>Table Consolidation Experience</h4><p>Consolidating hourly and daily data into one Iceberg table requires changes in both the write and read path. For the write path, to mitigate the aforementioned issues caused by small files, we force run a compaction during the partition spec switch. Tables 4 and 5 compare the statistics from our intelligent compaction jobs with the cost of a full rewrite of all the data files associated with the daily partition. For some large tables we obtain resource savings of &gt; 90% by leveraging Iceberg’s ability to avoid data copying during compaction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/649/0*yNW6DneHRxWZdlwq" /><figcaption><strong>Table 4.</strong> Compaction job comparison of example small-size dataset</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/672/0*R8NW6NHCpI2OUUxm" /><figcaption><strong>Table 5.</strong> Compaction job comparison of example large-size dataset</figcaption></figure><p>For the read path, since most data consumers use Airflow’s partition sensors, we updated the implementation of partition sensing. Specifically, we implemented a signal system to sense empty partitions in Iceberg tables, as opposed to the prior method of looking up each Hive partition as an actual row in Hive metastore.</p><h3>Results</h3><p>Comparing the prior TEZ and Hive stack, we see more than 50% compute resource saving and 40% job elapsed time reduction in our data ingestion framework with Spark 3 and Iceberg. From a usability standpoint, we made it simpler and faster to consume stored data by leveraging Iceberg’s capabilities for native schema and partition evolution.</p><h3>Conclusion</h3><p>In this post, we shared the upgrades we applied to Airbnb’s data compute and storage tech stack. We hope that readers enjoyed learning how our event data ingestion framework benefits from Adaptive Query Execution and Iceberg and that they consider applying similar tech stack changes to their use cases involving datasets of varying size and time granularity.</p><p>If this type of work interests you, please check out our open roles <a href="https://careers.airbnb.com/">here</a>!</p><h3>Acknowledgments</h3><p>Special thanks to Bruce Jin, Guang Yang, Adam Kocoloski and Jingwei Lu for their continued guidance and support!</p><p>Also countless thanks to Mark Giangreco, Surashree Kulkarni and Shylaja Ramachandra for providing edits and great suggestions to the post!</p><p>[1] One callout is that Spark AQE has a bug handling empty input (<a href="https://issues.apache.org/jira/browse/SPARK-35239">SPARK-35239</a>), and fixes are available in 3.2. Thus to take full advantage of AQE in lower Spark versions, we need to backport <a href="https://github.com/apache/spark/pull/32362">fix 1</a> and <a href="https://github.com/apache/spark/pull/31994">fix 2</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a4e18f09b6d5" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/upgrading-data-warehouse-infrastructure-at-airbnb-a4e18f09b6d5">Upgrading Data Warehouse Infrastructure at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb safeguards changes in production]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-c83e94bfc52?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c83e94bfc52</guid>
            <category><![CDATA[experiment]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[infrastructure]]></category>
            <dc:creator><![CDATA[Zack Loebel-Begelman]]></dc:creator>
            <pubDate>Tue, 06 Sep 2022 17:16:01 GMT</pubDate>
            <atom:updated>2022-09-06T17:16:00.927Z</atom:updated>
            <content:encoded><![CDATA[<h3>Part II: Near Real-time Experiments</h3><p>By: <a href="https://www.linkedin.com/in/michaelcl/">Mike Lin</a>, <a href="https://www.linkedin.com/in/preetiramasamy/">Preeti Ramasamy</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/zack-loebel-begelman-85407698/">Zack Loebel-Begelman</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/796/1*TwziuVGxkiaD4XKu4A2-pA.jpeg" /></figure><p>In our <a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446">first post</a> we discussed the need for a near real time Safe Deploy system and some of the statistics that power its decisions. In this post we will cover the architecture and engineering choices behind the various components that Safe Deploys comprises.</p><p>Designing a near real-time experimentation system required making explicit tradeoffs among speed, precision, cost, and resiliency. An early decision was to limit near real-time results to only the first 24 hours of an experiment — enough time to catch any major issues and transition to using comprehensive results from the batch pipeline. The idea being once batch results were available, experimenters would no longer need real time results. The following sections describe the additional design decisions in each component of the Safe Deploys system.</p><h3>High Level Design</h3><p>There are 3 major components that make up the technical footprint of the Safe Deploys system:</p><ol><li><strong>Ramp Controller</strong>, a <a href="https://flink.apache.org/">Flink</a> job that acts as a centralized coordinator, providing experiment configuration to NRT via Kafka and invoking statistical computations by calling Measured via HTTP.</li><li><strong>Near Real Time (NRT) pipeline</strong>, another Flink job that extracts measures, joins and enriches those measures with assignment information (treatment and subject information), and stores the enriched measures into S3.</li><li><strong>Measured</strong>, a python library (invoked via a Python HTTP server and worker pool) that consumes enriched measures from S3, aggregates them, and runs stats to determine if any change is significant.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1002/0*dO4dDyRoREQgfsh9" /><figcaption>Fig 1: Architecture Diagram of the Safe Deploy system</figcaption></figure><h3>Ramp Controller</h3><p>The Ramp Controller performs automated experiment ramping based on the results from Measured. It increases experiment exposure in stages, slowly exposing more subjects and monitoring metric impacts at each stage. If any egregiously negative metric is observed, the Ramp Controller will immediately shut down the experiment to minimize the impacts of bad changes. It supports several ramping algorithms, but most users leverage a simple time based algorithm.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/876/0*Uu8Z0bAhhrDH9CSQ" /><figcaption>Figure 2: Ramping process</figcaption></figure><p>Ramp Controller was designed to be stateless, and resilient to any job failures. Within seconds of an experiment starting, it publishes metadata to Kafka, triggering NRT to start joining events for that experiment. The metadata includes a path in S3 that NRT will write to. At this point the Ramp Controller’s core loop will begin:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ypoetTDVqKYg0ID8_EpTNA.png" /></figure><p>Results are computed for the first 24 hours of the experiment, with new metrics consumed as new files are published to S3. A metric is marked egregious when the percent change is smaller than -20% with an adjusted p-value of less than or equal to 0.01. By leveraging the Measured framework for metric computation, we get custom aggregations, richer statistical models, and the ability to compute performance metrics, and dimensional cuts for metrics.</p><p>After overcoming these technical challenges in scaling the pipeline and tuning decision making, we were ready to vet the system with experiment owners and drive adoption.</p><h3>Near Real Time (NRT) Pipeline</h3><p>We built the new NRT pipeline in Java and Scala using Apache Flink. It reads from a multitude of Kafka streams: event streams containing raw user based events (impressions, booking requests etc.), a stream that the Ramp Controller emits containing experiment metadata, and the streaming that contains the raw assignment events emitted for all experiments which are also consumed by the batch pipeline.</p><p>Previously Airbnb had attempted to build an online data store for all experiment assignments, however this did not scale and was eventually shut down. By reducing the scope, specifically limiting the NRT pipeline to the first 24 hours of an experiment, we are able to store a bounded subset of assignments. Using a <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/broadcast_state/">broadcast join</a> of experiment metadata lets us filter the assignment events and Flink makes <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/#state-time-to-live-ttl">aging out data</a> trivial.</p><p>The extraction is written in a stand-alone library so that the measure definitions can be re-used in both batch and streaming. In order to be highly performant, the measure extraction determines which events to extract first using an inverted index based on the existence and values of json fields then only running extraction on the relevant events. Not only do we extract measures, but also dimensions from each event. Because we want to limit the complexity in this job, we only support dimensions from the same event as a measure itself.</p><p>Our first difficulty was in how to handle measures and assignments coming in out of order. We want data to age out at different times when joining assignment events to measures and assignment data should be stored for the full 24 hours. Because the volume of measure events means we can’t keep them for 24 hours, we keep a short buffer dropping measures after 5 minutes. The outer join required to achieve this goal required building a custom join using the <a href="https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/streaming/api/functions/co/KeyedCoProcessFunction.html">keyed co-process api</a>.</p><p>Once the data is joined we buffer it internally within Flink to reduce the total number of files for small experiments. We wrote a simple keyed process stage that hashes the events based on the timestamp against how many concurrent files we want to output. It’s important that we hash on the timestamp since Flink requires the keying mechanism to be deterministic. The events are buffered based on event counts and time, emitting the buffered list either once a partition hits an event based or time based threshold. This stage allows us to have more fine grained control over the number of files we output.</p><p>We leverage Flink’s <a href="https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/filesystem/#parquet-format">built in support for parquet and S3 as a file sink</a> to write the files. In order to provide exactly-once semantics, Flink will only write files when checkpointing occurs. Files output by the NRT pipeline are consumed by the ramp controller to make decisions. To keep our latency low, we checkpoint every 5 minutes.</p><h3>Measured</h3><p>Measured is a framework for defining and computing metrics. It consists of a Scala library for extracting measures and dimensions from raw events that the NRT pipeline leverages, and a Python library for defining metrics (based off of those measures), statistical models, and visualizations. This section focuses on the Python library, and how it is used to compute metrics.</p><p>In order to provide consistent results across platforms we run the same Measured jobs that user’s run via a Python HTTP job server and worker pool. The NRT metric evaluation is one of those jobs, it downloads the event files from S3 using <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor">a Python worker pool</a>. Once the files are downloaded the job leverages <a href="https://duckdb.org/">duckdb</a>’s <a href="https://duckdb.org/docs/data/parquet">parquet reader functionality</a> to aggregate to the user level. Once we have a local user aggregate the job evaluates the various sequential models discussed in the first post. The results of these evaluations are stored in a MySQL database upon job completion to be retrieved by the UI or the Ramp Controller over HTTP.</p><h3>Adoption</h3><p>The full vision of Safe Deploys encompassed safeguarding any changes in production. However, to gain experience and trust, we initially focused our efforts on A/B tests. We knew that Safe Deploys, like any anomaly detection system, especially one that automates remediation steps, would face certain challenges in adoption, including:</p><ul><li>Trust in NRT metrics that were similar but not exactly the same as existing batch ones</li><li>Relinquishment of control in ramping and shutdown of experiments</li><li>False positives that could slow down experimentation by forcing restarts</li></ul><p>Before Safe Deploys, nearly a quarter of Airbnb teams had a manual process for ramping up experiments. This consisted of increasing the exposure of an experiment, manually verifying performance metrics, and repeating until reaching a target exposure. This often masked significant but not visually obvious negative impacts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/0*B_kTvecFqvuTWE17" /><figcaption>Figure 3: Illustration of how a controlled experiment provides greater sensitivity</figcaption></figure><p>We evangelized Safe Deploys as complementary to the existing process, providing increased sensitivity of detecting negative impacts, while still allowing experimenters to stop an experiment based on their own monitoring at any time. We also continually improved statistical methods used to decrease false positives and negatives. Since enabling Safe Deploys by default a year ago, it has been used for over 85% of experiment starts and helped prevent dozens of incidents, and flagged misconfigurations early, minimizing negative impacts to Airbnb’s business and wasted engineering resources on remediation.</p><h3>Tip of the Iceberg</h3><p>Safeguarding experiments was a significant step towards reducing incidents at Airbnb, however the full vision encompasses changes originating from other channels. The distribution of changes across different channels can be found in Figure 4.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/964/0*7Il9QJd_KJZcnBHp" /><figcaption>Figure 4: Distribution of changes pushed to production by channel</figcaption></figure><p>We tackled each remaining channel differently:</p><ul><li>Feature Flags were unified with Experiments to gain Safe Deploys capabilities</li><li>Content management systems were provided APIs to programmatically create experiments tied to content changes, and ramped with Safe Deploys</li><li>Code Deploys through Spinnaker ran Safe Deploys alongside pre-existing Automated Canary Analysis with each deploy</li></ul><p>(We considered infrastructure configs out of scope, since these lower level changes would require a fundamentally different approach to address.)</p><p>Code Deploys with Spinnaker account for an outsized majority of changes in production and required extensive work to enable. The next post in this series will cover how we achieved this through changes across traffic routing, service configs, and dynamically created configurations for Spinnaker.</p><p>Interested in working at Airbnb? Check out our <a href="https://careers.airbnb.com/">open roles</a>.</p><h3>Acknowledgements</h3><p>Safe Deploys was only made possible through the combined efforts across Airbnb’s infrastructure and data science teams. We would like to thank <a href="https://www.linkedin.com/in/jingwei-lu-5701222/">Jingwei Lu</a>, <a href="https://www.linkedin.com/in/wei-hou-93a069a4/">Wei Ho</a>, and the rest of the Stream Infrastructure team for helping implement, and subsequently scale the NRT pipeline. Also, thanks to Candace Zhang, <a href="https://www.linkedin.com/in/erikriverson/">Erik Iverson</a>, <a href="https://www.linkedin.com/in/minyong-lee-1a302466/">Minyong Lee</a>, Reid Andersen, <a href="https://www.linkedin.com/in/shant-torosean-606aa354/">Shant Toronsean</a>, <a href="https://www.linkedin.com/in/tatiana-xifara/">Tatiana Xifara</a>, and the many data scientists that helped build out metrics and verify their correctness. Also thanks to <a href="https://www.linkedin.com/in/kodnous/">Kate Odnus</a>, <a href="https://www.linkedin.com/in/kedar-bellare-3048128a/">Kedar Bellare</a> and <a href="https://www.linkedin.com/in/pmaccart/">Phil MacCart</a>, who were early adopters and provided us invaluable feedback. In addition <a href="https://www.linkedin.com/in/kocolosk/">Adam Kocoloski</a>, <a href="https://www.linkedin.com/in/rstata/">Raymie Stata</a> and <a href="https://www.linkedin.com/in/ronnyk/">Ronny Kohavi</a> for championing the effort across the company. We would also like to thank other members of the ERF team that contributed to Safe Deploys: <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/antoinecreux/">Antoine Creux</a>, <a href="https://www.linkedin.com/in/george-l-9b946655/">George Li</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, and <a href="https://www.linkedin.com/in/vincent-chan-70080423/">Vincent Chan</a>.</p><blockquote>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c83e94bfc52" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-c83e94bfc52">How Airbnb safeguards changes in production</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Veerabahu Chandran]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/70468aa3bc06</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[india]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Lauren Mackevich]]></dc:creator>
            <pubDate>Thu, 18 Aug 2022 18:50:01 GMT</pubDate>
            <atom:updated>2022-08-18T18:50:01.231Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Veerabahu Chandran</h3><p>Learning and growing in Airbnb’s new Bangalore Tech Center</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wwf3CMkjhKPlaxichQJd1g.jpeg" /></figure><p><em>Veera Chandran is an engineer in Airbnb’s new Bangalore Tech Center, where his team builds out technical systems to support hosts. As a lifelong learner, he has a passion for exploring new technologies and diving into practical problems. He’s excited to be tackling both the technical challenges of building new architecture and the organizational challenges of building out the capabilities of a new office.</em></p><p><em>Here’s Veera’s story:</em></p><h3>Learning and exploring</h3><p>I grew up in Tamil Nadu, in the South of India. I was always a curious kid, trying to understand how everything worked, so when it came to choosing a course of study, engineering was a natural fit. I feel lucky that I had a lot of education opportunities in front of me, and I was able to choose the path I wanted to take.</p><p>My first exposure to computers came when I was in 8th grade. These were still the relatively early days of the computer, and my dad brought one home so he could learn to use it. I found it fascinating and learned BASIC and Logo. These are simple languages, but I was excited that I could use them to make drawings and text appear on the screen. Those rudimentary programs opened me to the world of what’s possible with computer science.</p><h3>Studying and practical experience</h3><p>I went to the College of Engineering, Guindy to study Computer Science and Engineering. My studies covered a lot of subjects, but the one that excited me most was networking. I was really curious to understand how data moves from one place to another.</p><p>My first practical networking experience came while still in school. There were a bunch of gamers I knew, and they wanted to set up a LAN to play <em>Age of Empires</em> and <em>Quake </em>together. I got together with a couple of my friends and built out an inexpensive networking solution for them, covering everything from cabling to routers to setting up configurations. I’m actually not that big of a gamer myself, but building out a network was really exciting for me. I love to understand things on a practical level, because while theoretical understanding is important, I always find it most meaningful to see how things actually work.</p><h3>The power of engineering</h3><p>After graduation, I got a job in networking. There were several interesting companies in the space at the time, and I ended up joining one run by a group of IIT (Indian Institute of Technology) professors. It was a great opportunity to learn from some of the brightest minds in the field. I always tried to make sure I was learning, so I sought out whatever would help me continue to grow. Eventually, I moved on to opportunities at larger internet companies where I could use my networking knowledge but also expand into topics like large-scale, distributed systems.</p><p>Being a software engineer has always been exciting to me because it gives you the power to solve so many problems. When my daughter was born, my wife and I were looking for names that had to fit multiple constraints–e.g. it had to start with a <em>specific </em>letter–and it was a struggle to come up with viable options. As an engineer, I realized there was an easier way to find all our choices. I wrote a quick program that downloaded a list of millions of names and then ran them through our criteria. From that, I was able to narrow it down to a list of a few thousand options. My wife was amazed that I could generate so many names with just a couple hours of work.</p><p>The challenges of engineering are also interesting. You have to work hard to keep yourself informed. The industry moves so fast. When I started, I was using Java 4, and now we’re on Java 18. The way you would solve a problem in either of these versions is so different. All these newer languages have also emerged, and you can apply each to different situations. It feels like every day new machine learning research pushes the boundaries in unimaginable ways. I don’t know what’s going to come next, but I know it’s going to evolve quickly.</p><h3>Finding impact at Airbnb</h3><p>After a while in my previous role, I began to feel like my learning curve was getting saturated, so I wanted to look for a new challenge somewhere I could have a larger impact. I heard Airbnb was opening a tech center in Bangalore, and I was excited by the opportunity to be one of the first engineers there.</p><p>I joined in September 2021 as the first engineer in the Hosting org in Bangalore. I focus on tools for compliance, which is a complex problem space. Every region has their own laws on short-term rentals that hosts have to follow, and the laws can vary at different levels — the US has their laws, and then California might customize some of them, and San Francisco might have their own on top of that, and so on. These laws can also change quickly, like they did during Covid, so our products need to be versatile and adapt to new conditions.</p><p>Airbnb has been a great place to work. It’s startup-y in that there are challenging technical problems to work on, but the job is stable and the company respects your work-life balance. As a technical leader, there’s a great opportunity to be part of the evolution of our technology roadmap. The architecture recently transitioned from a monorepo to a Service Oriented Architecture, so we’re still figuring out the best approaches for the problems we’re solving.</p><p>I’ve also appreciated Airbnb’s culture, especially the focus on inclusivity and belonging. My coworkers want to make everyone feel comfortable. When they introduced themselves to me, they all included their pronouns, making it easier for anyone else to share theirs. The people here live the culture and make everyone feel included.</p><h3>Building our office in Bangalore</h3><p>One of my favorite things about working as an engineer in the Bangalore office is the ownership and accountability. We’re not just a delivery center, where we’re being passed requirements from elsewhere and building that one piece of software. We like to call ourselves a capability center. Our team is tasked with the whole span of product development, from identifying what user problems exist all the way through to delivering a solution for them. We work on the same roadmap, codebase, and tech stack as Airbnb HQ.</p><p>Our team is growing quickly, both in Bangalore and remotely across India. With the team being spread out, trust and team-building have been important. We have a social meeting every Friday, and the whole team shows up so we can get to know one another. It’s great for connecting with teammates, and the trust we’re creating helps us build more successful products.</p><p>Airbnb leadership has a clear roadmap for the future of the Bangalore Tech Center, and the team is growing quickly. It’s been exciting to build our first tech center outside of headquarters. We’re hiring for a number of teams and we’d love to hear from you!</p><p>Check out these related roles based out of Bangalore:</p><p><a href="https://grnh.se/777f0dbd1us">Engineering Manager, Ambassador Platform Products</a></p><p><a href="https://grnh.se/9b78e7f21us">Staff Software Engineer, Ambassador Platforms</a></p><p><a href="https://grnh.se/e0c9d3761us">Manager, BizTech</a></p><p><a href="https://grnh.se/d43963981us">Senior Software Engineer, Cities</a></p><p><a href="https://grnh.se/6a500ddd1us">Sr. Analytics &amp; Insight Analyst: CSA</a></p><p><a href="https://grnh.se/b6de7b661us">Operations Engineer, Biztech</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=70468aa3bc06" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-veerabahu-chandran-70468aa3bc06">My Journey to Airbnb — Veerabahu Chandran</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sisyphus and the CVE Feed: Vulnerability Management at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/sisyphus-and-the-cve-feed-vulnerability-management-at-scale-e2749f86a7a4?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e2749f86a7a4</guid>
            <category><![CDATA[scalability]]></category>
            <category><![CDATA[vulnerability-management]]></category>
            <category><![CDATA[security]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[airbnb]]></category>
            <dc:creator><![CDATA[Keziah Plattner]]></dc:creator>
            <pubDate>Wed, 10 Aug 2022 17:05:15 GMT</pubDate>
            <atom:updated>2022-08-10T17:05:15.891Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*McONkEbdjlQOssDo" /></figure><p><strong>Authors<br></strong><a href="https://www.linkedin.com/in/keziahsonderplattner">Keziah Perez Sonder Plattner</a>, Senior Software Engineer<br><a href="https://www.linkedin.com/in/kadia-m-58a18328">Kadia Mashal</a>, Engineering Manager</p><h3>Introduction</h3><p>Every engineer knows that security is a never-ending problem. Until we delete all our code and move into a cottage in the woods, we have to accept that there is no such thing as 100% secure software. You could be doing everything perfectly, and a publicly known vulnerability (<a href="https://www.redhat.com/topics/security/what-is-cve">CVE</a>) could emerge for the most updated version of a third party library in your infrastructure. Things are secure until they are not. Like with Sisyphus, the boulder will never reach the top of the hill.</p><p>Rather than eliminating vulnerabilities, the goal of a vulnerability management program should be to quickly and effectively detect and respond to the barrage of threats that surface every day. There are many scanners and vendor tools that purport to solve the problem. But with the scanners comes the problem of a never-ending flood of CVE reports, thus slowing down our ability to remediate in a timely manner.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/416/0*m80-F_w_BxVFZo0_" /></figure><h3>Vulnerability Management Lifecycle</h3><p>If you are new to vulnerability management, here are the basics of the lifecycle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UxDN236N9MBuYvDh" /><figcaption><em>Fig. 1: The Vulnerability Management Lifecycle</em></figcaption></figure><p><strong>Detection</strong></p><p>Find potential vulnerabilities in our infrastructure, anywhere from CVEs to insecure misconfigurations.</p><p><strong>Risk Assessment</strong></p><p>Apply a risk framework to the findings to identify true positives and weed out non-applicable vulnerabilities.</p><p><strong>Reporting</strong></p><p>Find the team and/or person best suited to address it and track progress in a methodical way. In addition, centrally track all vulnerabilities in order to have a full view of our attack surface.</p><p><strong>Remediation and Prevention</strong></p><p>Promptly remediate the vulnerability and invest in work to prevent the vulnerability from being introduced in the first place.</p><h3>Objectives</h3><p>In building a vulnerability management program, we want to focus on the following:</p><ul><li><strong>Visualize Known Attack Surface</strong>: We can’t properly assess risk if we don’t know our vulnerability status in the first place.</li><li><strong>Speed</strong>: Detection, reporting, and remediation should be completed in a timely manner.</li><li><strong>Prioritization</strong>: Focus on the highest-priority vulnerabilities before tackling less important or harder to exploit ones.</li><li><strong>Scaling</strong>: Support a constantly evolving and expanding cloud infrastructure.</li></ul><h3>Gaps and Challenges with Standard Industry Solutions</h3><p>Standard industry advice leans on out-of-the-box vendor deployments, manual risk assessment, and operationally-heavy reporting processes. Automation, if it exists, relies on limited vendor functionality without the flexibility to adjust to unique attributes of our environment. This led to major challenges in accomplishing our objectives.</p><p><strong>Lack of Vendor Agnostic Solutions</strong></p><p>As our infrastructure expands, the number of vulnerability types we want to track grows along with it. A variety of scanning solutions are needed to cover our bases, but they come with different setups and reporting processes. And there may be future scanning solutions that will work better for us, so we don’t want to lock ourselves into a single vendor or solution.</p><p><strong>Noisy and Inaccurate Severity Ratings</strong></p><p>In our experience, the majority of scanners provide inaccurate risk scoring. Vulnerability bulletins and assessments like the basic Common Vulnerability Scoring System (CVSS) may describe a worst-case scenario that is difficult to exploit or sensationalize an issue, leading to inflated severity. And when a major zero-day vulnerability is found, it’s more likely to be identified by an anonymous Twitter user than a scanner.</p><p>Additionally, internal mitigations can lower the impact of a vulnerability, and generic scanners rarely have ways to add internal context to customize risk ratings. Asset information and the location of the vulnerability play a massive role in determining its severity.</p><p><strong>Operational Work</strong></p><p>Many vulnerability management solutions assume the need for human intervention in the process. However, humans make mistakes, and every manual step leads to slower remediation times. Spending time on onerous tasks like manually assessing risk severity or making tickets takes away from our time to focus on root-cause remediation.</p><h3>Guiding Principles</h3><p>Before developing our solution, we wanted to establish guiding principles. While there will always be exceptions, our goal is to keep this as our north star.</p><p><strong>Limit the need for human intervention by reducing false positives. </strong><br>It can be tempting to design a solution that catches close to 100% of true positives.</p><p>However, when maximizing true positives, it’s inevitable that the false positive rate will increase too. False positives create onerous manual work for both the security team and the owning engineering team. We don’t want to “cry wolf” on vulnerabilities that are not worth addressing. Doing so is both unscalable and breaks trust in the security org. Not to mention that relying on manual triage to verify alert severity slows down response time, leaving vulnerabilities open for longer.</p><p>In addition, the idea that manually checking vulnerabilities will result in higher accuracy doesn’t take into account human error and alert fatigue. No human or automated process will ever get 100% true positive accuracy, and that knowledge must be built into the solution instead of fighting a losing battle against the barrage of noisy alerts.</p><p><strong>Pair detection with preventative measures.</strong><br>Now that we have established that true positives will slip through the cracks, we need to address how to handle those cases. We pair our detection and reporting workflows with preventative solutions that address the root cause of the vulnerabilities. For example, if we make sure to have a regular patch cadence, then all vulnerabilities–including lower-priority ones–will be addressed within a reasonable timeframe. If we fix a flawed design, we will reduce the number of vulnerabilities in the first place.</p><p><strong>Build relationships.</strong></p><p>All the automation in the world won’t be useful if we antagonize other engineering teams, rather than empowering and incentivizing them to remediate problems. Developer productivity matters — we don’t want to create an onerous system that frustrates developers and makes security the enemy. We want to avoid blocking solutions unless the priority calls for it, and we want to focus engineering efforts on our highest priority gaps rather than spreading engineers thin across many low and medium level vulnerabilities. Of course, there are always exceptions here–sometimes a vulnerability is severe enough to require being paged in the middle of the night. But we want to be sparing with that approach.</p><p>It helps to get teams on board by pairing security fixes with other benefits, or working it into existing workflows so the process is essentially invisible to outside engineers.</p><p>We have tried top-down solutions in the past, but nothing has been as effective as treating engineering teams as our partners and seeking their input as stakeholders, making it a mutually beneficial process.</p><p><strong>Maintain Accountability</strong>.<br>Detecting vulnerabilities doesn’t help if there is no accountability to fix them. We want to make sure they are correctly attributed to business units so that leadership is held responsible for fixing vulnerabilities. Keeping business units accountable means that they will be incentivized to allocate resources towards the problem instead of keeping security issues on the backburner.</p><p>There should be a company-wide Service-Level Agreement (SLA) based on the severity of the vulnerability that is part of the success metrics for each org. If an SLA cannot be met, we offer automated SLA extension requests to allow teams to make adjustments. We avoid exceptions except when absolutely necessary and periodically review the reasoning.</p><h3>Building an Automated, Vendor-Agnostic Vulnerability Management Pipeline</h3><p>Given the challenges, the standard industry advice just didn’t work for our use case. So, we decided to create our own engineering solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*G2KcgicUKe7FhslT" /><figcaption><em>Fig. 2: Automated Vulnerability Pipeline</em></figcaption></figure><h4>Step 1: Aggregate and Process Vulnerabilities</h4><p>First, we turned the barrage of alerts from our scanners into a standardized format, centralized in a single place, instead of having each scanning tool siloed from the others.</p><p>In order to cleanly track vulnerabilities throughout the pipeline, we have developed a UUID generating process for every vulnerability type we encounter. This UUID is mappable to the vulnerability and the asset it is found in. This can change per vulnerability type–for example, for our third-party packages, we track vulnerabilities by <em>asset</em> + <em>package name</em> + <em>package version.</em> It is less important to individually track every CVE present in the asset, given that fixing a package version will address every related CVE. When we detect that an asset is no longer using a specific package or version, we can feel confident that the vulnerability is no longer present. And unifying multiple CVEs into one UUID is easier to manage.</p><p>This also helps when scanners have some overlap. Instead of using the limited vendor features directly, we leverage their APIs to ingest the results so that we can process the alerts according to our needs, and allow us to deduplicate repetitive alerts and verify fixes. We can then combine results as needed or add additional information.</p><h4>Step 2: Contextualize Risk</h4><p>Our next step was to automate risk assessment by taking the default severity calculation provided by the scanner and integrating additional context.</p><p>All companies have different infrastructure setups and mitigation strategies. For example, vulnerabilities involving DDOS aren’t as impactful if the load balancers have existing mitigations. On the other hand, a vulnerability in an application that handles PII can be much more severe than initially assessed by a scanner.</p><p>To improve the accuracy of our risk assessment, we take into account the following:</p><ul><li><strong>Internal mitigations</strong>: Certain vulnerability types may not be relevant in our infrastructure.</li><li><strong>Common Vulnerability Scoring System (CVSS) vector</strong>: The type of exploit is important. For example, if the attack requires local privileges and user interaction, the odds of exploitation are significantly lower, even if the impact is severe. Breaking down the CVSS base score by attack vector allowed us to gain a better understanding of the vulnerability risk.</li><li><strong>Asset risk</strong>: Is the asset public facing? Or is it an internal service that handles low-priority metadata? Does it work with PII? How critical is it to production flows?</li><li><strong>Multiple external scoring systems and metadata</strong>: Is there a difference between the National Vulnerability Database (NVD) rating, the Red Hat rating, and the vendor rating? Is this a package that hasn’t been updated in 3 years? Is this an old, unmaintained third-party package?</li></ul><p>It’s critical that this step be automated as much as possible. We don’t want to have engineers reading through every CVE guide to determine how severe a vulnerability is.</p><p>Depending on a company’s situation, tracking asset risk context may require engineering resourcing from both the security org and other engineering teams. This could involve engineering work to gather metadata from the codebase or cloud infra provider, or sorting through large existing datasets and compiling the most useful information for reference.</p><p>While that information may not always be available depending on the stage of the company, keeping track of asset information is a long-term investment that will pay out in dividends as a security program matures.</p><p>There’s no need to wait until all the relevant information is available–even a partial dataset is helpful to start, and the risk algorithm can be tuned as more data comes in. For example, in a microservice architecture, owners could fill out a yml file with attributes of the service on creation, and eventually graduate to more automated evaluation of service metadata.</p><h4>Step 3: Reporting and Remediation</h4><p>Once we’d standardized the format of the vulnerabilities and tracked them via UUIDs, we created a generic reporting service. Shared logic for ticket creation, closing, and metadata tracking is handled in the service so that we don’t need to constantly reinvent the wheel. We will go into more detail on the reporting service in the <em>Implementation</em> section.</p><h4>Step 4. Verification</h4><p>Once the vulnerability has been marked as fixed by the owner, we want to programmatically verify that it is truly gone. As our guiding principle states, humans are always the weak link in a process. Anyone can mistakenly close a ticket as fixed, and we can’t have 100% confidence unless we verify that the vulnerability is truly gone.</p><p>For scanners that report state daily, once a UUID is no longer present, we can mark the vulnerability as verified. For more complicated ones, we can write separate jobs that pass the UUID status to the reporting service so tickets can be closed and/or verified. For example, if we are tracking a vulnerability ad-hoc, we can collect information from deployment pipelines to ensure that the patch has been successfully deployed.</p><h3>Implementation and Scaling</h3><p>We want to share our specific implementation for our pipeline, however, it is worth noting that there are many different technologies that could be leveraged to handle similar logic.</p><h4>Detection</h4><p>Vulnerability scanners can be performance heavy. Moreover, deploying many vendor solutions and/or agents can increase an organization’s attack surface. It is key to understand what data the vulnerability scanner will provide and consider if an already rolled-out agent (i.e. Osquery, AWS Systems Manager Agent (SSM), etc) could provide similar output. So aside from a few necessary traditional vendor solutions, we primarily leveraged agents that were already being used for other purposes to identify security vulnerabilities.</p><h4>Airflow</h4><p>Our process is primarily implemented in <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">Airflow</a>, an open-source data processing tool released by Airbnb a few years ago. Airflow allows us to create scheduled jobs with upstream and downstream dependency management. Datasets can be tracked on any timeframe, and it is easy to backfill or rerun jobs on specific dates (in contrast to a standard cronjob).</p><p>The steps are implemented in <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html">Directed Acyclic Graphs (DAGs)</a> that can be chained together. We use this to create a data processing pipeline to ingest the alerts and process them as mentioned in our pipeline explanation. Shared data like internal risk context, ingested vulnerability feeds, and NVD/Red Hat scoring criteria can be easily accessed at any point of the pipeline for writing contextualized risk logic.</p><h4>Reporting Service</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*J5vHCbaIQTgs_5sN" /><figcaption><em>Fig. 3: Reporting Service Logic</em></figcaption></figure><p>Now that we had the process for a single type of vulnerability, we wanted to be able to easily scale for any new type of vulnerability that we start tracking.</p><p>This is where the importance of the reporting service comes in. Our reporting service doesn’t care about the details of the vulnerability. All it needs is the data stored in a table with the expected schema and the client-provided callbacks to create the ticket copy.</p><p>The modular nature of Airflow DAGs and the reporting service makes it simple to add a new vulnerability source into our systems. Our team doesn’t have to be responsible for writing every ingest / risk assessment process, and external teams using our pipeline don’t have to worry about managing the shared vulnerability tracking logic. Shared metadata can be reused over and over again, so things like risk assessment logic get easier every time.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gqspSTfgjHG6PCSj" /><figcaption><em>Fig. 4: How we scale our reporting service for any number of alert types.</em></figcaption></figure><h3>Results</h3><h4>Scaling</h4><p>Before we standardized on this system, the vulnerability management team had to be much more involved in the vulnerability tracking process, creating a bottleneck. It was difficult to have a different team own a specific kind of scanner or vulnerability type, as we had to be deeply involved in how it was tracked.</p><p>After we rolled out this process, the number of vulnerabilities we were able to manage increased dramatically. <a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-3-34e592c45d46">Multiple security teams were able to integrate with our pipeline for their own purposes</a>, and unifying the functionality across our org allowed us to automatically get metrics that give us insight into our full attack surface.</p><h4>Operational Work</h4><p>Our work with contextualizing risk also made an enormous difference in managing the operational work around risk assessment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/455/0*rNTgLOG3QUSPktNn" /><figcaption><em>Fig. 5: False positive rate over time</em></figcaption></figure><p>For example, when we first deployed a new scanner, a large percentage of the alerts were false positives. We spent a lot of time going through the tickets daily to filter out the noise and identify the highest priority CVEs. Over the course of several months, we tuned our risk assessment algorithm to take in different kinds of criteria aside from the default severity score provided by our scanner. Now, we only occasionally have to manually review tickets to validate severity (primarily criticals, which can be difficult to distinguish from highs), and we trust that alerts are most likely accurate.</p><h4>Case Study: log4shell</h4><p>A side effect of having a scalable vulnerability management system is that it is significantly easier to react quickly during critical incidents.</p><p>Like the rest of the internet, we had to act fast when the log4j vulnerability occurred. Several years ago, the work would have been both operationally and programmatically difficult, and would require significant time and resources. However, our new pipeline allowed us to respond much more quickly. We simply wrote a new DAG to track all services running Java and passed it into the reporting service. While our engineers were busy patching the services, we wrote a second DAG programmatically detecting if a service had been patched. We then were able to confidently verify the status of a service, while reopening tickets for incorrectly fixed ones.</p><h3>Takeaways and Suggestions</h3><p>Vulnerability management is a hard problem to solve and getting to a solution that works best in a custom environment takes time. It is key for organizations to prioritize the problem space using automation to allow them to quickly address known attack surfaces.</p><p>Vulnerability management should be treated as an engineering problem, not as an operational problem. If you have not yet adopted this approach, hopefully the benefits we have described convince you to take steps towards this goal. And just like every engineering solution, you will learn to adjust your approach as you gather more datasets and metrics about your environment.</p><p>On top of detection and metrics tracking, it’s important to prioritize automation to address vulnerability root causes because metrics alone will not reduce attack surface. Prevention is always better than remediation.</p><p>Lastly, be creative with your solutions. Survey your existing tools, even ones not specifically geared towards security, to see if they can provide vulnerability insights. Your vulnerability management automation pipeline should be modular and vendor-agnostic to provide flexibility to incorporate all available data sources. The more you can reuse existing tools, the less additional attack surface you’ll need to maintain while still providing valuable signal.</p><h3>Acknowledgements</h3><p>Thanks to Deanna Bjorkquist who has helped drive the Vulnerability Management program and automation requirements. Thanks to Derek Wang for code excellence and feature expansion. Thanks to Christopher Barcellos for reviewing and providing feedback for our blog post. Thanks to Tina Nguyen for helping drive and make this blog post possible. Thanks to Mark Vlcek for his work on some of our scanning solutions. Thanks to the internal Airbnb Airflow team for their technology support.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2749f86a7a4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/sisyphus-and-the-cve-feed-vulnerability-management-at-scale-e2749f86a7a4">Sisyphus and the CVE Feed: Vulnerability Management at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Airbnb’s Approach to Access Management at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/airbnbs-approach-to-access-management-at-scale-cfa66c32f03c?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/cfa66c32f03c</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[access-control]]></category>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[authorization]]></category>
            <category><![CDATA[information-security]]></category>
            <dc:creator><![CDATA[Paul Bramsen]]></dc:creator>
            <pubDate>Mon, 08 Aug 2022 17:02:55 GMT</pubDate>
            <atom:updated>2022-08-18T17:00:00.023Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>How Airbnb securely manages permissions for our large team of employees, contractors, and call center staff.</strong></p><p><strong>By:</strong> <a href="https://www.linkedin.com/in/paul-bramsen-9a98638b/">Paul Bramsen</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l86VJi1iVOnE7ngb" /></figure><h3>Introduction</h3><p>Airbnb is a company that is built on trust. An important piece of this trust comes from protecting the data that our guests and hosts have shared with us. One of the ways we do this is by following the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least privilege</a>. Least privilege dictates that–in an ideal world–an employee has the exact permissions they need at the moment their job requires them. Nothing more, nothing less. Anything more introduces unnecessary risk–whether from a malicious employee, compromised laptop, or even just an honest mistake. Anything less inhibits productivity.</p><p>Not only has enforcing least privilege always been crucial for maintaining trust, it’s rapidly becoming a legal necessity. Airbnb operates in <a href="https://news.airbnb.com/about-us/">almost every country and region in the world</a> necessitating that we comply with an ever increasing set of data privacy regulations.</p><p>Administrators can effectively solve these problems with minimal tooling in small companies when an individual can track the work of all colleagues. But as a company grows, this approach does not scale. In this post, we will explain how Airbnb uses a novel software solution to maintain least privilege while enabling our large team of employees, contractors, and call center agents to do our jobs effectively and efficiently.</p><h3>Where We Started</h3><p>In Airbnb’s early days a combination of homegrown and vendor solutions were implemented, but the lack of a unifying architecture prevented us from scaling. The hodge-podge of systems used to control access made it difficult to hit either of our least privilege goals:</p><ul><li>It was often unclear where employees could get needed permissions, hampering productivity.</li><li>Projects aimed at reducing unnecessary access (i.e., drive least privilege) required significant effort across many systems. Integrating access control with a new system took months of engineering effort when it should have been one or two days.</li></ul><p>Ultimately, these factors led to growing operational burden, reduced security, and increased hours required for compliance efforts. This led us to the following conclusion: <strong>we need a single place to manage employee access</strong>.</p><h3>Clarifying Focus</h3><p>Having determined the need for centralized access control, we worked to set guiding principles for the solution we would implement. Ultimately, we boiled down the requirements for our system to two goals:</p><ol><li>The access control system should manage the entirety of the processes and logic around a permission’s lifecycle. This includes:<br>– Self-serve ways to request or revoke permissions.<br>– Settings to control who has to approve new permissions.<br>– Tools for managing groups of permissions.<br>– Settings for automatic permission expiration.<br>– Logging to meet operational and compliance requirements.<br>– Notifications about relevant permission updates like upcoming expirations or when an approval is required.<br>All of these features should be controlled declaratively for each available permission and the system should use these declarations to implement all necessary logic and actions.</li><li>We wanted to build a system that could easily and robustly integrate with any permission store (e.g., AWS IAM, LDAP, <a href="https://ranger.apache.org/">Apache Ranger</a>, MySQL, <a href="https://medium.com/airbnb-engineering/himeji-a-scalable-centralized-system-for-authorization-at-airbnb-341664924574">Himeji</a>, etc) without the need to modify it. To use a network analogy, the permission stores would be the data plane that enforces authorization while our access control system is the control plane that coordinates everything. This requirement led us to focus on providing the interface needed to efficiently synchronize permission changes from the central access control system into the permission stores. This would be accomplished using a little glue code for each store (allowing us to maintain the generality of the central system).</li></ol><p>We also clarified what the system would <em>not</em> be.</p><ul><li><strong>Not</strong> a hyper-reliable, hyper-low-latency way to answer online permission checks. The permission stores themselves would answer online authorization queries and, because we would sync with them, they could automatically act as a cache if the central access control system went down. While availability and performance are always important, our primary focus would be on the permission management logic.</li><li><strong>Not</strong> a place to dump one-off authorization code. Some of the prior permission management systems had evolved into authorization code dumping grounds incurring significant technical debt.</li><li><strong>Not</strong> a place to store permissions for our guests and hosts. Public product permission management requirements are generally quite different from permissions we grant to employees to access our internal tooling and data to do their jobs. The scales also generally differ by many orders of magnitude. Additionally, internal permissions are usually significantly more complex. So it makes sense to handle each case separately.</li></ul><p>If you could only take one thing away from this post, take away these goals. Clarifying our focus and using these two goals as our north star was the most critical step in building our centralized access control platform.</p><h3>Build Vs Buy</h3><p>We evaluated a number of products on the market but none of them solved for our specific goals. Generally, permissions were managed by a small group of knowledgeable administrators, operationalizing the approval process and failing our first goal. Additionally, integrations usually required modifying the client. While some of the permission stores already had plugins (e.g., LDAP plugin), others did not.</p><p>We hope that eventually a startup builds a solution that implements a centralized, self-serve, easy-to-plug-in model. We think this could provide a lot of value to other companies that don’t have the scale to justify building an in-house solution like ours.</p><h3>Architecture</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*L-FvRQ0fyPLiIS-LcYQiwg.png" /><figcaption>Each stage makes requests to the prior stage as updates flow through the system from left to right. Note that for the purposes of this post we are only considering stages 2 and 3. We can assume stages 1, 4, and 5 already exist.</figcaption></figure><p>We designed a system with a linear five-stage architecture. Changes flow from left to right. The architecture is linear in the sense that each stage can query the previous stage, but no others. For example the Access Control Platform can query Employee Data Systems and can be queried by Connectors, but never communicates directly with Permission Stores.</p><p>A stage can also have limited communication with the immediately following stage through loosely coupled channels like queues or callbacks. For example the Access Control Platform can enqueue an update message that will be consumed by a Connector to trigger a permission update.</p><ol><li><strong>Employee Data Systems<br></strong>These are the HR systems (e.g., LDAP) that contain employee data (e.g., title, location, status, management chain). The Access Control Platform ingests this data to enable features like dynamic groups based on title and approval flows based on management chain. These systems are owned by the IT team.</li><li><strong>Access Control Platform<br></strong>This is the core system. This includes all the business logic to manage permissions as well as the UI that employees use to make and/or approve changes. The Access Control Platform is highly configurable but does not directly interact with any permission stores that integrate with it. The security team owns this system.</li><li><strong>Connectors<br></strong>Connectors are the glue code that connects the Access Control Platform to the Permission Stores. They serve two purposes. First, connectors tell the Access Control Platform what permissions should be available for request. For example the data warehouse connector might make read access to the users and reservations Hive tables available for request. Secondly, if user bob received access to read reservations the data warehouse connector would synchronize this permission into the appropriate permission store — <a href="https://ranger.apache.org/">Apache Ranger</a> in this case. Since connectors are simply responding to messages on a queue by making the appropriate API calls, they can run in whatever environment their owner deems best (e.g., Kubernetes job, AWS Lambda, Airflow DAG). They are owned and operated by the team that owns the corresponding permission store. For example the storage team owns the MySQL connector.</li><li><strong>Permission Stores<br></strong>Permission stores are the systems that store the permissions and answer permission queries — for example, AWS IAM, LDAP, <a href="https://ranger.apache.org/">Apache Ranger</a>, MySQL’s built in permission system, <a href="https://medium.com/airbnb-engineering/himeji-a-scalable-centralized-system-for-authorization-at-airbnb-341664924574">Himeji</a>, or other internal systems. Note that in some cases Permission Stores may be built into clients in which case stages 4 and 5 would be combined, as is the case for MySQL.</li><li><strong>Clients<br></strong>The clients are all the systems that the end user needs — for example, SSH, Apache Superset, MySQL, internal customer support tools, Salesforce, etc.</li></ol><h3>Benefits Realized</h3><p>Two years ago we implemented this architecture and since then we’ve integrated many systems into this centralized Access Control Platform. Here we highlight a few of the benefits we’ve realized.</p><h4>Security</h4><p>One of the biggest wins for security is having a single place where we can implement new least privilege features and then apply them across the board (as opposed to implementing once for AWS, once for MySQL, once for SSH, etc). A great case study is usage-based expiration. Usage-based expiration is a feature where permissions that have not been used for a significant period of time are automatically revoked. This approach is good for security because unnecessary permissions are quickly cleaned up. But it is also good for the user experience because employees can rest assured that the permissions being removed aren’t the ones they use regularly. Before the revocations happen, the Access Control Platform notifies impacted employees about the upcoming change and provides instructions on what to do if they need to keep the permissions. The notifications also provide links to low-friction ways to get the permissions back after the revokes happen should they realize later that the permissions were needed.</p><p>The chart below shows relative change in users with access to a core production system we’ll call System X. After rolling out usage-based expiration at the end of April, users with access to System X hit a steady state of about one third peak. A significant least privilege win! We saw similar results in other systems where we rolled out usage-based expiration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/786/0*ahfDNc4H4XLmdXfF" /><figcaption>Users with System X access dropped by two thirds after enabling usage-based expiration in late April.</figcaption></figure><p>Another security benefit has been the ability to roll out consistent compliance changes across all systems as new regulations are introduced. For example, we could enable a rule that requires North American employees to get special approval from our European legal counsel in order to access certain protected data for European customers. This rule can be consistently applied across many systems such as online databases, offline datastores, and customer support tooling.</p><p>Another win has been having a centralized database, against which we can create cross-system least privilege metrics and track our progress over time. The chart above was generated using this database.</p><h4>Usability</h4><p>Having a centralized access control platform has been a big win for usability. By consolidating, users no longer need to be aware of the N different places they need to go to request access. Effectively we’ve been able to create a one-stop-shop for all access at Airbnb. Just search for what you need access to and we’ll guide you through the rest.</p><p>The self-serve features we’ve built into the platform have helped reduce operational overhead. Employees can request permissions without having to involve a support engineer. When a manager goes out of town they can delegate a peer to approve changes on their behalf. Users have self-serve revoke for their own permissions, their reports’ permissions, or permissions for systems they manage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/786/0*RItgoPuQHokQDkup" /><figcaption>Providing good self-serve access control tooling has significantly cut support costs.</figcaption></figure><h4>Developer Experience</h4><p>We’ve put significant effort into making it as easy as possible for developers to build the connectors that link the Access Control Platform with permission stores. A large portion of this effort has been building great tools.</p><p>As an example, a design decision we made that has proved extremely useful in providing a strong developer experience is notifying connectors about changes via an asynchronous message queue. Whenever a permission’s state changes, the Access Control Platform sends a message to the queue. The queue is processed by the connector that’s responsible for syncing the state of the updated permission into the permission store.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*Yj_z9ZaQScTCHMKN" /><figcaption>The permission’s state (granted or revoked) has to be fetched from the Access Control Platform. It is not included in the enqueued message.</figcaption></figure><p>The contents of the message are a critical part of the design. The message contains what the permission is and who it is for, but <em>not</em> whether the permission was granted or revoked. To get the current state (granted / revoked), the connector must query the platform. You can think of the message as a trigger to cause the system to resync permission X for user Y.</p><p>This design has the following properties:</p><ul><li>Because the latest state is always fetched (granted / revoked), message processing is idempotent.</li><li>This allows us to use at-least-once delivery semantics, greatly simplifying the process of ensuring that the proper messages are sent every time a permission changes. If a permission changes but the process is killed (perhaps due to a deploy) before we’ve recorded that the platform triggered the necessary update message, we just trigger the message again in a clean-up process.</li><li>Replay attacks are nullified. So we let connector developers freely enqueue messages to aid in debugging. As a connector developer, this is quite useful when trying to determine why a permission sync is failing.</li><li>If updates do fail too many times, the message goes to a dead letter queue and the team responsible for the connector is alerted. Developers then use our tools to read the messages from the dead letter queue and debug the failing updates. Once issues are fixed, all failed messages can be re-enqueued which will bring all permissions back in sync.</li><li>We run regular offline jobs to do bulk permission diffs and identify any permission changes that need backfilling. Then we trigger resyncs by enqueuing update messages for these permissions. This means that connector developers only need to write code to support incremental sync rather than both backfill and incremental sync. The backfills are free!</li></ul><h3>Conclusion</h3><p>Managing permissions and ensuring least privilege is a challenge at any company and especially difficult in large companies. Many companies come up with operationally heavy solutions that are expensive, insecure, and provide a negative user experience. At Airbnb, we’ve solved this challenge by implementing a centralized, self-serve access control platform. What made our investments such a success was solving Airbnb’s unique goals in a cohesive and scalable way, and what is very rare is the degree to which we’ve actually rolled this out in production. The majority of permissions at Airbnb are managed by our Access Control Platform. Our approach has enabled us to make huge strides in ensuring that we’re doing everything we can to keep our community’s data safe while at the same time enabling Airbnb’s employees to do our best work.</p><p>We’ve made a lot of progress in the access management space, but there is still a lot to do! If you’re interested in working on this or other efforts to protect Airbnb’s community, check out security and software engineering jobs on <a href="https://careers.airbnb.com/">our careers page</a>.</p><h3>Acknowledgments</h3><p>The Access Control Platform we’ve built was the result of hard work from many collaborators at Airbnb. <a href="https://www.linkedin.com/in/zhusamuel/">Samuel Zhu</a>, <a href="https://www.linkedin.com/in/alissara-rojanapairat/">Alissara Rojanapairat</a>, <a href="https://www.linkedin.com/in/kyler-mejia-a9a323101/">Kyler Mejia</a>, <a href="https://www.linkedin.com/in/stephynancy/">Stephy Nancy</a>, and Maryna Butovych built significant portions of the system and contributed to the architecture. <a href="https://www.linkedin.com/in/alanyao/">Alan Yao</a> and <a href="https://www.linkedin.com/in/abhishek-parmar-924b529a/">Abhishek Parmar</a> provided invaluable feedback that influenced the architecture. <a href="https://www.linkedin.com/in/julia-k-cline/">Julia Cline</a> ensured that we were building a product that would meet the needs of our customers. <a href="https://www.linkedin.com/in/brettbukowski/">Brett Bukowski</a>, Jacqui Watts, <a href="https://www.linkedin.com/in/julia-k-cline/">Julia Cline</a>, <a href="https://www.linkedin.com/in/patmoynahan/">Pat Moynahan</a>, and <a href="https://www.linkedin.com/in/chris408/">Christopher B</a> provided valuable feedback on this blog post. <a href="https://www.linkedin.com/in/tinamn/">Tina Nguyen</a> and <a href="https://www.linkedin.com/in/laurenmackevich/">Lauren Mackevich</a> shepherded this blog post through the process. And many other colleagues contributed in small and large ways to make this possible.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cfa66c32f03c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/airbnbs-approach-to-access-management-at-scale-cfa66c32f03c">Airbnb’s Approach to Access Management at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Incident Management]]></title>
            <link>https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/ae863dc5d47f</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[incident-management]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[reliability-engineering]]></category>
            <category><![CDATA[automation]]></category>
            <dc:creator><![CDATA[Vlad Vassiliouk]]></dc:creator>
            <pubDate>Wed, 27 Jul 2022 16:39:43 GMT</pubDate>
            <atom:updated>2022-07-27T16:39:43.843Z</atom:updated>
            <content:encoded><![CDATA[<h3>Automated Incident Management Through Slack</h3><p>How Airbnb automates incident management in a world of complex, rapidly evolving ensemble of microservices.</p><p><a href="https://www.linkedin.com/in/vladimir-vassiliouk">Vlad Vassiliouk</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hP8PSrLw_LTyLjhbR6LRxg.jpeg" /></figure><h3>Incident Management</h3><p>Incidents are unforeseeable events that disrupt normal business operations and are inevitable in complex systems that must be up and running 24/7. This is why it’s important to prepare and to train people to handle incidents in a timely and organized manner. Although each incident is unique, we follow the same procedure for detection, escalation, management, and resolution of incidents.</p><p>At Airbnb, we utilize a <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> which involves many interconnected services managed by small teams. Quickly figuring out what service is in trouble, and who to page is paramount to timely incident resolution. We found that our teams spent a lot of time switching between applications such as Slack, Pagerduty and Jira to raise an incident, page responders, and provide context. In order to have quick resolutions of incidents, we developed an incident management bot, a centralized automation tool for incident management.</p><h3>Incident Management Slack bot</h3><p>Our goal was to centralize incident management in Slack. Everyone at Airbnb is familiar with and has access to Slack, and it’s easy to bring people and resources together in an incident channel. In addition, the incident channel acts like a timeline of events which makes putting together a post mortem report easy.</p><p>Our requirements were as follows:</p><ul><li>Run in Airbnb’s <a href="https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c">service oriented infrastructure</a> and have full support from our team.</li><li>Standardize incident-related communications in all tools such as Jira, Slack, PagerDuty.</li><li>Centralize incident management in Slack.</li><li>Single intake funnel for incidents with clearly defined steps.</li><li>Automate post-incident tasks such as setting up meetings and archiving channels.</li><li>Provide incident timelines and metrics.</li></ul><p>We decided to build our own app to meet our exact specifications and allow us to easily customize and develop further. We also chose to build the app in Golang, because of the great community, and their well documented <a href="https://pkg.go.dev/github.com/slack-go/slack">slack library</a>.</p><p>Finally, we decided to use chat commands instead of <a href="https://slack.com/help/articles/201259356-Slash-commands-in-Slack">slash commands</a> so that all commands sent to the bot would be visible to the members of the Slack channel.</p><p>Our incident management bot achieves incident response automation through four key commands:</p><ul><li><strong>new incident &lt;summary&gt;: </strong>Create a Jira ticket and page incident managers.</li><li><strong>new channel &lt;ticket&gt;: </strong>Create an incident Slack channel for an open incident ticket.</li><li><strong>page &lt;service|user&gt;: </strong>Page the on-call(s) for a PagerDuty Service or a user directly.</li><li><strong>get timeline:</strong> Compile a concise timeline of important chat events for post-incident analysis.</li></ul><h3>Incident Response Lifecycle</h3><p>We have defined four separate phases of an incident: detection, communication, escalation and resolution. Each of the bot’s commands automates tasks that would normally require coordination during these distinct phases.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*x2vbM-iepMqqsrH9" /></figure><h3>Detection</h3><p>Most of our incidents are detected by our <a href="https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f">monitoring and alerting tools</a>, although sometimes we learn about incidents from our team members or customers. No matter how an incident is detected, having a single intake funnel for all incidents is crucial for effective incident detection. Our bot solves this by providing the “new incident” command.</p><h4>New incident &lt;summary&gt;</h4><p>This command creates a blank JIRA ticket with default settings and asks the user if they’d like to page an incident manager.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*W275nEZINolYZ6pn" /></figure><p>Regardless of the user’s choice to page an incident manager, a popup appears to the user asking for additional information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/519/0*cwNt9XjvCIiVXNS2" /></figure><p>This allows us to escalate incidents quickly while still allowing the incident responder to provide valuable information for the incident managers. These fields are optional in the interests of urgency and can be filled out later if needed.</p><h3>Communication</h3><p>Another important first step is to set up communication channels and provide as much context as possible to responders.</p><h4>New channel [Jira ticket]</h4><p>This command takes an optional Jira ticket as a URL or key. If none is provided, it will show the last 5 recently opened incident tickets for the user to choose. A channel is then created using the Jira ticket key, the summary as the title, and all incident managers are invited.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/495/0*TfXcDuFds_fv8K-8" /></figure><p>To provide context to all users invited, the channel’s topic is set to the Jira ticket link along with the summary of the Jira ticket. In addition, we update the Jira ticket with a link to the newly created Slack channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/328/0*2WJcyBs7LocZ4yjP" /></figure><h3>Escalation</h3><p>You may have heard about the<a href="https://nvd.nist.gov/vuln/detail/CVE-2021-44228"> Log4j security vulnerability</a> which was characterized as the single biggest and most critical vulnerability of the last decade. Within 72 hours of vulnerability disclosure, there were reports of <a href="https://arstechnica.com/information-technology/2021/12/hackers-launch-over-840000-attacks-through-log4j-flaw">840,000 attacks</a> on companies globally, which turned into 100 internet wide attacks per minute over the following weekend.</p><p>At Airbnb, we have over a thousand micro services with hundreds of small teams managing them, which offered a unique challenge for us. We had to identify all vulnerable services, and quickly reach out to their respective owners for quick mitigation. This is where our Slack bot really shined, allowing our Incident Managers to quickly reach out to service owners and coordinate rolling out the fix much quicker than before. In a matter of minutes, the bot was used to page over 300 teams to assist with assessing impact and deploying patches. This equated to 4 hours saved compared to paging these teams manually, not to mention reducing the time spent in a vulnerable state.</p><h4>Page &lt;shortcut|service name|slack user&gt;</h4><p>The page command can be given a service shortcut, service name, or a slack user.</p><p>To get started, the user can view a list of shortcuts by typing in “page list”</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/264/0*DVEqiuHCfuSqyruA" /></figure><p>Each shortcut corresponds to a PagerDuty service ID which will be used when creating a PagerDuty incident. The shortcuts are easily customizable by editing a YAML file.</p><p>If a user types in a service name which doesn’t match any shortcut, a search is done in the PagerDuty service directory and results are displayed for the user to choose.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/426/0*yym1kmhJGZTzsa4c" /></figure><p>Once a user chooses the service they want to page they’re asked to confirm and a new PagerDuty incident is created for that service.</p><p>We also allow paging Slack users directly for when additional responders are required outside of those on-call.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/620/0*pw130BpOe5IFD7CZ" /></figure><p>Once the page command is sent, the bot creates a new incident in PagerDuty with the Jira ticket, summary, and slack channel to provide context to the on-call person. After the on-call person is paged, the bot announces who was paged and invites them to the channel.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/457/0*f0cPvK_oG91okdgm" /></figure><h3>Resolution</h3><p>Once responders confirm there is no further user impact and a root cause is known, the incident is considered resolved and the team transitions to the post-incident phase. A robust timeline is required to have an effective post-incident review and an effective <a href="https://www.atlassian.com/incident-management/postmortem">post mortem</a> report.</p><h4>Get timeline</h4><p>This command will search the incident channel for all chat messages marked with a specific emoji which designates the message as a timeline event, and direct message the user a compiled timeline.</p><p>For example, we use the 📝 emoji to designate important events in the chat. As the incident is ongoing, anyone can add the emoji as a reaction to important chat events. Post-incident, the “get timeline” command will compile these chat events into an easy to copy paste timeline to be used in the post-incident report.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/466/0*vL094xETdA2Re2cR" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/472/0*Z2WAZ9sw8X_Fg0-J" /></figure><h4>Incident Review</h4><p>At Airbnb, we have after action review meetings (AAR) weekly where we review recent high severity incidents, post-incident reports, and ensure any corrective actions are called out and assigned. As soon as the Jira ticket tracking the incident is updated with the AAR meeting date, the bot will notify the person owning the Jira ticket when the meeting will be and what is expected of them.</p><h4>Followup Tracking</h4><p>Oftentimes, during our <a href="https://www.atlassian.com/incident-management/postmortem/blameless">blameless postmortem process</a>, tickets for corrective actions are created and assigned to teams to avoid similar incidents in the future. To encourage quick resolutions we set a strict deadline for these tickets. Our bot will send a warning message over Slack a couple of days before the deadline, and another message if the deadline has lapsed to the user assigned to the ticket.</p><h4>Archiving Incident Channels</h4><p>To keep our Slack workspace tidy, the bot automatically archives incident channels ten days after the incident’s Jira ticket has been closed.</p><h3>Results</h3><p>Since launch, our bot has saved our Incident Managers and responders many hours through its automation and centralization of incident management within Slack. By measuring the average amount of time each task takes to complete manually compared to the bot’s automation, we determined an estimated 44 hours of time saved so far in 2022.</p><h3>What’s Next?</h3><p>To further streamline our incident response from Slack, we plan to enhance our integration with PagerDuty.</p><p>Currently, every time the page command is used a new PagerDuty incident is created. Instead, we plan to unify all pages under a single PagerDuty incident to take advantage of PagerDuty’s incident metrics and to provide more context to responders.</p><p>Lastly, after a PagerDuty service is paged using the bot, we don’t have visibility of the status of the PagerDuty incident in Slack. Was the page acknowledged? Did the on-call not respond? Was it escalated and to who? We plan to build automation to follow the PagerDuty incident and report the current status to the incident’s channel. This will also allow us to record the timeline of actions taken in the PagerDuty incident after paging the service.</p><h3>Attribution and Thanks</h3><ul><li><a href="https://medium.com/u/af76cda83a53">Stephen</a>: for being a great partner on the Airbnb Incident Management team and helping to define the incident management bot’s feature roadmap</li></ul><p>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ae863dc5d47f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f">Incident Management</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Beti Gathegi]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/61c2db3d8546</guid>
            <category><![CDATA[program-management]]></category>
            <category><![CDATA[éducation]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[bootcamp]]></category>
            <category><![CDATA[onboarding]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 21 Jul 2022 17:28:47 GMT</pubDate>
            <atom:updated>2022-07-21T17:28:47.687Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Beti Gathegi</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mhJHc6Qqjet_mKa-bzwu-w.jpeg" /></figure><p>From exploring careers across continents to now helping others find their place at Airbnb.</p><p><em>After trying a series of careers ranging from television production to university communications and marketing, </em><a href="https://www.linkedin.com/in/betigathegi"><em>Beti Gathegi</em></a><em> works as a Senior Program Manager on the TechED (technical education) team at Airbnb. When she’s not lurking in the #bookworms Airbnb Slack channel, you can find Beti leading Bootcamp, our onboarding program for new technical hires, which takes engineers and data scientists through their first commit at Airbnb. Before this role, Beti was a recruiting program manager for Connect, Airbnb’s engineering apprenticeship program targeted at people from non-traditional technical backgrounds.</em></p><p><em>Beti herself has a non-traditional background, with a degree in journalism and several experiences outside the tech industry, including substantial time abroad. She is a major advocate for diversity and inclusion; part of her role in leading Bootcamp involves setting the company’s culture and encouraging new hires to shape the culture in their own unique ways.</em></p><h3>Setting my own direction</h3><p>I describe myself as half East Coast, half West Coast, with a bit of time abroad added in. I’m the child of Kenyan immigrants and I grew up in the San Francisco Bay Area, in a town called Albany, California. When I was 15, I moved to the East Coast, and it would be many years before I found myself back in the Bay Area.</p><p>For a long time, I wanted to be a journalist. To that end, I studied journalism in college as part of my communications degree. I was never fixated on a specific path and certainly explored a lot to reach where I am now. My father, who pivoted later in life by getting a law degree around age 40, was my guiding light in terms of being willing to try new things. I find personal exploration liberating — crafting my own, organic path gave me a chance to figure out my likes and dislikes, as well as my skills and growth opportunities.</p><p>Part of my outlook on life is that it’s okay to stop something that isn’t right for yourself. Sometimes there can be a lot of inertia that makes it hard to pause and change directions, but I think making a decision to pursue another path is really brave and can be worn as a badge of honor. In my case, I started a master’s in liberal arts in which I was studying the South Asian diaspora and the children of Indian immigrants in particular. Inspired by the stories of others, I was eager to discover more about my own background and history. I chose to leave my program to go live in Kenya and experience Kenyan culture for myself. Until that point, I’d only been to Kenya with my family, so this was a new lens to see the country on my own.</p><h3>Living and working in three continents</h3><p>Living in Kenya was a transformative experience and helped me understand my own identity more deeply. Having previously been told, by some, that I’m not Kenyan enough or not American enough, actually living in Kenya and encountering the sheer diversity of people made me realize there’s no singular way to be Kenyan, just like there’s no one way to be American or any culture for that matter. During my time abroad, I also realized I was ready to get more hands-on experience and enter the working world.</p><p>Ready to live a life of adventure, I moved to New York City but stumbled into a financial crisis when seemingly everyone was getting laid off. I worked retail for a little while but otherwise didn’t last too long in New York. This was just the first in a series of new experiences, my next being at a TV production firm where I was an assistant, and where to this day I have an IMDB credit for four episodes of the show <em>Swamp Men </em>on National Geographic. If that wasn’t enough, I also had jobs writing TV quizzes for Nielsen, doing marketing for the University of South Florida, and working at an Australian Aboriginal art gallery.</p><p>Eventually, I happened upon the tech industry when a friend recruited me to join Lyft in a customer support role. This was a completely new universe to me and I took every opportunity to get involved and apply my skills to a growing company. Practically by accident, my initiative to gather people via the company’s internal email list turned into Employee Resource Groups or ERGs. I helped form the Black ERG and Women’s luncheon, while also supporting others who wanted to create similar spaces for their communities. Very organically, I was taking a big part in the diversity and belonging conversation and making sure to educate myself to be a thoughtful contributor to these discussions. Later, this turned into an official job focused on Lyft’s culture, and afterward I moved to Pandora as a diversity and belonging program manager.</p><h3>Onboarding to Airbnb</h3><p>By the time I applied to join Airbnb, I had shifted to leading Pandora’s university recruiting program. I noticed a tremendous amount of potential in students, and found it really impactful to work on this key pipeline. That said, going directly from college to the tech industry isn’t the only way, my own career being a prime example. I jumped at the opportunity to join Airbnb as an apprenticeship program manager where I had the chance to revamp this pathway for engineers with unconventional backgrounds to join the company.</p><p>I ran the apprenticeship program for two years and while the role was primarily on the recruiting side, we worked very closely with engineering and I started to develop an interest in TechED, the team I’m on now that owns the onboarding process for Airbnb’s technical hires. I was already meeting regularly with my manager at the time, Leo, about growing in my career, so I started by sharing my goals in that setting. Leo was incredibly supportive and epitomizes how Airbnb has a culture of empowering people to explore their passions and what energizes them.</p><p>I reached out to the manager on the TechED team to express my interest in the space and not too long after, a role happened to open up. After interviewing, I got my current position leading Airbnb’s Bootcamp program, the onboarding process all software engineers and data scientists go through in their first weeks at the company. It took a ton of experiences across many other roles to arrive at my current spot, but that came with immeasurable learning and I wouldn’t have it any other way! I feel uniquely equipped to welcome people to a new experience or challenge, having gone through so many of my own.</p><h3>Leading Bootcamp and onboarding others to Airbnb</h3><p>My current role aligns with my passion for helping people acclimate and providing them with the resources they ended to be successful. It’s fulfilling to advocate for an incredible new hire experience, help new team members feel confident in their respective roles at Airbnb, and support them towards making their first commit or code change. I strive to be really respectful of people’s time and make Bootcamp as relevant, engaging, and valuable as possible to everyone who participates.</p><p>Onboarding is a challenging task because there are multiple variables. Typically you are onboarding people in various roles, at various levels, to various teams, which may use their own tools and process. There is a balancing act between providing general information and hyper-relevant but also highly specific information. Remote onboarding also adds its own set of challenges.</p><p>That being said, I love co-creating solutions. I get to work with incredibly smart people on the engineering and data science teams to identify and clarify our challenges, workshop ideas, execute solutions, monitor progress, and iterate. I get a ton of energy from that process and from our collaborations.</p><h3>How the first weeks at work can leave a lasting impact</h3><p>I’m also grateful to the many volunteers I partner with to shape the onboarding experience for our technical hires and set them up for success. For example, we pair each new hire with a buddy from their team. They serve both to scope the hire’s starter project as well as to answer the many questions that inevitably pop up in onboarding.</p><p>We have volunteers from various teams who raise their hands to host Bootcamp and lead sessions for each new hire cohort, and most of them are driven by providing a sense of belonging. Additionally, there’s a great community of collaborators across the industry to benchmark with and get mentorship from, since onboarding is a challenging problem that a lot of companies work on.</p><p>Beyond the technical parts of onboarding, Bootcamp plays a critical role in setting Airbnb’s culture. Especially in a remote work environment, the quality of onboarding can make or break whether new hires feel a sense of community and feel comfortable engaging with it themselves. We emphasize belonging and inclusivity as core values of our culture, and we welcome new hires to bring their own special qualities to integrate into our ever-evolving culture.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61c2db3d8546" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-beti-gathegi-61c2db3d8546">My Journey to Airbnb — Beti Gathegi</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Safeguards Changes in Production]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/9fc9024f3446</guid>
            <category><![CDATA[a-b-testing]]></category>
            <category><![CDATA[experimentation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[continuous-delivery]]></category>
            <category><![CDATA[spinnaker]]></category>
            <dc:creator><![CDATA[Michael Lin]]></dc:creator>
            <pubDate>Mon, 11 Jul 2022 17:02:38 GMT</pubDate>
            <atom:updated>2022-07-11T18:13:11.762Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part I: Evolution of Airbnb’s experimentation platform</h4><p>By: <a href="https://www.linkedin.com/in/michaelcl/">Michael Lin</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/zack-loebel-begelman-85407698/">Zack Loebel-Begelman</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0J4whYTNqGPUdUme" /></figure><h3>Introduction</h3><p>As Airbnb has grown to a company with over 1,200 developers, the number of platforms and channels for pushing changes to our product — and the number of daily changes we push into production — has also grown tremendously. In the face of this growth, we constantly need to scale our ability to detect errors before they reach production. However, errors inevitably slip past pre-production validation, so we also invest heavily in mechanisms to detect errors quickly when they do make it to production. In this blog post we will cover the motivations and foundations for a system for safeguarding changes in production, which we call Safe Deploys. Two following posts will cover the technical architecture in detail for how we applied this to traditional A/B tests, and code deploys respectively.</p><h3>Continuous Delivery and Beyond</h3><p>Airbnb’s continuous delivery team recently wrote about <a href="https://medium.com/airbnb-engineering/continuous-delivery-at-airbnb-6ac042bc7876">our adoption of Spinnaker</a>, a modern CI/CD orchestrator. Spinnaker supports <a href="https://spinnaker.io/docs/guides/user/canary/">Automated Canary Analysis (ACA)</a> during deployment, splitting microservice traffic by request to compare versions of code to see if performance, error rates, or other key metrics are negatively impacted. If metrics for the new version regress, Spinnaker automatically rolls back the deployment, significantly reducing the time to remediate a bad push.</p><p>ACA at Airbnb has indeed caught a large number of errors early in the deployment process. However, it has a number of limitations:</p><ul><li><strong>Channels: </strong>Spinnaker’s ACA tests against changes to microservices. However, microservice updates are not the only source of errors that can be pushed into production. For instance, Android and iOS apps follow a release process through their respective app stores. Many “production pushes” at Airbnb may involve no new code at all, and are strictly applied through configuration changes. These changes include marketing campaigns or website content created with Airbnb’s <a href="https://medium.com/airbnb-engineering/airbnbs-promotions-and-communications-platform-6266f1ffe2bd">internal content management systems</a>. While seemingly benign, pushes through these systems can have dramatic effects. For example an incident was once caused when a marketing campaign was mistakenly applied to all countries except one, instead of the original intent of targeting one specific country. This simple mistake led to empty search results for nearly all users globally, and required over an hour to identify and revert.</li><li><strong>End-to-end business metrics: </strong>Spinnaker’s ACA is driven by local system metrics, such as a microservice’s local performance and error rates; not end-to-end business metrics, such as search click-through rates and booking rates. While roll-backs based on local system metrics are valuable, they aren’t sufficient, as some of our most costly bugs impact end-to-end business metrics but not local system metrics. For instance in 2020, a simple frontend change was deployed to production without being tested on a specific browser that did not support the CSS used, preventing users on that browser from booking trips. This had no impact on system metrics, but directly impacted business metrics. <br><br>Unfortunately, adding business metrics to Spinnaker’s ACA system is not possible because Spinnaker randomizes traffic by request, therefore the same user may be exposed to multiple variants. Business metrics, however, are generally user based and require each user to have a fixed variant assignment. More fundamentally, it’s not possible because business metrics need to be measured end-to-end and when two microservices undergo ACA at the same time, Spinnaker has no way of distinguishing the respective impact of those two services on end-to-end business metrics.</li><li><strong>Granularity: </strong>Spinnaker’s ACA tests at the level of the entire microservice. However, it’s often the case that two features are being worked on at the same time within a microservice. When ACA fails, it can be hard to tell which feature caused the failure.</li></ul><p>While we heavily depend upon Spinnaker’s ACA at Airbnb, it became clear there was an opportunity to complement it and address the above limitations where the circumstances call for it.</p><h3>Experimentation Reporting Framework (ERF)</h3><p>A/B testing has long been a fixture in product development at Airbnb. While sharing some qualities with ACA in counterfactual analysis, A/B testing has focused on determining whether a new feature improves business outcomes, versus determining whether that feature causes a system regression. Over the years Airbnb has developed our Experimentation Reporting Framework (ERF) to run hundreds of concurrent A/B experiments across a half dozen platforms to determine whether a new feature will have a positive impact.</p><p>ERF addresses the limitations of ACA listed above:</p><ul><li><strong>Channels: </strong>With each new platform, an ERF client has been introduced to support A/B testing on it. This includes mobile, web, and backend microservices. APIs were also introduced to provide config systems an avenue to treat config changes as A/B tests.</li><li><strong>End-to-end business metrics: </strong>ERF is driven <em>primarily</em> by end-to-end business metrics. On the technical side, it randomizes by user, not request, and it is able to distinguish the impact of hundreds of experiments running concurrently. ERF taps into Airbnb’s <a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">central metrics system</a> to access the thousands of business metrics and dimensions Product and Business teams have defined to measure what matters most to Airbnb overall.</li><li><strong>Granularity: </strong>Where Spinnaker’s ACA runs its experiments at the level of an entire microservice, ERF runs its experiments based on what are basically feature flags embedded into the code. Thus, if multiple features are being developed concurrently in the same microservice, ERF can determine which one is impacting the business metrics.</li></ul><p>The above characteristics of ERF address the limitations of ACA, but ERF also had a limitation of its own: it was a daily-batch system generating interactive reports intended to be consumed by human decision makers. To address the limitation of Spinnaker’s ACA, ERF needed to evolve into a near real-time system that can directly control the deployment process without human intervention.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rOQST4-KYXGe253y" /><figcaption>Figure 1: Areas of the ERF Platform augmented to support near real-time experimentation</figcaption></figure><p>This evolution had implications on both the data science behind ERF, and its software architecture. We describe the former in this post, and will describe the latter in the next post of this series.</p><h3>Realtime ERF — The Data Science</h3><p>The foundation of solid data science is solid data engineering. On the data engineering side, we needed to revisit the definitions of the business metrics to be computed in real-time. The metrics computed by the batch ERF system were designed for accuracy, and could take advantage of complex joins and pre-processing to achieve this. Near real-time metrics did not have this luxury, and required simplification to meet low latency requirements.</p><p>Not only did we have to build new metrics, but we knew we would have to build new statistical tests as well. It is imperative for safe deployment systems to not be noisy, otherwise people will stop using it. Traditional methods like T-Test suffer from a variety of issues that would be extremely problematic when implemented in a real-time system. Two issues in particular are false positives due to (1) <a href="http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1517.pdf">peeking</a> (looking before a predetermined amount of time) and (2) heavily skewed data.</p><p>When monitoring whether or not a metric has changed in real-time, users want to be notified as soon as the model has the confidence that this is true. However, doing so naively results in the first issue, peeking. In traditional A/B testing, the statistical test is only applied once after a predetermined time, because there is a chance that a significant result is due to randomness and not an actual effect. For real-time ERF, we aren’t making just one test, since, depending on how long we wait to take the test, we’re at risk for either taking too long to detect some errors, or missing other errors that take longer to surface. Instead, we want to check (peek at) the model every 5 minutes so that we can react quickly. With a p-value of 0.05 running 100 A/A comparisons, one could expect to have ~5 significant results that are actually false positives. We can transfer this issue to computing p-values on the same data set multiple times. Each evaluation results in a 5% chance of a false positive and so over multiple evaluations, the chance of having 1 or more false positives approaches 100%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/894/0*GHdgh0pCLSG87czD" /><figcaption>Figure 2: Increasing evaluations inevitably lead to false positives</figcaption></figure><p>To balance early detection without noisiness, we utilize <a href="https://en.wikipedia.org/wiki/Sequential_analysis">sequential analysis</a>. Sequential methods do not assume a fixed sample size (i.e., checking the model once) and allow us to continually monitor a metric without worrying about false positives incurred due to peeking. One way to correct for false positives (<a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error">Type 1 Errors</a>) is by applying a <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>. If you check your model for statistical significance four times and want to guarantee a 5% overall false positive rate, you need to divide your p-value by four, meaning only results with p-value at or under 1.25% are valid. However, doing so is too conservative since each check is dependent. They are dependent because each check has the same base of data only adding additional observations as time goes on. Sequential models take this dependence into account while guaranteeing false positives rates more efficiently than Bonferroni. We use two different sequential models, SSRM (<a href="https://arxiv.org/abs/2011.03567">Sequential Sample Ratio Mismatch</a>) for count metrics, and <a href="https://arxiv.org/abs/1906.09712">Sequential Quantiles</a> (Howard, Ramdas) for quantile metrics.</p><p>The second issue that we needed to solve in order to be robust is handling skewed data. Performance metrics like latency can have extremely heavy tails. Models that assume a normal distribution won’t be effective because the Central Limit Theorem does not come into effect. By applying Sequential Quantiles, we can ignore assumptions about the metric’s distribution and directly measure the difference between arbitrary quantiles.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/892/0*_UUUc7wmcL3jUKei" /><figcaption>Figure 3: Metrics may have non-normal distributions</figcaption></figure><p>Lastly, many important measures are not independent. Metrics like latency and impressions have within-user correlation, so each event in the data cannot be treated as an independent unit. In order to counteract skew, we aggregate all measures into user metrics first before evaluating statistical models.</p><h3>Conclusion</h3><p>With the statistical methods in place to evaluate business metrics in near real-time, we could now detect problems that were invisible to Spinnaker, or required too much lead time to rely on traditional ERF experiments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*nmC6mnUBfe98hcxo" /><figcaption>Figure 4: How Real-time ERF fits between Spinnaker and Traditional ERF</figcaption></figure><p>Operationalizing the newly created near real-time metrics and statistical methods required further engineering, but more challenging, it required changing the experimentation culture at Airbnb. In the following post we will detail how our near real-time metrics pipeline was built, how these metrics powered automated decision making, and how we drove adoption across the company.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4262326/">Senior Software Engineer, Metric Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/4216371/">Staff Software Engineer, Real-time Stream Processing Platform</a></p><p><a href="https://careers.airbnb.com/positions/2403782/">Staff Software Engineer — ML Ops Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><h3>Appreciations</h3><p>Thanks to <a href="https://www.linkedin.com/in/adriankuhn/">Adrian Kuhn</a>, <a href="https://www.linkedin.com/in/alex-shaojie-deng-b572347/">Alex Deng</a>, <a href="https://www.linkedin.com/in/antoinecreux/">Antoine Creux</a>, <a href="https://www.linkedin.com/in/erikriverson/">Erik Iverson</a>, <a href="https://www.linkedin.com/in/george-l-9b946655/">George Li</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/preetiramasamy/">Preeti Ramasamy</a>, <a href="https://www.linkedin.com/in/rstata/">Raymie Stata</a>, Reid Andersen, <a href="https://www.linkedin.com/in/ronnyk/">Ronny Kohavi</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/tatiana-xifara/">Tatiana Xifara</a>, <a href="https://www.linkedin.com/in/vincent-chan-70080423/">Vincent Chan</a>, <a href="https://www.linkedin.com/in/xin-tu/">Xin Tu</a> and the OMNI team.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fc9024f3446" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446">How Airbnb Safeguards Changes in Production</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>