<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 25 Apr 2023 00:59:40 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Improving Istio Propagation Delay]]></title>
            <link>https://medium.com/airbnb-engineering/improving-istio-propagation-delay-d4da9b5b9f90?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d4da9b5b9f90</guid>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[istio]]></category>
            <category><![CDATA[service-mesh]]></category>
            <dc:creator><![CDATA[Ying Zhu]]></dc:creator>
            <pubDate>Thu, 23 Mar 2023 18:20:38 GMT</pubDate>
            <atom:updated>2023-03-23T18:20:38.120Z</atom:updated>
            <content:encoded><![CDATA[<h4>A case study in service mesh performance optimization</h4><p>by: <a href="https://www.linkedin.com/in/ying-zhu-763a3879/">Ying Zhu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tvfIu34QvOy1IBNTNGXoAQ.jpeg" /></figure><h3>Introduction</h3><p>In this article, we’ll showcase how we identified and addressed a service mesh performance problem at Airbnb, providing insights into the process of troubleshooting service mesh issues.</p><h4>Background</h4><p>At Airbnb, we use a microservices architecture, which requires efficient communication between services. Initially, we developed a homegrown service discovery system called Smartstack exactly for this purpose. As the company grew, however, we encountered scalability issues¹. To address this, in 2019, we invested in a modern service mesh solution called AirMesh, built on the open-source <a href="https://istio.io/latest/">Istio</a> software. Currently, over 90% of our production traffic has been migrated to AirMesh, with plans to complete the migration by 2023.</p><h4>The Symptom: Increased Propagation Delay</h4><p>After we upgraded Istio from 1.11 to 1.12, we noticed a puzzling increase in the propagation delay — the time between when the Istio control plane gets notified of a change event and when the change is processed and pushed to a workload. This delay is important for our service owners because they depend on it to make critical routing decisions. For example, servers need to have a graceful shutdown period longer than the propagation delay, otherwise clients can send requests to already-shut-down server workloads and get 503 errors.</p><h4>Data Gathering: Propagation Delay Metrics</h4><p>Here’s how we discovered the condition: we had been monitoring the Istio metric <em>pilot_proxy_convergence_time</em> for propagation delay when we noticed an increase from 1.5 seconds (p90 in Istio 1.11) to 4.5 seconds (p90 in Istio 1.12). <em>Pilot_proxy_convergence_time</em> is one of several metrics Istio records for propagation delay. The complete list of metrics is:</p><ul><li><em>pilot_proxy_convergence_time</em> — measures the time from when a push request is added to the push queue to when it is processed and pushed to a workload proxy. (Note that change events are converted into push requests and are batched through a process called <em>debounce</em> before being added to the queue, which we will go into details later.)</li><li><em>pilot_proxy_queue_time</em> — measures the time between a push request enqueue and dequeue.</li><li><em>pilot_xds_push_time</em> — measures the time for building and sending the xDS resources. Istio leverages Envoy as its data plane. Istiod, the control plane of Istio, configures Envoy through the xDS API (where x can be viewed as a variable, and DS stands for discovery service).</li><li><em>pilot_xds_send_time</em> — measures the time for actually sending the xDS resources.</li></ul><p>The diagram below shows how each of these metrics maps to the life of a push request.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Itxhfr6I8TygnWQwQgghhA.png" /><figcaption>A high level graph to help understand the metrics related to propagation delay.</figcaption></figure><h3>Investigation</h3><h4>xDS Lock Contention</h4><p>CPU profiling showed no noticeable changes between 1.11 and 1.12, but handling push requests took longer, indicating time was spent on some waiting events. This led to the suspicion of lock contention issues.</p><p>Istio uses four types of xDS resources to configure Envoy:</p><ul><li>Endpoint Discovery Service (EDS) — describes how to discover members of an upstream cluster.</li><li>Cluster Discovery Service (CDS) — describes how to discover upstream clusters used during routing.</li><li>Route Discovery Service (RDS) –describes how to discover the route configuration for an HTTP connection manager filter at runtime.</li><li>Listener Discovery Service (LDS) –describes how to discover the listeners at runtime.</li></ul><p>Analysis of the metric <em>pilot_xds_push_time</em> showed that only three types of pushes (EDS, CDS, RDS) increased after the upgrade to 1.12. The Istio changelog revealed that <a href="https://github.com/istio/istio/pull/33338">CDS</a> and<a href="https://github.com/istio/istio/pull/34243"> RDS</a> caching was added in 1.12.</p><p>To verify that these changes were indeed the culprits, we tried turning off the caches by setting PILOT_ENABLE_CDS_CACHE and PILOT_ENABLE_RDS_CACHE to “False”. When we did this, <em>pilot_xds_push_time</em> for CDS reverted back to the 1.11 level, but not RDS or EDS. This improved the <em>pilot_proxy_convergence_time</em>, but not enough to return it to the previous level. We believed that there was something else affecting the results.</p><p>Further investigation into the xDS cache revealed that all xDS computations shared one cache. The tricky thing is that Istio used an LRU Cache under the hood. The cache is locked not only on <a href="https://github.com/istio/istio/blob/1.12.9/pilot/pkg/model/xds_cache.go#L229">write</a>s, but also on <a href="https://github.com/istio/istio/blob/1.12.9/pilot/pkg/model/xds_cache.go#L266">read</a>s, because when you read from the cache, you need to promote the item to most recently used. This caused lock contention and slow processing due to multiple threads trying to access the same lock at the same time.</p><p>The hypothesis formed was that xDS cache lock contention caused slowdowns for CDS and RDS because caching was turned on for those two resources, and also impacted EDS due to the shared cache, but not LDS as it did not have caching implemented.</p><p>But why turning off both CDS and RDS cache does not solve the problem? By looking at where the cache was used when building RDS, we found out that the flag PILOT_ENABLE_RDS_CACHE was not respected. We fixed that <a href="https://github.com/istio/istio/pull/40719">bug</a> and conducted performance testing in our test mesh to verify our hypothesis with the following setup:</p><ul><li>Control plane:<br>- 1 Istiod pod (memory 26 G, cpu 10 cores)</li><li>Data plane:<br>- 50 services and 500 pods<br>- We mimicked changes by restarting deployments randomly every 10 seconds and changing virtual service routings randomly every 5 seconds</li></ul><p>Here were the results:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pRyD4EoUCmo_2WazdJoFnA.png" /><figcaption>A table of results² for the perfomance testing.</figcaption></figure><p>Because our Istiod pods were not CPU intensive, we decided to disable the CDS and RDS caches for the moment. As a result, propagation delays returned to the previous level. Here is the Istio <a href="https://github.com/istio/istio/issues/40744">issue</a> for this problem and potential future improvement of the xDS cache.</p><h4>Debounce</h4><p>Here’s a twist in our diagnosis: during the deep dive of Istio code base, we realized that <em>pilot_proxy_convergence_time</em> does not actually fully capture propagation delay. We observed in our production that 503 errors happen during server deployment even when we set graceful shutdown time longer than <em>pilot_proxy_convergence_time</em>. This metric does not accurately reflect what we want it to reflect and we need to redefine it. Let’s revisit our network diagram, zoomed out to include the debounce process to capture the full life of a change event.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vfzfyE92hlutqOFJbUcQqw.png" /><figcaption><em>A high level diagram of the life of a change event.</em></figcaption></figure><p>The process starts when a change notifies an Istiod controller³. This triggers a push which is sent to the push channel. Istiod then groups these changes together into one combined push request through a process called debouncing. Next, Istiod calculates the push context which contains all the necessary information for generating xDS. The push request together with the context are then added to the push queue. Here’s the problem: <em>pilot_proxy_convergence_time</em> only measures the time from when the combined push is added to the push queue, to when a proxy receives the calculated xDS.</p><p>From Istiod logs we found out that the debounce time was almost 110 seconds, even though we set PILOT_DEBOUNCE_MAX to 30 seconds. From reading the code, we realized that the <a href="https://github.com/istio/istio/blob/1.15.3/pilot/pkg/xds/discovery.go#L532">initPushContext</a> step was blocking the next debounce to ensure that older changes are processed first.</p><p>To debug and test changes, we needed a testing environment. However, it was difficult to generate the same load on our test environment. Fortunately, the debounce and init push context are not affected by the number of Istio proxies. We set up a development box in production with no connected proxies and ran custom images to triage and test out fixes.</p><p>We performed CPU profiling and took a closer look into functions that were taking a long time:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DKQZMY6fHpNmUOExjvvBSg.png" /><figcaption><em>A CPU profile of Istiod.</em></figcaption></figure><p>A significant amount of time was spent on the Service DeepCopy function. This was due to the use of the <a href="https://github.com/mitchellh/copystructure">copystructure</a> library that used <a href="https://go.dev/blog/laws-of-reflection">go reflection</a> to do deep copy, which has expensive performance. Removing the library⁴ was both easy and very effective at reducing our debounce time from 110 seconds to 50 seconds.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0XoORAP6XTCQDwKNVNzmog.png" /><figcaption><em>A CPU profile of Istiod after DeepCopy improvement.</em></figcaption></figure><p>After the DeepCopy improvement, the next big chunk from the cpu profile was the ConvertToSidecarScope function. This function took a long time to determine which virtual services were imported by each Istio proxy. For each proxy egress host, Istiod first computed all the virtual services exported to the proxy’s namespace, then selected the virtual services by matching proxy egress host name to the virtual services’ hosts.</p><p>All our virtual services were public as we did not specify the <em>exportTo</em> parameter, which is a list of namespaces to which this virtual service is exported. If this parameter is not configured, the virtual service is automatically exported to all namespaces. Therefore, <a href="https://github.com/istio/istio/blob/1.12.9/pilot/pkg/model/push_context.go#L829-L833">VirtualServicesForGateway</a> function created and copied all virtual services each time. This deep-copy of slice elements was very expensive when we had many proxies with multiple egress hosts.</p><p>We <a href="https://github.com/istio/istio/pull/41101">reduced</a> the unnecessary copy of virtual services: instead of passing a copied version of the virtual services, we passed the virtualServiceIndex directly into the select function, further reducing the debounce time from 50 seconds to around 30 seconds.</p><p>Another improvement that we are currently rolling out is to limit where virtual services are exported by setting the exportTo field, based on which clients are allowed to access the services. This should reduce debounce time by about 10 seconds.</p><p>The Istio community is also actively working on improving the push context calculation. Some ideas include <a href="https://github.com/istio/istio/issues/41453">adding multiple workers to compute the sidecar scope</a>, <a href="https://github.com/istio/istio/pull/41647">processing changed sidecars only instead of rebuilding the entire sidecar scope</a>. We also added <a href="https://github.com/istio/istio/pull/40523">metrics for the debounce time</a> so that we can monitor this together with the proxy convergence time to track accurate propagation delay.</p><h3>Conclusion</h3><p>To conclude our diagnosis, we learned that:</p><ul><li>We should use both <em>pilot_debounce_time</em> and <em>pilot_proxy_convergence_time</em> to track propagation delay.</li><li>xDS cache can help with CPU usage but can impact propagation delay due to lock contention, tune PILOT_ENABLE_CDS_CACHE &amp; PILOT_ENABLE_RDS_CACHE to see what’s best for your system.</li><li>Restrict the visibility of your Istio manifests by setting the <em>exportTo</em> field.</li></ul><p>If this type of work interests you, check out some of our related <a href="https://careers.airbnb.com/">roles</a>!</p><h3>Acknowledgments</h3><p>Thanks to the Istio community for creating a great open source project and for collaborating with us to make it even better. Also call out to the whole AirMesh team for building, maintaining and improving the service mesh layer at Airbnb. Thanks to Lauren Mackevich, Mark Giangreco and Surashree Kulkarni for editing the post.</p><p>[1]: Checkout our presentation <a href="https://events.istio.io/istiocon-2021/sessions/airbnb-on-istio/">Airbnb on Istio</a> for details.</p><p>[2]: Note that some CPU throttling occurred for the last two cases, so if we were to allocate more CPU we would expect propagation delay (especially P99) to improve further.</p><p>[3]: Istiod service controller monitors changes to registered services from different sources including kubernetes service, ServiceEntry created service, etc., Istiod config controller monitors changes to the Istio resources used to manage those services.</p><p>[4]: <a href="https://github.com/istio/istio/pull/40966/files">PR1</a>, <a href="https://github.com/istio/istio/pull/37932/files">PR2</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d4da9b5b9f90" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/improving-istio-propagation-delay-d4da9b5b9f90">Improving Istio Propagation Delay</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building Airbnb Categories with ML & Human in the Loop]]></title>
            <link>https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-human-in-the-loop-35b78a837725?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/35b78a837725</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Mihajlo Grbovic]]></dc:creator>
            <pubDate>Wed, 22 Mar 2023 22:03:24 GMT</pubDate>
            <atom:updated>2023-03-24T16:18:37.928Z</atom:updated>
            <content:encoded><![CDATA[<p>Airbnb Categories Blog Series — Part II : ML Categorization</p><p>by: <strong>Mihajlo Grbovic, Pei Xiong, Pratiksha Kadam, Ying Xiao, Sherry Chen, Weiping Peng, Shukun Yang, Chen Qian, Haowei Zhang, Sebastien Dubois, Nate Ney, James Furnary, Mark Giangreco, Nate Rosenthal, Cole Baker, Aaron Yin, Bill Ulammandakh, Shankar Shetty</strong>, <strong>Sid Reddy, Egor Pakhomov</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QYv0Kr3gpdWJFtzPgqkwJA.jpeg" /></figure><p><a href="https://news.airbnb.com/2022-summer-release/">Airbnb 2022 release</a> introduced Categories, a browse focused product that allows the user to seek inspiration by browsing collections of homes revolving around a common theme, such as <em>Lakefront, Countryside, Golf, Desert, National Parks</em>, <em>Surfing</em>, etc. In <a href="https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb">Part I</a> of our Categories Blog Series we covered the high level approach to creating Categories and showcasing them in the product. In this Part II we will describe the ML Categorization work in more detail.</p><p>Throughout the post we use the <strong><em>Lakefront</em> category</strong> as a running example to showcase the ML-powered category development process. Similar process was applied for other categories, with category specific nuances. For example, some categories rely more on points of interests, while others more on structured listing signals, image data, etc.</p><h4><strong>Category Definition</strong></h4><p>Category development starts with a product-driven category definition: “<em>Lakefront category should include listings that are less than 100 meters from the lake</em>”. While this may sound like an easy task at first, it is very delicate and complex as it involves leveraging multiple structured and unstructured listing attributes, points of interest (POIs), etc. It also involves training ML models that combine them, since none of the signals captures the entire space of possible candidates on their own.</p><h4>Listing Understanding Signals</h4><p>As part of various past projects multiple teams at Airbnb spent time on processing different types of raw data to extract useful information in structured form. Our goal was to leverage these signals for cold-start rule-based category candidate generation and later use them as features of the ML model that could find category candidates with higher precision:</p><ul><li><strong>Host provided listing information</strong>, such as <strong><em>property type</em></strong> (e.g. castle, houseboat), <strong><em>amenities &amp; attributes</em></strong><em> </em>(pool, fire pit, forest view, etc.). <strong><em>listing</em></strong> <strong><em>location</em></strong>, <strong><em>title, description, image captions</em></strong> that can be scanned for keywords (we gathered exhaustive sets of keywords in different languages per category).</li><li><a href="https://www.airbnb.com/resources/hosting-homes/a/create-a-guidebook-to-share-your-local-tips-23"><strong>Host guidebooks</strong></a>, where hosts recommend nearby places for guests to visit (e.g. a Vineyard, Surf beach, Golf course) which hold locations data that was useful for extracting <strong><em>POIs</em></strong></li><li><a href="https://www.airbnb.com/s/experiences"><strong>Airbnb experiences</strong></a>, such as <em>Surfing</em>, <em>Golfing, Scuba</em>, etc. <strong><em>Locations of these activities</em></strong> proved useful in identifying listing candidates for certain activity-related categories.</li><li><strong>Guest reviews<em> </em></strong>which is another source that can be scanned for <strong><em>keywords</em></strong>. We also collect supplemental guest reviews where guests provide<strong><em> feedback on listings quality, amenities and attributes.</em></strong></li><li><a href="https://www.airbnb.com/wishlists/popular"><strong>Wishlists</strong></a> that guests create when browsing, such as “Golf trip 2022”, “Beachfront”, “Yosemite trip”, are often related to one of the categories, which proved useful for candidate generation.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZK5ffGuT8bWNFXqE" /><figcaption>Figure 1. Popular wishlists created by airbnb users</figcaption></figure><p>The listing understanding knowledge base was further enriched using external data, such as <strong>Satellite data</strong> (tell us if a listing is close to an ocean, river or lake), <strong>Climate, Geospatial data</strong>, <strong>Population data</strong> (tells us if listing is in rural, urban or metropolitan area) and <strong>POI data </strong>that contains names and locations of places of interest from host guidebooks or collected by us via open source datasets and further improved, enriched and adjusted by in-house human review.</p><p>Finally, we leveraged our in-house ML models for additional knowledge extraction from raw listing data. These included <strong>ML models for</strong><a href="https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e"><strong> Detecting amenities and objects in listing images</strong></a>, <a href="https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3"><strong>Categorizing room types and outdoor spaces in listing images</strong></a>,, <a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"><strong>Computing embedding similarities between listings</strong></a> and <a href="https://medium.com/airbnb-engineering/when-a-picture-is-worth-more-than-words-17718860dcc2"><strong>Assessing property aesthetics</strong></a>. Each of these were useful in different stages of category development, candidate generation, expansion and quality prediction, respectively.</p><h4><strong>Rule-based candidate generation</strong></h4><p>Once a category is defined, we first leverage pre-computed listing understanding signals and ML model outputs described in the previous section to codify the definition with a set of rules. Our candidate generation engine then applies them to produce a set of rule-based candidates and prioritizes them for human review based on a category confidence score.</p><p>This confidence score is computed based on how many signals qualified the listing to the category and the weights associated with each rule. For example, considering <em>Lakefront</em> category, vicinity to a Lake POIs carried the most weight, host provided signals on direct lake access were next more important, lakefront keywords found in listing title, description, wishlists, reviews carried less weight, while lake and water detection in listing images carried the least weight. A listing that would have all these attributes would have a very high confidence score, while a listing that would have only one would have a lower score.</p><h4><strong>Human review process</strong></h4><p>Candidates were sent for human review daily, by selecting a certain number of listings from each category with the highest category confidence score. Human agents then judged if listing belongs to the category, choose the best cover photo and assessed the quality of the listing (Figure 3)</p><p>As human reviews started rolling in and there were enough listings with confirmed and rejected category tags it unlocked new candidate generation techniques that started contributing their own candidates:</p><ul><li><strong>Proximity based: </strong>leveraging distance to the confirmed listing in a given category, e.g. neighbor of a confirmed <em>Lakefront</em> listing it may also be <em>Lakefront</em></li><li><strong>Embedding similarity</strong>: leveraging listing embeddings to find listings that are most similar to confirmed listing in a given category.</li><li><strong>Training ML categorization</strong> <strong>models</strong>: once the agents reviewed 20% of rule-based candidates we started training ML models.</li></ul><p>In the beginning, only agent vetted listings were sent to production and featured on the homepage. Over time, as our candidate generation techniques produced more candidates and the feedback loop repeated, it allowed us to train better and better ML models with more labeled data. Finally, at some point, when ML models were good enough, we started sending listings with high enough model scores to production (Figure 2).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EwmASLm1l4gOGftsg40vSg.png" /><figcaption>Figure 2. Number of listings in production per category and fractions vetted by humans</figcaption></figure><h3>Aligning ML Models with Human review tasks</h3><p>In order to scale the review process we trained ML models that mimic each of the three human agent tasks (Figure 3). In the following sections we will demonstrate the training and evaluation process involved with each model</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*R6OBIRV-1C78xU89" /><figcaption>Figure 3. ML models setup for mimicking human review</figcaption></figure><h4><strong>ML Categorization Model</strong></h4><p>ML Categorization Model task was to confidently place listings in a category. These models were trained using Bighead (Airbnb’s ML platform) as XGBoost binary<em> per category </em>classification models. They used agent category assignments as labels and signals described in the Listing Understanding section as features. As opposed to a rule-based setting, ML models allowed us to have better control of the precision of candidates via model score threshold.</p><p>Although many features are shared across categories and one could train a single multiclass model, due to the high imbalance in category sizes and dominance of category-specific features we found it better to train dedicated ML per category models. Another big reason for this was that a major change to a single category, such as change in definition, large addition of new POIs or labels, did not require us to retrain, launch and measure impact on all the categories, but instead conveniently work on a single category in isolation.</p><p><strong>Lakefront ML model</strong></p><p><strong>Features</strong>: the first step was to build features, with the most important one being distance to Lake POI. We started with collecting Lake POIs represented as a single point and later added lake boundaries that trace the lake, which greatly improved the accuracy of being able to pull listings near the boundary. However, as shown in Figure 4, even then there were many edge cases that lead to mistakes in rule-based listing assignment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/344/0*6Nx7UjAUaqzjECuG" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/534/0*dqWLL502IAax6Z2G" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/864/0*GLKr66XZFXCOU2Jx" /><figcaption>Figure 4. Examples of imperfect POI (left) and complex geography: highway between lake and home (middle), long backyards (right)</figcaption></figure><p>These include imperfect lake boundaries that can be inside the water or outside on land, highways in between lake and houses, houses on cliffs, imperfect listing location, missing POIs, and POIs that are not actual lakes, like reservoirs, ponds etc. For this reason, it proved beneficial to combine POI data with other listing signals as ML model features and then use the model to proactively improve the Lake POI database.</p><p>One modeling maneuver that proved to be useful here was <strong>feature dropout</strong>. Since most of the features were also used for generating rule-based candidates that were graded by agents, resulting in labels that are used by the ML model, there was a risk of overfitting and limited pattern discovery beyond the rules.</p><p>To address this problem, during training we would randomly drop some feature signals, such as distance from Lake POI, from some listings. As a result, the model did not over rely on the dominant POI feature, which allowed listings to have a high ML score even if they are not close to any known Lake POI. This allowed us to find missing POIs and add them to our database.</p><p><strong>Labels</strong>: <strong>Positive labels </strong>were assigned to listings agents tagged as <em>Lakefront</em>, <strong>Negative labels </strong>were assigned to listings sent for review as <em>Lakefront</em> candidates but rejected (<strong>Hard negatives </strong>from modeling perspective). We also sampled negatives from related <em>Lake House </em>category<em> </em>that allows greater distance to lake (<strong>Easier negatives</strong>) and listings tagged in other categories (<strong>Easiest negatives</strong>)</p><p><strong>Train / Test split:</strong> 70:30 random split, where we had special handling of distance and embedding similarity features not to leak the label.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*TWLK6uKR-pQuMjms" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*YZfAoBLNtWzn-2wa" /><figcaption>Figure 5. Lakefront ML model feature importance and performance evaluation</figcaption></figure><p>We trained several models using different feature subsets. We were interested in how well POI data can do on its own and what improvements can additional signals provide. As it can be observed in Figure 5, the POI distance is the most important feature by far. However, when used on its own it cannot approach the ML model performance. Specifically, the ML model improves Average Precision by 23%, from 0.74 to 0.91, which confirmed our hypothesis.</p><p>Since the POI feature is the most important feature we invested in improving it by adding new POIs and refining existing POIs. This proved to be beneficial as the ML model using <em>improved</em> POI features greatly outperforms the model that used <em>initial</em> POI features (Figure 5).</p><p>The process of Lake POI refinement included leveraging trained ML model to<strong> find missing or imperfect POIs</strong> by inspecting listings that have a high model score but are far from existing Lake POIs (Figure 6 left) and <strong>removing wrong POIs</strong> by inspecting listings that have a low model score but are very close to an existing Lake POI (Figure 6 right)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6zlNgooTq5VZuw1d" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GuREMpTIxhzDPbMO" /><figcaption>Figure 6. Process of finding missing POIs (Left) and wrong POIs (Right)</figcaption></figure><p><strong>Sending confident listings to production: </strong>using the test set Precision-Recall curve we found a threshold that achieves 90% Precision. We used this threshold to make a decision on which candidates can go directly to production and which need to be sent for human review first.</p><h4><strong>Cover Image ML model</strong></h4><p>To carry out the second agent task with ML, we needed to train a different type of ML model. One whose task would be to choose the most appropriate listing cover photo given the category context. For example, choosing a listing photo with a lake view for the Lakefront category.</p><p>We tested several out of the box object detection models as well as several in-house solutions trained using human review data, i.e. (listing id, category, cover photo id) tuples. We found that the best cover photo selection accuracy was achieved by fine-tuning a <a href="https://huggingface.co/google/vit-base-patch16-224-in21k">Vision Transformer model</a> (VT) using our human review data. Once trained, the model can score all listing photos and decide which one is the best cover photo for a given category.</p><p>To evaluate the model we used a hold out dataset and tested if the agent selected listing photo for a particular category was within the top 3 highest scoring VT model photos for the same category. The average Top 3 precision on all categories was 70%, which we found satisfactory.</p><p>To further test the model we judged if the VT selected photo represented the category better than the Host selected cover photo (Figure 7). It was found that the VT model can select a better photo in 77% of the cases. It should be noted that the Host selected cover photo is typically chosen without taking any category into account, as the one that best represents the listing in the search feed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iavt2RDI2GWF1aME" /><figcaption>Figure 7. Vision Transformer vs. Host selected cover photo selection for the same listing for Lakefront category</figcaption></figure><p>In addition to selecting the best cover photo for candidates that are sent to production by the ML categorization model, the VT model was also used to speed up the human review process. By ordering the candidate listing photos in descending order of the VT score we were able to improve the time it takes the agents to make a decision on a category and cover photo by 18%.</p><p>Finally, for some highly visual categories, such as <em>Design</em>, <em>Creative spaces</em>, the VT model proved to be useful for direct candidate generation.</p><h4>Quality ML Model</h4><p>The final human review task is to judge the quality of the listing by selecting one of the four tiers: Most Inspiring, High Quality, Acceptable, Low Quality. As we will discuss in Part III of the blog series, the quality plays a role in ranking of listings in the search feed.</p><p>To train an ML model that can predict quality of a listing we used a combination of engagement, quality and visual signals to create a feature set and agent quality tags to create labels. The features included review ratings, wishlists, image quality, embedding signals and listing amenities and attributes, such as price, number of guests, etc.</p><p>Given the multi-class setup with four quality tiers, we experimented with different loss functions (pairwise loss, one-vs-all, one-vs-one, multi label, etc.). We then compared the ROC curves of different strategies on a hold-out set and the binary one-vs-all models performed the best.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CwdBkRcZn_r96Sh6" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*Of2jGUxhGbm5QFDn" /><figcaption>Figure 8: Quality ML model feature importance and ROC curve</figcaption></figure><p>In addition to playing a role in search ranking, the Quality ML score also played a role in the human review prioritization logic. With all three ML models functional for all three human review tasks, we could now streamline the review process and send more candidates directly to production, while also prioritizing some for human review. This prioritization plays an important role in the system because listings that are vetted by humans may rank higher in the category feed.</p><p>There were several factors to consider when prioritizing listings for human review, including listing category confidence score, listing quality, bookability and popularity of the region. The best strategy proved to be a combination of those factors. In Figure 9 we show the top candidates for human review for several categories at the time of writing this post.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*H4pV-jJu1XWSu7lI" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*RQGEp-CfzSwegbvQ" /><figcaption>Figure 9: Listing prioritized for review in 4 different categories</figcaption></figure><p>Once graded, those labels are then used for periodical model re-training in an active feedback loop that continuously improves the category accuracy and coverage.</p><h3>Future work</h3><p>Our future work involves iterating on the three ML models in several directions, including generating a larger set of labels using generative vision models and potentially combining them into a single multi-task model. We are also exploring ways of using Large Language Models (LLMs) for conducting category review tasks</p><p>If this type of work interests you, check out some of our related <a href="https://careers.airbnb.com/">roles</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=35b78a837725" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-human-in-the-loop-35b78a837725">Building Airbnb Categories with ML &amp; Human in the Loop</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Prioritizing Home Attributes Based on Guest Interest]]></title>
            <link>https://medium.com/airbnb-engineering/prioritizing-home-attributes-based-on-guest-interest-3c49b827e51a?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/3c49b827e51a</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[scalability]]></category>
            <category><![CDATA[named-entity-recognition]]></category>
            <category><![CDATA[prioritization]]></category>
            <dc:creator><![CDATA[Joy Jing]]></dc:creator>
            <pubDate>Thu, 16 Feb 2023 17:05:39 GMT</pubDate>
            <atom:updated>2023-02-16T17:05:38.931Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>How Airbnb leverages ML to derive guest interest from unstructured text data and provide personalized recommendations to Hosts</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yoNhrvu8spSI6SuHSCbSkQ.jpeg" /></figure><p><strong>By: </strong><a href="https://www.linkedin.com/in/joyjing1/"><strong>Joy Jing</strong></a><strong> and </strong><a href="https://www.linkedin.com/in/jing-julia-xia-029b3a123/"><strong>Jing Xia</strong></a></p><p>At Airbnb, we endeavor to build a world where anyone can belong anywhere. We strive to understand what our guests care about and match them with Hosts who can provide what they are looking for. What better source for guest preferences than the guests themselves?</p><p>We built a system called the <strong>Attribute Prioritization System</strong> (APS) to listen to our guests’ needs in a home: What are they requesting in messages to Hosts? What are they commenting on in reviews? What are common requests when calling customer support? And how does it differ by the home’s location, property type, price, as well as guests’ travel needs?</p><p>With this personalized understanding of what home amenities, facilities, and location features (i.e. “home attributes”) matter most to our guests, we advise Hosts on which home attributes to acquire, merchandize, and verify. We can also display to guests the home attributes that are most relevant to their destination and needs.</p><p>We do this through a scalable, platformized, and data-driven engineering system. This blog post describes the science and engineering behind the system.</p><p><strong>What do guests care about?</strong></p><p>First, to determine what matters most to our guests in a home, we look at what guests request, comment on, and contact customer support about the most. Are they asking a Host whether they have wifi, free parking, a private hot tub, or access to the beach?</p><p>To parse this unstructured data at scale, Airbnb built <strong>LATEX</strong> (<strong>L</strong>isting <strong>AT</strong>tribute <strong>EX</strong>traction), a machine learning system that can extract home attributes from unstructured text data like guest messages and reviews, customer support tickets, and listing descriptions. LATEX accomplishes this in two steps:</p><ol><li>A <strong>named entity recognition (NER) module</strong> extracts key phrases from unstructured text data</li><li>An <strong>entity mapping module</strong> then maps these key phrases to home attributes</li></ol><p>The <a href="https://spacy.io/usage/linguistic-features#named-entities">named entity recognition (NER)</a> module uses <a href="https://arxiv.org/pdf/1408.5882.pdf">textCNN (convolutional neural network for text)</a> and is trained and fine tuned on human labeled text data from various data sources within Airbnb. In the training dataset, we label each phrase that falls into the following five categories: Amenity, Activity, Event, Specific POI (i.e. “Lake Tahoe”), or generic POI (i.e. “post office”).</p><p>The entity mapping module uses an unsupervised learning approach to map these phrases to home attributes. To achieve this, we compute the cosine distance between the candidate phrase and the attribute label in the fine-tuned word embedding space. We consider the closest mapping to be the referenced attribute, and can calculate a confidence score for the mapping.</p><p>We then calculate how frequently an entity is referenced in each text source (i.e. messages, reviews, customer service tickets), and aggregate the normalized frequency across text sources. Home attributes with many mentions are considered more important.</p><p>With this system, we are able to gain insight into what guests are interested in, even highlighting new entities that we may not yet support. The scalable engineering system also allows us to improve the model by onboarding additional data sources and languages.</p><figure><img alt="An example of a listing’s description with keywords highlighted and labeled by the Latex NER model." src="https://cdn-images-1.medium.com/max/1024/1*4dJVVVQqLcv_kN6bWUtmJw.png" /><figcaption>An example of a listing’s description with keywords highlighted and labeled by the Latex NER model.</figcaption></figure><p><strong>What do guests care about for different types of homes?</strong></p><p>What guests look for in a mountain cabin is different from an urban apartment. Gaining a more complete understanding of guests’ needs in an Airbnb home enables us to provide more personalized guidance to Hosts.</p><p>To achieve this, we calculate a unique ranking of attributes for each home. Based on the characteristics of a home–location, property type, capacity, luxury level, etc–we predict how frequently each attribute will be mentioned in messages, reviews, and customer service tickets. We then use these predicted frequencies to calculate a customized importance score that is used to rank all possible attributes of a home.</p><p>For example, let us consider a mountain cabin that can host six people with an average daily price of $50. In determining what is most important for potential guests, we learn from what is most talked about for other homes that share these same characteristics. The result: hot tub, fire pit, lake view, mountain view, grill, and kayak. In contrast, what’s important for an urban apartment are: parking, restaurants, grocery stores, and subway stations.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nxdxj3lB48CeTvalc4Bn_A.jpeg" /><figcaption><strong>Image:</strong> An example image of a mountain cabin home</figcaption></figure><figure><img alt="An example of home attributes ranked for a mountain cabin vs an urban apartment." src="https://cdn-images-1.medium.com/max/369/1*kO7ePY4VQxE1lSPhHJ50pA.png" /><figcaption>An example of home attributes ranked for a mountain cabin vs an urban apartment.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*s_rEFw4LE1BQkHnVsU1-Ig.jpeg" /><figcaption><strong>Image:</strong> An example of an urban apartment home</figcaption></figure><p>We could directly aggregate the frequency of keyword usage amongst similar homes. But this approach would run into issues at scale; the cardinality of our home segments could grow exponentially large, with sparse data in very unique segments. Instead, we built an inference model that uses the raw keyword frequency data to infer the expected frequency for a segment. This inference approach is scalable as we use finer and more dimensions to characterize our homes. This allows us to support our Hosts to best highlight their unique and diverse collection of homes.</p><p><strong>How can guests’ preferences help Hosts improve?</strong></p><p>Now that we have a granular understanding of what guests want, we can help Hosts showcase what guests are looking for by:</p><ul><li>Recommending that Hosts acquire an amenity guests often request (i.e. coffee maker)</li><li>Merchandizing an existing home attribute that guests tend to comment favorably on in reviews (i.e. patio)</li><li>Clarifying popular facilities that may end up in requests to customer support (i.e. the privacy and ability to access a pool)</li></ul><p>But to make these recommendations relevant, it’s not enough to know what guests want. We also need to be sure about what’s already in the home. This turns out to be trickier than asking the Host due to the 800+ home attributes we collect. Most Hosts aren’t able to immediately and accurately add all of the attributes their home has, especially since amenities like a crib mean different things to different people. To fill in some of the gaps, we leverage guests feedback for amenities and facilities they have seen or used. In addition, some home attributes are available from trustworthy third parties, such as real estate or geolocation databases that can provide square footage, bedroom count, or if the home is overlooking a lake or beach. We’re able to build a truly complete picture of a home by leveraging data from our Hosts, guests, and trustworthy third parties.</p><p>We utilize several different models, including a Bayesian inference model that increases in confidence as more guests confirm that the home has an attribute. We also leverage a supervised neural network WiDeText machine learning model that uses features about the home to predict the likelihood that the next guest will confirm the attribute’s existence.</p><p>Together with our estimate of how important certain home attributes are for a home, and the likelihood that the home attribute already exists or needs clarification, we are able to give personalized and relevant recommendations to Hosts on what to acquire, merchandize, and clarify when promoting their home on Airbnb.</p><figure><img alt="Cards shown to Hosts to better promote their listings." src="https://cdn-images-1.medium.com/max/1024/1*vtzF4NTBo3XDhVdiXKWl1Q.png" /><figcaption>Cards shown to Hosts to better promote their listings.</figcaption></figure><p><strong>What’s next?</strong></p><p>This is the first time we’ve known what attributes our guests want down to the home level. What’s important varies greatly based on home location and trip type.</p><p>This full-stack prioritization system has allowed us to give more relevant and personalized advice to Hosts, to merchandize what guests are looking for, and to accurately represent popular and contentious attributes. When Hosts accurately describe their homes and highlight what guests care about, guests can find their perfect vacation home more easily.</p><p>We are currently experimenting with highlighting amenities that are most important for each type of home (i.e. kayak for mountain cabin, parking for urban apartment) on the home’s product description page. We believe we can leverage the knowledge gained to improve search and to determine which home attributes are most important for different categories of homes.</p><p>On the Host side, we’re expanding this prioritization methodology to encompass additional tips and insights into how Hosts can make their listings even more desirable. This includes actions like freeing up popular nights, offering discounts, and adjusting settings. By leveraging unstructured text data to help guests connect with their perfect Host and home, we hope to foster a world where anyone can belong anywhere.</p><p>If this type of work interests you, check out some of our related positions at <a href="https://careers.airbnb.com/">Careers at Airbnb</a>!</p><h3>Acknowledgments</h3><p>It takes a village to build such a robust full-stack platform. Special thanks to (alphabetical by last name) <a href="https://www.linkedin.com/in/uabbasi/">Usman Abbasi</a>, <a href="https://www.linkedin.com/in/deanchen1/">Dean Chen</a>, <a href="https://www.linkedin.com/in/guillaumeguy/">Guillaume Guy</a>, <a href="https://www.linkedin.com/in/noah-hendrix-2b148366/">Noah Hendrix</a>, <a href="https://www.linkedin.com/in/hwlical/">Hongwei Li</a>, <a href="https://www.linkedin.com/in/xiao-l-593679194/">Xiao Li</a>, <a href="https://www.linkedin.com/in/saraxliu/">Sara Liu</a>, <a href="https://www.linkedin.com/in/qianru-ma-91850749/">Qianru Ma</a>, <a href="https://www.linkedin.com/in/dan-nguyen-b8817a34/">Dan Nguyen</a>, <a href="https://www.linkedin.com/in/nguyenmartin/">Martin Nguyen</a>, <a href="https://www.linkedin.com/in/brennanpolley/">Brennan Polley</a>, <a href="https://www.linkedin.com/in/pontefederico/">Federico Ponte</a>, <a href="https://www.linkedin.com/in/jose-toti-rodriguez-0b840463/">Jose Rodriguez</a>, <a href="https://www.linkedin.com/in/peng-wang-13117371/">Peng Wang</a>, <a href="https://www.linkedin.com/in/rongru-yan-7077a036/">Rongru Yan</a>, <a href="https://www.linkedin.com/in/meng-yu-b013011/">Meng Yu</a>, <a href="https://www.linkedin.com/in/luzhangtracy/">Lu Zhang</a> for their contributions, dedication, expertise, and thoughtfulness!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3c49b827e51a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/prioritizing-home-attributes-based-on-guest-interest-3c49b827e51a">Prioritizing Home Attributes Based on Guest Interest</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Learning To Rank Diversely]]></title>
            <link>https://medium.com/airbnb-engineering/learning-to-rank-diversely-add6b1929621?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/add6b1929621</guid>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[airbnb]]></category>
            <dc:creator><![CDATA[Malay Haldar]]></dc:creator>
            <pubDate>Mon, 30 Jan 2023 20:54:33 GMT</pubDate>
            <atom:updated>2023-01-30T22:33:57.097Z</atom:updated>
            <content:encoded><![CDATA[<p>by <a href="https://medium.com/@malay.haldar">Malay Haldar</a>, <a href="https://medium.com/@liweihe">Liwei He</a> &amp; <a href="https://medium.com/@mooseabdool">Moose Abdool</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f7AlRXdrhZog2gONttd3TA.jpeg" /></figure><p>Airbnb connects millions of guests and Hosts everyday. Most of these connections are forged through search, the results of which are determined by a neural network–based ranking algorithm. While this neural network is adept at selecting <em>individual listings</em> for guests, we recently improved the neural network to better select the overall <em>collection of listings</em> that make up a search result. In this post, we dive deeper into this recent breakthrough that enhances the diversity of listings in search results.</p><h3>How Does Ranking Work?</h3><p>The ranking neural network finds the best listings to surface for a given query by comparing two listings at a time and predicting which one has the higher probability of getting booked. To generate this probability estimate, the neural network places different weights on various listing attributes such as price, location and reviews. These weights are then refined by comparing booked listings against not-booked listings from search logs, with the objective of assigning higher probabilities to booked listings over the not-booked ones.</p><p>What does the ranking neural network learn in the process? As an example, a concept the neural network picks up is that lower prices are preferred. This is illustrated in the figure below, which plots increasing price on the x-axis and its corresponding effect on normalized model scores on the y-axis. Increasing price makes model scores go down, which makes intuitive sense since the majority of bookings at Airbnb skew towards the economical range.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZTOExPdRXDdibtff" /><figcaption><em>Relation between model scores and percent price increase</em></figcaption></figure><p>But price is not the only feature for which the model learns such concepts. Other features such as the listing’s distance from the query location, number of reviews, number of bedrooms, and photo quality can all exhibit such trends. Much of the complexity of the neural network is in balancing all these various factors, tuning them to the best possible tradeoffs that fit all cities and all seasons.</p><h3>Can One Size Fit All?</h3><p>The way the ranking neural network is constructed, its booking probability estimate for a listing is determined by how many guests in the past have booked listings with similar combinations of price, location, reviews, etc. The notion of higher booking probability essentially translates to what the majority of guests have preferred in the past. For instance, there is a strong correlation between high booking probabilities and low listing prices. The booking probabilities are tailored to location, guest count and trip length, among other factors. However, within that context, the ranking algorithm up-ranks listings that the largest fraction of the guest population would have preferred. This logic is repeated for each position in the search result, so the entire search result is constructed to favor the majority preference of guests. We refer to this as the <em>Majority principle</em> in ranking — the overwhelming tendency of the ranking algorithm to follow the majority at every position.</p><p>But majority preference isn’t the best way to represent the preferences of the entire guest population. Continuing with our discussion of listing prices, we look at the distribution of booked prices for a popular destination — Rome — and specifically focus on two night trips for two guests. This allows us to focus on price variations due to listing quality alone, and eliminate most of other variabilities. Figure below plots the distribution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-_xIFlwmpe3tQ-w4" /><figcaption><em>Pareto principle: 50/50 split of booking value corresponds to roughly 80/20 split of bookings</em></figcaption></figure><p>The x-axis corresponds to booking values in USD, log-scale. Left y-axis is the number of bookings corresponding to each price point on the x-axis. The orange shape confirms the log-normal distribution of booking value. The red line plots the percentage of total bookings in Rome that have booking value less than or equal to the corresponding point on x-axis, and the green line plots the percentage of total booking value for Rome covered by those bookings. Splitting total booking value 50/50 splits bookings into two unequal groups of ~80/20. In other words, 20% of bookings account for 50% of booking value. For this 20% minority, cheaper is not necessarily better, and their preference leans more towards quality. This demonstrates the <em>Pareto principle</em>, a coarse view of the heterogeneity of preference among guests.</p><p>While the Pareto principle suggests the need to accommodate a wider range of preferences, the Majority principle summarizes what happens in practice. When it comes to search ranking, the Majority principle is at odds with the Pareto principle.</p><h3>Diversifying by Reducing Similarity</h3><p>The lack of diversity of listings in search results can alternatively be viewed as listings being too similar to each other. Reducing inter-listing similarity, therefore, can remove some of the listings from search results that are redundant choices to begin with. For instance, instead of dedicating every position in the search result to economical listings, we can use some of the positions for quality listings. The challenge here is how to quantify this inter-listing similarity, and how to balance it against the base booking probabilities estimated by the ranking neural network.</p><p>To solve this problem, we build another neural network, a companion to the ranking neural network. The task of this companion neural network is to estimate the similarity of a given listing to previously placed listings in a search result.</p><p>To train the similarity neural network, we construct the training data from logged search results. All search results where the booked listing appears as the top result are discarded. For the remaining search results, we set aside the top result as a special listing, called the antecedent listing. Using listings from the second position onwards, we create pairs of booked and not-booked listings. This is summarized in the figure below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6JpNCFVTujDdEZzQ" /><figcaption><em>Construction of training examples from logged search results</em></figcaption></figure><p>We then train a ranking neural network to assign a higher booking probability to the booked listing compared to the not-booked listing, but with a modification — we subtract the output of the similarity neural network that supplies a similarity estimate between the given listing vs the antecedent listing. The reasoning here is that guests who skipped the antecedent listing and then went on to book a listing from results down below must have picked something that is dissimilar to the antecedent listing. Otherwise, they would have booked the antecedent listing itself.</p><p>Once trained, we are ready to use the similarity network for ranking listings online. During ranking, we start by filling the top-most result with the listing that has the highest booking probability. For subsequent positions, we select the listing that has the highest booking probability amongst the remaining listings, after discounting its similarity to the listings already placed above. The search result is constructed iteratively, with each position trying to be diverse from all the positions above it. Listings too similar to the ones already placed effectively get down-ranked as illustrated below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4zCEcB5KZ56pFMD2" /><figcaption><em>Reranking of listings based on similarity to top results</em></figcaption></figure><p>Following this strategy led to one of the most impactful changes to ranking in recent times. We observed an increase of 0.29% in uncancelled bookings, along with a 0.8% increase in booking value. The increase in booking value is far greater than the increase in bookings because the increase is dominated by high-quality listings which correlate with higher value. Increase in booking value provides us with a reliable proxy to measure increase in quality, although increase in booking value is not the target. We also observed some direct evidence of increase in quality of bookings — a 0.4% increase in 5-star ratings, indicating higher guest satisfaction for the entire trip.</p><h3>Further Reading</h3><p>We discussed reducing similarity between listings to improve the overall utility of search results and cater to diverse guest preferences. While intuitive, to put the idea in practice we need a rigorous foundation in machine learning, which is described in <a href="https://arxiv.org/pdf/2210.07774.pdf">our technical paper</a>. Up next, we are looking deeper into the location diversity of results. We welcome all comments and suggestions for the technical paper and the blog post.</p><p><em>Interested in working at Airbnb? Check out </em><a href="https://careers.airbnb.com/positions/"><em>these open roles</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=add6b1929621" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/learning-to-rank-diversely-add6b1929621">Learning To Rank Diversely</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making Airbnb’s Android app more accessible]]></title>
            <link>https://medium.com/airbnb-engineering/making-airbnbs-android-app-more-accessible-75618172be6?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/75618172be6</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[android]]></category>
            <category><![CDATA[accessibility]]></category>
            <dc:creator><![CDATA[Julia Fu]]></dc:creator>
            <pubDate>Wed, 11 Jan 2023 19:09:48 GMT</pubDate>
            <atom:updated>2023-01-11T19:09:48.164Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>By:</strong> <a href="https://www.linkedin.com/in/julia-fu-3844b712/">Julia Fu</a>, <a href="https://www.linkedin.com/in/peter-elliott-777125144/">Peter Elliott</a></p><figure><img alt="Decorative image for the blog post." src="https://cdn-images-1.medium.com/max/1024/0*3kx7B-Au3UoHoMon" /></figure><p>At Airbnb, we have been consciously designing and building products to be equally usable by all users. Making our mobile apps and websites more accessible not only aligns with our company’s mission of creating a world where people can belong anywhere, but also supports the civil rights of people with disabilities and complies with the law.</p><p>In this article, we highlight some of the efforts we have made to make the app more accessible, for example, labeling UI elements, grouping related content, supporting large font scale, providing heading and page names. The Airbnb app is one of the most popular travel apps with millions of users and supports many features. Making such a complex app more accessible is a huge endeavor that we are continuously working on.</p><h3>Part I: Build for all: best practices we apply</h3><p>At Airbnb, we follow industry best practices to make the Android app accessible. If you are interested, you can find all best practices we follow from the <a href="https://developer.android.com/guide/topics/ui/accessibility/principles">official Android documentation</a> for platform specific guidelines and the <a href="https://www.w3.org/WAI/standards-guidelines/wcag/">Web Content Accessibility Guidelines</a> as an industry standard. Here we want to highlight a few examples where we apply the best practices:</p><h4>Best Practice: content descriptions</h4><p>Everything shall have accurate content descriptions unless they should be ignored by assistive technology. In these examples, the share button has a content description that TalkBack reads aloud. TalkBack skips the house icon.</p><figure><img alt="Share button highlighted on a listing page. TalkBack output says ‘Share, Button’." src="https://cdn-images-1.medium.com/max/720/0*rl6obaypyKaN2bZ8" /></figure><figure><img alt="Row highlighted with a star icon, bolded and underlined text, another icon with text, and subtitle text on a listing page. TalkBack output omits content for the icons and says ‘Rated 4.88 out of 5 from 203 reviews. Superhost Willington, Connecticut, United States’." src="https://cdn-images-1.medium.com/max/720/0*rYFpxuiMfj2wfJ5X" /></figure><h4>Best practice: grouping</h4><p>Elements of a natural group can be announced together with focusable containers for better usability and accuracy. For instance, Talkback reads all listing content on the card together.</p><figure><img alt="Listing card with a photo, rating, title, and price highlighted on a listing collections page. TalkBack output reads all the information together." src="https://cdn-images-1.medium.com/max/346/1*fMMUt2GVUIjcgw4nn6CY8w.png" /></figure><h4>Best practice: font scale</h4><p>UI shall be usable when the user increases the system font scale.</p><p>Default vs enlarged font scale:</p><figure><img alt="A listing search page with a search bar, tabs with category names, two listing cards, a floating Map button, and a navigation bar at the bottom" src="https://cdn-images-1.medium.com/max/800/0*pZh9ZmVQHUO5EQTV" /></figure><figure><img alt="Same page in a larger font, with only the first listing card visible" src="https://cdn-images-1.medium.com/max/800/0*ybfVcm1W02sTqpOo" /><figcaption>Default font scale on the left. Enlarged font scale on the right.</figcaption></figure><h4>Scaling best practices</h4><p>The Airbnb Android app is a large app with many screens. It would be exhausting and not scalable if we needed to add accessibility code everywhere. Fortunately, our <a href="https://airbnb.design/the-way-we-build/">Design Language System</a> enables us to broadly apply these best practices across product surfaces in a highly efficient way. Every screen is built with a collection of reusable UI components. When we improve the accessibility for one component, the change applies to all the pages with this component as part of the view. This has a long-lasting positive effect on our app’s accessibility improvements. Here’s an example:</p><figure><img alt="The accessibility filters screen for a stay includes a large icon, a heading, subheadings, multiple checkbox options for various features (such as “no stairs or steps to enter”), and a footer.." src="https://cdn-images-1.medium.com/max/800/0*PSzu5IzbOodIuoe4" /></figure><figure><img alt="The same screen with component names overlaid on top, such as DocumentMarquee for the heading, LeadingIconRow and CheckboxRows." src="https://cdn-images-1.medium.com/max/800/0*AsHDrHk6XYKmsrx_" /></figure><p>Take <em>SectionHeader</em> as an example. This UI component is used to communicate the structure on the page and group content together. We mark this component to be an accessibility heading in the component code so it is accessible in all screens that contain this component.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e09b44464bde190976231f18cf4b5653/href">https://medium.com/media/e09b44464bde190976231f18cf4b5653/href</a></iframe><h3>Part II: Empower engineers with automated checks</h3><p>We invested in automated accessibility testing and linting to run with every code commit, which creates a quick feedback loop for engineers and empowers them to make the app accessible at code writing time. The checks are fast, reliable, and scale well with our fast-growing features in the Android app.</p><h4>Automated testing</h4><p>We set up Espresso-based automated testing to check for accessibility issues. <a href="https://developer.android.com/guide/topics/ui/accessibility/testing#espresso">Espresso</a> is a popular testing library for Android UI with built-in accessibility checks. It supports a comprehensive set of accessibility rules and is easy to set up:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4b79832fa5516244fd0a65f3f9cc3eaa/href">https://medium.com/media/4b79832fa5516244fd0a65f3f9cc3eaa/href</a></iframe><p>If accessibility checks fail, the test outputs an error stack trace that engineers can use to debug the issue. For example:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0174ce8368a7f727459bd9821d20aa35/href">https://medium.com/media/0174ce8368a7f727459bd9821d20aa35/href</a></iframe><p>In this example, engineers can provide a content description to the image view to satisfy accessibility requirements.</p><p>We also screenshot test our components with a larger font size to ensure the behavior is correct using <a href="https://medium.com/airbnb-engineering/better-android-testing-at-airbnb-a77ac9531cab">Happo</a>.</p><figure><img alt="A screenshot test of the marquee component using a larger system font. The marquee component contains a vertical stack with an icon, smaller kicker text, larger title text, and smaller subtitle text." src="https://cdn-images-1.medium.com/max/346/1*2uy1-e-ZTJHMtE2ckTALxQ.png" /></figure><h4>Linting</h4><p>In addition to automated testing, we also enabled linting, including <a href="https://developer.android.com/studio/write/lint">Android Lint</a> rules for accessibility and custom lint rules built with <a href="https://github.com/pinterest/ktlint">Ktlint</a>.</p><p>Here is an example of an Android accessibility lint rule:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1b1b4a919b19bc68033e0a72e1071863/href">https://medium.com/media/1b1b4a919b19bc68033e0a72e1071863/href</a></iframe><p>Besides the built-in Android Lint, we also use Ktlint to build custom lint rules. For instance, when a user navigates to a new screen, we provide a page name for a screen reader to announce. We use the following rule to make sure that the page name is localized.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5fb8b7641ffa3f5b714a872bd46f55c9/href">https://medium.com/media/5fb8b7641ffa3f5b714a872bd46f55c9/href</a></iframe><p>Lint rules are straightforward to set up and provide timely feedback, but linting has limitations — it can only perform static code analysis.</p><p>Today, these automated checks run as part of CI (Continuous Integration) checks for every code commit. If a pull request does not pass the checks, it will be blocked from being merged into the primary code branch. We still use manual testing to cover the areas that automated checks do not cover, such as the traversal order of UI elements on a page. Automated and manual checks complement each other well.</p><h3>Part III: Looking into the future: Accessibility with Compose</h3><p>Over the past year, we have been integrating Jetpack Compose into our app. Google’s <a href="https://developer.android.com/jetpack/compose/accessibility">Accessibility in Compose documentation</a> has been a great resource to ensure our Compose components and screens remain accessible. While there are some notable things missing that existed with Views (e.g. focus order modification), Compose is still a young library and we look forward to future improvements. Here are a couple of things worth mentioning about our Compose-specific accessibility tooling:</p><h4>Proactively encourage content descriptions in the API</h4><p>One of our guidelines for UI components is that content descriptions exposed via a function parameter should not use a default value. This brings accessibility to the top of mind when an engineer uses the component as they need to consider what value to pass. A null value is still acceptable in cases where that UI element is not important for accessibility.</p><figure><img alt="A screenshot of an IconRow component that shows an icon beside two lines of text." src="https://cdn-images-1.medium.com/max/346/1*IeCWV4mFIJfWHe7lUJKdQg.png" /></figure><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/89979317838408078250e7e3e940a6f0/href">https://medium.com/media/89979317838408078250e7e3e940a6f0/href</a></iframe><h4>Page name announcements</h4><figure><img alt="A screenshot of several photos of a listing with the up button focused in the toolbar. The TalkBack output says “Photo tour of the listing”. We allow hosts to add captions for photos. If captions are provided, they are announced by Talkback when a user clicks on the photo. If no captions are provided, we do not generate them." src="https://cdn-images-1.medium.com/max/346/1*zZ6NlICykhaRaVG_hkbVBg.png" /></figure><p>When using Fragments and Views, we use the <em>View.setAccessibilityPaneTitle()</em> and <em>View.announceForAccessibility()</em> APIs when navigating to a new screen to announce a descriptive page name to the user. These APIs do not exist in Compose but we wanted to keep the functionality since it helps to provide more context as to what the new screen displays. Our current workaround sets certain semantics on the screen’s outer composable:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d81a055b12947d2895a8d7f6e5b89cfb/href">https://medium.com/media/d81a055b12947d2895a8d7f6e5b89cfb/href</a></iframe><p>We use the <em>liveRegion</em> property so changes can be announced when the content description changes. This is useful for pages whose entire content is determined by a response from the server. In this case, TalkBack would announce “Content Loading” while the network request is pending, followed by “Content Loaded” when it completes, and finally the page description defined in the server response. One downside of this approach is that it requires the outer container to be focusable, which requires an additional navigation action to get to the content.</p><h3>Closing thoughts</h3><p>Making our Android app more accessible has been an impactful journey. Improving app accessibility involves following best practices, adding rigorous enforcements, continually learning from mistakes, and putting in the work. All of these are worthy efforts to make sure an app works for all users.</p><p>If you are excited about building highly accessible products and the framework to support them, check out some of our related open positions:</p><p><a href="https://careers.airbnb.com/positions/4590099/">Staff Android Software Engineer, Guest</a></p><p><a href="https://careers.airbnb.com/positions/4648432/">Senior iOS Software Engineer, Infrastructure</a></p><h3>Acknowledgments</h3><p>It is a huge endeavor to make a complex app like the Airbnb Android app more accessible. This work wouldn’t be possible without the enormous efforts from the digital accessibility team and the close-knit Android community at Airbnb. Every engineer has contributed to making the features they own accessible. Making the Android app more accessible is an ongoing effort and it could not succeed without all of them.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><p><em>All bookings included in this blog post are intended to illustrate. Airbnb does not endorse or promote these listings or any other accommodations or experiences on the platform.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=75618172be6" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/making-airbnbs-android-app-more-accessible-75618172be6">Making Airbnb’s Android app more accessible</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[When a Picture Is Worth More Than Words]]></title>
            <link>https://medium.com/airbnb-engineering/when-a-picture-is-worth-more-than-words-17718860dcc2?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/17718860dcc2</guid>
            <category><![CDATA[similarity-search]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[deep-learning]]></category>
            <category><![CDATA[aesthetics]]></category>
            <category><![CDATA[computer-vision]]></category>
            <dc:creator><![CDATA[Yuanpei Cao]]></dc:creator>
            <pubDate>Thu, 08 Dec 2022 18:03:50 GMT</pubDate>
            <atom:updated>2022-12-08T18:03:50.795Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb uses visual attributes to enhance the Guest and Host experience</p><p><em>By </em><a href="https://www.linkedin.com/in/yuanpei-cao-792b103b/"><em>Yuanpei Cao</em></a><em>, </em><a href="https://www.linkedin.com/in/bulam/"><em>Bill Ulammandakh</em></a><em>, </em><a href="https://www.linkedin.com/in/hao-wang-2661553/"><em>Hao Wang</em></a><em>, and </em><a href="https://www.linkedin.com/in/hwangtt/"><em>Tony Hwang</em></a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qlUYnDrTVDAYZfqE" /></figure><h3><strong>Introduction</strong></h3><p>On Airbnb, our hosts share unique listings all over the world. There are hundreds of millions of accompanying listing photos on Airbnb. Listing photos contain crucial information about style and design aesthetics that are difficult to convey in words or a fixed list of amenities. Accordingly, multiple teams at Airbnb are now leveraging computer vision to extract and incorporate intangibles from our rich visual data to help guests easily find listings that suit their preferences.</p><p>In previous blog posts titled <a href="https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c"><em>WIDeText: A Multimodal Deep Learning Framework</em></a>,<em> </em><a href="https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3"><em>Categorizing Listing Photos at Airbnb</em></a> and <a href="https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e"><em>Amenity Detection and Beyond — New Frontiers of Computer Vision at Airbnb</em></a>, we explored how we utilize computer vision for room categorization and amenity detection to map listing photos to a taxonomy of discrete concepts. This post goes beyond discrete categories into how Airbnb leverages image aesthetics and embeddings to optimize across various product surfaces including ad content, listing presentation, and listing recommendations.</p><h3>Image aesthetics</h3><p>Attractive photos are as vital as price, reviews, and description during a guest’s Airbnb search journey. To quantify “attractiveness” of photos, we developed a deep learning-based image aesthetics assessment pipeline. The underlying model is a deep convolutional neural network (<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>) trained on human-labeled image aesthetic rating distributions. Each photo was rated on a scale from 1 to 5 by hundreds of photographers based on their personal aesthetic measurements (the higher the rating, the better the aesthetic). Unlike traditional classification tasks that classify the photo into low, medium and high-quality categories, the model was built upon the Earth Mover’s Distance (<a href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance">EMD</a>) as the loss function to predict photographers’ rating distributions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9uDKRsxap29D2jTg" /><figcaption><em>Figure 1. The model that predicts image aesthetics distribution is CNN-based and trained with the EMD loss function. Suppose the ground truth label of a photo is: 10% of users give ratings 1 and 2, respectively, 20% give rating 3, and 30% give ratings 4 and 5, respectively. The corresponding prediction is [0.1, 0.1, 0.2, 0.3, 0.3]</em></figcaption></figure><p>The predicted mean rating is highly correlated with image resolution and listing booking probability, as well as high-end Airbnb listing photo distribution. Rating thresholds are set based on use cases, such as ad photo recommendation on social media and photo order suggestion in the listing onboarding process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oX4zLXa1xuW5dZ1u" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kPzH6oeThUYOm-Lu" /><figcaption><em>Figure 2. Examples of Airbnb listing photos with aesthetics scores higher than the 90% percentile</em></figcaption></figure><h3>Image aesthetic-based ads quality improvement</h3><p>Airbnb uses advertising on social media to attract new customers and inspire our community. The social media platform chooses which ads to run based on millions of Airbnb-provided listing photos.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/750/0*Cs_Sp9Db3uebMDQA" /><figcaption><em>Figure 3. Airbnb Ads displayed on Facebook</em></figcaption></figure><p>Since a visually appealing Airbnb photo can effectively attract users to the platform and considerably increase the ad’s click-through rate (CTR), we utilized the image aesthetic score and room categorization to select the most attractive Airbnb photos of the living room, bedroom, kitchen, and exterior view. The criterion for “good quality” listing photos was set based on the top 50th percentile of the aesthetic score and tuned based on an internal manual aesthetic evaluation of 1K randomly selected listing cover photos. We performed A/B testing for this use case and found that the ad candidates with a higher aesthetic score generated a substantially higher CTR and booking rate.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qkiOeZNUrFiQ9hp0" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3wAi8A6I4pgZh2ti" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EAZedvb_PXHZT4j0" /><figcaption><em>Figure 4. Pre-selected Airbnb Creative Ads through image aesthetics and room type filters</em></figcaption></figure><h3>Automated photo ranking based on home design and room type</h3><p>When posting a new listing on Airbnb, hosts upload numerous photos. Optimally arranging these photos to highlight a home can be time-consuming and challenging. A host may also be uncertain about the ideal arrangement for their images because the work requires making trade-offs between photo attractiveness, photo diversity, and content relevance to guests. More specifically, the first five photos are the most important for listing success as they are the most frequently viewed and crucial to forming the initial guest impression. Accordingly, we developed an automated photo ranking algorithm that selects and orders the first five photos of a home leveraging two visual signals: home design evaluation and room categorization.</p><p>Home design evaluation estimates how well a home is designed from an interior design and architecture perspective. The CNN-based home design evaluation model is trained on Airbnb<em> Plus </em>and<em> Luxe</em> qualification data that assess the aesthetic appeal of each photo’s home design. Airbnb <em>Plus</em> and <em>Luxe</em> listings have passed strict home design evaluation criteria and so the data from their qualification process is well-suited to be used as training labels for a home design evaluation model. The photos are then classified into different room types, such as living room, bedroom, bathroom etc, through the room categorization model. Finally, an algorithm makes trade-offs between photo home design attractiveness, photo relevance, and photo diversity to maximize the booking probability of a home. Below is an example of how a new photo order is suggested. The photo auto-rank feature was launched in Host’s listing onboarding product in 2021, leading to significant lifts in new listing creation and booking success.</p><p><strong>Original ordering</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pnBb2R9FSRaKolwe" /></figure><p><strong>Auto-suggested ordering</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ycy7KI2DK_dhX98a" /><figcaption><em>Figure 5. The example of original photo order (top) uploaded by Airbnb Host and auto-suggested order (bottom) calculated by the proposed algorithm</em></figcaption></figure><h3>Image similarity</h3><p>Beyond aesthetics, photos also capture the general appearance and content. To efficiently represent this information, we encode and compress photos into image embeddings using computer vision models. Image embeddings are compact vector representations of images that represent visual features. These embeddings can be compared against each other with a distance metric that represents similarity in that feature space.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qJaLeY5a3rvkR87m" /><figcaption><em>Figure 6. Image embeddings can be compared by distance metrics like cosine similarity to represent their similarity in the encoded latent space</em></figcaption></figure><p>The features learned by the encoder are directly influenced by the training image data distribution and training objectives. Our labeled room type and amenity classification data allows us to train models on this data distribution to produce semantically meaningful embeddings for listing photo similarity use cases. However, as the quantity and diversity of images on Airbnb grow, it becomes increasingly untenable to rely solely on manually labeled data and supervised training techniques. Consequently, we are currently exploring self-supervised contrastive training to improve our image embedding models. This form of training does not require image labels; instead, it bootstraps contrastive learning with synthetically generated positive and negative pairs. Our image embedding models can then learn key visual features from listing photos without manual supervision.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hU4Ij4D2PUCVyN0V" /><figcaption><em>Figure 7. Introducing random image transformations to synthetically create positive and negative pairs helps refine our image encoders without additional labeling.</em></figcaption></figure><h3>Scalable embedding search</h3><p>It is often impractical to compute exhaustive pairwise embedding similarity, even within focused subsets of millions of items. To support real-time search use cases, such as (near) duplicate photo detection and visual similarity search, we instead perform an approximate nearest neighbor (<a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor">ANN</a>) search. This functionality is largely enabled by an efficient embedding index preprocessing and construction algorithm called Hierarchical Navigable Small World (<a href="https://arxiv.org/abs/1603.09320">HNSW</a>). HNSW builds a hierarchical proximity graph structure that greatly constrains the search space at query time. We scale this horizontally with AWS OpenSearch, where each node contains its own HNSW embedding graphs and Lucene-backed indices that are hydrated periodically and can be queried in parallel. To add real-time embedding ANN search, we have implemented the following index hydration and index search design patterns enabled by existing Airbnb internal platforms.</p><p>To hydrate an embedding index on a periodic basis, all relevant embeddings computed by <a href="https://ieeexplore.ieee.org/document/8964147">Bighead</a>, Airbnb’s end-to-end machine learning platform, are aggregated and persisted into a Hive table. The encoder models producing the embeddings are deployed for both online inference and offline batch processing. Then, the incremental embedding update is synced to the embedding index on AWS OpenSearch through Airflow, our data pipeline orchestration service.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*701twLpGC7cAqooo" /><figcaption><em>Figure 8. Index hydration data pathway</em></figcaption></figure><p>To perform image search, a client service will first verify whether the image’s embedding exists in the OpenSearch index cache to avoid recomputing embeddings unnecessarily. If the embedding is already there, the OpenSearch cluster can return approximate nearest neighbor results to the client without further processing. If there is a cache miss, Bighead is called to compute the image embedding, followed by a request to query the OpenSearch cluster for approximate nearest neighbors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gPnnwDRV1gU6VJ9g" /><figcaption><em>Figure 9. Image similarity search for a previously unseen image</em></figcaption></figure><p>Following this embedding search framework, we are scaling real-time visual search in current production flows and upcoming releases.</p><h3>Expanding Airbnb categories</h3><p><a href="https://www.airbnb.com/2022-summer">Airbnb Categories</a> help our guests discover unique getaways. Some examples are “Amazing views”, “Historical homes”, and “Creative spaces”. These categories do not always share common amenities or discrete attributes, as they often represent an inspirational concept. We are exploring automatic category expansion by identifying similar listings based on their photos, which do capture design aesthetics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*_m9Kv0hZdqYRDAZL" /><figcaption><em>Figure 10. Listing photos from the “Creative spaces” category</em></figcaption></figure><h3>Similar listing recommendations in rebooking assistance</h3><p>In the 2022 Summer Release, Airbnb introduced rebooking assistance to offer guests a smooth experience from Community Support ambassadors when a Host cancels on short notice. For the purpose of recommending comparable listings throughout the rebooking process, a two-tower reservation and listing embedding model ranks candidate listings, updated on a daily basis. As future work, we can consider augmenting the listing representation with image embeddings and enabling real-time search.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*at3MPM2B3Mj-etNu" /><figcaption><em>Figure 11. The example of a landing page that recommends similar listings to guests and Community Support ambassadors in the Rebooking assistance.</em></figcaption></figure><h3>Conclusion</h3><p>Photos contain aesthetic and style-related signals that are difficult to express in words or map to discrete attributes. Airbnb is increasingly leveraging these visual attributes to help our hosts highlight the unique character of their listings and to assist our guests in discovering listings that match their preferences.</p><p>Interested in working at Airbnb? Check out our <a href="https://careers.airbnb.com/">open roles</a>.</p><h3>Acknowledgements</h3><p>Thanks to Teng Wang, Regina Wu, Nan Li, Do-kyum Kim, Tiantian Zhang, Xiaohan Zeng, Mia Zhao, Wayne Zhang, Elaine Liu, Floria Wan, David Staub, Tong Jiang, Cheng Wan, Guillaume Guy, Wei Luo, Hanchen Su, Fan Wu, Pei Xiong, Aaron Yin, Jie Tang, Lifan Yang, Lu Zhang, Mihajlo Grbovic, Alejandro Virrueta, Brennan Polley, Jing Xia, Fanchen Kong, William Zhao, Caroline Leung, Meng Yu, Shijing Yao, Reid Andersen, Xianjun Zhang, Yuqi Zheng, Dapeng Li, and Juchuan Ma for the product collaborations. Also thanks Jenny Chen, Surashree Kulkarni, and Lauren Mackevich for editing.</p><p>Thanks to Ari Balogh, Tina Su, Andy Yasutake, Joy Zhang, Kelvin Xiong, Raj Rajagopal, and Zhong Ren’s leadership support on building computer vision products at Airbnb.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=17718860dcc2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/when-a-picture-is-worth-more-than-words-17718860dcc2">When a Picture Is Worth More Than Words</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Motion Engineering at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/motion-engineering-at-scale-5ffabfc878?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/5ffabfc878</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[animation]]></category>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[swift]]></category>
            <dc:creator><![CDATA[Cal Stephens]]></dc:creator>
            <pubDate>Wed, 07 Dec 2022 18:00:35 GMT</pubDate>
            <atom:updated>2022-12-07T18:00:35.371Z</atom:updated>
            <content:encoded><![CDATA[<p>How Airbnb is applying declarative design patterns to rapidly build fluid transition animations</p><p><strong>By: </strong><a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fRCT2OZX8i42aJrDKPIrSg.jpeg" /></figure><p>Motion is a key part of what makes a digital experience both easy and delightful to use. Fluid transitions between states and screens are key for helping the user preserve context as they navigate throughout a feature. Quick flourishes of animation make an app come alive, and help give it a distinct personality.</p><p>At Airbnb we launch hundreds of features and experiments that have been developed by engineers across many teams. When building at this scale, it’s critical to consider efficiency and maintainability throughout our tech stack–and motion is no exception. Adding animations to a feature needs to be fast and easy. The tooling must compliment and fit naturally with other components of our feature architecture. If an animation takes too long to build or is too difficult to integrate with the overall feature architecture, then it’s often the first part of a product experience that gets dropped when translating from design to implementation.</p><p>In this post, we’ll discuss a new framework for iOS that we’ve created to help make this vision a reality.</p><h3>Imperative UIKit Transitions</h3><p>Let’s consider this transition on the Airbnb app’s homepage, which takes users from search results to an expanded search input screen:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*myWLTeEaraOgt8dMdzNWKQ.gif" /><figcaption>An example transition from Airbnb’s iOS app of expanding and collapsing the search input screen</figcaption></figure><p>The transition is a key part of the design, making the entire search experience feel cohesive and lightweight.</p><p>Within traditional UIKit patterns, there are two ways to build a transition like this. One is to create a single, massive view controller that contains both the search results and the search input screens, and orchestrates a transition between the two states using imperative <a href="https://developer.apple.com/documentation/uikit/uiview/1622418-animate"><em>UIView</em> animation blocks</a>. While this approach is easy to build, it has the downside of tightly coupling these two screens, making them far less maintainable and portable.</p><p>The other approach is to implement each screen as a separate view controller, and create a bespoke <a href="https://developer.apple.com/documentation/uikit/uiviewcontrolleranimatedtransitioning?language=objc"><em>UIViewControllerAnimatedTransitioning</em></a> implementation that extracts relevant views from each view hierarchy and then animates them. This is typically more complicated to implement, but has the key benefit of letting each individual screen be built as a separate <em>UIViewController</em> like you would for any other feature.</p><p>In the past, we’ve built transitions with both of these approaches, and found that they both typically require hundreds of lines of fragile, imperative code. This meant custom transitions were time consuming to build and difficult to maintain, so they were typically not included as part of a team’s main feature development flow.</p><p>A common trend has been to move away from this sort of <em>imperative</em> system design and towards <em>declarative</em> patterns. We use declarative systems extensively at Airbnb–we leverage frameworks like <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670">Epoxy</a> and SwiftUI to declaratively define the layout of each screen. Screens are combined into features and flows using <a href="https://github.com/airbnb/epoxy-ios#epoxypresentations">declarative</a> <a href="https://github.com/airbnb/epoxy-ios#epoxynavigationcontroller">navigation</a> APIs. We’ve found these declarative systems unlock substantial productivity gains, by letting engineers focus on defining how the app should behave and abstracting away the complex underlying implementation details.</p><h3>Declarative Transition Animations</h3><p>To simplify and speed-up the process of adding transitions to our app, we’ve created a <strong>new</strong> <strong>framework for building transitions declaratively</strong>, rather than imperatively as we did before. We’ve found that this new approach has made it much simpler to build custom transitions, and as a result far more engineers have been able to easily add rich and delightful transitions to their screens even on tight timelines<strong><em>.</em></strong></p><p>To perform a transition with this framework, you simply provide the <em>initial</em> state and <em>final</em> state (or in the case of a screen transition, the <em>source</em> and <em>destination</em> view controllers<em>)</em> along with a declarative <em>transition definition</em> of how each individual element on the screen should be animated. The framework’s generic <em>UIViewControllerAnimatedTransitioning</em> implementation handles everything else automatically.</p><p>This new framework has become instrumental to how we build features. It powers many of the new features included in Airbnb’s <a href="https://news.airbnb.com/2022-summer-release/">2022 Summer Release</a><em> </em>and <a href="https://www.airbnb.com/2022-winter">2022 Winter Release</a><em>,</em> helping make them easy and delightful to use:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*KW_2f2UzNCHBOdmXEdKIyw.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*LEVniwcfps4EQYWn4wvo3g.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*_fK31gs2yfPQPoewqmP_9A.gif" /><figcaption>Example transitions in Airbnb’s iOS app from new features introduced in 2022</figcaption></figure><p>As an introduction, let’s start with a example. Here’s a simple “search” interaction where a date picker in a bottom sheet slides up over a page of content:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*XbjUC8R9CnWR3x2qWbwBbw.gif" /><figcaption>An example transition for a simple “search” feature</figcaption></figure><p>In this example, there are two separate view controllers: the search results screen and the date picker screen. Each of the components we want to animate are tagged with an identifier to establish their identity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*H9kFx3NvIlPDqTLirV1LbQ.png" /><figcaption><em>Diagram showing the search results screen and date picker screen annotated with component identifiers</em></figcaption></figure><p>These identifiers let us refer to each component semantically by name, rather than by directly referencing the <em>UIView </em>instance. For example, the <em>Explore.searchNavigationBarPill</em> component on each screen is a separate <em>UIView</em> instance,<em> </em>but since they’re tagged with the same identifier the two view instances are considered separate “states” of the same component.</p><p>Now that we’ve identified the components that we want to animate, we can define <em>how</em> they should animate. For this transition we want:</p><ol><li>The background to fade in</li><li>The bottom sheet to slide up from the bottom of the screen</li><li>The navigation bar to animate between the first state and second state (a “shared element” animation).</li></ol><p>We can express this as a simple transition definition:</p><pre>let transitionDefinition: TransitionDefinition = [<br>  BottomSheet.backgroundView: .crossfade,<br>  BottomSheet.foregroundView: .edgeTranslation(.bottom),<br>  Explore.searchNavigationBarPill: .sharedElement,<br>]</pre><p>Revisiting the example above for expanding and collapsing the search input screen, we want:</p><ol><li>The background to blur</li><li>The top bar and bottom bars to slide in</li><li>The home screen search bar to transition into the “where are you going?” card</li><li>The other two search cards to fade in while staying anchored relative to the “where are you going? card</li></ol><p>Here’s how that animation is defined using the declarative transition definition syntax:</p><pre>let transitionDefinition: TransitionDefinition = [<br>  SearchInput.background: .blur,<br>  SearchInput.topBar: .translateY(-40),<br>  SearchInput.bottomBar: .edgeTranslation(.bottom),<br>  <br>  SearchInput.whereCard: .sharedElement,<br>  SearchInput.whereCardContent: .crossfade,<br>  SearchInput.searchInput: .crossfade,<br>  <br>  SearchInput.whenCard: .anchorTranslation(relativeTo: SearchInput.whereCard),<br>  SearchInput.whoCard: .anchorTranslation(relativeTo: SearchInput.whereCard),<br>]</pre><h3>How It Works</h3><p>This declarative <em>transition definition </em>API is powerful and flexible, but it only tells half the story. To actually perform the animation, our framework provides a generic <em>UIViewControllerAnimatedTransitioning</em> implementation that takes the transition definition and orchestrates the transition animation. To explore how this implementation works, we’ll return to the simple “search” interaction.</p><p>First, the framework traverses the view hierarchy of both the <em>source</em> and <em>destination</em> screens to extract the <em>UIView</em> for each of the identifiers being animated. This determines whether or not a given identifier is present on each screen, and forms an <em>identifier hierarchy</em> (much like the view hierarchy of a screen).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2s84RSAuG2YFPJN94EIuiw.png" /><figcaption><em>The “identifier hierarchy” of the source and destination screens</em></figcaption></figure><p>The identifier hierarchies of the <em>source</em> and <em>destination</em> are diffed to determine whether an individual component was added, removed, or present in both. If the view was added or removed, the framework will use the animation specified in the transition definition. If the view was present in both states, the framework instead performs a “shared element animation” where the component animates from its initial position to its final position while its content is updated. These shared elements are animated recursively–each component can provide its own identifier hierarchy of child elements, which is diffed and animated as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-KCx3tfFxT3inrQss6yKHw.png" /><figcaption><em>The final identifier hierarchy after diffing the source and destination screens</em></figcaption></figure><p>To actually perform these animations, we need a single <em>view hierarchy</em> that matches the structure of our <em>identifier hierarchy</em>. We can’t just combine the source and destination screens into a single view hierarchy by layering them on top of each other, because the ordering would be wrong. In this case, if we just placed the destination screen over the source screen then the source <em>Explore.searchNavigationBarPill</em> view would be below the destination <em>BottomSheet.backgroundView</em> element, which doesn’t match the identifier hierarchy.</p><p>Instead, we have to create a separate view hierarchy that matches the structure of the identifier hierarchy. This requires making copies of the components being animated and adding them to the UIKit transition container. Most <em>UIView</em>s<em> </em>aren’t trivially copyable, so copies are typically made by “snapshotting” the view (rendering it as an image). We temporarily hide the “original view” while the animation is playing, so only the snapshot is visible.</p><p>Once the framework has set up the transition container’s view hierarchy and determined the specific animation to use for each component, the animations just have to be applied and played. This is where the underlying imperative <em>UIView</em> animations are performed.</p><h3>Conclusion</h3><p>Like with <a href="https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670">Epoxy</a> and other declarative systems, abstracting away the underlying complexity and providing a simple declarative interface makes it possible for engineers to focus on the <em>what</em> rather than the <em>how</em>. The declarative transition definition for these animations are only a few lines of code, which is by itself a <em>huge</em> improvement over any feasible imperative implementation. And since our declarative feature-building APIs have first-class support for UIKit <a href="https://developer.apple.com/documentation/uikit/uiviewcontrolleranimatedtransitioning?language=objc"><em>UIViewControllerAnimatedTransitioning</em></a> implementations, these declarative transitions can be integrated into existing features without making any architecture changes. This significantly accelerates feature development, making it easier than ever to create highly polished transitions, while also enabling long-term flexibility and maintainability.</p><p>We have a packed roadmap ahead. One area of active work is improving interoperability with SwiftUI. This lets us seamlessly transition between UIKit and SwiftUI-based screens, which unlocks incremental adoption of SwiftUI in our app without having to sacrifice motion. We’re also exploring making similar frameworks available on web and Android. Our long-term goal here is to make it as easy as possible to translate our designer’s great ideas into actual shipping products, on all platforms.</p><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4693375/">Staff Software Engineer, Wishlists</a></p><p><a href="https://careers.airbnb.com/positions/4665949/">Staff Software Engineer, Guests &amp; Hosts</a></p><p><a href="https://careers.airbnb.com/positions/4590099/">Staff Android Software Engineer, Guest</a></p><h3>Acknowledgments</h3><p>Many thanks to Eric Horacek and Matthew Cheok for their major contributions to Airbnb’s motion architecture and our declarative transition framework.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5ffabfc878" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/motion-engineering-at-scale-5ffabfc878">Motion Engineering at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing Lottie 4.0 for iOS]]></title>
            <link>https://medium.com/airbnb-engineering/announcing-lottie-4-0-for-ios-d4d226862a54?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/d4d226862a54</guid>
            <category><![CDATA[animation]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[mobile]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[lottie]]></category>
            <dc:creator><![CDATA[Cal Stephens]]></dc:creator>
            <pubDate>Tue, 06 Dec 2022 20:01:37 GMT</pubDate>
            <atom:updated>2022-12-07T00:27:14.640Z</atom:updated>
            <content:encoded><![CDATA[<p>A new rendering engine with significant performance improvements powered by Core Animation</p><p><strong>By: </strong><a href="https://www.linkedin.com/in/calstephens/">Cal Stephens</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Y4aQ-u0Mnh0S5b71sKr-Fg.jpeg" /></figure><p><a href="https://airbnb.design/lottie/"><strong>Lottie</strong></a> is Airbnb’s <a href="https://airbnb.io/lottie/#/README">cross-platform</a>, <a href="https://github.com/airbnb/lottie-ios">open source</a> library for rendering vector motion graphics. We use Lottie extensively at Airbnb, and it also powers animations in thousands of other apps throughout the industry.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*YncHlVnpvqU3hnjwo4g7dg.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*-69VqnYxgYdp8xOyiaDGSQ.gif" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*Ie-4EfSyrNjA9sZnAo6ASA.gif" /><figcaption>Example Lottie animations included in Airbnb’s iOS app</figcaption></figure><p>Today we’re releasing <a href="https://github.com/airbnb/lottie-ios/releases/tag/4.0.0"><strong>Lottie 4.0</strong></a><strong> </strong>for iOS. This major new release brings <strong>significant performance improvements</strong> to all Lottie animations, with a brand new rendering engine powered by Core Animation.</p><p>Using Lottie at scale for many years, we’ve learned a lot about its performance characteristics in real-world use cases. We found that it was relatively common for Lottie animations to drop frames in some of our more complex screens. To understand why, we first have to take a look at how Lottie previously rendered animations.</p><p>Previous versions of Lottie played animations on the app’s main thread, effectively using a <a href="https://developer.apple.com/documentation/quartzcore/cadisplaylink?language=objc"><em>CADisplayLink</em></a>. Once per frame, Lottie would execute code on the main thread to advance the progress of the animation and re-render its content. This meant that animations would consume 5–20%+ of the CPU while playing, leaving fewer CPU cycles available for the rest of the app:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z4DbWMMV2dHCQoETuW0taQ.gif" /><figcaption><em>Playing an animation with Lottie 3.5.0, using the original main thread rendering engine</em></figcaption></figure><p>This also meant that animations would not update when the main thread was busy. This could cause animations to drop frames or freeze entirely, which results in a poor user experience:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Nnlh9Fk_Im--KvAyL1v5KQ.gif" /><figcaption><em>Lottie animations dropping frames when the main thread is overloaded</em></figcaption></figure><p>These issues are inherent limitations of using a main-thread-bound rendering architecture.</p><p>On iOS, the most performant and power-efficient way to play animations is by using <a href="https://developer.apple.com/documentation/quartzcore?language=objc">Core Animation</a>. This system framework renders animations out-of-process with GPU hardware acceleration. Animation playback is managed by a separate system process called the “render server”. This means Core Animation-powered animations don’t contribute to the CPU utilization of the app process itself, and can continue even when its main thread is blocked or busy.</p><p>Throughout 2022, we’ve been working on a new rendering engine implementation for Lottie built on top of Core Animation. For each of the layers in the animation JSON file, the new engine builds a <em>CALayer</em> and applies <em>CAAnimation</em>s with keyframes for the layer’s animated properties. Lottie passes these animation keyframes off to Core Animation, which takes care of actually rendering them on-screen and updating the animation each frame.</p><p>This new engine eliminates the CPU overhead from playing a Lottie animation, and effectively guarantees that Lottie animations will animate smoothly at 60 or 120 fps regardless of the app’s CPU load.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QmNCLz3e_Zuez3nFBvu_LA.gif" /><figcaption><em>Playing an animation with Lottie 4.0, using the new Core Animation rendering engine</em></figcaption></figure><p>Since animations rendered by the new engine don’t execute any code on the app’s main thread, apps now have more resources available for other functionality. This is especially valuable when running tasks with high CPU load. As an example, the Airbnb app displays a Lottie animation when starting up for the first time. We ran an experiment here and found that switching to the new rendering engine <em>reduces</em> our app’s total launch time, while <em>also</em> improving the frame-rate and UX of the startup animation.</p><p>We <a href="https://github.com/airbnb/lottie-ios/discussions/1627">first introduced</a> the Core Animation rendering engine in Lottie 3.4.0 earlier this year, behind an opt-in feature flag. We’ve been using the new engine by default for all Lottie animations in the Airbnb app for over six months, and have been hard at work fixing issues reported by early-adopters in the community.</p><p>Starting in today’s Lottie 4.0 release for iOS, the Core Animation rendering engine is <strong>enabled by default</strong> for all apps using Lottie, with no additional work or migration required by app developers. This is a major milestone that we’ve been working towards for a long time, and we hope it helps raise the bar for animation quality and performance even higher throughout the industry!</p><p>Lottie 4.0 for iOS also includes several significant enhancements contributed by members of the community:</p><ul><li>Support for <a href="https://dotlottie.io/">dotLottie animation files</a>, which are much smaller in size than standard JSON files</li><li>A new animation decoding implementation that is ~2x faster than the previous <em>Codable</em>-based implementation</li></ul><p>You can learn more about Lottie, and our commitment to open source, in previous posts we’ve published:</p><ul><li><a href="https://airbnb.design/introducing-lottie/">Introducing Lottie</a>: Behind the scenes of our new open-source animation tool</li><li><a href="https://medium.com/airbnb-engineering/lottie-and-swift-at-airbnb-e0c85dc365e7">Moving Lottie Swiftly into the Future</a>:<strong> </strong>A personal story on how Airbnb rewrote the popular open source library Lottie in a new language</li></ul><p>Interested in working at Airbnb? Check out these open roles:</p><p><a href="https://careers.airbnb.com/positions/4693375/">Staff Software Engineer, Wishlists</a></p><p><a href="https://careers.airbnb.com/positions/4665949/">Staff Software Engineer, Guests &amp; Hosts</a></p><h3>Acknowledgments</h3><p>Many thanks to Eric Horacek for first proposing this project and reviewing 100+ pull requests over the past year. Also thanks to Brandon Withrow, the original author of Lottie, plus the <a href="https://github.com/airbnb/lottie-ios/graphs/contributors">many other contributors</a> who have helped out over the years.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d4d226862a54" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/announcing-lottie-4-0-for-ios-d4d226862a54">Announcing Lottie 4.0 for iOS</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How AI Text Generation Models Are Reshaping Customer Support at Airbnb]]></title>
            <link>https://medium.com/airbnb-engineering/how-ai-text-generation-models-are-reshaping-customer-support-at-airbnb-a851db0b4fa3?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/a851db0b4fa3</guid>
            <category><![CDATA[customer-experience]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Gavin Li]]></dc:creator>
            <pubDate>Wed, 23 Nov 2022 17:38:57 GMT</pubDate>
            <atom:updated>2022-11-23T17:38:57.385Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>Leveraging text generation models to build more effective, scalable customer support products.</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*74YI6FqsvcKKWNIDPhDVPw.jpeg" /></figure><p><a href="https://www.linkedin.com/in/gavin-li-64354117/">Gavin Li</a>, <a href="https://www.linkedin.com/in/mia-zhao-964a9213/">Mia Zhao</a> and <a href="https://www.linkedin.com/in/zhenyu-zhao-30b8632a/">Zhenyu Zhao</a></p><p>One of the fastest-growing areas in modern Artificial Intelligence (AI) is <a href="https://huggingface.co/tasks/text-generation">AI text generation models</a>. As the name suggests, these models generate natural language. Previously, most industrial natural language processing (NLP) models were classifiers, or what might be called discriminative models in machine learning (ML) literature. However, in recent years, generative models based on large-scale language models are rapidly gaining traction and fundamentally changing how ML problems are formulated. Generative models can now obtain some domain knowledge through large-scale pre-training and then produce high-quality text — for instance answering questions or paraphrasing a piece of content.</p><p>At Airbnb, we’ve heavily invested in AI text generation models in our community support (CS) products, which has enabled many new capabilities and use cases. This article will discuss three of these use cases in detail. However, first let’s talk about some of the beneficial traits of text generation models that make it a good fit for our products.</p><h3>About Text Generation Models</h3><p>Applying AI models in large-scale industrial applications like Airbnb customer support is not an easy challenge. Real-life applications have many long-tail corner cases, can be hard to scale, and often become costly to label the training data. There are several traits of text generation models that address these challenges and make this option particularly valuable.</p><h3>Encoding Knowledge</h3><p>The first attractive trait is the capability to encode domain knowledge into the language models. As illustrated by <a href="https://arxiv.org/abs/1909.01066">Petroni et al. (2019)</a>, we can encode domain knowledge through large-scale pre-training and transfer learning. In traditional ML paradigms, input matters a lot. The model is just a transformation function from the input to the output. The model training focuses mainly on preparing input, feature engineering, and training labels. While for generative models, the key is the knowledge encoding. How well we can design the pre-training and training to encode high-quality knowledge into the model — and how well we design prompts to induce this knowledge — is far more critical. This fundamentally changes how we solve traditional problems like classifications, rankings, candidate generations, etc.</p><p>Over the past several years, we have accumulated massive amounts of records of our human agents offering help to our guests and hosts at Airbnb. We’ve then used this data to design large-scale pre-training and training to encode knowledge about solving users’ travel problems. At inference time, we’ve designed prompt input to generate answers based directly on the encoded human knowledge. This approach produced significantly better results compared to traditional classification paradigms. A/B testing showed significant business metric improvement as well as significantly better user experience.</p><h3>Unsupervised Learning</h3><p>The second trait of the text generation model we’ve found attractive is its “unsupervised” nature. Large-scale industrial use cases like Airbnb often have large amounts of user data. How to mine helpful information and knowledge to train models becomes a challenge. First, labeling large amounts of data by human effort is very costly, significantly limiting the training data scale we could use. Second, designing good labeling guidelines and a comprehensive label taxonomy of user issues and intents is challenging because real-life problems often have long-tail distribution and lots of nuanced corner cases. It doesn’t scale to rely on human effort to exhaust all the possible user intent definitions.</p><p>The unsupervised nature of the text generation model allows us to train models without largely labeling the data. In the pre-training, in order to learn how to predict the target labels, the model is forced to first gain a certain understanding about the problem taxonomy. Essentially the model is doing some data labeling design for us internally and implicitly. This solves the scalability issues when it comes to intent taxonomy design and cost of labeling, and therefore opens up many new opportunities. We’ll see some examples of this when we dive into use cases later in this post.</p><h3>More Natural and Productive Language Models</h3><p>Finally, text generation models transcend the traditional boundaries of ML problem formulations Over the past few years, researchers have realized that the extra dense layers in autoencoding models may be unnatural, counterproductive, and restrictive. In fact, all of the typical machine learning tasks and problem formulations can be viewed as different manifestations of the single, unifying problem of language modeling. A classification can be formatted as a type of language model where the output text is the literal string representation of the classes.</p><p>In order to make the language model unification effective, a new but essential role is introduced: the <strong>prompt</strong>. A prompt is a short piece of textual instruction that informs the model of the task at hand and sets the expectation for what the format and content of the output should be. Along with the prompt, additional natural language annotations, or hints, are also highly beneficial in further contextualizing the ML problem as a language generation task. The incorporation of prompts has been demonstrated to significantly improve the quality of language models on a variety of tasks. The figure below illustrates the anatomy of a high-quality input text for universal generative modeling.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fYmXfhyBq6uw7uKa" /><figcaption>Figure 1.1 An example of the prompt and input feature design of our text generation model</figcaption></figure><p>Now, let’s dive into a few ways that text generation models have been applied within Airbnb’s Community Support products. We’ll explore three use cases — content recommendation, real-time agent assistance, and chatbot paraphrasing.</p><h3>Content Recommendation Model</h3><p>Our content recommendation workflow, powering both Airbnb’s Help Center search and the support content recommendation in our <a href="https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9">Helpbot</a>, utilizes pointwise ranking to determine the order of the documents users receive, as shown in Figure 2.1. This pointwise ranker takes the textual representation of two pieces of input — the current user’s issue description and the candidate document, in the form of its title, summary, and keywords. It then computes a relevance score between the description and the document, which is used for ranking. Prior to 2022, this pointwise ranker had been implemented using the XLMRoBERTa, however we’ll see shortly why we’ve switched to the MT5 model.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9aM_OnaEBDRA5kzd" /><figcaption>Figure 2.1 How we utilized encoder-only architecture with an arbitrary classification head to perform pointwise document ranking</figcaption></figure><p>Following the design decision to introduce prompts, we transformed the classic binary classification problem into a prompt-based language generation problem. The input is still derived from both the issue description and the candidate document’s textual representation. However, we contextualize the input by prepending a prompt to the description that informs the model that we expect a binary answer, either “Yes” or “No”, of whether the document would be helpful in resolving the issue. We also added annotations to provide extra hints to the intended roles of the various parts of the input text, as illustrated in the figure below. To enable personalization, we expanded the issue description input with textual representations of the user and their reservation information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fnUD_3FEKSEJgzph" /><figcaption>Figure 2.2. How we leveraged an encoder-decoder architecture with a natural language output to serve as a pointwise ranker</figcaption></figure><p>We fine-tuned the MT5 model on the task described above. In order to evaluate the quality of the generative classifier, we used production traffic data sampled from the same distribution as the training data. The generative model demonstrated significant improvements in the key performance metric for support document ranking, as illustrated in the table below.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2Fcd16baf468&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2Fcd16baf468&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="294" frameborder="0" scrolling="no"><a href="https://medium.com/media/a63a53f8082e510013ab23e0bd9af2b4/href">https://medium.com/media/a63a53f8082e510013ab23e0bd9af2b4/href</a></iframe><p>In addition, we also tested the generative model in an online A/B experiment, integrating the model into Airbnb’s Help Center, which has millions of active users. The successful experimentation results led to the same conclusion — the generative model recommends documents with significantly higher relevance in comparison with the classification-based baseline model.</p><h3>‘Real-Time Agent Assistant’ Model</h3><p>Equipping agents with the right contextual knowledge and powerful tools leads to better experiences for our customers. So we provide our agents with just-in-time guidance, which directs them to the correct answers consistently and helps them resolve user issues efficiently.</p><p>For example, through agent-user conversations, suggested templates are displayed to assist agents in problem solving. To make sure our suggestions are enforced within CS policy, suggestion templates are gated by a combination of API checks and model intent checks. This model needs to answer questions to capture user intents such as:</p><ul><li>Is this message about a cancellation?</li><li>What cancellation reason did this user mention?</li><li>Is this user canceling due to a COVID sickness?</li><li>Did this user accidentally book a reservation?</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bL5t68bbHTkbVOAgrhx5hg.png" /><figcaption>Figure 3.1 AI-generated recommendation template</figcaption></figure><p>In order to support many granular intent checks, we developed a mastermind Question-Answering (QA) model, aiming to help answer all related questions. This QA model was developed using the generative model architecture mentioned above. We concatenate multiple rounds of user-agent conversations to leverage chat history as input text and then ask the prompt we care about at the point in time of serving.</p><p>Prompts are naturally aligned with the same questions we ask humans to annotate. Slightly different prompts would result in different answers as shown below. Based on the model’s answer, relevant templates are then recommended to agents.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2Fb70c8e7a4e&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2Fb70c8e7a4e&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="384" frameborder="0" scrolling="no"><a href="https://medium.com/media/fee7ae07a9080a402770f606b972ba44/href">https://medium.com/media/fee7ae07a9080a402770f606b972ba44/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*h5mMZvyhnEehuUxq" /><figcaption>Figure 2.2 Mastermind QA model architecture</figcaption></figure><p>We leveraged backbone models such as t5-base and Narrativa and did experimentations on various training dataset compositions including annotation-based data and logging-based data with additional post-processing. Annotation datasets usually have higher precision, lower coverage, and more consistent noise, while logging datasets have lower precision, higher case coverage, and more random noises. We found that combining these two datasets together yielded the best performance.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2F073ba3d3d1&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2F073ba3d3d1&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="248" frameborder="0" scrolling="no"><a href="https://medium.com/media/b4750e4a42e072d98c28b557fa80ab99/href">https://medium.com/media/b4750e4a42e072d98c28b557fa80ab99/href</a></iframe><p>Due to the large size of the parameters, we leverage a library, called <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>, to train the generative model using multi GPU cores. DeepSpeed helps to speed up the training process from weeks to days. That being said, it typically requires longer for hyperparameter tunings. Therefore, experiments are required with smaller datasets to get a better direction on parameter settings. In production, online testing with real CS ambassadors showed a large engagement rate improvement.</p><h3>Paraphrase Model in Chatbot</h3><p>Accurate intent detection, slot filling, and effective solutions are not sufficient for building a successful AI chatbot. Users often choose not to engage with the chatbot, no matter how good the ML model is. Users want to solve problems quickly, so they are constantly trying to assess if the bot is understanding their problem and if it will resolve the issue faster than a human agent. Building a paraphrase model, which first rephrases the problem a user describes, can give users some confidence and confirm that the bot’s understanding is correct. This has significantly improved our bot’s engagement rate. Below is an example of our chatbot automatically paraphrasing the user’s description.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UBV5lgJYxCp9uxwi" /><figcaption>Figure 4.1 An actual example of the chatbot paraphrasing a user’s description of a payment issue</figcaption></figure><p>This method of paraphrasing a user’s problem is used often by human customer support agents. The most common pattern of this is “I understand that you…”. For example, if the user asks if they can cancel the reservation for free, the agent will reply with, “I understand that you want to cancel and would like to know if we can refund the payment in full.” We built a simple template to extract all the conversations where an agent’s reply starts with that key phrase. Because we have many years of agent-user communication data, this simple heuristic gives us millions of training labels for free.</p><p>We tested popular sequence-to-sequence transformer model backbones like <a href="https://arxiv.org/abs/1910.13461">BART</a>, <a href="https://doi.org/10.48550/ARXIV.1912.08777">PEGASUS</a>, <a href="http://arxiv.org/abs/1910.10683">T5</a>, etc, and autoregressive models like <a href="https://doi.org/10.48550/ARXIV.1907.05774">GPT2</a>, etc. For our use case, the T5 model produced the best performance.</p><p>As found by <a href="https://arxiv.org/abs/1905.05709">Huang et al. (2020)</a>, one of the most common issues of the text generation model is that it tends to generate bland, generic, uninformative replies. This was also the major challenge we faced.</p><p>For example, the model outputs the same reply for many different inputs: “I understand that you have some issues with your reservation.” Though correct, this is too generic to be useful.</p><p>We tried several different solutions. First, we tried to build a backward model to predict <em>P(Source|target)</em>, as introduced by <a href="https://arxiv.org/abs/1911.00536">Zhang et al. (2020)</a>, and use it as a reranking model to filter out results that were too generic. Second, we tried to use some rule-based or model-based filters.</p><p>In the end, we found the best solution was to tune the training data. To do this, we ran text clustering on the training target data based on pre-trained similarity models from <a href="https://www.sbert.net/">Sentence-Transformers</a>. As seen in the table below, the training data contained too many generic meaningless replies, which caused the model to do the same in its output.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2F4fc4479886&amp;dntp=1&amp;display_name=Sheetsu&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2F4fc4479886&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="294" frameborder="0" scrolling="no"><a href="https://medium.com/media/87416f6754eb610f173dbe0c77e18976/href">https://medium.com/media/87416f6754eb610f173dbe0c77e18976/href</a></iframe><p>We labeled all clusters that are too generic and used Sentence-Transformers to filter them out from the training data. This approach worked significantly better and gave us a high-quality model to put into production.</p><h3>Conclusion</h3><p>With the fast growth of large-scale pre-training-based transformer models, the text generation models can now encode domain knowledge. This not only allows them to utilize the application data better, but allows us to train models in an unsupervised way that helps scale data labeling. This enables many innovative ways to tackle common challenges in building AI products. As demonstrated in the three use cases detailed in this post — content ranking, real-time agent assistance, and chatbot paraphrasing — the text generation models improve our user experiences effectively in customer support scenarios. We believe that text generation models are a crucial new direction in the NLP domain. They help Airbnb’s guests and hosts solve their issues more swiftly and assist Support Ambassadors in achieving better efficiency and a higher resolution of the issues at hand. We look forward to continuing to invest actively in this area.</p><h3>Acknowledgments</h3><p>Thank you <a href="https://www.linkedin.com/in/weipingpeng/">Weiping Pen</a>, <a href="https://www.linkedin.com/in/xin-liu-908b6b18/">Xin Liu</a>, <a href="https://www.linkedin.com/in/mukundn/">Mukund Narasimhan</a>, <a href="https://www.linkedin.com/in/cmujoy/">Joy Zhang</a>, <a href="https://www.linkedin.com/in/tina-su-saratoga/">Tina Su</a>, <a href="https://www.linkedin.com/in/ayasutake/">Andy Yasutake</a> for reviewing and polishing the blog post content and all the great suggestions. Thank you <a href="https://www.linkedin.com/in/cmujoy/">Joy Zhang</a>, <a href="https://www.linkedin.com/in/tina-su-saratoga/">Tina Su</a>, <a href="https://www.linkedin.com/in/ayasutake/">Andy Yasutake</a> for their leadership support! Thank you <a href="https://www.linkedin.com/in/elaineliu5/">Elaine Liu</a> for building the paraphrase end-to-end product, running the experiments, and launching. Thank you to our close PM partners, <a href="https://www.linkedin.com/in/shuangyi-cassie-cao/">Cassie Cao</a> and <a href="https://www.linkedin.com/in/jerryhong/">Jerry Hong</a>, for their PM expertise. This work could not have happened without their efforts.</p><p><em>Interested in working at Airbnb? Check out </em><a href="https://careers.airbnb.com/"><em>these</em></a><em> open roles.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a851db0b4fa3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-ai-text-generation-models-are-reshaping-customer-support-at-airbnb-a851db0b4fa3">How AI Text Generation Models Are Reshaping Customer Support at Airbnb</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building Airbnb Categories with ML and Human-in-the-Loop]]></title>
            <link>https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e97988e70ebb</guid>
            <category><![CDATA[artificial-intelligence]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Mihajlo Grbovic]]></dc:creator>
            <pubDate>Mon, 21 Nov 2022 16:54:21 GMT</pubDate>
            <atom:updated>2023-01-31T18:52:19.638Z</atom:updated>
            <content:encoded><![CDATA[<h4><strong>Airbnb Categories Blog Series — Part I</strong></h4><p>By: <strong>Mihajlo Grbovic, Ying Xiao, Pratiksha Kadam, Aaron Yin, Pei Xiong, Dillon Davis, Aditya Mukherji, Kedar Bellare, Haowei Zhang, Shukun Yang, Chen Qian, Sebastien Dubois, Nate Ney, James Furnary, Mark Giangreco, Nate Rosenthal, Cole Baker, Bill Ulammandakh, Sid Reddy, Egor Pakhomov</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RrtVCKycvPPwPuukCLDT-Q.jpeg" /><figcaption>Figure 1. Browsing listings by categories: <strong>Castles</strong>, <strong>Desert</strong>, <strong>Design</strong>, <strong>Beach </strong>&amp;<strong> Countryside</strong></figcaption></figure><h3>25 Years of Online Travel Search</h3><p>Online travel search hasn’t changed much in the last 25 years. The traveler enters her destination, dates, and the number of guests into a search interface, which dutifully returns a list of options that best meet the criteria. Eventually, Airbnb and other travel sites made improvements to allow for better filtering, ranking, personalization and, more recently, to display results slightly outside of the specified search parameters–for example, by accommodating flexible dates or by suggesting nearby locations. Taking a page from the travel agency model, these websites also built more “inspirational” browsing experiences that recommend popular destinations, showcasing these destinations with captivating imagery and inventory (think digital “catalog”).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*atw4D5q1UfxGH6dj" /><figcaption>Figure 2. Airbnb Destination Recommendation Example</figcaption></figure><p>The biggest shortcoming of these approaches is that the traveler must have a specific destination in mind. Even travelers who are flexible get funneled to a similar set of well-known destinations, reinforcing the cycle of mass tourism.</p><h3>Introducing Airbnb Categories</h3><p>In our recent release, we flipped the travel search experience on its head by having the inventory dictate the destinations, not the other way around. In this way, we sought to inspire the traveler to book unique stays in places they might not think to search for. By leading with our unique places to stay, grouped together into cohesive “categories”, we inspired our guests to find some incredible places to stay off the beaten path.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*clTHm13EoXvGjmEt5PpUVQ.png" /><figcaption>Figure 3. Unique travel worthy inventory in lesser known destinations that users are unlikely to search for</figcaption></figure><p>Though our goal was an intuitive browsing experience, it required considerable work behind the scenes to pull this off. In this three-part series, we will pull back the curtain on the technical aspects of the <a href="https://news.airbnb.com/2022-summer-release/">Airbnb 2022 Summer Launch</a>.</p><ul><li><strong>Part I </strong>(this post) is designed to be a high-level introductory post about how we applied machine learning to build out the listing collections and to solve different tasks related to the browsing experience–specifically, quality estimation, photo selection and ranking.</li><li><strong>Part II </strong>of the series focuses on ML Categorization of listings into categories. It explains the approach in more detail, including signals and labels that we used, tradeoffs we made, and how we set up a human-in-the-loop feedback system.</li><li><strong>Part III</strong> focuses on ML Ranking of Categories depending on the search query. For example, we taught the model to show the Skiing category first for an Aspen, Colorado query versus Beach/Surfing for a Los Angeles query. That post will also cover our approach for ML Ranking of listings within each category.</li></ul><h3>Grouping Listings into Categories</h3><p>Airbnb has thousands of very unique, high quality listings, many of which received design and architecture awards or have been featured in travel magazines or movies. However, these listings are sometimes hard to discover because they are in a little-known town or because they are not ranked highly enough by the search algorithm, which optimizes for bookings. While these unique listings may not always be as bookable as others due to lower availability or higher price, they are great for inspiration and for helping guests discover hidden destinations where they may end up booking a stay influenced by the category.</p><p>To showcase these special listings we decided to group them into collections of homes organized by what makes them unique. The result was <strong>Airbnb Categories, </strong>collections of homes revolving around some common themes including the following:</p><ul><li><strong>Categories that revolve around a location or a place of interest (POI)</strong> such as Coastal, Lake, National Parks, Countryside, Tropical, Arctic, Desert, Islands, etc.</li><li><strong>Categories that revolve around an activity</strong> such as Skiing, Surfing, Golfing, Camping, Wine tasting, Scuba, etc.</li><li><strong>Categories that revolve around a home type</strong> such as Barns, Castles, Windmills, Houseboats, Cabins, Caves, Historical, etc.</li><li><strong>Categories that revolve around a home amenity</strong> such as Amazing Pools, Chef’s Kitchen, Grand Pianos, Creative Spaces, etc.</li></ul><p>We defined 56 categories and outlined the definition for each category. Now all that was left to do was to assign our entire catalog of listings to categories.</p><p>With the Summer launch just a few months away, we knew that we could not manually curate all the categories, as it would be very time consuming and costly. We also knew that we could not generate all the categories in a rule-based manner, as this approach would not be accurate enough. Finally, we knew we could not produce an accurate ML categorization model without a training set of human-generated labels. Given all of these limitations, we decided to combine the accuracy of human review with the scale of ML models to create a human-in-the-loop system for listing categorization and display.</p><h4>Rule-Based Candidate Generation</h4><p>Before we could build a trained ML model for assigning listings to categories, we had to rely on various listing- and geo-based signals to generate the initial set of candidates. We named this technique <strong><em>weighted sum of indicators</em></strong><em>. </em>It consists of building out a set of signals (indicators) that associate a listing with a specific category. The more indicators the listing has, the better the chances of it belonging to that category.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*l8v0iDnqsyguo85jgk75Lw.png" /><figcaption>Figure 4. Rule-based weighted sum of indicators approach to produce candidates for human review</figcaption></figure><p>For example, let’s consider a listing that is within 100 meters of a Lake POI, with keyword “lakefront” mentioned in listing title and guest reviews, lake views appearing in listing photos and several kayaking activities nearby. All this information together strongly indicates that the listing belongs to the <em>Lakefront</em> category. The weighted sum of these indicators totals to a high <strong><em>score</em></strong>, which means that this listing-category pair would be a strong candidate for human review. If a rule-based candidate generation created a large set of candidates we would use this score to prioritize listings for human review to maximize the initial yield.</p><h4>Human Review</h4><p>The manual review of candidates consists of several tasks. Given a listing candidate for a particular category or several categories, an agent would:</p><ul><li><strong>Confirm/reject the category or categories</strong> assigned to the listing by comparing it to the category definition.</li><li><strong>Pick the photo</strong> that best represents the category. Listings can belong to multiple categories, so it is sometimes appropriate to pick a different photo to serve as the cover image for different categories.</li><li><strong>Determine the quality tier </strong>of the selected photo. Specifically, we defined <strong>four quality tiers:</strong> <strong><em>Most Inspiring</em></strong>, <strong><em>High Quality</em></strong>, <strong><em>Acceptable Quality</em></strong>, and <strong><em>Low Quality. </em></strong>We use this information to rank the higher quality listings near the top of the results to achieve the “wow” effect with prospective guests.</li><li>Some of the categories rely on signals related to <strong>Places of Interest (POIs) data</strong> such as the locations of lakes or national parks, so the reviewers could add a POI that we were missing in our database.</li></ul><h4>Candidate Expansion</h4><p>Although the rule-based approach can generate many candidates for some categories, for others (e.g., Creative Spaces, Amazing Views) it may produce only a limited set of listings. In those cases, we turn to candidate expansion. One such technique leverages pre-trained listing embeddings. Once a human reviewer confirms that a listing belongs to a particular category, we can find similar listings via cosine similarity. Very often the 10 nearest neighbors are good candidates for the same category and can be sent for human review. We detailed one of the embedding approaches in our previous<a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"> blog post</a> and have developed new ones since then.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W3G7kW0PpbKJQ6wbWyZQ_Q.jpeg" /><figcaption>Figure 5. Listing similarity via embeddings can help find more listings that are from the same category</figcaption></figure><p>Other expansion techniques include keyword expansion, location-based expansion (i.e. considering neighboring homes for same POI category), etc.</p><h4>Training ML Models</h4><p>Once we collected enough human-generated labels, we trained a binary classification model that predicts whether or not a listing belongs to a specific category. We then used a holdout set to evaluate performance of the model using a precision-recall (PR) curve. Our goal here was to evaluate if the model was good enough to send highly confident listings directly to production.</p><p>Figure 6 shows a trained ML model for the Lakefront category. On the left we can see the feature importance graph, indicating which signals contribute most to the decision of whether or not a listing belongs to the Lakefront category. On the right we can see the hold out set PR curve of different model versions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*n0HbLnOu9iqC4txk" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*viXUC31IIWQn13Wb" /><figcaption>Figure 6. Lakefront ML model feature importance and performance evaluation</figcaption></figure><p><strong>Sending confident listings to production: </strong>using a PR curve we can set a threshold that achieves 90% precision on a downsampled hold out set that mimics the true listing distribution. Then we can score all unlabeled listings and send ones above that threshold to production, with the expectation of 90% accuracy. In this particular case, we can achieve 76% recall at 90% precision, meaning that with this technique we can expect to capture 76% of the true Lakefront listings in production.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*M2RCGu7LEgn-x6_l" /><figcaption>Figure 7. Basic ML + Human in the Loop setup for tagging listings with categories</figcaption></figure><p><strong>Selecting listings for human review: </strong>given the expectation of 76% recall, to cover the rest of the Lakefront listings we also need to send listings below the threshold for human evaluation. When prioritizing the below-threshold listings, we considered the photo quality score for the listing and the current coverage of the category to which the listing was tagged, among other factors. Once a human reviewer confirmed a listing’s category assignment, that tag would be made available to production. Concurrently, we send the tags back to our ML models for retraining, so that the models improve over time.</p><p><strong>ML models for quality estimation and photo selection. </strong>In addition to the ML Categorization models described above, we also trained a Quality ML model that assigns one of the four quality tiers to the listing, as well as a Vision Transformer Cover Image ML model that chooses the listing photo that best represents the category. In the current implementation the Cover Image ML model takes the category information as the input signal, while the Quality ML model is a global model for all categories. The three ML models work together to assign category, quality and cover photo. Listings with these assigned attributes are sent directly into production under certain circumstances and also queued for review.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iwRE1sSgmYpJCuM0" /><figcaption>Figure 8. Human vs. ML flow to production</figcaption></figure><h4>Two New Ranking Algorithms</h4><p><a href="https://news.airbnb.com/2022-summer-release/">The Airbnb Summer release</a> introduced categories both to homepage (Figure 9 left), where we show categories that are popular near you, and to location searches (Figure 9 right), where we show categories that are related to the searched destination. For example, in the case of a Lake Tahoe location search we show <em>Skiing, Cabins, Lakefront, Lake House, etc.</em>, and <em>Skiing</em> should be shown first if searching in winter.</p><p>In both cases, this created a need for two new ranking algorithms:</p><ul><li><strong>Category ranking </strong>(green arrow in Figure 9 left): How to rank categories from left to right, by taking into account user origin, season, category popularity, inventory, bookings and user interests</li><li><strong>Listing Ranking</strong> (blue arrow in Figure 9 left): given all the listings assigned to the category, rank them from top to bottom by taking into account assigned listing quality tier and whether a given listing was sent to production by humans or by ML models.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vN4exaTinuCSGCHth6FtlA.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Odmoseo1jwOCnRrTurqglA.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Own3vspDyitaRu7GBpmIOA.png" /><figcaption>Figure 9. Listing Ranking Logic for Homepage and Location Category Experience</figcaption></figure><h3>Putting it all together</h3><p>To summarize, we presented how we create categories from scratch, first using rules that rely on listing signals and POIs and then with ML with humans in the loop to constantly improve the category. Figure 10 describes the end-to-end flow as it exists today.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*o6lekCM1lHkZt8uNeTWKAw.png" /><figcaption>Figure 9: Logic for Category Creation and Improvement over time</figcaption></figure><p><em>Our approach was to </em><strong><em>define </em></strong><em>an acceptable delivery; </em><strong><em>prototype </em></strong><em>several categories to acceptable level; </em><strong><em>scale </em></strong><em>the rest of the categories to the same level;</em><strong><em> revisit </em></strong><em>the acceptable delivery and improve the product over time.</em></p><p>In Part II, we’ll explain in greater detail the models that categorize listings into categories.</p><h3>Acknowledgments</h3><p><em>We would like to thank everyone involved in the project. Building Airbnb Categories holds a special place in our careers as one of those rare projects where people with different backgrounds and roles came together to work jointly to build something unique.</em></p><p><em>Interested in working at Airbnb? Check out our open roles </em><a href="https://careers.airbnb.com/"><em>here</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e97988e70ebb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb">Building Airbnb Categories with ML and Human-in-the-Loop</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>