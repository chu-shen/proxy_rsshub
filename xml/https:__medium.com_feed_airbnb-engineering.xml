<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[The Airbnb Tech Blog - Medium]]></title>
        <description><![CDATA[Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium]]></description>
        <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>The Airbnb Tech Blog - Medium</title>
            <link>https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Mon, 01 Nov 2021 16:11:29 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/airbnb-engineering" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Meet Ottr: A Serverless Public Key Infrastructure Framework]]></title>
            <link>https://medium.com/airbnb-engineering/meet-ottr-a-serverless-public-key-infrastructure-framework-f6580010ae0c?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/f6580010ae0c</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[airbnb]]></category>
            <category><![CDATA[security]]></category>
            <category><![CDATA[public-key-infrastructure]]></category>
            <category><![CDATA[aws]]></category>
            <dc:creator><![CDATA[Kenneth Yang]]></dc:creator>
            <pubDate>Tue, 26 Oct 2021 18:09:19 GMT</pubDate>
            <atom:updated>2021-10-26T18:58:43.010Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Ottr is a serverless Public Key Infrastructure framework that handles end-to-end certificate rotations without the use of an agent. The purpose of the blog is to provide an overview on Ottr with sample reference architecture, logical and network flows, and highlight the benefits of the solution. For installation instructions, skip to the Open Source section of the article.</em></p><p><a href="https://www.linkedin.com/in/kenneyan/">Kenneth Yang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/900/1*6C4m508jFNo-FtFNIK1acw.png" /></figure><h3>Introduction</h3><p>Managing certificates for Public Key Infrastructure (PKI) is a difficult problem to solve at scale for any organization. While there are a number of <a href="https://letsencrypt.org/how-it-works/">agent-based solutions</a> to automate certificate rotations for Linux and Windows distributions, the process to broker certificates for network infrastructure commonly involves either manual intervention from engineering teams or use of enrollment protocols such as Certificate Management Protocol (CMP), Simple Certificate Enrollment Protocol (SCEP), or Enrollment over Secure Transport (EST), which all have their security issues.</p><p>We built Ottr at Airbnb to be a scalable and configurable serverless framework on AWS with little operational overhead or reliance on enrollment protocols. Ottr can be extended to handle end-to-end certificate rotations for any hosts (e.g., network infrastructure, Linux, Windows) capable of managing their own X.509 certificates from a remote session (e.g., API, SSH, SSM Agent).</p><h4>Background</h4><p>PKI governs the issuance of digital certificates to protect sensitive data, provide unique digital identities, and ensure secure end-to-end communication. Certificate Authorities (CA) are responsible for brokering these X.509 certificates and own the policies, practices, and procedures for vetting recipients and the issuing process. The CA used to generate the digital certificate can be from a Private CA, which your organization manages, or a Public CA, such as <a href="https://letsencrypt.org/">Let’s Encrypt</a>, which is managed by the <a href="https://www.abetterinternet.org/">Internet Security Research Group (ISRG)</a>.</p><p>At Airbnb, engineers are responsible for ensuring that end-to-end encryption is in place for compute nodes as well for firewalls, load balancers, and other network devices. The diagram below illustrates the typical process for certificate reissue.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*_wSqMv2Z_AWmThkHg0a5uQ.png" /></figure><p>As you can see, this is a heavily manual process requiring an approval step, which creates operational overhead for multiple teams. The details are broken down below:</p><ol><li><strong>Generate Private Key and Certificate Signing Request (CSR):</strong> A CSR is a cryptographically signed request that contains information around organization details as well as the Common Name (CN) and Subject Alternative Names (SANs) for which the certificate will be valid. A CSR is typically generated using OpenSSL, where a Private Key is created on the target device (and never leaves the host) and the associated Public Key is embedded within the CSR.</li><li><strong>Send CSR to Certificate Authority (CA):</strong> In order for a CSR to be signed by the CA, the domain must be validated. This can be done through a number of different ways (e.g., HTTP-01, DNS-01 Challenges). The CA can either be a Private CA for which your organization controls the trust chain, or a Public CA such as Let’s Encrypt whose <a href="https://letsencrypt.org/certificates/">chain of trust</a> is outside of your control.</li><li><strong>Approve CSR: </strong>Due to the sensitive nature of the certificate request process, an<strong> </strong>approval will typically be required from a security team to allow the CA to generate the certificate for the CSR that was submitted.</li><li><strong>Download Certificate:</strong> After approval, the certificate, intermediate certificate, root certificate, or full chain will be available from your CA and can be downloaded in a base64 format (e.g., .pem, .cer, .p7b).</li><li><strong>Upload Certificate: </strong>Depending on the platform, the full chain certificate will then be uploaded to the target device in a format that is supported (e.g., .pem) and restarted if applicable.</li></ol><h3>Why Ottr?</h3><p>When we were first designing Ottr, Airbnb needed a framework to manage X.509 certificates for hosts that could not run agents to manage their X.509 certificates; we needed a solution that would be customizable and scalable, while still emphasizing security. The diagram below illustrates how certificate reissue works with Ottr.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*21rKmxuIKBD3bSF0BUN1-g.png" /></figure><p>There are many advantages of this new framework:</p><ul><li><strong>Serverless:</strong> No underlying infrastructure to manage, which means we do not have to patch or harden new servers.</li><li><strong>Limited Dependencies:</strong> Only major dependency is upon the ACME Client (acme.sh), which is well-maintained.</li><li><strong>Customizable:</strong> Ottr is modular in design, meaning it provides developers the ability to build custom integrations when additional platforms are introduced to the infrastructure. Developers can use Certificate Authorities outside of Let’s Encrypt, so long as they support the ACME protocol.</li><li><strong>Scalable:</strong> Ability to perform thousands of certificate rotations per day (based off the rate-limit the CA sets).</li><li><strong>Security:</strong> Infrastructure security is a default; the Terraform modules that build Ottr have hardened configurations and follow the principle of least privilege.</li><li><strong>Automated:</strong> Ottr handles the end-to-end certificate rotation lifecycle without any manual intervention.</li><li><strong>Portability:</strong> Ottr builds 100+ resources through Terraform that are easily configurable through modules and deployable across any AWS environment.</li><li><strong>Cost:</strong> Ottr can be used with a Private or Public CA (e.g., Let’s Encrypt) running ACME at no additional cost.</li><li><strong>Error Handling:</strong> Provides instantaneous feedback through Slack on any potential errors during runtime.</li><li><strong>Open Source:</strong> Anyone can contribute, and new platform support can be introduced as the framework matures.</li></ul><h3>Getting Under the Hood</h3><p><em>In this section, we’ll dive into the different components that comprise Ottr and explain how they connect together to abstract the complexities of PKI from the end user.</em></p><h4>High Level Diagram</h4><p><em>Ottr Architecture</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yUXlXD6mzsLGmMEgAGeIhQ.png" /></figure><p>Let’s take a look at the architecture of Ottr and how each component works in relation to the overall flow:</p><ul><li><strong>CloudWatch Event:<em> </em></strong>Automated entrypoint that triggers the Lambda Router at a configurable interval (e.g., once per day).</li><li><strong>Ottr API:</strong> Alternative entrypoint that can be used to execute one-off certificate rotations.</li><li><strong>Lambda Synchronizer:</strong> Aggregates host metadata from datacenters and/or AWS used to update the DynamoDB database via the Ottr API.</li><li><strong>Lambda Router:</strong> Scans the DynamoDB database and determines which hosts are eligible for certificate rotations and forwards data to Step Function.</li><li><strong>Step Function:</strong> Processes batch of device data in parallel from Router Lambda or API and executes an ECS Container for each host that is targeted for a certificate rotation.</li><li><strong>ECS Container:</strong> Pulls down platform specific image from Elastic Container Registry (ECR) based on the ECS Task Definition metadata element that is retrieved from the Step Function.</li><li><strong>Lambda Handler:</strong> In cases where a container runtime error occurs, there is an external integration with Slack that will provide device details and a link to entry within CloudWatch Logs.</li></ul><p><strong>Container Runtime:</strong></p><ul><li>Establish connection to device to generate a Public/Private Key Pair and CSR on the device; pull the CSR onto the container filesystem.</li><li>ACME Client binds the organization’s ACME credentials on the container and sends the CSR to the CA (e.g., Let’s Encrypt) to begin the certificate signing flow.</li><li>ACME Client writes DNS TXT Record(s) to the DNS Subdelegate Zone in Route53 for each Common Name (CN) and Subject Alternative Name (SAN) from the CSR.</li><li>CA validates domain ownership through a DNS-01 Challenge; when validated, a certificate is generated and the ACME Client writes the fullchain certificate to the container filesystem.</li><li>Depending on platform logic, the certificate is applied to the device and a number of validation checks are performed.</li><li>Upon success, the new certificate expiration date is updated for the device in the DynamoDB database.</li></ul><h4>Database</h4><p>The API is not only an alternative entrypoint for Ottr, but it is also the preferred endpoint for managing assets within the DynamoDB database. The elements within the database provide device details that are used both to determine when a certificate is expiring as well as the metadata used to map a host to a platform specific ECS Task Definition for runtime logic.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n-dEsmMrkZBTg6YrK4JwQA.png" /></figure><p><em>Database Asset Output Example</em></p><pre>{<br>   &quot;system_name&quot;: &quot;test.example.com&quot;,<br>   &quot;common_name&quot;: &quot;test.example.com&quot;,<br>   &quot;certificate_authority&quot;: &quot;lets_encrypt&quot;,<br>   &quot;certificate_expiration&quot;: &quot;2021-01-01T00:00:00&quot;,<br>   &quot;certificate_validation&quot;: &quot;True&quot;,<br>   &quot;data_center&quot;: &quot;DC1&quot;,<br>   &quot;device_model&quot;: &quot;PA-XXXX&quot;,<br>   &quot;host_platform&quot;: &quot;panos&quot;,<br>   &quot;ip_address&quot;: &quot;10.0.0.1&quot;,<br>   &quot;origin&quot;: &quot;API&quot;,<br>   &quot;os_version&quot;: &quot;9.x.x&quot;,<br>   &quot;subject_alternative_name&quot;: [<br>   &quot;subdomain.example.com&quot;<br>   ]<br>}</pre><h4>Task Routing</h4><p>During the routing process, the database is first scanned to build a list of devices that have a certificate expiration within 30 days. That list is further narrowed down depending on if the host has valid Route53 Records within the DNS Subdelegate Zone. If these are both true, the logic moves to map each host to a corresponding ECS Task Definition based on the Routing Configuration that is set.</p><p>Following the routing configuration example below, if there is a PAN-OS device running 9.x.x with a model PA-XXXX and has the Certificate Authority set for Let’s Encrypt, an ECS Task of <em>otter-panos-9x-lets-encrypt</em> will be returned. By having this routing logic, it enables end users to perform different types of device rotation logic all under one platform.</p><p><em>Routing Configuration Example</em></p><pre>{<br> &quot;note&quot;: {<br>   &quot;description&quot;: &quot;Routes to trigger certificate renewal or generation based on Platform, OS Version, and Certificate Authority.&quot;<br> },<br> &quot;platform&quot;: {<br>   &quot;panos&quot;: {<br>     &quot;os&quot;: {<br>       &quot;9.x.x&quot;: {<br>         &quot;certificate_authority&quot;: {<br>           &quot;lets_encrypt&quot;: &quot;otter-panos-9x-lets-encrypt&quot; # ECS Task Definition<br>         },<br>         &quot;model&quot;: [<br>           &quot;PA-XXXX&quot;,<br>           &quot;PA-YYYY&quot;,<br>         ]<br>       },<br>   ...</pre><p><em>​​</em>After the router builds the mappings between hosts and task definitions from the routing configuration, the payload is sent into the Step Function and gets processed as a Map, which is used to run a set of steps for each element. The Step Function launches an ECS Fargate Container that pulls an ECR Image defined within the ECS Task Definition. The process is performed in parallel with a max concurrency of the number of Elastic Network Interfaces (ENI) available within the subnet. If there are more containers that are required to run than the available number of ENIs, the Step Function will queue jobs until the previous executions finish and network interfaces become available.</p><p>Looking at the Step Function input below, it shows that <em>test.example.com</em> has an ECS Task Definition of <em>otter-panos-9x-lets-encrypt </em>while <em>test.airbnb.com</em> has an ECS Task Definition of <em>otter-linux-aws-ssm-lets-encrypt. </em>Although both the platforms and the domains are different, Ottr can execute both these rotations in parallel independently of each other because a dedicated container is spun up for each host.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HXAuO66-776Hr6h6fr0o7g.png" /></figure><h4>DNS Subdelegation</h4><p>One of the core security design decisions of the service was to limit access to Route53 (DNS). Ottr performs domain validation using a DNS-01 challenge, which means that the ACME Client needs to write a DNS TXT Record to _acme-challenge.[FQDN] in order for the Certificate Authority to validate ownership of the domain. The security concern is we do not want to provide access that allows write permissions across the organization’s primary hosted zone. While Ottr may only require the ability to write a TXT record to _acme-challenge.test.example.com, as of the time of this writing, AWS does not provide the granularity necessary to specify write access to TXT Records only, which would mean Ottr would be granted access to write any record types including A, CNAME, PTR, and MX Records to your organization’s domain(s).</p><p>To limit access, we introduced DNS subdelegation to the ACME Client. When the infrastructure for Ottr is built, there will be a new Route53 Hosted Zone that is created depending on the configuration such as <em>example-acme.com</em>. When the ACME Client sends the Certificate Signing Request (CSR) to the Certificate Authority (CA), the CA will subsequently look for the TXT record within the challenge-alias field, which will be <em>example-acme.com</em>.</p><p>What this means is that before the domain validation process occurs, DNS needs to be configured to set up a CNAME mapping between your host <em>test.example.com</em> to forward records to <em>example-acme.com</em>.</p><p><em>Terraform DNS Module</em></p><pre>module &quot;dns_example&quot; {<br> source = &quot;./modules/dns&quot;</pre><pre>certificate_common_name = &quot;test.example.com&quot;<br> subject_alternative_names = [&quot;subdomain.example.com&quot;,  &quot;dev.example.com&quot;]<br>}</pre><p><em>DNS CNAME Record Mapping</em></p><pre>_acme-challenge.test.example.com <br>  =&gt;   _acme-challenge.test.example-acme.com<br><br>_acme-challenge.subdomain.example.com <br>  =&gt;   _acme-challenge.test.example-acme.com</pre><pre>_acme-challenge.dev.example.com <br>  =&gt;   _acme-challenge.test.example-acme.com</pre><p>By adding this mapping, all TXT records are written and read within <em>example-acme.com</em>. As a result the permissions within Ottr can be limited to read only for domains such as <em>example.com</em> and write permissions would be granted to the subdelegate zone <em>example-acme.com</em>.</p><h4>External Integrations</h4><p>By default, Ottr includes an external integration with Slack for error handling. After each ECS Task is completed, a new certificate expiration is added to the database pending a successful run. If an error occurs in the container runtime, it results in a notification being generated in Slack to provide operational teams instantaneous feedback and a link that directly points you to the CloudWatch Logs of the failed task. While Slack is the default integration, custom logic can be written if your organization prefers to use other platforms or triaging methods for error handling.</p><h4>Network Architecture</h4><p><em>Ottr Network Data Flow</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XMgiF7zl2A_2UWm5Z65Hsw.png" /></figure><p>Let’s take a look at network connectivity for routes outside of the AWS infrastructure. After the Step Function determines valid hosts to perform certificate rotations on, it will execute a container for each device in parallel. These containers are launched depending on ENI availability in one of two subnets that is predefined when building the infrastructure with Terraform. After connecting to the device and retrieving the CSR, the ACME Client sends the CSR to the Certificate Authority. By default this is Let’s Encrypt, but Ottr has the capability of integrating with any CA that supports the ACME Protocol.</p><p>During the signing process, the ACME Client requires routes to both the Certificate Authority as well as Cloudflare DNS endpoints. Since Let’s Encrypt is the default CA, access to the production endpoint <a href="https://acme-v02.api.letsencrypt.org"><em>acme-v02.api.letsencrypt.org</em></a> as well as the staging endpoint <a href="https://acme-staging-v02.api.letsencrypt.org"><em>acme-staging-v02.api.letsencrypt.org</em></a> are required over 443 (SSL). Cloudflare DNS <a href="https://cloudflare-dns.com"><em>cloudflare-dns.com</em></a> is also used to poll DNS status using DNS over HTTPS (DoH) to determine when the DNS TXT record used for domain validation has been posted by the ACME Client. By using DoH, the DNS resolver runs queries over TLS which improves security since DNS queries are encrypted and not run via DNS over UDP. Performance is also improved since the ACME Client polls DNS compared to using sleep to wait for a set time before querying DNS to validate the domain.</p><h3>Results and ROI</h3><p>From deploying Ottr within Airbnb, our organization has realized several benefits. We’ve seen returns on investment due both to time saved and to the reduced operational overhead for engineering teams. Since the introduction of Ottr at the beginning of the year, thousands of certificate rotations have been performed without any human intervention. This has alleviated a pain point for multiple teams including Operations, which was responsible for monitoring and triaging tickets for expired certificates, Engineering which was responsible for the manual certificate rotation process, and Security which was involved in request approvals.</p><p>Another important return was related to improvements in security. By having Ottr act as a broker between the CA and host, engineers would no longer need to make changes to DNS records to validate domain ownership. This resulted in the reduction of AWS IAM permissions across a number of teams, improving least privilege. In addition, Ottr provides a repeatable framework in which the private key never leaves the host during its lifespan, rather than having engineers generate a private key locally and upload it to the host.</p><p>Most importantly, certificate rotations are being run in more frequent intervals instead of certificate renewals, which means the private key is switched out for each execution; this results in shorter certificate lifespans, which in the case of private key compromise lessens the timeframe data can be decrypted.</p><h3>Conclusion</h3><p>Although Public Key Infrastructure can be a complex problem to solve at scale, Ottr was built to abstract a number of challenges associated with certificate provisioning while also providing additional benefits around operations and security.</p><p>By open sourcing Ottr, we hope to create a community to share, collaborate, and expand the framework to help fit the needs of other organizations. If you’re interested in helping protect people and data, Airbnb Security is hiring. Check out our <a href="https://www.airbnb.com/careers/departments/engineering">open positions</a> and apply today!</p><h3>Open Source</h3><p><strong>Setup<br></strong>Ottr is now open sourced on <a href="https://github.com/airbnb/ottr">Github</a>. You can begin building the infrastructure by going to the Setup resource page and learn more about our current implementations through the Supported Platforms link.</p><p><strong>Contributing<br></strong>Please feel free to reach out or submit pull requests with any suggestions. If you are currently leveraging Ottr and running rotations against platforms that currently aren’t supported, please view the <a href="https://github.com/airbnb/ottr/tree/master/docs/CONTRIBUTE.md">contributions page</a> and consider helping add your implementations to the platform!</p><p><strong>Credits and Contributions:<br></strong>Ben Paradis (Staff Security Engineer, Airbnb)<br>Aaron von Hungen (Senior Security Program Manager)<br>John Borromeo (Senior Network Engineer, Airbnb)<br>Ryan Diers (Security Engineer, Airbnb)<br>Sean Corcran (Senior Systems Engineer, Airbnb)<br>Jeff Nanney (Staff Network Architect, Airbnb)<br>Mark Vlcek (Security Engineer, Airbnb)<br>Zeeshan Khadim (Former Manager, Airbnb)<br>Tina Nguyen (Senior Project Manager, Airbnb)</p><p>Development Community Supporting acme.sh</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><h3>Appendix</h3><p>All trademarks are the property of their registered owners; Airbnb claims no responsibility for nor proprietary interest in them.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f6580010ae0c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/meet-ottr-a-serverless-public-key-infrastructure-framework-f6580010ae0c">Meet Ottr: A Serverless Public Key Infrastructure Framework</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automating Data Protection at Scale, Part 2]]></title>
            <link>https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-2-c2b8d2068216?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c2b8d2068216</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[privacy]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[elizabeth nammour]]></dc:creator>
            <pubDate>Tue, 19 Oct 2021 17:15:31 GMT</pubDate>
            <atom:updated>2021-10-19T17:15:30.962Z</atom:updated>
            <content:encoded><![CDATA[<p>Part two of a series on how we provide powerful, automated, and scalable data privacy and security engineering capabilities at Airbnb</p><p><a href="https://www.linkedin.com/in/elizabethnammour/">Elizabeth Nammour</a>, <a href="https://www.linkedin.com/in/pinyao-guo-6b621684/">Pinyao Guo</a>, <a href="https://www.linkedin.com/in/wendy-jing-jin-81452921/">Wendy Jin</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VdwIh29PIUkVLcaU" /></figure><h3>Introduction</h3><p>In <a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">Part 1 of our blog series</a>, we introduced the Data Protection Platform (DPP), which enables us to protect data in compliance with global regulations and security requirements. We stressed that understanding our data, by keeping track of where personal and sensitive data is stored in our ecosystem, is a necessary building block to protecting the data. In this blog post, we will discuss the challenges companies often face when trying to pinpoint the exact location of personal and sensitive data. As a stopgap, many companies rely on engineers to manually keep track of where and how personal and sensitive data flows within their environments. However, relying on manual data classifications presents some challenges:</p><ol><li><strong>Data is constantly evolving</strong>. This makes it challenging for engineers to have a global understanding of the data and how the data flows throughout a company’s infrastructure. Data can be replicated and propagated into different data stores. Also, new types of data can be collected as products arise or change.</li><li><strong>Manual classification is more prone to error.</strong> Engineers may forget if a data asset contains personal data, or perhaps, as is the case in freeform user entries, may not know what an asset contains.</li><li><strong>Security and privacy personal data elements keep expanding. </strong>Engineers have to perform the manual data classification exercise again for any new data elements required by new privacy regulations and security compliance, which incurs a high cost and a low labor efficiency to companies.</li><li><strong>Secrets can leak into our codebase and various data stores.</strong> Secrets, such as production API keys, vendor secrets, and database credentials, are commonly used by engineers. Secret leakage in a codebase is a known issue in the tech industry, usually due to accidental or unintentional code committed by engineers, and they are not always caught by reviewers. Once checked in, secrets become needles in a haystack and cannot be easily discovered.</li></ol><p>To address these challenges, we built data classification tools to detect personal and sensitive data in our data stores, logs, and source code. Continue reading as we walk through the architecture of our data classification tools. Specifically, we will dive deep into the technical components of Inspekt, our data classification system for our data stores and logs, and Angmar, our secrets detection and prevention system for our codebase on Github Enterprise.</p><h3>Inspekt: A Data Classification Service</h3><p>Inspekt is an automated and scalable data classification tool that determines where personal and sensitive data is stored in our ecosystem. Inspekt consists of two services: the first service, called the Task Creator, determines what needs to be scanned, and the second system, called the Scanner, samples and scans the data to detect personal and sensitive data.</p><h4>Task Creator</h4><p>The task creation system is responsible for determining what to scan and splitting it into tasks for the scanning system to ingest.</p><p>The Inspekt Task Creator periodically calls Madoka, our metadata service <a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">described in our previous blog post</a>, to get a list of the data assets that exist at Airbnb. Fr MySQL and Hive data stores, the service fetches a list of all tables. For AWS S3, the service fetches a list of buckets for each AWS account and their corresponding list of object keys. Due to the sheer volume of data, the Task Creator randomly samples a small percentage of object keys from each bucket. For application logs, the service fetches the list of all services at Airbnb and their corresponding Elasticsearch clusters that store the logs. The Task Creator then creates an SQS message for each table/object/application, which is referred to as a task, and adds it to the scanning SQS queue, which will be consumed by the Scanner in a later stage.</p><h4>Scanner</h4><p>The scanning system is responsible for sampling and scanning the data to detect personal information. Inspekt provides an interface to define scanning methods, algorithms to scan sampled data. For each data element, we define a “verifier” as a combination of one or multiple scanning methods.</p><p>Inspekt currently supports four types of scanning methods:</p><ul><li><strong>Regular expressions (Regexes):</strong> Regexes are useful for data elements that follow a fixed format, such as longitude and latitude coordinates, birthdates, email addresses, etc. Inspekt allows us to define regexes to either match with the metadata (e.g. column name, object key name) of the data asset or with the content of the asset. Inspekt allows us to define regexes as both allowlists and denylists. For example, we can define a regex to detect data assets where the column name contains “birthdate”, or where the content contains the word “birthdate”, or where the content does not contain the word “birthdate”.</li><li><strong>Tries:</strong> Some data elements that we collect don’t follow a fixed pattern, such as first and last name, and cannot be detected using regexes. When the data element is stored in a known source data store, we can make use of the <a href="https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm">Aho-corasick algorithm</a>, which uses tries to detect substring matches of a finite sample of that data.</li><li><strong>Machine Learning (ML) models:</strong> A number of data elements cannot be detected accurately or effectively using regex-based scanning for several reasons. First, some data elements, such as physical addresses, have varying formats or non-exhaustive amounts of content. Second, as a global company that operates in more than 200 countries, Airbnb hosts data in different languages. Third, some data, such as images, are not text-based and thus cannot be recognized using regular scanning methods. Machine learning-based algorithms are natural fits to handle these challenges. We developed different machine/deep learning models, such as multitask CNN, Bert-NER, and WiDeText Classification Model, for the detection of several complex data elements. The models are trained either using data samples from our production database, such as user addresses from Airbnb’s listings tables, or using public datasets or models pre-trained on large text corps. We host these models on the Airbnb machine learning platform called <a href="https://databricks.com/session/bighead-airbnbs-end-to-end-machine-learning-platform">Bighead</a>, which serves API endpoints for Inspekt to detect each data element using machine learning scanning methods.</li><li><strong>Hardcoded methods:</strong> Some data elements that we collect follow a fixed pattern, but are either too complicated to describe in a regex, or there already exists an open-source solution that detects the data elements with high quality. Inspekt allows us to define a code block to detect a data element. For instance, we created an International Bank Account Number (IBAN) data element verifier leveraging a validator from an open-source library.</li></ul><p>In Inspekt, verifiers are defined as JSON blobs and stored in a database that the Scanner reads from. This allows us to easily modify existing verifiers or add new verifiers to detect new data elements on the fly without redeploying the service.</p><p>Here is an example of a verifier configuration that aims to detect any column name that contains the word “birthdate”, or where the content contains the word “birthdate”:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bae9a2599f8265fe04be018e37d6d1a3/href">https://medium.com/media/bae9a2599f8265fe04be018e37d6d1a3/href</a></iframe><p>Inspekt Scanner is a distributed system using Kubernetes. Depending on the workload (i.e., tasks in queue), it can scale horizontally as needed. Each Scanner node picks up task messages from the SQS task queue. For scanning robustness, each message reappears N times back into the queue until a scanner node deletes it. A diagram of the Scanner architecture is shown below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/968/0*Wo44CMWwVR2VRFYw" /><figcaption>Figure 1: Inspekt Scanner Architecture</figcaption></figure><p>At startup, each Inspekt Scanner node fetches and initializes the verifiers from the database. The verifiers are periodically refreshed to pick up new verifiers or verifier configuration changes. After verifier initialization, each Scanner node fetches a task from the task queue created by the Task Creator. Each task contains a specification of the task to be executed, i.e., which data asset to scan, the sampling amount, etc. The node then submits each task to a thread pool that performs the sampling and the scanning job. The scanning job runs as follows:</p><ol><li>Inspekt scanner connects to the data store that is specified in the task and samples data from the data store. For MySQL, the scanner node will connect to the MySQL database and sample a subset of rows for each table. To scan a different set of rows each time without causing a full table scan, we randomly generate a value X smaller than the maximum value of the primary key and select a subset of rows where the primary key &gt;= X. For Hive, we sample a subset of rows for each table from the latest partition. For service logs, we sample a subset of logs for each service per day. To get better coverage over different logs, we make queries to our Elasticsearch log clusters to select logs from distinct logging points. For S3, we generate a random offset smaller than the object size and sample a customizable set of bytes starting from that offset. We also support scanning across AWS accounts. If an object is in a different AWS account than that where the scanner is running, Inspekt automatically uses the proper Assume Role IAM permissions to access and read the objects from the foreign account.</li><li>For each piece of sampled datum from data stores, Inspekt Scanner runs each verifier against the datum to determine whether any match was found.</li><li>Inspekt Scanner stores the matching results in a database. For each match, we will store the metadata of the data asset where the match was found, the matched content, and the verifier it matched with. We also store a subset of this information, containing just the data asset and what data element was found, in a separate table. We periodically delete records from the matching results table to ensure the security and privacy of our data.</li><li>Inspekt Scanner deletes the SQS message</li></ol><h3>Inspekt Quality Measurement Service</h3><p>As described in our <a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">previous blog</a>, our Data Protection Platform leverages the classification results to initiate protection measurements. For downstream stakeholders to trust and adopt classification results from Inspekt, we need to continuously ensure that each data element is being detected with high quality. Causing too many false positives is disruptive to teams that are alerted with the findings, and it would discredit the team. Causing too many false negatives means we aren’t successfully catching all occurrences of the data element, casting privacy and security concerns.</p><h4>Quality Measurement Strategy</h4><p>To continuously monitor and improve the quality of each data element verifier, we built an Inspekt Quality Measurement service to measure their precision, recall, and accuracy.</p><p>For each data element, we store the true positive data and true negative data as the ground truth in our Inspekt Quality Measurement database. We then run the verifier against the ground truth dataset. From the true positive data, we output the number of true positives (TP) and the number of false negatives (FN) the verifier generated. From the true negative data, we output the number of false positives (FP) and true negatives (TN) the verifier generated. We can then calculate precision, recall, and accuracy from the TP, FN, FP, TN counts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CWFlmfUc5x82fyjo" /><figcaption>Figure 2: Inspekt Quality Calculation</figcaption></figure><h4>Sampling and Populating Test Data</h4><p>As discussed above, for each personal data element, we need to gather true positive and true negative data sets. For these metrics to be accurate, the data sets used for testing must be as comprehensive and similar to production data as possible. We populate this data set by periodically sampling data from the following sources:</p><ul><li>Known datasets in production: Some columns in our online databases or our data warehouse are known to contain and represent a specific data element, e.g., a MySQL column that is known to store email addresses. We can use these columns as true positives.</li><li>Inspekt results: As Inspekt runs and generates results, the result can either represent a true positive or a false positive. We can therefore use these data to populate the data set.</li><li>Known freeform/unstructured data: Some columns in our online databases or our data warehouse represent freeform user-entered data or unstructured blobs, such as messages, JSON objects, etc. These columns could contain any type of data elements and represent a good test data source to ensure our system detects data elements in unstructured formats and different edge cases.</li><li>Generated fake, synthesized data: Some data elements, such as a user’s height and weight, are not present very often in our data stores, and there are no known source columns that store them. To have enough test data for these data elements, we generate fake data in the proper format and populate our test database with it.</li></ul><h4>Labeling</h4><p>After sampling the data, we need to ensure whether each sample represents a true positive or true negative before storing it in our test data sets. To achieve this, we manually label the sampled data using <a href="https://aws.amazon.com/sagemaker/groundtruth/">AWS Ground Truth</a>. For each data element, we’ve developed instructions and trained Airbnb employees to correctly label each sample as true or false. Our system will then upload the raw data that is sampled for each data element onto AWS S3 and create a labeling job with the proper instructions onto Ground Truth. Once the employees finish labeling the data, the labeled output will be stored in an S3 bucket for our system to read. The Inspekt Quality Measurement service will periodically check the bucket to determine whether the labeled data is ready. If ready, it will fetch and store the data in our test data sets, and then delete the raw and labeled data from S3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*klZH8DK00f_Sh_Fj" /><figcaption>Figure 3: Inspekt Test Labeling Pipeline</figcaption></figure><h4>Re-Training ML Models</h4><p>The labeled data from the Inspekt Quality Measurement service can be of value to improve the performance of Inspekt verifiers. Specifically, the labeled results can be a useful source to reinforce the performance of Inspekt machine learning models. We feed the labeled data into the training samples of some machine learning models. During the re-training, the newly labeled data are used along with the training samples. We obtain better models for corresponding data elements after each re-training.</p><h3>Angmar: Secrets Detection and Prevention in Code</h3><p>In the previous sections, we described how Inspekt focuses on detecting personal and sensitive data in data stores. However, some sensitive data, such as business and infrastructure secrets, may also exist in the company codebase, which could lead to serious vulnerabilities if leaked to unintended parties. We expanded our scope to a secret detection solution called Angmar, which leverages detection and prevention approaches to protect Airbnb secret data in Github Enterprise (GHE).</p><h4>Architecture</h4><p>Angmar is built as two parts, a CI check that intends to detect secrets pushed to GHE and a Github pre-commit hook that intends to prevent secrets from entering GHE in the first place.</p><p>We built a CI check to scan every commit pushed to the Airbnb GHE server. When a commit is pushed to GHE, a CI job, which is required to pass before a merge will be allowed into the main branch, is kicked off. The CI job downloads an up-to-date customized version library of an open-source library <a href="https://github.com/Yelp/detect-secrets">Yelp/detect-secrets</a> and runs a secret scanning job over each modified or added file in the commit. Once secrets are detected in the CI job, it triggers the Data Protection Platform to create a JIRA ticket and automatically assigns the ticket to the code author for resolution. The details of the ticket generation will be discussed in part 3 of this blog series. We require all production secrets to be removed, rotated, and checked in again using our production secret management tool named <a href="https://medium.com/airbnb-engineering/production-secret-management-at-airbnb-ad230e1bc0f6">Bagpiper</a> within SLA.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*onZNDxoAXUqhyTeR" /><figcaption>Figure 4: Angmar Architecture</figcaption></figure><p>However, with the CI check shepherding every push to the codebase, the secret exposure time window still casts a certain amount of security risks on the company infrastructure. Also, in some cases, secret rotations could be very costly in terms of engineering hours and expenses. Therefore, we propose a proactive approach to prevent secrets from entering GHE in the first place to further eliminate secret exposure and to save the effort of rotating secrets. We built an Angmar pre-commit tool using the same custom detection library for developers to block committing secrets into Git commits. Once secrets are detected in a commit, the Git commit command would prompt an error and block the new commit.</p><h4>Customization</h4><p>We made a few customizations to the detect-secrets open-source library for Airbnb’s use case:</p><ul><li>We added a few secret data elements specific to Airbnb into the library plugins.</li><li>According to our analysis on false positive detections from the library, some test secrets, staging secrets, or placeholders are falsely detected as secrets. We added a path-filtering logic to skip certain files within a commit.</li><li>We also implemented some deduplication logic that uses hashing to reduce repetitive tickets due to modifications on the same file in different commits.</li><li>In rare cases when false positives occur, we allow developers to skip certain lines of code or certain files to avoid blocking emergency code merges into production. The security team reviews the skipped code regularly to make sure no actual secrets are bypassed.</li></ul><h3>Future Work</h3><p>We are continuously improving and expanding Inspekt and Angmar to scan more data sources and detect more privacy and sensitive data elements. A few initiatives we are currently exploring or working on include:</p><ol><li>Scanning Thrift interface description language API requests and responses to keep track of how personal and sensitive data flows between services.</li><li>Scanning our third-party applications, such as Google Drive, Box, to understand data lineage around how data flows into third-party applications, and how data is accessed both internally and externally.</li><li>Expanding our scanning capabilities to more data stores that are used at Airbnb, such as DynamoDB, Redis, etc.</li></ol><h3>Alternatives</h3><p>There are several commercial solutions on the market that tackle data classification. Before building out our solution, we evaluated a few commercial vendors to see if we could leverage existing tools rather than building our own solution. We decided to build an in-house solution for the following reasons:</p><ul><li>Data store coverage: We needed a tool that could cover most of the data stores that exist in our ecosystem, since building a custom tool for a subset of data stores would require a very similar amount of effort as building it for all of our data stores. Most vendors only support scanning SAAS applications and S3 buckets.</li><li>Customized scanning: We needed to be able to customize what scanning algorithms to scan against. This is important since we want to make sure we can scan for all personal and sensitive data elements and also ensure we are getting the best performance (precision, recall, accuracy) for all data elements. Many vendors support scanning against a custom regex, but none support scanning against a custom ML model.</li><li>Cost efficiency: We found that for our purposes, building our solution would be much more cost-efficient than using a commercial solution.</li></ul><h3>Conclusion</h3><p>In this second post, we dove deep into the motivations and architecture of our data classification system that enables us to detect personal and sensitive data at scale. In our next post, we will deep dive into how we’ve used the data protection platform to enable various security and privacy use cases.</p><h3>Acknowledgments</h3><p>Inspekt and Angmar were made possible by all team members of the data security team: Shengpu Liu, Jamie Chong, Zi Liu, Jesse Rosenbloom, Serhi Pichkurov, and PM team Julia Cline, and Gurer Kiratli. Thank Bo Zeng from the AI Labs team for helping develop Inspekt machine learning models. Thanks to our leadership, Marc Blanchou, Joy Zhang, Brendon Lynch, Paul Nikhinson, and Vijaya Kaza, for supporting our work. Thank Aaron Loo and Ryan Flood from the Security Engineering team for their support and advice. Thank you to the data governance team members for partnering and supporting our work: Andrew Luo, Shawn Chen, and Liyin Tang. Thank you Tina Nguyen and Cristy Schaan for helping drive and make this blog post possible. Thank you to previous members of the team who contributed greatly to the work: Lifeng Sang, Bin Zeng, Prasad Kethana, Alex Leishman, and Julie Trias.</p><p>If this type of work interests you, see <a href="https://careers.airbnb.com/">https://careers.airbnb.com</a> for current openings.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c2b8d2068216" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-2-c2b8d2068216">Automating Data Protection at Scale, Part 2</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating Kafka transparently between Zookeeper clusters]]></title>
            <link>https://medium.com/airbnb-engineering/migrating-kafka-transparently-between-zookeeper-clusters-e68a75062f65?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e68a75062f65</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[zookeeper]]></category>
            <category><![CDATA[kafka]]></category>
            <dc:creator><![CDATA[Edmund Mok]]></dc:creator>
            <pubDate>Tue, 12 Oct 2021 18:58:41 GMT</pubDate>
            <atom:updated>2021-10-12T18:58:41.773Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Learn more about how to migrate your Kafka cluster from one Zookeeper cluster to another without any user impact.</em></p><p><strong>By</strong>: <a href="https://www.linkedin.com/in/edmund-mok">Edmund Mok</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rlubHQYNxZB-GWwTgIKrVQ.jpeg" /></figure><h3>Introduction</h3><p>Kafka is an open-source distributed event-streaming platform. It depends on Zookeeper, another open-source distributed coordination system, to store cluster metadata. At Airbnb, Kafka forms the backbone of our data infrastructure, powering use cases such as event logging and change data capture that help us better understand our guests and hosts, and make decisions that improve our product.</p><p>We run several production Kafka clusters, the largest of which being our oldest cluster consisting of hundreds of brokers and supporting over 1GB/s of incoming traffic. Prior to our migration, this Kafka cluster relied on a legacy Zookeeper setup — a multi-tenant Zookeeper cluster shared between many different production use cases, which means that any incident on the cluster would affect all dependent services, including Kafka. Moreover, this Zookeeper cluster also lacks a clear owner. We wanted to migrate Kafka out of this Zookeeper cluster into a separate, dedicated Zookeeper cluster with clear ownership and better isolation from other use cases.</p><p>The goal was for the migration to be done transparently, without any data loss, downtime or impact to Kafka users, as well as other Zookeeper users.</p><p>The recently released Kafka 2.8.0 includes changes for <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum">KIP-500</a>, the community’s proposal to remove Kafka’s Zookeeper dependency. Since this feature in Kafka 2.8.0 is neither complete nor production tested, it would take another few releases before we would have the confidence to try running Kafka without Zookeeper. Thus, we decided to migrate Kafka between Zookeeper clusters instead.</p><p>Our approach was largely inspired by <a href="https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html">Yelp’s original approach</a>, but modified to use Zookeeper observers in the migration to minimize configuration changes to the source Zookeeper cluster. We were also migrating Kafka clusters running version 2.3.1 and Zookeeper clusters running version 3.5.8. If you are also attempting a migration, be sure to test carefully on your setup, especially if you are running a different Kafka or Zookeeper version!</p><h3>High Level Strategy</h3><p>We’ll use zk-source to refer to the original Zookeeper cluster that we are migrating Kafka out of, and zk-dest to refer to the new target Zookeeper cluster that we want to move the Kafka dependency to. Note that zk-dest must be a fresh cluster that does not contain any pre-existing data.</p><p>Our plan for the migration consisted of the following phases:</p><ol><li>First, add zk-dest hosts as observers of the zk-source cluster</li><li>Next, switch the Kafka cluster’s Zookeeper connections from zk-source to zk-dest</li><li>Finally, reconfigure zk-dest observers to form their own separate ensemble</li></ol><h3>Phase 1: Add zk-dest hosts as observers of zk-source</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sdbX5HT3HPQpf19jGWwUOA.png" /><figcaption><em>Phase 1: After adding zk-dest hosts as observers of zk-source, all zk-dest hosts will be connected to the leader of zk-source and begin replicating data from zk-source.</em></figcaption></figure><p>In the Zookeeper configuration of each zk-dest host, include all zk-source servers with their respective roles (participants or observers) and all zk-dest servers as observers in the server list. Starting Zookeeper in zk-dest hosts with such a configuration will configure them to join the zk-source cluster as observers and start replicating all Zookeeper data in the cluster.</p><p>One advantage of using observers here is that in Zookeeper, observers do not participate in the voting process for pending commits and only learn from the leader about committed proposals. Thus, observers can be added to the source cluster to replicate existing data without affecting the quorum.</p><p>Another benefit of using observers is that we do not have to modify anything within zk-source, such as the configuration files in zk-source hosts. Observers can join the zk-source cluster without being present in the server list of the zk-source cluster configuration. The leader does not reject such observers despite being aware that they are not listed in the original configuration. This was particularly important for us since we wanted to avoid making changes to (or worse, having to restart) the source cluster if possible.</p><h3>Phase 2: Switch Kafka connections from zk-source to zk-dest</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*117hDPP3WxjoI0sHOmzm9w.png" /><figcaption><em>Phase 2: After switching Kafka’s Zookeeper connection over from zk-source to zk-dest, Kafka will still be operating on the original data in zk-source, but through zk-dest observers.</em></figcaption></figure><p>This phase involves updating the Kafka configuration on each broker to use Zookeeper connection strings that point only to zk-dest observers, followed by running a rolling restart of every Kafka broker to pick up this new configuration. Remember to confirm that the observers are correctly replicating data from the zk-source cluster before switching over. At the end of this phase, the Kafka cluster will continue to work with data in the original zk-source cluster, but now doing so through zk-dest observers.</p><h3>Phase 3: Reconfigure zk-dest observers to form a separate ensemble</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yjs_J80JTV3x5MeKxvzA0w.png" /><figcaption><em>Phase 3: After restarting zk-dest observers into a separate ensemble, zk-dest hosts will form an isolated cluster with a copy of the original data from zk-source but independent from the original cluster.</em></figcaption></figure><p>Here, we perform the following actions:</p><ol><li>Block network communications between zk-source and zk-dest by inserting iptables rules in zk-dest hosts to reject any incoming and outgoing Zookeeper TCP packets to zk-source</li><li>Check that zk-dest hosts have the latest data within the Kafka subtree</li><li>Update zk-dest configurations to include only zk-dest hosts in the server list as participants</li></ol><p>Zookeeper will be unavailable to Kafka after the first action, but availability will resume after the final action, and Kafka will eventually reconnect through retries. At the end, zk-dest hosts will pick up the new configuration and form a quorum consisting of only zk-dest hosts. Kafka will be working on a copy of zk-source’s data, but within the zk-dest cluster.</p><p>We first block network communications to prevent any further writes from Kafka to Zookeeper to minimize the chance of missing an update being replicated from zk-source to zk-dest as we do the migration, which can result in inconsistent data. This step is just an extra precaution since we have observed Kafka to have low transaction rates to Zookeeper, mostly used for topic creation and updates.</p><p>Alternatively, we can either shut down all zk-dest hosts, or block traffic directly from Kafka brokers to Zookeeper. One advantage of blocking traffic between zk-source and zk-dest is that we have found it faster to update iptables rules than to restart Zookeeper processes gracefully. If we want to restore zk-dest back as observers of the original cluster, we can do so more quickly to restore Zookeeper availability to Kafka. It can also help with preserving Zookeeper sessions in the zk-dest cluster, since the zk-source leader may propagate session expirations to zk-dest observers if traffic between them is not blocked. In a later section, we will explain more about why we would like to preserve these sessions during the migration.</p><p><strong>Even though Zookeeper is considered unavailable to Kafka after the first action, producers and consumers can still interact with the Kafka cluster and do not experience any disruptions, as long as all brokers are healthy.</strong></p><p>In the second action, we can confirm the data within the Kafka subtree is up-to-date by checking whether the zxid (the <a href="https://zookeeper.apache.org/doc/r3.2.2/zookeeperInternals.html#sc_guaranteesPropertiesDefinitions">Zookeeper transaction id</a> used to guarantee total ordering of proposals) for the latest transaction in zk-dest is greater than or equal to the largest czxid, mzxid or pzxid of all ZNodes in the Kafka subtree. These zxids correspond to the zxids of: the change that created a particular ZNode, the change that last modified a particular ZNode, and the change that last modified children of a particular ZNode. If this condition holds true, then any transaction that modified the Kafka subtree of zk-source would have been captured in the transaction logs of zk-dest and we are confident that the data in zk-dest is up-to-date.</p><p>In order to perform this check quickly during our migration, we modified the transaction log dump tool bundled with Zookeeper to print only the latest transactions instead of the entire log. In our case, the average log was about 500MB. Dumping the whole log would take around 20 seconds, even though we were only concerned about the latest transactions. We also found that using memory-mapped file I/O instead of standard file I/O further improved performance slightly. These changes reduced the time to extract the latest transactions from the log to around 1 second.</p><p>Since Zookeeper will be unavailable to Kafka during the migration, the controller will be unable to detect any brokers that go down during this time. If a broker fails, the controller will not elect new leaders for any partitions that have leader replicas on the failed broker. Since consumers and producers can only read from and write to leader replicas, and the replicas are offline on the failed broker, these partitions will become unavailable to consumers and producers. Depending on your producer retry policy, the offline leader replicas may cause new incoming messages to be dropped and lost forever. Thus, it is important to monitor the broker metrics in order to be aware if any brokers do fail. If this happens during the migration, we can either proceed with the migration and allow the controller to handle the failure once it resumes connectivity with the zk-dest at the end of the migration, or we can revert zk-dest hosts back as observers of zk-source and unblock network communications between zk-source and zk-dest to bring back Zookeeper availability to Kafka and allow the controller to handle the failure.</p><h3>Preserving Zookeeper sessions during migration</h3><p>Zookeeper clients (such as Kafka brokers) connect to a Zookeeper cluster through sessions. Ephemeral nodes are associated with these sessions — for example in Kafka, they include the controller and broker ID ZNodes. Ephemeral nodes are removed when sessions expire, and sessions are kept alive through client heartbeats. The Zookeeper leader is responsible for tracking all sessions and determining if they have expired.</p><p>Ideally, we want to preserve Zookeeper sessions across the migration because of how Kafka depends on these sessions and the associated ephemeral nodes. Specifically, we would like to preserve the ephemeral broker ID ZNodes (under path /brokers/ids) throughout the migration. During the migration, Zookeeper will only be unavailable to Kafka for a short period of time, but if the migration takes long enough, the Zookeeper leader may expire the session.</p><p>The Kafka controller checks for offline leader replicas by monitoring the ephemeral broker ZNodes and identifying which brokers are offline. If a Kafka broker’s Zookeeper session expires, its ephemeral broker ID ZNode will be removed. At the end of the migration, when Zookeeper becomes available again, if the broker does not reconnect to Zookeeper fast enough, the controller will detect the missing broker ZNode and assume the broker has failed. The controller will attempt to relocate leader replicas to other brokers to maintain partition availability, and once more back to the original broker when it reconnects. During this time, Kafka consumers and producers may experience disruption as they switch between the brokers holding the leader replicas.</p><p>For our migration, we had to work with the existing session timeout of 40 seconds in the zk-source cluster to avoid making any changes to the cluster. Since the leader determines session expiry, by blocking traffic between zk-source and zk-dest, we prevent the possibility of any existing sessions expiring in zk-dest (though they might still expire in zk-source) and avoid the aforementioned race condition.</p><h3>Conclusion</h3><p>Migrations can be tricky to execute, especially when trying to ensure no disruptions to users and no data loss on complex distributed systems like Kafka and Zookeeper, but through careful planning and testing, we were able to migrate with no downtime. This approach can be used on other systems that depend on Zookeeper, not just Kafka — ideally, such systems have low transaction rates to Zookeeper and are able to gracefully handle Zookeeper unavailability. Once again, we would like to acknowledge Yelp’s original blog post for the inspiration and proving that such a migration can be done without user impact.</p><h3>Acknowledgements</h3><p>Huge thanks to Xu Zhang, Tong Wei, Xuting Zhao, Meng Guo, and Mocheng Guo for collaborating closely and contributing invaluable feedback to ensure a successful migration. Also special thanks to Abhishek Parmar, Chandramouli Rangarajan, Chiyoung Seo, Jingwei Lu, Liuyang Li, Liyin Tang, and Zheng Liu for their help in reviewing the migration strategy. Finally, thanks to Brett Bukowski and Dylan Hurd for reviewing and improving this blog post.</p><p>Check out these related roles:</p><p><a href="https://careers.airbnb.com/positions/3045137/">Senior Software Engineer, Storage &amp; Database</a></p><p><a href="https://careers.airbnb.com/positions/1858035/">Senior Staff Software Engineer, Backend Data Platform</a></p><p><a href="https://careers.airbnb.com/positions/2410642/">Staff Software Engineer, Cloud Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3477507/">Senior/Staff Software Engineer, Analytics Infrastructure</a></p><p><a href="https://careers.airbnb.com/positions/3069302/">Staff Software Engineer, Service Platform</a></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e68a75062f65" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/migrating-kafka-transparently-between-zookeeper-clusters-e68a75062f65">Migrating Kafka transparently between Zookeeper clusters</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Designing for Productivity in a Large-Scale iOS Application]]></title>
            <link>https://medium.com/airbnb-engineering/designing-for-productivity-in-a-large-scale-ios-application-9376a430a0bf?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/9376a430a0bf</guid>
            <category><![CDATA[ios-app-development]]></category>
            <category><![CDATA[mobile-app-development]]></category>
            <category><![CDATA[xcode]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[swift]]></category>
            <dc:creator><![CDATA[Michael Bachand]]></dc:creator>
            <pubDate>Tue, 05 Oct 2021 17:29:02 GMT</pubDate>
            <atom:updated>2021-10-05T17:33:19.984Z</atom:updated>
            <content:encoded><![CDATA[<p><em>How innovation in technology and people processes have enabled iOS developers to remain productive in a large codebase.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YoPXkM2HntwCXznpl96Ouw.png" /></figure><p>Every iOS engineer remembers the joy of seeing their first application running on an iOS device. The human-centric interface of the iPhone brings the program to life. When you choose iOS development as a career, that joy grows as your application touches more people’s lives.</p><p>Affecting more users often involves new iOS features, flows, and functionality. But as an application grows to serve more users, new features and functionality can introduce additional weight and complexity, which slows product iteration and precludes atomic refactors.</p><p>We have undergone this journey as an iOS team at Airbnb. There is a profound joy knowing that the code we ship every week enables unforgettable vacations for guests and new revenue streams for host entrepreneurs. A focus on design is in our DNA and we take immense pride in perfecting every detail of what we show to our users. At the same time we have not been immune to the challenges of developing at scale.</p><p>In this article, we will walk through difficulties that we have encountered in accommodating the growing business needs of Airbnb. We will outline how investments in technology, ownership, and processes have allowed the Airbnb iOS team to have the best of both worlds: working in a globally impactful codebase without feeling its weight.</p><h3>Challenges of developing a large-scale iOS app</h3><p>An Airbnb intern made the first commit to our iOS application on June 16th, 2010. Since then, that same Xcode project has evolved into a codebase with 1.5 million lines of first-party code. About 75 iOS engineers work on our application today. We ship the app weekly in 62 languages supporting a community of guests and hosts in nearly every country on the planet.</p><p>At Airbnb’s scale, code organization becomes a challenge. We welcome and encourage experimentation and new ideas. Then, once we’ve sufficiently explored a solution space, we value the consistency of an “Airbnb way.” Until recently, most of our code was organized into modules within a flat directory called lib/. Without any hierarchy or categorization of our code, it became hard for engineers to find the existing implementations of general-purpose capabilities. We began to notice many ways to accomplish the same task in our application code, which bloated the binary that we shipped to users. Moreover, we found that each competing implementation of the same capability tended to be of lower quality than one implementation that received more investment and attention.</p><p>While Xcode is the tool in which iOS engineers feel most comfortable working, we found that Xcode does not scale gracefully to a codebase of our size and complexity. Not only are Xcode project files challenging to review in pull requests, but the incidence of merge conflicts and race conditions in these project files increased with a larger team of engineers moving at a high velocity. Even opening Xcode can become a chore with a large codebase. Over a year ago, we measured that Xcode would take between one and two minutes to become interactive when loading a workspace with all of our source code.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bbD6oAGQtHcVKz0XXU6SCw.png" /><figcaption><em>A pull request circa 2018 adding one new module. Over half of the changes to the pbxproj Xcode project file are not shown in this screenshot.</em></figcaption></figure><p>Most iOS engineers became frustrated with long build times and slow iteration loops. At one point, a particularly creative engineer found that his laptop would compile code faster if he unplugged his external monitor. Many engineers have trained themselves to plug the USB-C charging cable into the <a href="https://www.imore.com/heres-why-you-should-probably-charge-your-macbook-using-ports-its-right-side">right side</a> of their MacBook Pro to avoid the productivity loss of thermal throttling. When Airbnb’s codebase was less than 500k lines of first-party code, some of these problems could be rectified with more powerful hardware, though we identified a practical limit to that solution as well. It became hard to feel like you were doing your best work when large amounts of your day were spent waiting for builds to complete.</p><p>These challenges grew organically. Feedback from new hires provided valuable input for how we should prioritize infrastructure needs, as iOS engineers who came from companies with smaller projects had not yet become used to the sluggishness and workarounds of an overgrown codebase. We knew that something had to change to ensure that Airbnb continued to ship a world-class iOS application.</p><h3>Solutions we’ve implemented</h3><p>We investigated and implemented many solutions over the years to solve the problems stated above. In this post we will discuss the three biggest levers that have allowed us to operate efficiently at scale. We expect that these high-level themes will be applicable to other small- to medium-sized iOS teams undergoing rapid growth.</p><h4>Adopting a modern build system</h4><p>Xcode remains the preferred IDE for iOS engineers at Airbnb. At the same time, we’ve seen features in other build systems that we knew could improve the productivity of iOS developers. A few stood out: network caches of build artifacts, a query interface for the build graph, and a seamless way to add custom steps as dependencies. We believe that these capabilities are table stakes for a modern build system.</p><p>Facebook’s <a href="https://buck.build/">Buck</a> build system met these requirements. We began discussing Buck seriously in late 2016 and began explorations in earnest in 2017. In 2019, we fully transitioned to Buck’s declarative build system. We have benefited greatly from Buck though we found that public documentation left much to be desired. Accordingly, we have shared our Buck setup in a <a href="https://github.com/airbnb/BuckSample">public GitHub repository</a>.</p><p>As part of this transition, we removed our manually managed Xcode projects in favor of declarative BUCK files, which live adjacent to each module’s code. BUCK files are defined using the <a href="https://github.com/bazelbuild/starlark">Starlark</a> language, which is interoperable with Bazel, another popular modern build system. Below is the structure of an existing Airbnb module.</p><pre>~/apps/ios/features/WifiSpeedFeature&gt; tree -L 1<br>.<br>├── BUCK<br>├── Sources<br>├── Tests<br>└── _infra</pre><pre>3 directories, 1 file</pre><p>Our infrastructure teams have taken the approach that we should meet engineers where they’re at while supercharging their development experience under the hood. Accordingly, iOS engineers continue to develop in an Xcode workspace that is generated from the build graph defined in Buck.</p><p>Initially, only our command line Buck builds of the Airbnb iOS application could benefit from the Buck HTTP cache. This alone was a great improvement since it enabled us to validate that an App Store build would succeed on every pull request without slowing down engineers. Local Xcode builds, however, could not pull artifacts from the cache as the generated Xcode workspace continued to use the standard Xcode build system.</p><p>We have continued to leverage the modern build system at the foundation of our application to tighten the iteration loop for engineers. We <a href="https://github.com/airbnb/BuckSample/pull/134">made it possible</a> to generate an Xcode workspace that internally builds the application using the Buck build system. All of the standard Xcode tools (breakpoints, console, errors) that iOS engineers use every day work as expected.</p><p>The Buck-based Xcode workspace improved local build speeds as it can participate in Buck’s cache. To improve the launch time of Xcode, we also made it possible to generate an Xcode workspace with only a subset of our entire codebase. The entire application continues to be built with Buck, and Xcode becomes interactive in a fraction of the time.</p><h4>Designing module types</h4><p>To address the lack of hierarchy in our code, and therefore lack of discoverability, we have designed an organizational structure for our first-party code. Modules are organized into semantically meaningful groups, called <em>module types</em>.</p><p>We have written precise documentation for our module types. Since the concept of module types is so fundamental to the way iOS developers work at Airbnb, this documentation is hosted on our internal developer portal and managed in source control. We summarize each module type in just a few paragraphs, explaining the purpose of the module type and the types of code it was designed to support.</p><p>We considered both application programming and build system best practices when designing this architecture. Each module type has a strict set of <a href="https://buck.build/concept/visibility.html">visibility rules</a>. These visibility rules define the allowed dependencies between modules of that type. An individual module may tighten its visibility, a technique used by some larger teams who enjoy the benefits of modularity and want to avoid unexpected inbound dependencies on their modules. An individual module cannot expand its visibility beyond the limits imposed by its module type.</p><blockquote><strong>Let’s look at an example…</strong></blockquote><blockquote>A <em>feature</em> is one of our core module types. At Airbnb, features are user-facing destinations. In our iOS code, a user-facing destination is a UIViewController that will be presented modally or installed into a UINavigationController. A feature module should be scoped to a single user-facing destination when possible, though it may contain multiple UIViewControllers that implement the destination.</blockquote><blockquote>Feature modules are not visible to other feature modules (i.e. a feature module cannot depend on a feature module); however, they share lightweight types via a sibling module type called a <em>feature interface</em>.</blockquote><blockquote>Each feature has a corresponding feature interface, which has broader visibility. A feature can depend on any number of feature interface modules and always depends on its own interface module. The interface functions similarly to how header files function in Clang programs.</blockquote><blockquote>The visibility rules of the feature module type ensure that all feature modules are independent of each other. The interface module type allows features to share simple types (protocols, enumerations, value types), enabling capabilities like strongly typed <a href="https://youtu.be/ray2vMjg2ug?t=661s">routing</a> between features.</blockquote><blockquote>In addition to the feature module type, the <em>service</em> module type is home to non-UI objects that are responsible for managing state that is shared between features. Any service module may optionally have an interface sibling module as well.</blockquote><blockquote>We have twelve iOS module types at Airbnb today.</blockquote><p>Our semantically meaningful module types act as a table of contents for our very large codebase. Engineers immediately have a reasonably accurate mental model for a module based on its type. 90% of our first-party code has been migrated from lib/ to module types. A great <a href="https://www.youtube.com/watch?v=KhZcSRXJHFs&amp;t=190s">talk</a> by my colleague Francisco describes in greater detail how our code organization strategy evolved from folders to module types and also sheds light on how we operationalized this large migration.</p><h4>Creating Dev Apps</h4><p>Our investments in build systems and iOS application architecture enabled a third innovation: Dev Apps. A <em>Dev App</em> is an on-demand, ephemeral Xcode workspace for a single module and its dependencies.</p><p>Dev Apps originated in the Airbnb Android ecosystem. The popularity and success of both Android and iOS Dev Apps derive from a simple axiom: minimizing your IDE scope to only the files that you are editing tightens the development loop. When there is less code in your Xcode workspace, Xcode can index and compile that code more quickly.</p><p>Adopting module types in our codebase broke costly dependencies between functional units. Now modules have minimal dependencies. For example, building any feature module and all of its dependencies is always much cheaper than building the entire Airbnb application. Since feature modules cannot depend on other feature modules, we have defined away the possibility of mega features that transitively build the entire application.</p><p>iOS engineers create Dev Apps using a robust and user-friendly command line interface. The command to generate a Dev App follows Unix best practices with a focus on being accessible to engineers who may not be comfortable in Terminal. Under the hood the tool uses Buck’s query interface to assemble the full list of source files.</p><p>The Dev App command line tool generates a container iOS application to host the feature and opens a generated Xcode workspace. Developers define variants of their feature in non-production code. These variants enable one-tap access to any possible UI state. The Dev App container application provides conveniences for common workflows, like attaching an OAuth token to HTTP requests.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Q_-p6QlfNnCINf_2PdBYOQ.png" /><figcaption><em>A Dev App for an existing Airbnb module. Developers can test all states of their feature by defining variants (left). Developer settings support live network requests (right).</em></figcaption></figure><p>A Dev App allows a product developer to iterate on their feature’s UI and much of its business logic while building a fraction of the overall Airbnb application. Although Dev Apps were designed for feature and UI modules, we now support creating a Dev App for any module type. We have found that many iOS developers also prefer to work on non-UI modules in this minimal Xcode environment.</p><p>It remains critical to build and run the entire Airbnb application in many situations, especially for testing how features interact with each other. However, when you are working on code that is sufficiently isolated and well-tested, it can be possible to ship that change confidently with a Dev App alone.</p><h3>Breaking through to the other side</h3><p>Our efforts have created an ecosystem where teams can operate independently on their surface areas. Coding in a Dev App recalls the joy of coding in a simpler, smaller project with all of the benefits of being supported by mature first-party tools and frameworks.</p><p>Dev Apps now drive over 50% of local builds. The 75th percentile of Dev App build times is under two minutes, with the 50th percentile well under one minute. We encourage Dev Apps to use mocked dependencies as much as possible to reduce the code required to be built.</p><p>Our application architecture has ushered in an era of 100% code ownership. We have maintained 100% ownership through team reorganizations due in part to our highly modularized codebase, which allows the ownership to be transferred and repartitioned with minimal refactoring. Today, our first-party code is divided into nearly 1,500 modules.</p><p>We see over 50% test coverage for code in our modern module structure while code that remains in the legacy module structure has 23% coverage. This is in part due to interface modules, which strongly push developers to write services using the protocol-oriented programming technique. When code interacts primarily with protocols, it is easy to create test doubles for dependencies.</p><p>By retaining strict visibility rules between module types, we have achieved a highly parallelized build graph. When examining the trace for a build of the full Airbnb app on a 8-core, 16-inch 2019 MacBook Pro we see full utilization of the available CPU resources for nearly 80% of the compilation phase.</p><p>Our modern build system has enabled a healthy ecosystem of command line tools which greatly simplify common tasks. Creating a module previously involved following a long checklist of error-prone steps. Now engineers create modules with an interactive Rake command. We even leveraged our Buck query interface to build a command line tool which guides engineers through the steps necessary to migrate their lib/ modules to the new module structure.</p><p>And last, but certainly not least, by no longer managing Xcode projects in source control, it is now trivial to add and remove module dependencies with an easy path to resolving any merge conflicts.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dsA7iuafUp4j47gd6IdELA.png" /><figcaption><em>A portion of a recent PR that adds a dependency to a feature module. Starlark build files make the code easy to write and review.</em></figcaption></figure><h3>Pushing the edge of the envelope together</h3><p>At Airbnb, we are passionate about advancing the state of iOS development in the industry at large. We believe in the power of native applications running on mobile devices and want other companies to leverage the work that we’ve done so that they can focus more energy on building experiences that users love.</p><p>We know that we are not the only company whose application has grown organically from a small project to something larger than what the original author may have conceived to be possible. We know that many of our peer companies have undergone similar transformations to our own. Each of our aforementioned solutions are specific to our circumstances and culture, though we have seen the themes to be evergreen. We are excited to continue this work in the public domain with our peer companies as part of the <a href="https://mobilenativefoundation.org/">Mobile Native Foundation</a>.</p><p>Our journey of improving productivity is not done yet. As we look to the future we see a massive opportunity to use <a href="https://github.com/apple/swift-syntax">Swift static analysis</a> to generate boilerplate code and increase code portability of features and services. We will continue to tighten the build/test/run iteration loop of iOS product development so that the weight of a mega iOS application does not stifle the joy of indie iOS development. And we yearn for a modern build system blessed by Apple.</p><p>We believe that we’ve only scratched the surface of mobile computing. We will continue to improve upon the tools, technologies, and people processes necessary to innovate at scale.</p><p>Many thanks to <a href="https://www.linkedin.com/in/fdiazmeza">Francisco Díaz</a> for advising on content and voice through multiple revisions of this article. The work described in this article is the product of many talented Airbnb iOS engineers.</p><p>If you are interested in joining us on our continuing quest to make the best iOS apps in the App Store, please see our <a href="https://careers.airbnb.com/">careers</a> page for open iOS roles.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9376a430a0bf" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/designing-for-productivity-in-a-large-scale-ios-application-9376a430a0bf">Designing for Productivity in a Large-Scale iOS Application</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Tech Fosters a Culture of Learning]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-tech-fosters-a-culture-of-learning-854be0f9fe9d?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/854be0f9fe9d</guid>
            <category><![CDATA[learning]]></category>
            <category><![CDATA[tech-education]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[education]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Tamera Scholz]]></dc:creator>
            <pubDate>Thu, 30 Sep 2021 20:11:24 GMT</pubDate>
            <atom:updated>2021-09-30T20:24:23.633Z</atom:updated>
            <content:encoded><![CDATA[<p>Leveraging technical learning and development to enable engineers to do their best work.</p><p>Authors: Hanna Dooley, Jennifer Rice, Tamera Scholz</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2MJ-_TsCSo4MvyD_" /></figure><h3>Introduction</h3><p>The Airbnb TechED team believes each individual’s success is critical to the health of our technical teams. This fundamental belief in the power of human potential drives us to bring high quality, relevant educational content to our technical teams to meet both their needs and the needs of Airbnb. We work with our technical leaders and subject matter experts (SMEs) to build and deliver unique, interactive, multimodal learning experiences at scale.</p><p>We have three principles for approaching technical learning:</p><ul><li>Embrace the adventure</li><li>Share what you know</li><li>Enable the success of technical priorities</li></ul><p>At Airbnb, one of our <a href="https://careers.airbnb.com/">core values</a> is <em>Embrace the Adventure</em> which from a behavioral perspective, means being curious, asking for help, and demonstrating an ability to grow. Another core value is to <em>Be a Host</em>, which in the context of enabling our technical team’s best work, means not only sharing knowledge, but doing so sustainably. Relying on institutional knowledge of more tenured team members is not inclusive, accessible, or sustainable; whereas systematically encapsulating and disseminating the right knowledge can enable more positive outcomes. On TechED, we build educational programs that provide opportunities for our technical teams to embody these values while also reinforcing the direction set by our Airbnb Tech Priorities.</p><h3>How We Started</h3><p>Prior to 2017, technical education was driven by a few invested individuals working outside of their core responsibilities to ensure new hires were onboarded effectively. As an early start-up this approach worked, but it was not sustainable as Airbnb grew. Thus, the TechED team was formed. Much of the initial effort at the time focused on building onboarding programs. More recently, TechED has expanded its focus to develop a shared understanding of technical quality and best practices across our community and deliver them through a variety of programs which reinforce <a href="https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a">technical mastery and quality</a> throughout individual’s careers, regardless of tenure or role.</p><h3>Importance of Organizational Support</h3><p>The TechED team’s <a href="https://careers.airbnb.com/">core values</a> are closely tied to the mission of our company. We rely primarily on employees to <em>Be a Host</em> and share their technical knowledge through varying levels of commitment. We have team members contribute by developing technical curriculum — anything from codelabs to lectures. In cases where (<em>pre-covid)</em> an in-person lecture is preferred, we rely on SMEs to present the material. We also offer more informal means of sharing knowledge through serving as a “new hire buddy”, hosting a summer intern, or being the onboarding host for an entire cohort of new hires. This volunteer based strategy is only successful if people are motivated to participate. To incentivize participation, knowledge sharing is built into the tech career ladder competencies. Both ICs and managers are expected to share what they know, and help when they can through the opportunities offered. Participants are recognized for these efforts during the performance review cycle.</p><h3>Our Programs</h3><p>Knowledge must be shared in systematic and clearly defined ways. We do this though the following TechED programs:</p><ul><li><strong>Tech Bootcamp:</strong> an onboarding program ensuring all new hires successfully integrate into our global tech community and are prepared to approach their first code contributions</li><li><strong>Tech Manager Bootcamp:</strong> an onboarding program for managers designed to enable new tech managers to build an effective and healthy team</li><li><a href="https://medium.com/airbnb-engineering/how-airbnb-is-boosting-data-literacy-with-data-u-intensive-training-a6399dd741a2#:~:text=Data%20University%20is%20Airbnb%E2%80%99s%20dynamic%20data%20education%20program%2C,teach%20over%2020%20unique%20curriculums%20globally%20each%20year."><strong>Data University (DataU)</strong></a><strong>:</strong> a program to empower every Airbnb employee to make data-informed decisions</li><li><strong>Eng University (EngU):</strong> a continuing education program for engineers on areas of our tech priorities</li><li><strong>Food on Nerds:</strong> our internal tech talk series</li><li><a href="https://medium.com/airbnb-engineering/how-we-enable-airbnb-team-members-to-code-like-a-mobile-engineer-d7181a20399f"><strong>Code Like a Mobile Engineer</strong></a><strong>:</strong> a certification program designed to grow native engineering knowledge</li><li><a href="https://medium.com/airbnb-engineering/inside-connect-airbnbs-engineering-apprenticeship-program-c26d6eb2768c#:~:text=Inside%20Connect%3A%20Airbnb%E2%80%99s%20Engineering%20Apprenticeship%20Program%201%20About,The%20Program%20Structure.%20...%203%20Moving%20Forward.%20"><strong>Connect Program</strong></a><strong>:</strong> a six-month apprenticeship program for individuals from non-traditional backgrounds. If you’re curious, you can read about this program from both a <a href="https://medium.com/airbnb-engineering/inside-connect-supporting-apprentices-as-an-engineering-leader-the-third-of-a-three-part-blog-ef2e631b4899">manager perspective</a> and an <a href="https://medium.com/airbnb-engineering/inside-connect-an-apprentice-perspective-c9f299e11e51">apprentice perspective</a>.</li></ul><h3>Content Creation and Governance</h3><p>The TechED team, in partnership with SMEs, leverages the <a href="https://www.instructionaldesign.org/models/addie/">Addie Model</a>. This instructional systems design framework illustrates the flow through which all new training materials are built: analysis &gt; design &gt; development &gt; implementation &gt; evaluation. Our TechED Program Managers work closely with SMEs to embed sound learning methodology into each course during the early stages. This includes clearly stating and reinforcing learning objectives, promoting participant engagement with the material, and enforcing best practices for delivering content. Throughout design and development, TechED relies on curriculum advisory groups for each program to consult and course correct. Comprised of senior ICs and/or managers, these groups are responsible for sharing insights that influence our program roadmaps and amplifying opportunities for involvement across Airbnb’s technical teams.</p><p>While TechED programs benefit from the support of the organization and its incentive structure, we are accountable for supporting our organization in areas of need. One program that demonstrates how TechED responds to urgent organizational needs is the <a href="https://medium.com/airbnb-engineering/how-we-enable-airbnb-team-members-to-code-like-a-mobile-engineer-d7181a20399f"><strong>Code Like a Mobile Engineer</strong></a> program. In 2019, a substantial number of daily active hosts were using their mobile devices to visit Airbnb. As this continued to increase, so did the demand for new features. With only around 100 mobile engineers across our product teams, there was more work than we had engineers. To accelerate mobile development, we designed and launched Code Like a Mobile Engineer. This full-time, 12-day program utilizes our model of combining lectures, codelabs, buddy support, and a capstone project to quickly train mobile engineers. Since this program’s inception, we have seen 360+ merged PRs to our mobile codebase from 30 participants and have had one engineer move into a full-time native engineering role.</p><h3>Building Flexible and High Impact Onboarding Programs</h3><p>The COVID-19 pandemic created enormous challenges and opportunities for all learning organizations. For the TechED team, one of the more urgent shifts was pivoting our in-person, 2.5-week <strong>Tech Bootcamp</strong> onboarding program to be consumed virtually. While meeting the virtual needs of our new hires, we also took the opportunity to incorporate prior program feedback implementing a hybrid of both synchronous learning (lectures) and asynchronous learning (past recordings, codelabs, etc.).</p><p>In early 2021, TechED expanded the program from two general tracks (Engineering and Data Science) to five function-specific tracks (Backend, Frontend, Native, Data Engineering, and Data Science). With the help of SMEs from these critical functions, TechED developed flows for each track that equip new hires with highly relevant training. Through the ongoing retrospectives, we found that new hires appreciated the balance and autonomy that comes with consuming both live lectures and recordings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/969/0*PId0F1paics4Yxt9" /></figure><p>The TechEd team also supports onboarding people new to a management role with our Tech Manager Bootcamp. In partnership with our HR and Central Learning teams we offer a holistic training journey over the course of a tech manager’s tenure. The <strong>Tech Manager Bootcamp</strong> program is offered quarterly, and the goal is to onboard new tech managers to the fundamentals of effective team and people development at Airbnb. It is offered alongside complementary programs developed by our cross-functional partners. The success of these complementary programs is gauged through shared objectives and key results.</p><h3>Continuing Education</h3><p>The TechED team strives to enable Engineers and Data Scientists to continuously grow in their careers, regardless of tenure or role. To this effect, the TechED team has several offerings.</p><p>For frequent, informational knowledge exchanges, we host a one-hour, bi-weekly webinar called <strong>Food on Nerds (FON)</strong>. Started in 2015 as a grassroots lunch-hour tech talk with pizza and snacks, Food on Nerds is now the primary way for technical teams to broadly share their work with their peers. FON offers all engineers and data scientists the opportunity to amplify their work within our internal tech community, build their personal brand at Airbnb, and, in some cases, test drive their presentation for an external audience. Talks are recorded and available at any time through our internal AirTV channel.</p><p>These Food on Nerds tech talks fuel our ongoing <strong>EngU</strong> program. <strong>EngU</strong>, our more formal continuing education program for engineers, is a catalog of courses available to engineers to further their technical growth. This is one of the main levers through which TechED is able to disseminate critical training on tech priorities. These sessions are hosted live on a regular basis, with recordings available to employees on demand.</p><p>Lastly, unlike TechED’s other programs, which are built specifically for a technical audience, <a href="https://medium.com/airbnb-engineering/how-airbnb-democratizes-data-science-with-data-university-3eccc71e073a"><strong>DataU</strong></a> is an educational program available to all Airbnb employees. We have intro level courses designed to empower every employee to make data-informed decisions. The advanced courses are tailored to more experienced users in modeling and experimentation. The continued learning doesn’t stop there. We also offer access to <strong>O’Reilly Safari Online</strong> and <strong>LinkedIn Learning</strong> to augment any additional topics of interest for our tech community.</p><h3>Measuring Our Impact</h3><p>Our goal is to ensure essential information moves quickly and efficiently into the heads of those who need it. We maintain a dashboard to measure both engagement with our programs from participants as well as contributions from SMEs. These reports are used by both managers and ICs to inform performance reviews. We share monthly, team-wide impact reports with tech leaders, as well as more granular impact reports with stakeholders for each individual course. Agility is more important than polished presentations, however. It’s most important to ensure our tech community has the information and knowledge needed to do their job.</p><h3>Conclusion</h3><p>In May of 2020, Airbnb felt the turbulence of the pandemic first hand. As a response to this, we <a href="https://www.forbes.com/sites/deniselyohn/2020/11/10/how-airbnb-survived-the-pandemic--and-how-you-can-too/?sh=2e7614b49384">refocused our roadmap on the core business of home rentals</a>. While we have been pleasantly surprised by the rebound of our business in the later months of the pandemic, we remember the challenges that came with losing a significant portion of our workforce. People are our biggest asset at Airbnb, and on TechED, we strive to keep people growing and learning. Since the onset of the pandemic, over 50% of our tech community has engaged with one or more TechED offerings, and with strong organizational support and core values that prioritize community participation, we remain excited to continue bringing unique and engaging learning experiences to the Airbnb tech community.</p><h3>Acknowledgements</h3><p>The Airbnb TechED team depends on the knowledge and expertise of our technical community. We would like to thank all who have contributed to our programs, dedicated time to being a buddy, developed new courses, and delivered meaningful content. We would like to express our gratitude for those past employees who worked with Airbnb TechED and paved the way for our success.</p><p><em>— —</em></p><p><em>This work, and many other exciting initiatives, are always happening at Airbnb. If you want to join us, check out our </em><a href="https://careers.airbnb.com/"><em>career page</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=854be0f9fe9d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-tech-fosters-a-culture-of-learning-854be0f9fe9d">How Airbnb Tech Fosters a Culture of Learning</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Airflow Smart Sensor Service]]></title>
            <link>https://medium.com/airbnb-engineering/the-airflow-smart-sensor-service-221f96227bcb?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/221f96227bcb</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[data-quality]]></category>
            <category><![CDATA[engineering]]></category>
            <dc:creator><![CDATA[Yingbo Wang]]></dc:creator>
            <pubDate>Tue, 28 Sep 2021 17:24:05 GMT</pubDate>
            <atom:updated>2021-09-28T18:29:09.737Z</atom:updated>
            <content:encoded><![CDATA[<p>Consolidating long-running, lightweight tasks for improved resource utilization</p><p><strong>By:</strong> <a href="https://www.linkedin.com/in/yingbo-wang-86aa3027/">Yingbo Wang</a>, <a href="https://www.linkedin.com/in/ruiqinyang/">Kevin Yang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3r30u7rnBhR7BJSc" /></figure><h3>Introduction</h3><p>Airflow is a platform to programmatically author, schedule, and monitor data pipelines. A typical Airflow cluster supports thousands of workflows, called DAGs (directed acyclic graphs), and there could be tens of thousands of concurrently running tasks at peak hours. Back in 2018, Airbnb’s Airflow cluster had several thousand DAGs and more than 30 thousand tasks running at the same time. This amount of workload would often result in Airflow’s database being overloaded. It also made the cluster quite expensive since it required a lot of resources to support those concurrent tasks.</p><p>In order to make the system more stable, and to reduce the cost of the cluster, we looked to optimize the Airflow system. We soon found that the long-running lightweight (LRLW) tasks waste a lot of resources, so we proposed a Smart Sensor to consolidate them and address the waste.</p><h3>Long-Running Lightweight Tasks</h3><p>When we investigated the Airflow performance issues, we found that a few kinds of tasks shared the same LRLW patterns. They are the sensor tasks, the subDAGs, and the SparkSubmitOperator.</p><p><strong>Sensors</strong>, or sensor tasks, are a special kind of operator that will keep running until a certain criterion is met. The criterion can be a file landing in HDFS or S3, a partition appearing in Hive, whether some other external task succeeded, or even if it is a specific time of the day.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HPMY9cRlDg7_Y7zj" /><figcaption><strong>Figure 1. The lifespan of a sensor task</strong></figcaption></figure><p>When a sensor task is running, it calls its “poke” function to check the criterion periodically, usually every 3 minutes, and marks the sensor tasks with ‘success’ if their “poke” functions return true or ‘fail’ if sensor timeout. The execution of a “poke” is very fast, mostly less than 100ms, so most of the time sensors are idle, waiting for the next “poke” time to come. The lifespan of a sensor task is from the checking time to the time when the condition is met, which can range from a few minutes to several days.</p><p><strong>SubDAGs</strong> are another example of long-running lightweight tasks. They are used to encapsulate a set of tasks in a DAG and make a complicated DAG’s structure cleaner and more readable. The DAG run is created for a subDAG in the pre_execute function and then subDAG task “poke” the DAG run status in the execute function.</p><p>The <strong>SparkSubmitOperator</strong> is also an example of a long-running lightweight task. The Spark client in Airflow submits the job and polls until completion. All these tasks, after some initialization work, fall into a lightweight and, at times, a long-running status.</p><p>From the previous examples, we can see that these tasks fall into the same “long-running, lightweight” pattern, characterized by the following:</p><ul><li><strong>The resource utilization is very low.</strong> Worker processes for these tasks remain idle 99% of the time.</li><li><strong>These tasks often account for a very large portion of the concurrent running tasks in a large scale cluster.</strong> At Airbnb, more than 70% of running tasks are sensors. At peak hour, they take more than 20Kworker slots.</li><li><strong>There are a lot of duplicate sensor tasks.</strong> More than 40% of sensor jobs are duplicates because many downstream DAGs usually wait for the same partitions from just a few important upstream DAGs.</li></ul><h3>Smart Sensor</h3><p>We proposed the Smart Sensor to consolidate these LRLW tasks. Though originally created to consolidate long-running sensor tasks, it was later expanded to consolidate all LRLW tasks. We kept the name Smart Sensor for this service.</p><h3>How It Works</h3><p>The main idea of the Smart Sensor service is to use centralized processes to execute long-running tasks in batches, instead of using one process for each task.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*H6QTUtYBpgbn2ijm" /><figcaption><strong>Figure 2. Sensors before and after enabling smart sensor</strong></figcaption></figure><p>With the Smart Sensor service, a sensor task is executed in two steps:</p><ol><li>First, each task parses the DAG, gets the task object, runs the pre_execute function, and then registers itself to the Smart Sensor service. In the registration, it persists information required to poll external resources to the Airflow metaDB. After registration succeeds, the task exits and frees up the worker slots.</li><li>Then, a few centralized processes (the Smart Sensor tasks from a built-in DAG) keep checking the database for the latest records of all registered tasks and execute the “poke” function for these tasks in batches. Normally, one Smart Sensor task is able to handle several hundred sensor tasks easily. The Smart Sensor can also combine duplicate sensor tasks into a single instance to save even more resources.</li></ol><p><strong>The Smart Sensor deduplicates tasks and balances workloads by defining the sensor task shards.</strong> The number of concurrently running sensors could be large and there will be multiple Smart Sensor tasks to execute all these jobs in a short period. How to assign sensor tasks to Smart Sensors was one of our key challenges when designing this system. We sought to balance the workload of all Smart Sensor tasks. At the same time, the `duplicated` sensor tasks have to be assigned to the same Smart Sensor so that we can avoid multiple pokes for the same target.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fM_bvm_cykKVz7qd" /><figcaption><strong>Figure 3. Deduplicating tasks by shardcode</strong></figcaption></figure><p>In the Smart Sensor service, the `poke_context` is the signature of a sensor job. It is a dictionary of arguments needed to execute the sensor’s poke function. Two sensors with the same operator class and same `poke_context` are running the same `poke` function and are considered duplicated tasks. By using the hashcode of `poke_context` to do the sharding and make each Smart Sensor task take care of tasks whose hashcode is in a specific range, it should be able to assign `duplicated` sensors to the same smart sensor. Since hashcodes are long, we optimized by using the mod of the hashcode, which can be indexed in the database. We refer to this key as the `shardcode`.</p><p>Figure 3 shows how the sharding works in the Smart Sensor service. Sensor1 and sensor2 have the same `poke_context` and so they have the same `hashcode` and `shardcode`. At runtime, they will be picked up by the same Smart Sensor — e.g., `SmartSensor1`. All duplicated sensors will be poked only once in one poking loop.</p><p><strong>Smart Sensor is a general service for all sensor classes.</strong> The centralized Smart Sensor task is a general framework. It is designed to support various classes. As long as the class has a poke function and the argument for this poke function can be serialized, the Smart Sensor tasks can support them.</p><p><strong>Logs are handled similarly to unconsolidated processes.</strong> Although task execution is consolidated into fewer processes, the Smart Sensor service supports the same ability to read or download logs from the Airflow UI. Users can read logs from the original sensor task’s URL.</p><p><strong>Smart Sensor can be easily applied to an Airflow cluster.</strong> Enabling and disabling the Smart Sensor service is simple, we only need to do a system level configuration change on the `smart_sensor` session in airflow.cfg. The change is transparent to the individual users and there is no need to change existing DAGs. Also, rotating centralized smart sensor tasks will not cause any user’s sensor task to fail.</p><h3>The Efficiency Improvement</h3><p>Upon deploying the first version of Smart Sensor, Airbnb was able to reduce the number of peak-hour, concurrently running tasks by more than 60%. We also reduced the running sensor tasks by 80%. The process slots needed for sensors were reduced from 20,000 to 80. The database load is also greatly reduced due to much fewer running tasks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lkiBWjq8_ezvYC-e" /><figcaption><strong>Figure 4. Number of running tasks after Smart Sensor deployed</strong></figcaption></figure><p>In Smart Sensor, the deduplicate mechanism reduced about 40% of requests to the Hive metastore and hence reduced both the absolute sensor traffic and the load on the underlying data warehouse.</p><h3>Conclusion</h3><p>Smart Sensor is a service which consolidates small, lightweight task traffic into bigger centralized tasks. It can reduce Airflow’s infrastructure cost and improve cluster stability. This is especially true for large clusters with a considerable amount of sensor tasks. For Airbnb’s gigantic Airflow clusters, Smart Sensor reduced a significant amount of cost and greatly improved the overall cluster stability.</p><p>The smart sensor service was released as one of the majority new features in <a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/smart-sensors.html">Apache Airflow 2.0</a>, since which it has been used to improve the resource utilization for more airflow users. Because the smart sensor service introduced the idea of splitting task lifespan into multiple processes and unlocked the `async` mode for task execution, the open source community has started to invest in more generic use cases for `async` solutions, among which the <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177050929">deferrable (“Async”) operator</a> is an operator aiming to extend the async mode to more tasks.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=221f96227bcb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/the-airflow-smart-sensor-service-221f96227bcb">The Airflow Smart Sensor Service</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Journey to Airbnb — Rachel Zhao]]></title>
            <link>https://medium.com/airbnb-engineering/my-journey-to-airbnb-rachel-zhao-3302e70c5a54?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/3302e70c5a54</guid>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[search]]></category>
            <category><![CDATA[hiring]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Mon, 27 Sep 2021 22:49:28 GMT</pubDate>
            <atom:updated>2021-09-23T20:45:51.761Z</atom:updated>
            <content:encoded><![CDATA[<h3>My Journey to Airbnb — Rachel Zhao</h3><p>From an uncertain software engineering student to Head of Search Engineering.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D5ycTIwTPF9Oo6Onbgrc9w.jpeg" /></figure><p><em>If there’s one thing travel teaches us, it’s that the journey is just as important as the destination. With this in mind, we’re launching a new series of blog posts to bring you the personal stories of our amazing Airfam! How did they initially connect with their passion, what brought them to Airbnb, and what’s fueling them every day?</em></p><p><em>We could think of no one better to kick off this series than </em><a href="https://www.linkedin.com/in/rachelzhao/"><em>Rachel Zhao,</em></a><em> the head of engineering for our Search product group, which contributed to the set of </em><a href="https://news.airbnb.com/2021-release/"><em>incredible features</em></a><em> this year to respond to the changing world of travel. In addition to major initiatives in search and mobile, Rachel’s team is expanding to our new Atlanta hub, which goes hand-in-hand with her personal passion to improve representation in tech. As an immigrant and a woman, Rachel knows the importance of community building and how hard and intimidating it can be to find your place in an engineering career path. Read on for Rachel’s own words on seeing code as a way to communicate, working at the crossroads of UX and data, the tech talent in Atlanta, and more.</em></p><h3>From Beijing to Waterloo</h3><p>I grew up in Beijing and then went to Canada to study engineering at the University of Waterloo. It felt like my classmates all knew that they wanted to do computer science from a very early age. On the other hand, I was a complete newbie. Growing up, I’d been more interested in the arts and communication media professions. But my family decided to immigrate to Canada, and as I knew very little English, I needed to find a major that was more practical for me. It was very random — I just Googled for the top schools and programs in Canada. And that’s how I got into software engineering.</p><p>It was really stressful because it seemed like everyone knew how to program already. I struggled a lot in the first years and felt so behind compared to everyone in the class (learning English and Java at the same time was rough!). But I quickly realized that everyone has strengths and weaknesses. While I was weaker in computer science, I was stronger in other subjects. I could offer help on calculus and physics thanks to my advanced high school curriculum, and in return they would share their programming experience. I was lucky that I had classmates who supported each other, and that got me through the years of uncertainty.</p><h3>Finding my niche in software engineering</h3><p>As a visual person, the typical perception of programming (dark screen with green text, “Matrix”-style) was daunting to me. However, things changed when I took an introduction to user interface class. It made me realize that a big part of software engineering is to create a way to communicate effectively. I could bring my interest in art and media into my career path as an engineer by working on user experience.</p><p>Through many internships during the undergrad years, I also learned that when writing code, making it functional is not the only goal — we’re writing for other developers, current and future, who share the same codebase. They need to understand what you’re trying to achieve and your code needs to be maintainable. It’s not just about the algorithms, but also about organization and communication. And those skills you have as a person also apply to engineering.</p><h3>Discovering my place and my people at Airbnb</h3><p>Throughout my career, I’ve found that I’m motivated to solve user problems: working with research to understand the needs, and prioritizing work that brings value to customers. Therefore the opportunity to come to Airbnb and lead the search group was very exciting to me. Teams in this org are responsible for a crucial path of a guest’s journey: helping people discover and onboard to Airbnb (SEO), showcasing what Airbnb has to offer (Storefronts), capturing what guests are looking for (Search Input) and finding the best matches for them (Search Feed).</p><p>The space we work on is both a product surface and a platform. It’s also the crossroad of user experience and data. As a result, we get to work on many different types of projects, from new user facing features, systems that power product pages, platforms that enhance developer productivity, to work that enhances product experience such as performance improvements… The possibilities are endless.</p><p>Not only is this area a great fit for my passions, but Airbnb is a product that I’ve admired for a long time (ever since discovering the platform as a host back in 2012, when I literally had guests staying on an air mattress in my flat in San Francisco!). One thing I love about Airbnb as a product is that it’s really good at storytelling. You land on the homepage and you see there’s a narrative here, rather than a lot of components.</p><p><strong>“I think that how you deal with a crisis — how you deal with the lows of a company — shows more character of the company than the highs, when everything is going smoothly.”</strong></p><p>What really confirmed my desire to join was seeing how Airbnb handled the challenges of 2020. While seeing layoffs happening all over the tech industry during the pandemic, I was impressed by Airbnb’s response. The communication was very clear from leadership, and the company was generous and considerate in helping people find and land their next job and get through this period financially. There was also <a href="https://news.airbnb.com/a-message-from-co-founder-and-ceo-brian-chesky/">a widely-shared blog post by Brian</a> which I felt set the bar for how to communicate with empathy and compassion while making a difficult decision.</p><p>I think that how you deal with a crisis — how you deal with the lows of a company — shows more character of the company than the highs, when everything is going smoothly. I felt like how Airbnb handled things was a really good sign of the company’s culture and the leadership. And that’s what ultimately inspired me to join the team.</p><h3>Expanding our team in Atlanta</h3><p>I put a lot of value on community building, inclusive communication, and representation. When there’s a lack of diversity at the table, we don’t get to hear different perspectives, and those perspectives are not considered in decision making. That could lead to biased technical decisions, or a product direction with many blindspots.</p><p>That’s why I’m so excited about the team we’re starting in Atlanta. It’s a very important tech hub. Atlanta has great schools and great talent. And the office there will help us operate in a way that’s less Silicon Valley-centric. I think it’s very important to bring different ways of thinking into the company and strengthen the culture instead of simply fitting in the culture.</p><p>I’m also feeling confident that we’ve built up the “muscle” of working remotely and learned how to make everyone feel supported over that past year and half. We’re making sure to integrate new engineers in Atlanta with our existing teams first, so they can learn how Airbnb works, what our tech stack is like, and so on — we’re being very careful about how we equip everyone with that domain knowledge so they’re set up for success.</p><p>At Airbnb, we have a healthy engineering culture of collaboration and knowledge sharing. People are willing to help each other out. We’re here to build products with a mission that everyone can belong anywhere, and there are so many ways to contribute to a team where everyone is sharing their strengths.</p><p>Our team is dedicated to perfecting our new features that help travelers to embrace more flexibility in <a href="https://techcrunch.com/2021/05/24/airbnb-doubles-down-on-flexible-search-improves-the-host-flow-in-preparation-for-summer-2021/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAACTqKJxxNCEClv9r2IB9uYeSHOeH_XMduLIman-CPW8aQtcgupj8ioXo2ZeXXaCEjW2uzTG-JF2fPuRuyLWz5IDZZd4QpXpS-DLrlMyDgtCc2H6mm7raLLBhTRI3Mpo3Dj8MfPSGqeETUld2Cc9JIKIl6I8PF8yGenT4ocEWbhaB">date</a> and <a href="https://news.airbnb.com/unique-stays-hosts-earn-more-than-300-million-since-start-of-pandemic/">destination</a>, as well as improving core functionalities all over the onboarding and search flow. On the platform side, we continue to invest in mobile, to scale and evolve our tech stack and set a new standard for app development. We’re hiring in Atlanta, the Bay Area, and a number of other locations and we’d love to hear from you!</p><p>Check out these related roles:</p><ul><li><a href="https://careers.airbnb.com/positions/2607507/">Senior iOS Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/2756046/">Senior Android Software Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/3178314/">Senior Software Engineer, Guest Experience</a></li><li><a href="https://careers.airbnb.com/positions/3206883/">Staff Fullstack Engineer, Guest Experience</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3302e70c5a54" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/my-journey-to-airbnb-rachel-zhao-3302e70c5a54">My Journey to Airbnb — Rachel Zhao</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Airbnb Enables Consistent Data Consumption at Scale]]></title>
            <link>https://medium.com/airbnb-engineering/how-airbnb-enables-consistent-data-consumption-at-scale-1c0b6a8b9206?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/1c0b6a8b9206</guid>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[metrics]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Shao Xie]]></dc:creator>
            <pubDate>Tue, 21 Sep 2021 17:00:25 GMT</pubDate>
            <atom:updated>2021-09-21T17:00:25.435Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part-III: Building a coherent consumption experience</h4><p><strong>By: </strong><a href="https://www.linkedin.com/in/apahwa/">Amit Pahwa</a>, <a href="https://www.linkedin.com/in/cristianrfr/">Cristian Figueroa</a>, <a href="https://www.linkedin.com/in/donghan-zhang-670990135/">Donghan Zhang</a>, <a href="https://www.linkedin.com/in/haimgrosman/">Haim Grosman</a>, <a href="https://www.linkedin.com/in/john-bodley-a13327133/">John Bodley</a>, <a href="https://www.linkedin.com/in/jonathan-parks-15617820/">Jonathan Parks</a>, <a href="https://www.linkedin.com/in/jialingliu1020/">Jenny Liu</a>, <a href="https://www.linkedin.com/in/krishna-bhupatiraju-1ba1a524/">Krishna Bhupatiraju</a>, <a href="https://www.linkedin.com/in/shengnan-zhu-89403124/">Maggie Zhu</a>, <a href="https://www.linkedin.com/in/michaelcl/">Mike Lin</a>, <a href="https://www.linkedin.com/in/philip-weiss-391021b1/">Philip Weiss</a>, <a href="https://www.linkedin.com/in/robert-ih-chang/">Robert Chang</a>, <a href="https://www.linkedin.com/in/shao-xie-0b84b64/">Shao Xie</a>, <a href="https://www.linkedin.com/in/sylviatomiyama/">Sylvia Tomiyama</a>, <a href="https://www.linkedin.com/in/toby-mao/">Toby Mao</a>, <a href="https://www.linkedin.com/in/xiaohui-sun-24bb3017/">Xiaohui Sun</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4n3siF32QWxo31_6okaE9g.jpeg" /></figure><h3>Introduction</h3><p>In the <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">first post</a> of this series, we highlighted the role Minerva plays in transforming how Analytics works at Airbnb. In the <a href="https://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486">second post</a>, we dove into Minerva’s core compute infrastructure and explained how we enforce data consistency across datasets and teams. In this third and final post, we will focus our story on how Minerva drastically simplifies and improves the data consumption experience for our users. Specifically, we will showcase how a unified metric layer, which we call the Minerva API, helps us build versatile data consumption experiences tailored to users with a wide range of backgrounds and varying levels of data expertise.</p><h3>A Metric-Centric Approach</h3><p>When data consumers use data to frame a business question, they typically think in terms of metrics and dimensions. For example, a business leader may wonder what percentage of bookings (a metric) is made up of long-term stays (a dimension). To answer this question, she needs to find the right set of tables from which to query (where), apply the necessary joins or filters (how), and then finally aggregate the events (how) to arrive at an answer that is, hopefully, correct.</p><p>While many traditional BI tools attempt to abstract this work on behalf of their users, most of their data-serving logic still relies heavily on the users to figure out the “where” and the “how”. At Airbnb, we aspired to build a better user experience — one in which users simply ask for metrics and dimension cuts, and receive the answers without having to worry about the “where” or the “how”. This vision, what we call a “metric-centric approach”, turned out to be a difficult engineering challenge.</p><h4>Challenge One: The “Where”</h4><p>In most traditional data warehouses, data is organized in tables. This means that to answer an inquiry, a BI tool needs to associate the metrics and dimensions in question to the physical tables that contain the relevant answers. However, for a given metric and dimension combination, there might be many datasets from which to source the answers. These tables often have varying degrees of data quality and correctness guarantees, so picking the right tables to serve the data is nontrivial.</p><h4>Challenge Two: The “How”</h4><p>Moving beyond the “where”, the data-serving logic responsible for the “how” also has many nuances. To start, there are different metric types: <em>simple metrics</em> are composed of single materialized events (e.g., bookings); <em>filtered metrics</em> are composed of simple metrics filtered on a dimension value (e.g., bookings in China); and <em>derived metrics</em> are composed of one or more non-derived metrics (e.g. search-to-book rate). Furthermore, while many metrics are additive (e.g., bookings), many other metrics are not: count distincts, percentiles, and point-in-time snapshots cannot simply be calculated by summing individual events. Consistently calculating these various metric types correctly, across all scenarios, is a big challenge.</p><h4>Challenge Three: Integration With Downstream Applications</h4><p>Finally, to make data-informed decisions, data must be used in a wide variety of contexts, applications, and tools. The more prevalent and important the metric is, the more likely it is to be used in a wide variety of settings. For example, gross booking value (GBV), nights booked, and revenue are among the most frequently used metrics at Airbnb. They are used to track business performance, calculate guardrail metrics for randomized controlled experiments, and leveraged as features for machine learning models. Serving these metrics in different use cases, while providing contextual information for users to use them the right way is yet another core challenge for us.</p><h4>Our Solution</h4><p>We have addressed these challenges by building the Minerva API, a metric-serving layer that acts as an interface between upstream data models and downstream applications. With Minerva API, any downstream application is able to serve data consistently and correctly without knowing where the data is stored or how metrics should be computed. In essence, the Minerva API serves as the “how” by connecting the “what” with the “where”.</p><h3>Minerva API</h3><p>The Minerva API consists of the API web server, a metadata fetcher application, and several clients that integrate with <a href="https://superset.apache.org/">Apache Superset</a>, <a href="https://www.tableau.com/">Tableau</a>, <a href="https://www.python.org/">Python</a>, and <a href="https://www.r-project.org/">R</a>. These components serve native NoSQL and SQL metric queries to the downstream applications.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FUP1m7DFm8B6XmaF" /><figcaption>Minerva API serves as the interface between the consumers and the underlying datasets</figcaption></figure><h4>Metadata Fetcher: Abstracting the “Where”</h4><p>We mentioned previously that users simply ask Minerva for metrics and dimension cuts without having to figure out the “where”. When a data request is issued, Minerva spends a great deal of effort figuring out which of its datasets should be used to honor that request.</p><p>Under the hood, Minerva takes into account several factors before picking an optimal data source — one of the most important factors being data completeness. This means that any data source chosen to serve the query should contain all the columns needed for a given user’s query request and must cover the time range required from the query request.</p><p>To accomplish this, we built a service called Metadata Fetcher that periodically fetches data source metadata and caches it in a MySQL database every 15 minutes. Specifically, we periodically fetch the latest copy of the Minerva configuration (stored in Thrift binary) from S3 to get the list of every valid Minerva data source in Druid. For each data source, we query the Druid broker to get its name and a list of associated metrics and dimensions. Furthermore, we can also get the min date, max date, and count of distinct dates from the broker to figure out if there is any missing data. Every time new information is fetched, we update the MySQL database to maintain the source of truth. With this metadata fetcher, we are able to serve the data request using the best data source at any given time.</p><h4>Data API: Abstracting the “How”</h4><p>Imagine a scenario in which a user is interested in knowing the trend of average daily price (ADR), cut by destination region, excluding private rooms for the past 4 weeks in the month of August 2021. The full spec of the example query might look like the following:</p><pre>{</pre><pre>      metric: ‘price_per_night’,</pre><pre>      groupby_dimension: ‘destination_region’,</pre><pre>      global_filter: ‘dim_room_type!=”private-room”’,</pre><pre>      aggregation_granularity: ‘W-SAT’,</pre><pre>      start_date: ‘2021–08–01’,</pre><pre>      end_date: ‘2021–09–01’,</pre><pre>      truncate_incomplete_leading_data: ‘true’,</pre><pre>      truncate_incomplete_trailing_data: ‘true’,</pre><pre>}</pre><p>When Minerva receives such a request, it needs not only to figure out where to fetch the data, but also how to filter, combine, and aggregate the data to create the final result. It employs a strategy for achieving this via the <a href="https://www.jstatsoft.org/article/view/v040i01">Split-Apply-Combine paradigm</a>, commonly used in data analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZBoswC0q8u0DvM82" /><figcaption>Split-Apply-Combine in action for `price_per_night` metric</figcaption></figure><h4>Step 1: Split the Request into Atomic Metric Requests</h4><p>When Minerva API receives a query request such as the one above, the first thing it does is to break up any derived metrics into what we call a Minerva “atomic” metric by creating a set of associated subqueries. If a user query only specifies an atomic Minerva metric, then this first step is essentially a no-op.</p><p>In the example above, given that the `price_per_night` metric is a ratio metric (a special case of derived metric) that contains a numerator (`gross_booking_value_stays`) and a denominator (`nights_booked`), Minerva API breaks up this request into two sub-requests.</p><h4>Step 2: Apply and Execute Each Subquery</h4><p>With the atomic metrics identified from step 1, Minerva leverages metric configurations stored in S3 to extrapolate the associated metric expressions and metadata in order to generate the subqueries. Let’s stick with the same example: Minerva data API looks up the metric definition of `gross_booking_value_stays` and sees that it is a SUM aggregation, and similarly for the `nights_booked` metric. In both requests, a global filter ‘dim_room_type!=”private-room”’ is applied to ensure that private rooms are excluded from the calculation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*utscy1Byao92TR23" /><figcaption>The split-apply-combine paradigm in action for the ADR metric</figcaption></figure><p>Once the associated subqueries are generated for each atomic metric, Minerva API finally sends the queries over to Druid or Presto. It chops up the query into several “slices” that span a smaller time range and then combines the results into a single dataframe if resource limitation is reached. The API also truncates any incomplete leading or trailing data before rolling up the dataframe based on the aggregation granularity.</p><h4>Step 3: Combine Atomic Metric Results Into a Single Dataframe</h4><p>Once Minerva rolls up the dataframes for each atomic metric, it then combines them into a single dataframe by joining the dataframes on the timestamp column. As a final step, Minerva API performs any necessary post-aggregation calculations, applies ordering, and limits before returning the final result to the client in serialized JSON format.</p><p>To recapitulate, with Minerva’s data source API and data API, we are able to abstract away the process of identifying “where” to fetch the data and “how” to return the data. This API serves as the single layer of abstraction for Minerva to honor any request coming from downstream applications. However, our story does not simply end here: many of our engineering challenges involve how to integrate different applications with this API. We will explore these challenges in the next section.</p><h3>The Data Consumption Experience</h3><p>Bearing in mind the diverse set of data consumers within Airbnb, we set out to build tools tailored to different personas and use cases. With the Minerva API, we built a wide range of user interfaces that provide a consistent and coherent data consumption experience. As we mentioned briefly in the first <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70">post</a>, there are four major integration endpoints, each supporting a different set of tools and audience:</p><ul><li><strong>Data Analysis: </strong>Integration with Python and R, used mostly for advanced data analytics</li><li><strong>Data Exploration: </strong>Integration with BI tools such as Superset, <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd#c576">Metric Explorer</a>, and Tableau, tailored for data-savvy analysts who drive insights</li><li><strong>Reporting: </strong>Integration with<strong> </strong><a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#efb9">XRF</a> (eXecutive Reporting Framework), tailored for an executive audience who wish to know the current state of the business</li><li><strong>Experimentation: </strong>Integration with<strong> </strong><a href="https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166">ERF</a> (Experimentation Reporting Framework), tailored for any data scientists, engineers, or product managers who run A/B tests at Airbnb</li></ul><p>When building out these features, we were constantly trading off between consistency, flexibility, and accessibility. For example, Metric Explorer is built mostly for non-technical users who are not data experts. This means that it needs to optimize consistency and accessibility over flexibility. Metric Explorer enforces strict guardrails that prevent users from doing the wrong thing, and there is very little opportunity to go off the “paved path”.</p><p>At the other extreme, the R and Python clients that are typically favored by data scientists are much more flexible. Users have full controls on how to leverage the clients’ API to perform custom analysis or visualization. In the next few sections, we will explain how some of these consumption experiences are created behind the scenes.</p><h4>Integration with Metric Explorer</h4><p>Metric Explorer was created at Airbnb so anyone, regardless of their level of data expertise, can leverage data to make informed decisions. Because of its broad target audience, Metric Explorer optimizes accessibility and data consistency over flexibility.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OknXUhNkPk0DtU8P" /><figcaption>The Metric Explorer is great for a non-technical audience who wants to answer high-level business questions</figcaption></figure><p>Under the hood, all of Metric Explorer’s metrics, dimensions, and relevant metadata are sourced from Minerva’s metric repository and ingested into <a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a>. These metadata are conveniently presented on the right sidebar as contexts before users perform any operations on the data.</p><p>When a user chooses to perform data operations such as Group By and Filter, Metrics Explorer presents dimensions in ranked order so that users with little or no business context can easily drill down, without needing to know the dimension values ahead of time — as illustrated above.</p><p>As users slice and dice the data, the Minerva API automatically determines which combination is valid and only surfaces cuts that exist. Nowhere in the experience does a user need to know anything about the underlying physical table from which the metric in question is sourced.</p><h4>Integration with Apache Superset</h4><p>While Metrics Explorer provides high-level information about metrics, more adventurous users who wish to slice and dice the data more can do so in Superset. <a href="https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd">Apache Superset</a> is a homegrown tool at the core of Airbnb’s self-serve BI solutions. Given the ubiquity of Superset inside the company, we knew that we needed to provide a functional SQL-like integration with Superset in order for Minerva to be widely adopted.</p><p>While many applications can be built on top of the Minerva API by talking to its RESTful endpoints directly, the client interfaces for BI tools such as Apache Superset and Tableau are more complex. Commonly, these BI tools speak SQL (via a client), not HTTP requests. This meant that Minerva API needed to support a SQL-like interface that adheres to the <a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a> type query structure. To build such an interface, we added to Minerva API a SQL parser — leveraging <a href="https://pypi.org/project/sqlparse/v">sqlparse</a> — to parse the SQL statement into an AST which is then validated and transformed into native HTTP requests.</p><p>Adhering to the DRY principle, we leveraged <a href="https://calcite.apache.org/avatica/">Apache Calcite Avatica</a>, which defines a generic database wire API between a client and server. The Minerva API serves as the Avatica HTTP server and the client is either a custom <a href="https://www.python.org/dev/peps/pep-0249/">Python Database API</a> database driver with <a href="https://www.sqlalchemy.org/">SQLAlchemy</a> dialect (Superset) or Avatica provided JDBC connector (Tableau).</p><p>Unlike traditional BI tools for which custom business logic is implemented in the tools themselves, Minerva consolidates and obfuscates all this logic via pseudo SQL-like AGG metric expressions. In the table below, we compare and contrast the queries run in a traditional BI tool to those run in Superset:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HK1IgDif4Uz5Qmm8LLiFPA.png" /></figure><p>In the query on the left, a user need not specify where the metric should be computed from, nor do they need to specify the correct aggregation function — these details are abstracted away by Minerva.</p><p>Finally, given that there are 12,000 metrics and 5,000 dimensions in Minerva, not all metric-dimension combinations are valid. For example, active listing can be cut by where the host is located, but not by where the guest is from (i.e. this guest attribute could be different for each booking reservation). We added event listeners to the chart controls to make sure that only eligible metric and dimension combinations are surfaced in the left pane. This design helps to reduce cognitive load and to simplify the data exploration process.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CqG4C2B6ND0XW17h" /><figcaption>Superset is metric-centric. Users can query all metrics and dimensions from a single virtual source</figcaption></figure><h4>Integration with XRF — eXecutive Reporting Framework</h4><p>As presented in <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#efb9">Part-I</a>, XRF is a framework for producing succinct, high-fidelity, business critical reports that are consumed by executives and leadership teams. This framework is configured via the Minerva configs and powered entirely by Minerva API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hedx5FI-tvWKqhuA" /><figcaption>XRF automates a lot of repeated manual work and allows us to standardize high fidelity business critical reports</figcaption></figure><p>To curate an XRF report, users first define the reporting config and specify the desired business metrics, dimensional cuts, and global filters to apply. In addition, users can configure other controls such as whether a metric should be calculated as a running aggregation (e.g., MTD, QTD, or YTD), and the appropriate unit for growth rate time ratio comparisons (e.g., YoY, MoM, or WoW). Once these settings are specified, the Minerva API performs the necessary aggregations and final pivots to produce the final report.</p><p>The data output by XRF can be rendered in a Google sheet via a custom GoogleSheetHook as well as in Tableau via Presto connection. By leveraging the metric definitions in Minerva and its aggregation logic, we enforce consistency safeguards in the users’ choice of presentation layer.</p><h4>Integration with ERF — Experimentation Reporting Framework</h4><p>Unlike the analytics or reporting use cases, the experimentation use case is unique in that the metrics used for reporting are only a starting point. To make proper causal inferences, metrics must be joined with experiment assignment data before transforming them into summary statistics that can be used for valid statistical comparisons.</p><p>Typically, Minerva supplies the “raw events” to ERF. Depending on the unit of randomization and unit of analysis, we join the Minerva data to the assignment logs using different subject keys so that each event will have the associated subject, as well as the experiment group attached to it. Summary statistics such as means, percent changes, and p-values are then calculated and surfaced in the ERF scorecard.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wuT_vRG_w43SvZtf" /><figcaption>ERF scorecard showing summary statistics for experiments</figcaption></figure><p>The Experimentation UI also exposes relevant Minerva metadata directly in the tool. Users can view the description and ownership information of the underlying Minerva events. A lineage view, overlayed with ETA information, allows users to <a href="https://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710">track the progress of ERF metrics</a> and helps them contact the relevant Minerva metric owners in case of delays.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZtOE_1oe6GUtuY1T" /><figcaption>ERF displaying metrics metadata, which links to<a href="https://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710"> SLA Tracker</a> to visualize data lineage and timeliness</figcaption></figure><p>In summary, Minerva and its various integrations enable users to easily track metrics within their scheduled reporting, measure movements due to experimentation, and explore unexpected changes — all with the confidence that the data is correct and consistent. This confidence drastically reduces the time spent deriving insights, increases trust in data, and helps to support data-driven decision making.</p><h3>Closing</h3><p>Minerva introduced a novel way of thinking about data, not only is it centered around a business- and metric-centric user interface, we also need to adapt traditional BI tools (that mostly talk SQL) to the interface of Minerva API. In some sense, it is akin to fitting a new square peg (Minerva) into an existing round hole ( BI Tools).</p><p>As more organizations embrace the concept of a metric layer similar to Minerva, we believe there will be a new set of challenges awaiting us. That said, some of this pioneering work will surely bring analytics to the next level, and we are grateful for contributing to the leading edge of this landscape. We hope that soon more companies will follow suit.</p><h3>Acknowledgements</h3><p>Thanks to everyone who <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#8a0a">contributed to the work and outcomes</a> represented in this blog post. In addition to <a href="https://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70#8a0a">previous acknowledgements</a> we would also like to thank those who have partnered with us to adopt Minerva within our consumption landscape.</p><p>All trademarks are the properties of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship or endorsement.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1c0b6a8b9206" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/how-airbnb-enables-consistent-data-consumption-at-scale-1c0b6a8b9206">How Airbnb Enables Consistent Data Consumption at Scale</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Commitment to Craft]]></title>
            <link>https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/e36d5a8efe2a</guid>
            <category><![CDATA[culture]]></category>
            <category><![CDATA[people]]></category>
            <category><![CDATA[work-experience]]></category>
            <dc:creator><![CDATA[AirbnbEng]]></dc:creator>
            <pubDate>Thu, 16 Sep 2021 16:59:30 GMT</pubDate>
            <atom:updated>2021-09-16T20:03:27.761Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://news.airbnb.com/cto-2018/"><em>Ari Balogh</em></a><em>, CTO at Airbnb</em>, shares how striving towards excellence has served us well in a time of uncertainty.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EoAwRoDa43XnZypIQz7oUQ.jpeg" /></figure><p>If you’ve ever been part of a startup, you understand the importance of speed. Sometimes it feels like nothing else matters, since getting your products to market quickly can determine whether you survive. When Airbnb was an emerging company with a radical new vision for travel, we often had to prioritize speed in making tough engineering tradeoffs.</p><p>These decisions paid off, and Airbnb grew into a platform that supports millions of Hosts and Guests globally. Now, in addition to delivering fast, our success depends on providing an experience of exceptional quality that considers every detail for a diverse community that spans virtually every country. With this in mind, two years ago our tech organization took a holistic look at what we’d built and where to make changes. To prioritize excellence, we committed to creating an environment that enables people to do their best work and nurtures a mindset of quality — a Commitment to Craft.</p><p>Nobody could have predicted what came next. COVID-19 changed the world and impacted travel in unprecedented ways. With our Commitment to Craft already in place, we were better prepared for these challenges. In turn, we’ve developed an even deeper appreciation for quality and efficiency across our technology organization.</p><h3>What is Commitment to Craft?</h3><p>Commitment to Craft consists of several principles. First, it’s about systems built on sound technical foundations that enable easy adaptation and innovation, while ensuring quality. Second, it’s about enabling the people behind the work. To build on these foundations, it takes great talent combined with an environment that fosters creativity and collaboration, with clear accountability for deliverables. People then need the right tools and, importantly, time to do excellent work. Third, it’s about setting measurable goals: problems are clearly defined, and the individual teams create their plans.</p><p>A critical outcome is that these elements combine to create so much of the magic in our products. When people feel their craft is supported, their personal touches of excellence come through to bring true delight for our Hosts and Guests.</p><h3>Building with craft</h3><p>Change takes time. We cascaded goals to every engineer and data scientist (together we call them technologists) so everyone was part of the transformation. We started small, identifying a few concrete areas of improvement around site reliability, performance and developer tooling. These investments laid a foundation for quality and, over time, transformed how people work.</p><p>Commitment to Craft led to many important outcomes across Airbnb, including improving our page performance, <a href="https://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7">investing in our data quality</a>, <a href="https://medium.com/airbnb-engineering/achieving-insights-and-savings-with-cost-data-ec9a49fd74bc">using our compute resources more efficiently</a>, and improving our development practices, including testing.</p><h4>Example: Improving page performance</h4><p>Faster websites lead to happier users. But while we were focused on product innovation and adding more and more features, our pages became significantly slower. One of our Commitment to Craft goals was to reduce page load times. To set a specific target, we decided that “page performance score” — a composite score of user-centric metrics measuring the time it takes for a page to load and feel responsive — was a key measure.</p><p>To enable craft, we built a set of tools to measure fine-grained latencies across many page components. At the same time, our performance experts wrote a “how-to” guide on improving load times. With these resources, individuals across the entire org took ownership of improving the performance of their pages. The improvements were exceptional — we’ll share details in an upcoming blog post.</p><h3>Craft in a crisis</h3><p>We were progressing well on our Commitment to Craft goals when the world changed overnight. COVID-19 hit in early 2020 and Airbnb lost 80% of its business in eight weeks.</p><p>Prior to COVID, one of the elements of the Commitment to Craft program is what we called <em>Performance Efficiency:</em> delivering a reliable, performant experience in a cost-efficient manner. To support this, we started to build tools and techniques to help teams understand and optimize their cost of serving. With COVID, we doubled-down on these tools and techniques — and the use of these tools to improve efficiency and resource utilization. The result was a dramatic reduction in operating costs that helped us weather the storm.</p><h3>Looking ahead</h3><p>Today, Commitment to Craft is more than a set of projects — it’s a philosophy that people have rallied around. We’re starting to see teams across tech choose to prioritize craft simply because it’s “how we do things here.”</p><p>I’ve never been more proud of the work our teams are doing. We still have much to do, but we’re confident that this philosophy will help us navigate the future and keep us focused on what matters. Our goal is to achieve “agility with stability” — have the development agility of a startup combined with the product quality expected of a company of our scale. To get there, we’ll continue investing in the people behind the craft.</p><p><em>-Ari</em></p><p><em>If a culture of craft resonates with you, you might be a great fit for Airbnb. We’re currently hiring for a variety of technical roles, so check out our </em><a href="https://careers.airbnb.com/"><em>career page</em></a><em> for more information.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e36d5a8efe2a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a">Commitment to Craft</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automating Data Protection at Scale, Part 1]]></title>
            <link>https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08?source=rss----53c7c27702d5---4</link>
            <guid isPermaLink="false">https://medium.com/p/c74909328e08</guid>
            <category><![CDATA[data]]></category>
            <category><![CDATA[privacy]]></category>
            <category><![CDATA[distributed-systems]]></category>
            <category><![CDATA[security]]></category>
            <dc:creator><![CDATA[elizabeth nammour]]></dc:creator>
            <pubDate>Tue, 14 Sep 2021 17:00:16 GMT</pubDate>
            <atom:updated>2021-09-14T17:00:16.451Z</atom:updated>
            <content:encoded><![CDATA[<p>Part one of a series on how we provide powerful, automated, and scalable data privacy and security engineering capabilities at Airbnb.</p><p><a href="https://www.linkedin.com/in/elizabethnammour/">Elizabeth Nammour</a>, <a href="https://www.linkedin.com/in/wendy-jing-jin-81452921/">Wendy Jin</a>, <a href="https://www.linkedin.com/in/shengpu-liu/">Shengpu Liu</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*u5ErTNsWp-x1GE72" /></figure><p>Our community of hosts and guests trust that we will keep their data safe and honor their privacy rights. With frequent news reports of data security breaches, coupled with global regulations and security requirements, monitoring and protecting data has become an even more critical problem to solve.</p><p>At Airbnb, data is collected, stored, and propagated across different data stores and infrastructures, making it hard to rely on engineers to manually keep track of how user and sensitive data flows through our environment. This, in turn, makes it challenging for them to protect it. While many vendors exist for different aspects of data security, no one tool met all of our requirements when it came to data discovery and automated data protection, nor did they support all of the data stores and environments in our ecosystem.</p><p>In this three-part blog series, we’ll be sharing our experience building and operating a data protection platform at Airbnb to address these challenges. In this first post, we will give an overview of why we decided to build the Data Protection Platform (DPP), walk through its architecture, and dive into the data inventory component, Madoka.</p><h3>Data Protection Platform (DPP)</h3><p>Since no one tool was meeting our needs, we decided to build a data protection platform to enable and empower Airbnb to protect data in compliance with global regulations and security requirements. However, in order to protect the data, we first needed to understand it and its associated security and privacy risks.</p><h4>Understanding Airbnb’s Data</h4><p>At Airbnb, we store petabytes of data across different file formats and data stores, such as MySQL, Hive, and S3. Data is generated, replicated, and propagated daily throughout our entire ecosystem. In order to monitor and gain an understanding of the ever-changing data, we built a centralized inventory system that keeps track of all the data assets that exist. This inventory system also collects and stores metadata around the security and privacy properties of each asset, so that the relevant stakeholders at Airbnb can understand the associated risks.</p><p>Since some data assets may contain sensitive business secrets or public information, understanding what type of data is stored within a data asset is crucial to determining the level of protection needed. In addition, privacy laws, such as the European Union General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA), have granted users the right to access and delete their personal data. However, personal data is a less-than-precise term that represents many different data elements, including email addresses, messages sent on the platform, location info, etc. In order to comply with these laws, we need to pinpoint the exact location of all personal data. To do this, we built a scalable data classification system that continuously scans and classifies our data assets to determine what type of data is stored within them.</p><h4>Enabling Automated Data Protection</h4><p>Based on the understanding of the data, the DPP strives to automate its protection, or enables and notifies teams across the company to protect it. This automation focuses on a few key areas: data discovery, prevention of sensitive data leakages, and data encryption.</p><p>Discovering personal data is the first step to privacy compliance. This is especially true as personal data needs to be deleted or returned to a user upon request. Our platform enables us to automatically notify data owners when new personal data is detected in their data stores and integrate this data with our privacy orchestration service to ensure it gets deleted or returned if needed.</p><p>A common cause of data breaches is when sensitive secrets, such as API keys or credentials, are leaked internally and then make their way into the hands of an attacker. This can come from an engineer logging the secret within their service or committing the secret to code. Our data protection platform identifies potential leaks from various endpoints and notifies the engineer to mitigate the leakage by deleting the secret from the code or log, rotating the secret, and then hiding the new secret with our encryption tool sets.</p><p>One of the most popular and important methods of data protection is encryption, since even in case of an infiltration, attackers won’t be able to get their hands on sensitive data. However, breaches due to unencrypted sensitive data are unfortunately a common occurrence within the industry.</p><p>Why does it still happen? Secure encryption with proper key management is technically challenging, and organizations do not always know where sensitive data is stored. The DPP aims to abstract these challenges by providing a data encryption service and client library that engineers can use. It automatically discovers sensitive data, so that we don’t rely on manual identification.</p><h4>Platform Architecture</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SUuqoIcTshHVhQQ_" /><figcaption>Figure 1: DPP Overview</figcaption></figure><p>The DPP aims to discover, understand, and protect our data. It integrates the services and tools we built to tackle different aspects of data protection. This end-to-end solution includes:</p><ul><li><strong>Inspekt</strong> is our data classification service. It continuously scans Airbnb’s data stores to determine what sensitive and personal data types are stored within them.</li><li><strong>Angmar</strong> is our secret detection pipeline that discovers secrets in our codebase.</li><li><a href="https://medium.com/airbnb-engineering/one-step-forward-in-data-protection-8071e2258d16"><strong>Cipher</strong></a> is our data encryption service that provides an easy and transparent framework for developers across Airbnb to easily encrypt and decrypt sensitive information.</li><li><strong>Obliviate</strong> is our orchestration service, which handles all privacy compliance requests. For example, when a user requests to be deleted from Airbnb, obliviate will forward this request to all necessary Airbnb services to delete the user’s personal data from their data stores.</li><li><strong>Minister</strong> is our third party risk and privacy compliance service that handles and forwards all privacy data subject rights requests to our external vendors.</li><li><strong>Madoka</strong> is our metadata service that collects security and privacy properties of our data assets from different sources.</li><li>Finally, we have our <strong>Data Protection Service</strong>,<strong> </strong>a presentation layer where we define jobs to enable automated data protection actions and notifications using information from Madoka (e.g., automate integrations with our privacy framework)</li></ul><h3>Madoka: A Metadata System</h3><p>Madoka is a metadata system for data protection that maintains the security and privacy related metadata for all data assets on the Airbnb platform. It provides a centralized repository that allows Airbnb engineers and other internal stakeholders to easily track and manage the metadata of their data assets. This enables us to maintain a global understanding of Airbnb’s data security and privacy posture, and provides an essential role in automating security and privacy across the company.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/920/0*OEag9YxDW3CT1iQy" /><figcaption>Figure 2: Madoka Architecture</figcaption></figure><p>Implemented by two different services, a crawler and a backend, Madoka has three major responsibilities: collecting metadata, storing metadata, and providing metadata to other services.The Madoka crawler is a daily crawling service that fetches metadata from other data sources, including Github, MySQL databases, S3 buckets, Inspekt (data classification service), etc. It then publishes the metadata onto an AWS Simple Queue Service (SQS) queue. The Madoka backend is a data service that ingests the metadata from the SQS queue, reconciles any conflicting information, and stores the metadata in its database. It provides APIs for other services to query the metadata findings.</p><p>The primary metadata collected by Madoka includes:</p><ul><li>Data assets list</li><li>Ownership</li><li>Data classification</li></ul><p>For each of the above we handle both MySQL and S3 formats.</p><h4>Data Assets List</h4><p>The first type of metadata that needs to be collected is the list of all data assets that exist at Airbnb, along with their basic metadata such as the schema, the location of the asset, and the asset type.</p><p>For MySQL, the crawler collects the list of all columns that exist within our production AWS account. It calls the AWS APIs to get the list of all clusters in our environment, along with their reader endpoint. The crawler then connects to that cluster using JDBI and lists all the databases, tables, and columns, along with the column data type.</p><p>The crawler retains the following metadata information and passes it along to the Madoka backend for storage:</p><ul><li>Cluster Name</li><li>Database Name</li><li>Table Name</li><li>Column Name</li><li>Column Data Type</li></ul><p>For S3, the crawler collects the list of all objects that exist within all of our AWS accounts.</p><p>At Airbnb, we use <a href="https://www.terraform.io/">Terraform</a> to configure AWS resources in code, including S3 buckets. The crawler parses the Terraform files to fetch the S3 metadata.</p><p>The crawler first fetches the list of all AWS account numbers and names, which are stored in a configuration file in our Terraform repository. It then fetches the list of all bucket names, since each bucket configuration is a file under the account’s subrepo.</p><p>In order to fetch the list of objects within a bucket, the crawler uses <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html">S3 inventory reports</a>, a tool provided by AWS. This tool produces and stores a daily or weekly CSV file of all the objects contained in the bucket, along with their metadata. This is a much faster and less costly way of getting the list compared to calling the List AWS API. We’ve enabled inventory reports on all production S3 buckets in Terraform, and the bucket configuration will specify the location of the inventory report.</p><p>The crawler retains the following information and passes it along to Madoka backend for storage:</p><ul><li>Account Number</li><li>Account Name</li><li>Assume Role Name</li><li>Bucket Name</li><li>Inventory Bucket Account Number</li><li>Inventory Assume Role Name</li><li>Inventory Bucket Prefix</li><li>Inventory Bucket Name</li><li>Object key</li></ul><h4>Ownership</h4><p>Ownership is a metadata property that describes who owns a specific data asset.</p><p>We decided to collect service ownership, which allows us to link a data asset to a specific codebase, and therefore automate any data protection action that requires code changes.</p><p>We also decided to collect team membership, which is crucial to perform any data protection action that requires an engineer to do some work or that requires a stamp of approval. We chose to collect team ownership and not user/employee ownership since team members constantly change, while the data asset remains with the team.</p><p>At Airbnb, since we migrated to a service-oriented architecture (SOA), most MySQL clusters belong to a single service and a single team. To determine service ownership, the crawler fetches the list of the services that connect to a MySQL cluster and will set the service with the most number of connections within the last 60 days as the owner of all the tables within that cluster. There are many services that connect to all clusters for monitoring, observability, and other common purposes, so we created a list of roles that should be filtered out when determining ownership.</p><p>There are still some legacy clusters in use that are shared amongst many services, where each service owns specific tables within the clusters. For those clusters, not all tables will have the correct service owner assigned, but we allow for a manual override to correct these mistakes.</p><p>The crawler uses service ownership to determine team ownership, since at Airbnb, team ownership is defined within the service’s codebase on Git.</p><p>At Airbnb, all S3 buckets have a project tag in their Terraform configuration file, which defines which service owns the bucket. The crawler fetches the service ownership from that file and uses it to determine the team ownership, as described above for MySQL.</p><h4>Data Classification</h4><p>Data classification is a metadata property that describes what type of data elements are stored within the asset — e.g., a MySQL column which stores email addresses or phone numbers would be classified as personal data. Gathering data classifications allows us to understand the riskiness of each data set so we can determine the level of protection needed.</p><p>The crawler fetches the data classification from two different sources. First, it fetches data classifications from our Git repositories, since data owners can manually set the classifications in their data schema. However, relying on manual classifications is insufficient. Data owners do not always know what an asset contains, or they may forget to change the classifications when the data asset is updated to store new data elements.</p><p>The crawler will then fetch data classifications from our automated data classification tool, called Inspekt, which we will describe in detail in a later blog post. Inspekt continuously scans and classifies all of our major data stores, such as MySQL and S3. It outputs what data elements were found in each data asset. This ensures that our data is constantly monitored, and classifications are updated as data changes. As with any automated detection tool, precision and recall are never 100%, so false positives and false negatives may occur.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uArO328-uBKl2WQ-" /><figcaption>Figure 3: Classification Reconciliation</figcaption></figure><p>Since the crawler fetches the data classifications from two different sources, some discrepancies may arise, where the manual classification contains data elements not found by Inspekt or vice versa. The crawler will forward all findings to the Madoka backend, which will resolve any conflicts. The status of the manual classification is marked as <em>new</em> by default and the status of the Inspekt classification is marked as suggested. If the manual classification aligns with the Inspekt result, the classification is automatically confirmed. If there is any discrepancy, we file tickets to the data owners through the data protection service. If the Inspekt classification is correct, the owners may update the data schema in the Git repository, or they can mark the Inspekt classification as incorrect to resolve the conflict.</p><h4>Other Security and Privacy Related Attributes</h4><p>Madoka also stores how data assets have integrated with our security and privacy tools. For example, we may store whether or not the data asset is encrypted using Cipher or is integrated with our privacy compliance service, Obliviate, for data subject rights requests. We built Madoka to be easily extensible and are constantly collecting and storing more security and privacy related attributes.</p><h3>Conclusion</h3><p>In this first post, we provided an overview of why we built the DPP, described the platform’s architecture, and dove into the data inventory component, Madoka. In our next post, we will focus on our data classification system that enables us to detect personal and sensitive data at scale. In our final post we will deep dive into how we’ve used the DPP to enable various security and privacy use cases.</p><h3>Acknowledgements</h3><p>The DPP was made possible thanks to many members of the data security team: Pinyao Guo, Julia Cline, Jamie Chong, Zi Liu, Jesse Rosenbloom, Serhi Pichkurov, and Gurer Kiratli. Thank you to the data governance team members for partnering and supporting our work: Andrew Luo, Shawn Chen, and Liyin Tang. Thank you Tina Nguyen for helping drive and make this blog post possible. Thank you to our leadership, Marc Blanchou, Brendon Lynch, Paul Nikhinson and Vijaya Kaza, for supporting our work. Thank you to previous members of the team who contributed greatly to the work: Lifeng Sang, Bin Zeng, Alex Leishman, and Julie Trias.</p><p>If this type of work interests you, see <a href="https://careers.airbnb.com/">our career page</a> for current openings.</p><p>Tags: data, security</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c74909328e08" width="1" height="1" alt=""><hr><p><a href="https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08">Automating Data Protection at Scale, Part 1</a> was originally published in <a href="https://medium.com/airbnb-engineering">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>